We use a transform function to transform the evaluation dataset and we wrote the transformed evaluation data. For what type of data did we use analyze and transform dataset? Right, the training data. And we use transform dataset for the evaluation data. Even though we created the preprocessed features using Beam, the preprocessed method couldn't have arbitrary Python code. It had to consist solely of TensorFlow functions. The reason these functions needed to be in TensorFlow was that they're part of the prediction graph. And why are they part of the prediction graph? So that the end user can give the model raw data and the model can do the necessary preprocessing. But, how will the model know what functions to call? In order for the model to know what functions to call, we need to save the transform function. And that's what I'm doing here. I'm saving the transform function itself into a directory called metadata alongside by trained model, then we tell the input function to pickup the metadata. Which input function? All three. First, let's look at the training and evaluation input functions. They read the preprocessed features. So, notice that I specify that the schema corresponds to the transformed metadata. Change the training and evaluation input functions to read the preprocessed features. TensorFlow transform comes with a nice helper function called build training input function. I'm using this for both training and evaluation by changing the input paths variable to point to either the train data path, or the eval data path depending on the mode. The serving input function accepts the raw data. So here, I'm passing in the raw data metadata, not the transformed metadata. Well, the raw data alone isn't enough, we could also have arbitrary TensorFlow functions in the preprocessing code. Those operations are stored in saved_model.pb. But again, nor does a nice TensorFlow transform helper function build parsing transforming serving input function. Parse the Json according to raw data schema. Transform the raw data based on the TensorFlow operations in saved_model.pb, then send it along to the model. The client code just needs to send the raw input variables, so that hasn't changed. The serving input function receives the input variables and remains the same as before. It accepts raw data and sends it to the model. So, why does a model work? The DNN regressor or whatever model we use cannot deal with a string at DHU. The reason it works is that all the code that you wrote in preprocessed, that code is now part of the model graph itself. This happens because a model reads the metadata and it includes a preprocessing code. So, that's how TensorFlow transform works. Let's now use it on a taxi fare prediction problem.