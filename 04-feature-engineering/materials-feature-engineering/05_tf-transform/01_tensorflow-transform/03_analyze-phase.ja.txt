分析フェーズについてみていきましょう トレーニング用データセットの
分析を思い出してください まず想定されるデータの種類を
Beamに伝える必要があります これはスキーマの設定で行います 1行目にraw_data_schemaという
ディクショナリを設定します すべての文字列カラムに対して
エントリを追加します この文字列はTensorFlowの
データタイプです そしてtf.float32タイプの全カラムを追加し raw_data_schemaを更新します これで raw_data_schemaのデータセットに
すべてのカラムが取り込まれ DataFlowでBeamによって
処理されるようになります raw_data_schemaはメタデータ
テンプレートの作成に使われます 次にトレーニング用データセットに対して
analyzeandtransform PTransformを実行し 前処理用学習データと変換関数を取得します まず beam.io.readで
トレーニング用データを読み込みます これはBeamの以前のモジュールで見た
すべてのBeamパイプラインと同じです ここでBigQueryから読み込みます 次にトレーニングで使用したくない
データをフィルタリングします これはis_valid関数で行います この関数について今は触れず
後でこのメソッドを説明します 読み込みとフィルタリングを行った生データと 前のスライドで取得した生データのメタデータを AnalyzeAndTransformDataset PTransform
に渡します Beamではこの変換が分散的な手法で行われ 命じられたすべての分析が
preprocessメソッドで行われます このpreprocessメソッド
についても後ほど説明します これでis_validメソッドと
preprocessメソッドが Beamでトレーニング用
データセットに対して実行され フィルタリングと前処理が行われます 前処理されたデータは
PCollectionで戻されます このparallel Collectionでは
変換されたデータセットをコールします ただここで注意すべきなのは
前処理で実行した変換結果は 2番目の戻り値に格納されるということです つまり変換関数です
この点が重要になります 変換データを書き出します ここではTFレコードとして書き出します これはTensorFlowで最も有効な
フォーマットです 書き出しはTensorFlow transformの writetoTFrecord PTransformで行います ファイルは自動的に終了されます ただ このときに使用されるスキーマは 生データ用のスキーマではなく
変換されたスキーマです なぜでしょうか もちろんそれは書き出したデータが
変換されたデータだからです 生データではなく
前処理済みのデータだからです