1
00:00:00,000 --> 00:00:05,325
Wir haben über drei mögliche Orte
für Feature Engineering gesprochen.

2
00:00:05,325 --> 00:00:09,615
Sie können Feature Engineering
in TensorFlow durchführen.

3
00:00:09,615 --> 00:00:13,755
Sie können dort Merkmalspalten verwenden
oder ein Merkmalwörterbuch zusammenstellen

4
00:00:13,755 --> 00:00:16,570
und beliebigen TensorFlow-Code hinzufügen.

5
00:00:16,570 --> 00:00:22,245
Das ist großartig, weil TensorFlow-Code
auf einer GPU oder TPU effizient ist.

6
00:00:22,250 --> 00:00:26,640
Aber warum spreche ich
von beliebigem TensorFlow-Code?

7
00:00:26,640 --> 00:00:31,305
Der Code muss
als Teil Ihrer Modellfunktion,

8
00:00:31,305 --> 00:00:35,265
Ihres TensorFlow-Graphen,
ausgeführt werden.

9
00:00:35,265 --> 00:00:38,675
Sie können also keine Abfrage in der
Unternehmensdatenbank durchführen

10
00:00:38,675 --> 00:00:40,815
und dort einen Wert eintragen.

11
00:00:40,815 --> 00:00:44,490
Sie könnten eigenen TensorFlow-Code
in C++ schreiben und aufrufen.

12
00:00:44,490 --> 00:00:46,215
Aber das ignorieren wir erst einmal.

13
00:00:46,215 --> 00:00:48,597
Sie können auch nur Aufgaben ausführen,

14
00:00:48,597 --> 00:00:53,300
die ausschließlich
auf diesem Eingabewert basieren.

15
00:00:53,300 --> 00:00:58,680
Es ist schwierig,
einen gleitenden Durchschnitt zu berechnen.

16
00:00:58,680 --> 00:01:01,550
Später betrachten wir Sequenzmodelle,

17
00:01:01,550 --> 00:01:06,620
in denen scheinbar Zeitreihen, also
mehrere Eingabewerte, verarbeitet werden,

18
00:01:06,620 --> 00:01:09,545
aber die Eingabe dort
ist eine ganze Sequenz.

19
00:01:09,545 --> 00:01:13,590
Die Grenze für die
TensorFlow-Verarbeitung besteht darin,

20
00:01:13,590 --> 00:01:17,755
dass wir nur eine einzige 
Eingabe vorverarbeiten können.

21
00:01:17,755 --> 00:01:25,810
TensorFlow-Modelle, abgesehen
vom Sequenzmodell, sind meist zustandslos.

22
00:01:25,810 --> 00:01:28,510
In den letzten beiden Kapiteln

23
00:01:28,510 --> 00:01:32,735
haben wir uns
mit Vorverarbeitung und Merkmalerstellung

24
00:01:32,735 --> 00:01:36,395
in Apache Beam in Cloud Dataflow befasst.

25
00:01:36,400 --> 00:01:41,660
In Dataflow können wir beliebigen 
Python- oder Java-Code ausführen

26
00:01:41,660 --> 00:01:46,850
und so mehrere Eingabewerte
zustandsorientiert verarbeiten.

27
00:01:46,850 --> 00:01:51,510
Sie können z. B. den Durchschnitt
für ein Zeitfenster berechnen,

28
00:01:51,510 --> 00:01:57,090
wie die mittlere Anzahl von Fahrrädern
an einer Kreuzung in der letzten Stunde.

29
00:01:57,090 --> 00:02:03,050
Allerdings müssen Sie Ihren Vorhersagecode
auch innerhalb einer Pipeline ausführen,

30
00:02:03,050 --> 00:02:06,620
damit Sie die durchschnittliche Anzahl
der Fahrräder an einer Kreuzung

31
00:02:06,620 --> 00:02:08,510
in der letzten Stunde erhalten.

32
00:02:08,510 --> 00:02:12,199
Das eignet sich gut für Beispiele wie
Durchschnittswerte in einem Zeitfenster,

33
00:02:12,199 --> 00:02:14,780
bei denen Sie auf jeden Fall
eine Pipeline benötigen.

34
00:02:14,780 --> 00:02:20,910
Aber was ist, wenn Sie nur
ein Minimum oder Maximum benötigen,

35
00:02:20,910 --> 00:02:24,880
damit Sie die Werte skalieren oder
das Vokabular erhalten können,

36
00:02:24,880 --> 00:02:28,175
um kategorische Werte
in Zahlen umzuwandeln.

37
00:02:28,175 --> 00:02:32,275
Das Ausführen einer
Dataflow-Pipeline in der Vorhersage,

38
00:02:32,275 --> 00:02:37,554
nur um Min- und Max-Werte zu erhalten,
scheint ein bisschen übertrieben.

39
00:02:37,554 --> 00:02:40,845
Hier kommt tf.transform ins Spiel.

40
00:02:40,845 --> 00:02:44,490
Das ist ein Hybrid
aus den ersten beiden Ansätzen.

41
00:02:44,490 --> 00:02:49,405
Mit TensorFlow Transform sind Sie
auf TensorFlow-Methoden beschränkt.

42
00:02:49,405 --> 00:02:52,665
Aber dadurch erhalten Sie
auch die Effizienz von TensorFlow.

43
00:02:52,665 --> 00:02:57,330
Sie können Ihr vollständiges
Trainings-Dataset aggregiert verwenden,

44
00:02:57,330 --> 00:03:05,915
weil tf.transform im Training Dataflow nutzt,
aber in der Vorhersage nur TensorFlow.

45
00:03:05,915 --> 00:03:10,220
Sehen wir uns an, wie
TensorFlow Transform funktioniert.

46
00:03:10,220 --> 00:03:15,640
TensorFlow Transform ist ein Hybrid
aus Apache Beam und TensorFlow.

47
00:03:15,640 --> 00:03:17,490
Es liegt zwischen den beiden.

48
00:03:17,490 --> 00:03:22,420
Die Dataflow-Vorverarbeitung
funktioniert nur im Kontext einer Pipeline.

49
00:03:22,420 --> 00:03:25,750
Denken Sie an eingehende Streamingdaten

50
00:03:25,750 --> 00:03:30,335
wie IdD-Daten – Internet der Dinge –
oder Flugdaten.

51
00:03:30,335 --> 00:03:35,240
Die Dataflow-Pipeline
kann die Vorhersagen einbeziehen

52
00:03:35,240 --> 00:03:39,715
und sie sogar aufrufen
und in Bigtable speichern.

53
00:03:39,715 --> 00:03:43,560
Diese Vorhersagen
werden dann jedem bereitgestellt,

54
00:03:43,560 --> 00:03:46,495
der die Website in den
nächsten 60 Sekunden besucht.

55
00:03:46,495 --> 00:03:50,220
Danach ist eine neue Vorhersage
in BigTable verfügbar.

56
00:03:50,220 --> 00:03:54,195
Mit anderen Worten,
wenn Sie Dataflow hören,

57
00:03:54,195 --> 00:03:59,095
denken Sie an Sicherung und Vorverarbeitung
für Modelle für maschinelles Lernen.

58
00:03:59,095 --> 00:04:02,060
Sie können Dataflow 
zur Vorverarbeitung verwenden,

59
00:04:02,060 --> 00:04:06,075
wenn der Status erhalten bleiben muss,
wie bei einem Zeitfenster.

60
00:04:07,155 --> 00:04:13,225
Denken Sie bei direkter Vorverarbeitung
für ML-Modelle an TensorFlow.

61
00:04:13,225 --> 00:04:20,680
Nutzen Sie TensorFlow, wenn Vorverarbeitung
nur auf der aktuellen Eingabe basiert.

62
00:04:20,680 --> 00:04:24,260
Wenn Sie also alle Aufgaben
im gepunkteten Feld

63
00:04:24,260 --> 00:04:28,890
in den TensorFlow-Graphen einfügen,
ist es für Clients sehr einfach,

64
00:04:28,890 --> 00:04:35,630
eine Webanwendung aufzurufen, die die
gesamte Vorverarbeitung für sie übernimmt.

65
00:04:36,590 --> 00:04:39,980
Aber was ist mit den Aufgaben dazwischen?

66
00:04:39,980 --> 00:04:43,372
Sie möchten z. B. Ihre Eingaben

67
00:04:43,372 --> 00:04:48,245
basierend auf dem Min-
oder Max-Wert im Dataset skalieren.

68
00:04:48,245 --> 00:04:53,940
Dazu müssen Sie
Ihre Daten in Dataflow analysieren,

69
00:04:53,940 --> 00:04:57,880
um das ganze Dataset zu verarbeiten
und Min- und Max-Werte zu finden.

70
00:04:57,880 --> 00:05:00,600
Dann müssen Sie
die Transformation in Dataflow ausführen,

71
00:05:00,600 --> 00:05:04,035
damit Sie jeden einzelnen
Eingabewert skalieren können.

72
00:05:04,035 --> 00:05:07,245
Darum geht es bei tf.transform.

73
00:05:07,245 --> 00:05:11,555
Es ist ein Hybrid
aus Apache Beam und TensorFlow.

74
00:05:11,555 --> 00:05:19,525
Zum Verständnis: Im Allgemeinen
besteht Vorverarbeitung aus zwei Phasen.

75
00:05:19,535 --> 00:05:24,000
Stellen Sie sich zum Beispiel vor,
Sie möchten Ihre Eingaberohdaten skalieren,

76
00:05:24,000 --> 00:05:26,935
damit GradientDescent besser funktioniert.

77
00:05:26,935 --> 00:05:33,885
Dazu müssen Sie zuerst das Minimum
und das Maximum des numerischen Merkmals

78
00:05:33,885 --> 00:05:37,365
für das gesamte Trainings-Dataset finden.

79
00:05:37,365 --> 00:05:41,025
Dann skalieren Sie jeden Eingabewert

80
00:05:41,025 --> 00:05:46,035
anhand der Min- und Max-Werte, die
aus dem Trainings-Dataset berechnet wurden.

81
00:05:46,035 --> 00:05:49,030
Oder Sie möchten das Vokabular

82
00:05:49,030 --> 00:05:52,025
für Schlüssel
einer kategorischen Variable ermitteln.

83
00:05:52,025 --> 00:05:57,420
Sie haben vielleicht den Fahrzeughersteller
als kategorisches Merkmal.

84
00:05:57,420 --> 00:06:00,920
Sie analysieren
das gesamte Trainings-Dataset,

85
00:06:00,920 --> 00:06:04,640
um alle möglichen Werte
eines bestimmten Merkmals zu finden.

86
00:06:04,640 --> 00:06:08,520
Im Wesentlichen
erhalten Sie eine Liste aller Hersteller.

87
00:06:08,520 --> 00:06:13,785
Wenn Sie dann 20 verschiedene Hersteller
in Ihrem Trainings-Dataset gefunden haben,

88
00:06:13,785 --> 00:06:19,530
One-Hot-codieren Sie die Herstellerspalte
in einen Vektor mit der Länge 20.

89
00:06:19,530 --> 00:06:23,140
Sehen Sie, wie das funktioniert?

90
00:06:23,140 --> 00:06:28,440
Im ersten Schritt wird
das gesamte Dataset einmal durchlaufen.

91
00:06:28,440 --> 00:06:31,640
Das ist die Analysephase.

92
00:06:31,640 --> 00:06:38,770
Im zweiten Schritt werden
die Eingabedaten direkt transformiert.

93
00:06:38,770 --> 00:06:43,635
Das ist die Transformationsphase.

94
00:06:43,635 --> 00:06:52,660
Eignet sich Beam oder TensorFlow besser
für die Analyse des Trainings-Datasets?

95
00:06:52,660 --> 00:06:57,480
Eignet sich Beam oder TensorFlow besser

96
00:06:57,480 --> 00:07:04,000
für die direkte Transformation
der Eingabedaten?

97
00:07:04,000 --> 00:07:10,630
Analysieren Sie in Beam,
transformieren Sie in TensorFlow.

98
00:07:10,630 --> 00:07:15,975
Es gibt zwei PTransforms in tf.transform.

99
00:07:15,975 --> 00:07:21,330
"AnalyzeAndTransformDataset",
das in Beam ausgeführt wird,

100
00:07:21,330 --> 00:07:25,450
um das Trainings-Dataset vorzuverarbeiten,

101
00:07:25,450 --> 00:07:30,190
und "TransformDataset",
das in Beam ausgeführt wird,

102
00:07:30,190 --> 00:07:33,530
um das Bewertungs-Dataset zu erstellen.

103
00:07:33,530 --> 00:07:36,825
Denken Sie daran, dass die Berechnung
des Minimums und Maximums usw.

104
00:07:36,825 --> 00:07:40,965
nur für das Trainings-Dataset
durchgeführt wird.

105
00:07:40,965 --> 00:07:43,780
Wir können das Bewertungs-Dataset
dafür nicht verwenden.

106
00:07:43,780 --> 00:07:50,840
Dieses wird also mit dem Min- und
Max-Wert der Trainingsdaten skaliert.

107
00:07:50,840 --> 00:07:54,750
Aber was, wenn das Maximum
in der Bewertung größer ist?

108
00:07:54,750 --> 00:07:59,830
Das simuliert eine Situation,
in der Sie Ihr Modell bereitstellen

109
00:07:59,830 --> 00:08:03,280
und dann taucht ein größerer Wert
für die Vorhersage auf.

110
00:08:03,280 --> 00:08:04,825
Das ist dasselbe.

111
00:08:04,825 --> 00:08:10,310
Sie können für ein Bewertungs-Dataset
keine Min- und Max-Werte usw. berechnen.

112
00:08:10,310 --> 00:08:12,270
Das lässt sich nicht ändern.

113
00:08:12,270 --> 00:08:16,685
Der aufgerufene
Transformationscode wird jedoch

114
00:08:16,685 --> 00:08:21,510
während der Vorhersage in
TensorFlow ausgeführt.

115
00:08:23,540 --> 00:08:27,870
Sie können sich das auch
als zwei Phasen vorstellen.

116
00:08:27,870 --> 00:08:30,345
Die Analysephase.

117
00:08:30,345 --> 00:08:34,684
Diese wird beim Erstellen des
Trainings-Datasets in Beam ausgeführt.

118
00:08:34,684 --> 00:08:37,155
Die Transformationsphase.

119
00:08:37,155 --> 00:08:41,820
Diese wird während der Vorhersage
in TensorFlow ausgeführt.

120
00:08:41,820 --> 00:08:47,380
Analysieren Sie also in Beam, um Trainings-
und Bewertungs-Datasets zu erstellen.