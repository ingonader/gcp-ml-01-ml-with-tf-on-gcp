Let's look at the analyze phase. Remember that you analyze the training dataset. You first have to tell beam what data to expect. You do that by setting up a schema. So, in the first line I set up a dictionary called raw data schema. I add entries for all the string columns. The string here is that TensorFlow datatype. I then update the raw data schema by adding all the tf.float 32 typed columns. After this I have a raw data schema that has all the columns in the dataset that will be processed by beam on dataflow. The raw data schema is used to create a metadata template. Next, run the analyze-and-transform Ptransform on the training dataset to get back pre-process training data and the transform function. First, do beam.io.read to read in the training data. This is similar to all the beam pipelines that you saw in the previous module on beam. Here I'm reading from BigQuery. Next, filter out the data that you don't want to train with. I'm doing that with a function is valid that I'm not showing you on this slide. I will show you this method later. Third, take the raw data that you get from reading and filtering and the raw data metadata that you got from the previous slide and pass it to the analyze and transform data set Ptransform. Beam will execute this transform in a distributed way and do all the analysis that you told it to do in the method preprocess. I'll show you this method also later. For now, the is valid method and the pre-process method are executed by beam on the training dataset to filter it and to preprocess it. The pre-process data comes back in a P collection. In a parallel collection that I'm calling transformed dataset. But notice that the transformations that you carried out, in pre-process are saved in the second return value. Transform function, this is important. Take the transform data and write it out. Here I'm writing it out as TFRecords which is the most efficient format for TensorFlow. I can do that by using the right to TFRecord, P transform that comes with TensorFlow transform. The files will be shutted automatically. But notice what schema is being used. Not the raw data schema, the transformed schema. Why? Because of course, what we are writing out is a transformed data, the preprocess data, not the raw data.