1
00:00:00,000 --> 00:00:04,230
In diesem Lab sehen wir uns die
Verwendung von TensorFlow Transform an.

2
00:00:04,230 --> 00:00:06,355
Wir schreiben eine Beam-Pipeline,

3
00:00:06,355 --> 00:00:09,780
um die Trainingsdaten
zu analysieren und zu transformieren.

4
00:00:09,780 --> 00:00:12,640
In der gleichen Beam-Pipeline

5
00:00:12,640 --> 00:00:15,590
transformieren wir
auch die Berwertungsdaten

6
00:00:15,590 --> 00:00:18,082
und speichern die Transformationsfunktion

7
00:00:18,082 --> 00:00:20,325
zur Verwendung in der Vorhersage.

8
00:00:20,325 --> 00:00:24,350
Wir modifizieren die Eingabefunktionen
für Training und Bewertung,

9
00:00:24,350 --> 00:00:26,850
um die vorverarbeiteten Dateien zu lesen.

10
00:00:26,850 --> 00:00:29,520
Danach trainieren wir
das Modell ganz normal.

11
00:00:29,520 --> 00:00:32,895
Da wir die Daten
jedoch vorverarbeitet haben,

12
00:00:32,895 --> 00:00:36,510
können wir
diese Vorverarbeitung während des Trainings

13
00:00:36,510 --> 00:00:40,485
mithilfe von Dataflow
für große Datasets durchführen.

14
00:00:40,485 --> 00:00:44,790
Zudem können wir die Vorverarbeitung
während der Bereitstellung in TensorFlow

15
00:00:44,790 --> 00:00:49,095
effizient
als Teil des Modellgraphen ausführen.

16
00:00:49,095 --> 00:00:53,280
Auf diese Weise können Sie
die Skalierbarkeit der Cloud nutzen,

17
00:00:53,280 --> 00:00:59,310
um die Vorverarbeitung
auf mehrere CPUs zu verteilen

18
00:00:59,310 --> 00:01:04,980
und von der Effizienz von CPUs, GPUs

19
00:01:04,980 --> 00:01:08,700
und TensorFlow-Verarbeitungseinheiten
während der Vorhersage zu profitieren.

20
00:01:08,710 --> 00:01:12,790
Öffnen Sie als Nächstes Qwiklabs
und probieren Sie dieses Lab aus.