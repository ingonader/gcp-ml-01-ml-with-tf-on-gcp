ご説明したとおり 特徴エンジニアリングは
3か所で処理できます 1つはTensorFlow自体の中です 特徴列を使用して特徴辞書をラップし 任意のTensorFlowコードを追加します TensorFlowコードとGPU/TPUを使う
効率的な方法です 任意のTensorFlowコードと言ったのは このコードが モデル関数の一部、
TensorFlowグラフの一部として 実行されるからです 社内データベースをクエリして
値を挿入することはできません C++でTensorFlowを記述して
呼び出すことはできますが それは別の話です また 実行できるのは
この入力値のみに依存する処理だけです つまり ローリング平均の計算などは
実行が難しいと言えます 後で 時系列を処理しているような
シーケンスモデルを紹介します 複数の入力値がありますが
その入力がシーケンスになっています TensorFlow処理の制限は 単一入力しか前処理できないことです TensorFlowモデルは シーケンスモデルを除いて
通常 ステートレスです 前の2つの章では Apache Beamを使ったCloud Dataflowでの
前処理と特徴の作成について説明しました Dataflowを使用すると
任意のPythonコードやJavaコードを実行でき 複数の入力値をステートフルに処理できます たとえば時間枠の平均を計算します 過去1時間に交差点を通った
自転車の平均数などです ただし これを行うには予測コードも
パイプライン内で実行する必要があります この時間枠での平均は
パイプラインが必要になる良い例です では 必要なのが最小値や最大値のみで
値を増減するのが目的の場合や カテゴリを数値に変換する
ボキャブラリを得たい場合はどうでしょうか 予測でDataflowパイプラインを実行し 単に最小値と最大値を取得するのは
無駄に思われるかもしれません ここでtf.transformを投入します これは2つの方法のハイブリッドです TensorFlow変換では
TensorFlowメソッドに制限されますが TensorFlowの効率性を活かせます トレーニングデータセット全体の
集約も使用できます これは tf.transformは
トレーニングではDataflowを使用し 予測中はTensorFlowのみを使用するためです TensorFlow変換の仕組みを見てみましょう TensorFlow変換は
Apache BeamとTensorFlowのハイブリッドです Dataflowの前処理は
パイプラインでのみ実行可能です 入力ストリーミングデータの観点で考えます IoTデータやフライトデータなどです Dataflowパイプラインは
予測を伴う場合があり その予測を呼び出して
Bigtableに保存する場合があります その予測はその後60秒以内の ウェブページ訪問者すべてに適用されます この時点で新しい予測を利用できます つまりDataflowは 機械学習モデルの
バックエンドでの前処理に適しています ステートの維持が必要な
時間枠などの前処理に使用します 機械学習モデルのオンザフライ前処理には
TensorFlowが適しています 入力値のみに基づく前処理に使用します この図の点線内のすべてを
TensorFlowグラフに投入すると クライアントは
ウェブアプリケーションを呼び出して すべての処理を簡単に完了できます では これらの中間の場合を考えてみます データセット内の最小値や最大値をもとに
入力をスケーリングしたい場合などです そのような場合は Dataflow内でデータを分析して
データセット全体を処理し 最小値と最大値を見つけ 変換とDataflowを実行し 個別の入力値を増減させます ここで tf.transformの出番です Apache BeamとTensorFlowの
ハイブリッドです この仕組みを理解するために 前処理には2つの段階があることを
思い出してください 入力生データをスケーリングして 勾配降下法の処理を改善したいとします そのためには
トレーニングデータセット全体の 数値特徴の最小値と最大値を
見つける必要があります 次にトレーニングデータセットで計算された 最小値と最大値により
各入力値をスケーリングします 今度は カテゴリ型変数のキーのボキャブラリを
検索する場合を考えます 車の製造元という
カテゴリ型特徴があるとします トレーニングデータセット全体を処理し 特定の特徴のすべての値を見つけます つまり すべての製造元のリストを取得します 見つかった製造元の数が20であった場合 製造元の列を長さ20のベクターに
ワンホットエンコーディングします ご理解いただけたでしょうか 最初のステップではデータセット全体を
一度トラバースしています これは分析フェーズと呼ばれます 2番目のステップは
入力データのオンザフライ変換です これは変換フェーズと呼ばれます BeamとTensorFlowの
どちらのテクノロジーが トレーニングデータセットの
分析に適しているでしょうか？ BeamとTensorFlowの
どちらのテクノロジーが 入力データのオンザフライ変換に
適しているでしょうか？ 答えは 分析にはBeam
変換にはTensorFlowが適しています tf.transformには
2つのPTransformがあります AnalyzeAndTransformDatasetは
Beamで実行され 前処理済みの
トレーニングデータセットを作成します TransformDatasetは
Beamで実行されて 評価データセットを作成します 最小値と最大値などの計算では 分析の対象は
トレーニングデータセットのみです 評価データセットは使用できません 評価データセットは トレーニングデータの
最小値/最大値を使用してスケーリングされます では 評価の最大値の方が大きい場合は
どうすればよいでしょう これはたとえば モデルをデプロイした後で 予測時になって
さらに大きい値が見つかるような状況です 条件は同じです 評価データセットから最小値/最大値や
ボキャブラリは計算できません この制限は変わりません ただし 呼び出された変換コードは 予測時にTensorFlow内で実行されます つまり 2つのフェーズがあるということです まずは分析フェーズです トレーニングデータセットの作成中に
Beam内で実行されます 次に変換フェーズです これは予測時にTensorFlowで実行されます さらに Beam内で実行されて
トレーニング/評価データセットが作成されます