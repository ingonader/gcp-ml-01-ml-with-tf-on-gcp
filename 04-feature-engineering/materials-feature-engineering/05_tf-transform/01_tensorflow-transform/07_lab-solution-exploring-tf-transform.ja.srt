1
00:00:00,000 --> 00:00:04,305
このラボでは
TF Transformを試してみましょう

2
00:00:04,305 --> 00:00:06,910
TF Transformを使用する理由は

3
00:00:06,910 --> 00:00:11,290
Apache Beamで前処理を行えるからです

4
00:00:11,290 --> 00:00:14,840
しかし今回はTensorFlowで
前処理を行ってみましょう

5
00:00:14,840 --> 00:00:19,170
TensorFlowでは大規模な
前処理用データセットを作成できます

6
00:00:19,170 --> 00:00:21,750
そしてトレーニングや評価を
行うことができます

7
00:00:21,760 --> 00:00:25,630
さらにその前処理を入力データに適用し

8
00:00:25,630 --> 00:00:28,910
予測を実行できます

9
00:00:28,910 --> 00:00:32,955
TensorFlowグラフ自体の
一部としても実行できます

10
00:00:32,955 --> 00:00:34,770
では実際に見てみましょう

11
00:00:34,770 --> 00:00:39,640
まず TF Transformは
TensorFlowの一部ではありません

12
00:00:39,640 --> 00:00:42,500
オープンソースの別個のライブラリです

13
00:00:42,500 --> 00:00:48,905
そのため まずはTF Transformの
特定のバージョンをインストールします

14
00:00:48,905 --> 00:00:52,190
使用しているTensorFlowの
バージョンを把握して

15
00:00:52,190 --> 00:00:56,035
次にTF Transformの
対応するバージョンを確認します

16
00:00:56,035 --> 00:01:00,780
この動画で使用しているのは
TensorFlow 1.5です

17
00:01:00,780 --> 00:01:05,965
1.5に対応するTF Transformは

18
00:01:05,965 --> 00:01:07,835
バージョン0.5です

19
00:01:07,835 --> 00:01:10,355
みなさんのバージョンは違うかもしれません

20
00:01:10,355 --> 00:01:14,270
ノートブックは常に更新されているので

21
00:01:14,270 --> 00:01:19,070
TensorFlowに対応するバージョンが
インストールされているはずです

22
00:01:19,070 --> 00:01:23,170
今回はTF Transform0.5をインストールします

23
00:01:23,170 --> 00:01:27,345
また Apache Beam-GCPパッケージ
もインストールします

24
00:01:27,345 --> 00:01:29,915
では もう一度すべて正しいか確認します

25
00:01:29,915 --> 00:01:32,810
すでにDataflowがあるはずですが
これはアンインストールします

26
00:01:32,810 --> 00:01:37,300
基本的にApache Beam-GCPおよび
Google Cloud Dataflowと同じものだからです

27
00:01:37,300 --> 00:01:41,105
今回はすべてオープンソースのものを
利用したいと思います

28
00:01:41,110 --> 00:01:46,310
pip installおよびuninstallを実行します

29
00:01:46,310 --> 00:01:49,030
通常は数分で完了するはずです

30
00:01:49,030 --> 00:01:55,270
完了後 ノートブックのpipパッケージが
最新であるか確認しましょう

31
00:01:55,270 --> 00:01:59,190
最新であるか確認するには
[リセット]をクリックします

32
00:01:59,190 --> 00:02:03,810
そして黒丸のボタンが
再度有効になるまで待ちます

33
00:02:03,810 --> 00:02:06,635
有効になると 
特定のセルが処理を完了し

34
00:02:06,635 --> 00:02:10,250
pip installが適切に
実行されていることになります

35
00:02:10,250 --> 00:02:12,410
では少し待ちましょう

36
00:02:15,130 --> 00:02:18,970
完了したようです

37
00:02:18,970 --> 00:02:22,850
黒丸のボタンが再度有効になりました

38
00:02:22,850 --> 00:02:25,370
セルの処理が完了したことになります

39
00:02:25,370 --> 00:02:27,445
実際にセルをみてみましょう

40
00:02:27,445 --> 00:02:32,460
複数の処理が滞りなく行われています

41
00:02:32,750 --> 00:02:35,155
終わりまで進めましょう

42
00:02:35,155 --> 00:02:39,875
インストールとアンインストールが
確認できます

43
00:02:39,875 --> 00:02:43,760
TF Transform0.5の部分がありますね

44
00:02:43,760 --> 00:02:45,370
では 確認してみましょう

45
00:02:45,370 --> 00:02:47,620
ここでできることは何でしょうか

46
00:02:47,620 --> 00:02:50,510
まず 取り込んだものをみましょう

47
00:02:50,510 --> 00:02:53,225
そうするにはリセットする必要があります

48
00:02:53,225 --> 00:02:55,010
[リセット]をクリックします

49
00:02:55,010 --> 00:02:56,694
セッションが再び開始されます

50
00:02:56,695 --> 00:03:00,765
この時点で新しいpipパッケージが
取り込まれています

51
00:03:00,765 --> 00:03:03,865
下にスクロールしてみましょう

52
00:03:03,865 --> 00:03:09,710
このセルでpip freezeが行われています

53
00:03:09,711 --> 00:03:15,080
ノートブックで実行されている
Dockerコンテナの内容がわかるはずです

54
00:03:15,080 --> 00:03:21,705
grepしてflowまたはBeam
を含むパッケージを検索します

55
00:03:21,705 --> 00:03:24,890
ここで 縦棒はorを意味しています

56
00:03:24,898 --> 00:03:26,600
実行してみましょう

57
00:03:26,600 --> 00:03:31,535
TF TtransformとApache Beamの両方が

58
00:03:31,535 --> 00:03:34,485
インストールされているのがわかります

59
00:03:34,485 --> 00:03:36,295
TensorFlow自体もインストールされています

60
00:03:36,295 --> 00:03:39,800
Tensor BoardとApache airflowもあるようです

61
00:03:39,800 --> 00:03:42,750
今回は必要ありませんが
インストールされました

62
00:03:42,755 --> 00:03:46,565
これでTensorFlowをインポートする準備が
ほぼ完了しました

63
00:03:46,565 --> 00:03:49,310
transflow_transformも
TFTとしてインポートしています

64
00:03:49,310 --> 00:03:55,140
次にプロジェクトのバケットを変更して
Quick Labsプロジェクトを反映させます

65
00:03:55,160 --> 00:03:56,620
すでに反映済みのようです

66
00:03:56,620 --> 00:04:02,355
それでは セルを実行して
Bashで取り込まれるのを確認します

67
00:04:02,355 --> 00:04:04,745
これはos.environで行われています

68
00:04:04,745 --> 00:04:10,950
プロジェクトとコンピュータリージョンが
その通りになっているか確認しましょう

69
00:04:10,950 --> 00:04:14,755
次はBigQueryからデータを取得します

70
00:04:14,760 --> 00:04:17,875
ただ前の例とは異なります

71
00:04:17,875 --> 00:04:22,190
緯度や経度などでフィルタリングしません

72
00:04:22,190 --> 00:04:24,320
それはApache Beamで行います

73
00:04:24,320 --> 00:04:30,595
つまり 予測の最中に誤った
入力があってもホストしません

74
00:04:30,615 --> 00:04:34,450
ではいくつかやってみましょう

75
00:04:34,450 --> 00:04:37,805
前処理で運賃などを取得します

76
00:04:37,805 --> 00:04:41,860
クエリ自体は以前よりもシンプルです

77
00:04:41,860 --> 00:04:46,480
ほとんどの前処理は
Apache Beamで行うからです

78
00:04:46,480 --> 00:04:52,185
ここではまず
有効なDataFrameを作成します

79
00:04:52,185 --> 00:04:54,590
どうなるかお見せします

80
00:04:54,590 --> 00:04:59,095
クエリを動作させて実行し
Pandas DataFrameを作成します

81
00:04:59,095 --> 00:05:04,970
Pandas DataFrameが作成されたら
headをコールして最初の数行を表示させます

82
00:05:04,970 --> 00:05:11,125
次にdescribeをコールすると
平均値やその他の数値が表示されます

83
00:05:11,125 --> 00:05:17,160
特定のDataFrameの平均値
標準偏差、数量が表示されます

84
00:05:20,600 --> 00:05:22,385
問題ないですね
それでは 戻りましょう

85
00:05:22,385 --> 00:05:28,195
有効なDataFrameが取得できました

86
00:05:28,195 --> 00:05:34,610
fare_amountやhourofdayなどのカラムで
11,181が表示されています

87
00:05:34,625 --> 00:05:39,700
このようにクエリに
問題のないことが確認できます

88
00:05:39,705 --> 00:05:43,430
ではこのクエリを機械学習の
データセットの作成に使ってみましょう

89
00:05:43,430 --> 00:05:46,160
ここでは TF Transformと
Dataflowを使用します

90
00:05:46,160 --> 00:05:49,429
これまでのDataflowのジョブとは
異なります

91
00:05:49,429 --> 00:05:54,670
Dataflowの実行マシンに別のパッケージを
インストールする必要があります

92
00:05:54,670 --> 00:05:58,980
これを行うには
requirements.textを記述します

93
00:05:58,980 --> 00:06:02,890
pip installを実行したときのことを
思い出してください

94
00:06:02,890 --> 00:06:07,660
TF Transform 0.5.0はpipで
インストールしました

95
00:06:07,660 --> 00:06:09,565
TF Transform 0.5.0が
インストールされていますね

96
00:06:09,565 --> 00:06:13,465
requirements.textを記述していきます

97
00:06:13,465 --> 00:06:20,340
TensorFlow transform 0.5.0の
インストールが必要と記述します

98
00:06:20,350 --> 00:06:22,265
そのように記述されていますね

99
00:06:22,265 --> 00:06:24,730
記述できたら次に進みます

100
00:06:24,731 --> 00:06:28,445
Dataflowジョブを実行して

101
00:06:28,445 --> 00:06:33,055
要件ファイルとして
このrequirements.textを渡します

102
00:06:33,055 --> 00:06:35,990
これによりDataflowにrequirements.text
を参照する必要があることを伝え

103
00:06:35,990 --> 00:06:42,085
必須の全Pythonパッケージを
pip installするように伝えることができます

104
00:06:42,085 --> 00:06:44,760
このジョブでは
何が行われるのでしょうか

105
00:06:44,760 --> 00:06:50,470
このジョブでは前のジョブと同様に
BigQueryからデータを読み込みます

106
00:06:50,475 --> 00:06:54,245
レコードも作成します

107
00:06:54,245 --> 00:06:58,080
前のケースと違うのは
CSVレコードを作成することです

108
00:06:58,100 --> 00:07:00,740
TensorFlowの例を作成します

109
00:07:00,740 --> 00:07:02,465
より効率的だからです

110
00:07:02,465 --> 00:07:07,185
評価用データセットにトレーニング用
データセットを作る必要もあります

111
00:07:07,185 --> 00:07:10,300
では順を追ってやってみましょう

112
00:07:10,300 --> 00:07:15,355
まず どのような前処理を行うか
決定します

113
00:07:15,355 --> 00:07:18,290
たとえば2種類の前処理を
行うとしましょう

114
00:07:18,290 --> 00:07:25,195
1つ目は入力行が有効かどうか
確認する前処理としましょう

115
00:07:25,195 --> 00:07:27,135
これで入力行が確実に有効になります

116
00:07:27,135 --> 00:07:29,220
次に入力用のディクショナリを用意します

117
00:07:29,220 --> 00:07:32,601
BigQueryからのデータを
ディクショナリにします

118
00:07:32,610 --> 00:07:39,525
またJSONからの予測も
ディクショナリにすることができます

119
00:07:39,535 --> 00:07:45,510
同じコードがBigQueryデータセット
と入力されるJSONデータで動作するのです

120
00:07:45,510 --> 00:07:47,105
実際にやってみましょう

121
00:07:47,115 --> 00:07:49,520
入力を取得します

122
00:07:49,520 --> 00:07:52,440
pickuplon、dropofflon、pickuplat

123
00:07:52,440 --> 00:07:55,686
dropofflat、hour of the day、
day of the week

124
00:07:55,695 --> 00:07:57,700
その他すべてを取得します

125
00:07:57,700 --> 00:08:02,365
取得できないものがあれば
それは有効でないことになります

126
00:08:02,370 --> 00:08:04,240
それらは除外しましょう

127
00:08:04,240 --> 00:08:06,460
基本的にすべてを処理します

128
00:08:06,470 --> 00:08:10,375
例外が発生したら
有効ではないとみなします

129
00:08:10,375 --> 00:08:16,500
データが取得できたのであれば
有効で条件に一致したということです

130
00:08:16,500 --> 00:08:19,010
運賃の値が2.5以上である

131
00:08:19,010 --> 00:08:22,675
取得した経度が西経78度以上である

132
00:08:22,675 --> 00:08:25,475
これらすべての条件にパスしたとします

133
00:08:25,485 --> 00:08:28,065
その場合その入力は
有効であるということです

134
00:08:28,065 --> 00:08:30,545
では次に前処理に進みましょう

135
00:08:30,545 --> 00:08:37,050
データを取得しニューラルネットワーキングの
トレーニングを改善しましょう

136
00:08:37,059 --> 00:08:38,530
具体的に何をするのでしょうか

137
00:08:38,530 --> 00:08:43,890
入力されるfare_amountを
変更せずに渡します

138
00:08:43,890 --> 00:08:48,265
そのまま渡すこともできますし
他の関数をコールすることもできます

139
00:08:48,265 --> 00:08:51,870
今回はTF identityをコールして
それを渡してみましょう

140
00:08:51,870 --> 00:08:56,055
dayofweekの値は整数型です

141
00:08:56,055 --> 00:08:59,690
BigQueryでは1、2、3、4などの
整数型が使われます

142
00:08:59,690 --> 00:09:05,795
前回の機能エンジニアリングラボでは
どうだったでしょうか

143
00:09:05,800 --> 00:09:09,140
基本的にはボキャブラリー内で
ハードコードしていました

144
00:09:09,140 --> 00:09:12,485
今回はTF Transformに
トレーニング用のデータセットから

145
00:09:12,485 --> 00:09:15,255
ボキャブラリーを学習させるのです

146
00:09:15,255 --> 00:09:20,850
それぞれの数値が何を意味するか
必ずしも知る必要はありません

147
00:09:20,850 --> 00:09:25,500
どのような入力データであっても
予測の際に自動的に変換されます

148
00:09:25,520 --> 00:09:27,570
ここではdayofweekを
整数型に変換しましょう

149
00:09:27,580 --> 00:09:33,025
ボキャブラリーに基づいて文字列を
整数型に変換するのです

150
00:09:33,025 --> 00:09:34,860
string_to_intで行っているのは
この処理です

151
00:09:34,860 --> 00:09:40,985
hourofdayはすでに整数型ですので
変更せずにそのまま渡します

152
00:09:40,985 --> 00:09:46,800
pickuplonは浮動小数点型ですので
これもそのまま渡します

153
00:09:46,810 --> 00:09:51,425
しかしニューラルネットワークの学習や

154
00:09:51,425 --> 00:09:53,895
最急降下法は入力値が小さい値

155
00:09:53,895 --> 00:09:57,725
つまり0から1の間などの場合に

156
00:09:57,725 --> 00:09:59,715
優れた性能を発揮します

157
00:09:59,715 --> 00:10:02,805
そこでTF Transformに入力値を
調整させます

158
00:10:02,817 --> 00:10:07,417
入力値を0から1の間に調整させます

159
00:10:08,060 --> 00:10:10,910
ただこのような調整を行うには

160
00:10:10,910 --> 00:10:16,440
TF Transformに最小値と最大値を
認識させる必要があります

161
00:10:16,440 --> 00:10:18,610
データセットから学習させるのです

162
00:10:18,610 --> 00:10:20,540
そのためのフェーズが2つあり

163
00:10:20,540 --> 00:10:25,800
分析フェーズと変換フェーズが
用意されます

164
00:10:25,800 --> 00:10:29,870
仮にここで0から1の間に値を変換する
変換フェーズがあったとしても

165
00:10:29,870 --> 00:10:34,495
分析フェーズで最小値と最大値が
認識されていなければ

166
00:10:34,495 --> 00:10:36,690
この調整を行うことはできません

167
00:10:36,690 --> 00:10:39,350
その他の値にも同じ処理を行います

168
00:10:39,350 --> 00:10:43,210
passengersの入力値を
浮動小数に型変換します

169
00:10:43,210 --> 00:10:46,880
inputs.passengersの値を取得し

170
00:10:46,880 --> 00:10:50,780
ones_likeで処理します

171
00:10:50,780 --> 00:10:55,390
同じ数を取得して
文字列に型変換します

172
00:10:55,390 --> 00:10:59,270
その結果 すべてのキーが
基本的には文字列の値になります

173
00:10:59,270 --> 00:11:02,450
これはあくまでも一例に過ぎません

174
00:11:02,450 --> 00:11:05,685
他にも任意のTensorFlow関数を
コールできます

175
00:11:05,685 --> 00:11:10,065
重要なのは 前処理をすべて
TensorFlow関数で行えることです

176
00:11:10,065 --> 00:11:13,700
では前処理が終わったので
エンジニアリングも行いましょう

177
00:11:13,700 --> 00:11:16,060
やはりここでも
TensorFlow関数を使用します

178
00:11:16,060 --> 00:11:19,080
経度の取り込みやドロップ
それらの減算

179
00:11:19,096 --> 00:11:21,663
緯度の取り込みやドロップ
それらの減算

180
00:11:21,663 --> 00:11:25,075
さらに経度や緯度の差分の取得など

181
00:11:25,075 --> 00:11:27,618
コンピューティングや
スケーリングが可能です

182
00:11:27,618 --> 00:11:33,340
つまり 差分やスケーリングを
気にかける必要はないのです

183
00:11:33,375 --> 00:11:35,920
それこそがTF Transformの役割であり

184
00:11:35,920 --> 00:11:38,695
適切にスケーリングできるよう
最大値や最小値をみつけてくれます

185
00:11:38,695 --> 00:11:42,580
ではもう一度
スケーリングされた値をとって

186
00:11:42,602 --> 00:11:46,390
その値からユークリッド距離を
計算しましょう

187
00:11:46,390 --> 00:11:48,271
再びスケーリングする必要はありません

188
00:11:48,272 --> 00:11:51,242
距離の値が0から1の間にあることが
わかっているからです

189
00:11:51,250 --> 00:11:54,045
平方根も0から1の間です

190
00:11:54,045 --> 00:11:56,985
すべて平方根内なので問題ありません

191
00:11:56,985 --> 00:11:59,150
実際には少しだけ大きいですね

192
00:11:59,150 --> 00:12:02,770
両方が1である場合1.4になりますが
十分近い値です

193
00:12:02,770 --> 00:12:06,280
小さい数ですので
この時点で調整する必要はありません

194
00:12:06,280 --> 00:12:11,320
前処理の関数はすべて完了しています

195
00:12:11,320 --> 00:12:15,480
ですがis_validメソッドと

196
00:12:15,480 --> 00:12:18,240
preprocess_tftメソッドを
コールする必要があります

197
00:12:18,240 --> 00:12:23,644
これらのメソッドはBeam Transformから
コールする必要があります

198
00:12:23,644 --> 00:12:24,995
どのように行うのでしょうか

199
00:12:24,995 --> 00:12:29,070
まず最初に行うのは

200
00:12:29,070 --> 00:12:33,465
読み込みを行う生データの
メタデータを設定することです

201
00:12:33,465 --> 00:12:37,515
ここでいう生データとは
BigQueryからのデータのことです

202
00:12:37,520 --> 00:12:43,472
dayofweekやkeyは文字列です

203
00:12:43,490 --> 00:12:49,480
fare_amount、pickuplon、pickuplat
はすべて浮動小数です

204
00:12:49,490 --> 00:12:52,350
新しい生データの処理スキームも
作成します

205
00:12:52,350 --> 00:12:54,540
これは基本的にはディクショナリで

206
00:12:54,540 --> 00:13:00,175
カラムの名前から文字列、浮動小数、
整数の判断までカバーします

207
00:13:00,175 --> 00:13:03,030
hourofdayやpassengersは整数型です

208
00:13:03,030 --> 00:13:06,655
これらは生データに存在し
BigQueryからのデータとなります

209
00:13:06,670 --> 00:13:10,675
では生データをセルで処理します

210
00:13:10,675 --> 00:13:15,135
rawdata_metadataを書き込みます

211
00:13:15,135 --> 00:13:18,040
この書き込みを行うのは

212
00:13:18,040 --> 00:13:24,005
ユーザーからのJSON入力も
rawdata_metadataにするためです

213
00:13:24,005 --> 00:13:26,970
このような形式にすることで

214
00:13:26,970 --> 00:13:30,540
入力用の関数に認識させます

215
00:13:30,540 --> 00:13:34,620
次に 先ほど作成したクエリを使用して

216
00:13:34,630 --> 00:13:37,660
BigQueryのデータを読み込みます

217
00:13:37,660 --> 00:13:41,635
そしてis_validメソッドを使用して
フィルタリングします

218
00:13:41,635 --> 00:13:43,810
is_validメソッドがどのように
入力されるかがわかると思います

219
00:13:43,810 --> 00:13:46,935
このメソッドはbeamフィルタの
一部としてコールされます

220
00:13:46,935 --> 00:13:53,730
beamフィルタはis_valid関数で指定した
ルールに基づいて実行されます

221
00:13:53,730 --> 00:13:58,870
データセットをコールし
分析と変換を行います

222
00:13:58,870 --> 00:14:02,890
この場合 変換用の関数を
指定する必要があります

223
00:14:02,890 --> 00:14:06,265
変換用関数はpreprocess_tftです

224
00:14:06,265 --> 00:14:08,965
この関数はスケーリングなどの
処理をすべて行います

225
00:14:08,965 --> 00:14:14,459
この時点で 変換用データセットと
変換用関数が得られました

226
00:14:14,459 --> 00:14:21,210
次に 変換データを取得し
TFレコードとして書き込みます

227
00:14:21,210 --> 00:14:23,590
TFレコードとして書き込むのですが

228
00:14:23,599 --> 00:14:26,945
容量節約のためにGzipで圧縮します

229
00:14:26,945 --> 00:14:30,580
同じことをテストデータにも行います

230
00:14:30,580 --> 00:14:33,480
トレーニングデータでは
クエリが1で作成されています

231
00:14:33,490 --> 00:14:36,285
テストデータではクエリは
2で作成されています

232
00:14:36,285 --> 00:14:42,365
つまり 渡される値が1か2かによって
クエリの設定方法が異なります

233
00:14:42,365 --> 00:14:43,725
このphaseの部分です

234
00:14:43,725 --> 00:14:49,625
ハッシュバケットの最初か最後の部分の
どちらかを取得しています

235
00:14:49,625 --> 00:14:54,070
このようにしてトレーニングデータセットか
評価用データセットのどちらかを取得します

236
00:14:55,530 --> 00:14:58,075
下にスクロールしましょう

237
00:14:58,075 --> 00:14:59,716
完了しました

238
00:14:59,716 --> 00:15:05,136
変換用テストデータを書き込みます

239
00:15:05,136 --> 00:15:09,659
評価用データにも書き込みます

240
00:15:09,671 --> 00:15:11,820
そしてこれがとても重要なのですが

241
00:15:11,820 --> 00:15:15,725
変換用のメタデータも
書き込む必要があります

242
00:15:15,725 --> 00:15:19,485
これが コールした全TFメソッドが

243
00:15:19,485 --> 00:15:21,375
グラフに格納される方法です

244
00:15:21,375 --> 00:15:25,840
ここで実際に行われているのは
モデルの書き込みです

245
00:15:25,840 --> 00:15:28,380
モデルはトレーニングするものではありません

246
00:15:28,380 --> 00:15:33,200
モデルはTensorFlowの処理で構成され

247
00:15:33,200 --> 00:15:38,260
標準のモデルグラフの前に位置します

248
00:15:38,260 --> 00:15:43,570
ユーザーからの入力が

249
00:15:44,800 --> 00:15:48,225
TensorFlow関数を通じて
標準モデルに渡されるということです

250
00:15:48,225 --> 00:15:52,350
これで前処理用データセットを

251
00:15:52,350 --> 00:15:55,270
作成する準備ができました

252
00:15:55,270 --> 00:15:58,320
Trueに設定すれば
小規模なデータセットが作成されます

253
00:15:58,327 --> 00:16:02,680
しかし今回はFalseに設定して
Dataflowで実行させて

254
00:16:02,680 --> 00:16:06,119
データセットを作成します

255
00:16:06,119 --> 00:16:09,820
この時点でエラーが発生する場合は

256
00:16:09,820 --> 00:16:14,445
Dataflow APIが有効に
なっていないということです

257
00:16:14,445 --> 00:16:17,925
Quick Labsプロジェクトで
Dataflow APIを有効にしてください

258
00:16:17,925 --> 00:16:21,260
有効にすれば
Dataflowジョブが起動します

259
00:16:21,260 --> 00:16:27,650
これが完了するとpreproc_tftで
ファイルを確認できるはずです

260
00:16:27,650 --> 00:16:34,025
そして トレーニングは既存のものと
ほぼ同じものになっているでしょう

261
00:16:34,025 --> 00:16:37,240
実際に違いを確認してみましょう

262
00:16:37,240 --> 00:16:42,355
taxifare_tftで
TF Transformを確認します

263
00:16:42,355 --> 00:16:45,725
model.pyを確認してみましょう

264
00:16:45,725 --> 00:16:51,605
model.py内の違いは何でしょうか

265
00:16:51,605 --> 00:16:56,015
input columnsの取得方法は以前と同じです

266
00:16:56,015 --> 00:16:58,495
BucketizeやFeature cross
の部分は同じです

267
00:16:58,495 --> 00:17:01,780
wide columnsも
deep columnsも作成しています

268
00:17:01,780 --> 00:17:05,819
すべて前処理のときと同じです

269
00:17:05,819 --> 00:17:08,299
以前はDataFlowを使用して行っており

270
00:17:08,299 --> 00:17:15,280
追加の関数を作成して
3ヶ所のすべてでコールを行っていました

271
00:17:15,280 --> 00:17:16,865
ただ今回は

272
00:17:16,865 --> 00:17:19,750
そのようなことをする必要はなく
作成した追加の関数も必要ありません

273
00:17:19,750 --> 00:17:22,210
作成した追加の関数で
以前行っていたことは

274
00:17:22,210 --> 00:17:26,405
TF Transformが
グラフの一部として行います

275
00:17:26,405 --> 00:17:32,240
したがって 処理関数が与えられた場合

276
00:17:32,250 --> 00:17:36,720
この変換用関数から

277
00:17:36,720 --> 00:17:39,625
実行されたすべての操作が
読み込まれます

278
00:17:39,625 --> 00:17:41,805
入力された生データを取得します

279
00:17:41,805 --> 00:17:43,470
ここが生データの部分です

280
00:17:43,470 --> 00:17:49,495
TensorFlowのtransform関数で
行われた処理がすべて適用されます

281
00:17:49,495 --> 00:17:51,170
これまでのすべての処理です

282
00:17:51,170 --> 00:17:55,690
基本的にはpreprocess_tftで
コールしたコードすべてです

283
00:17:55,690 --> 00:18:00,870
すべてをfeaturesに適用します

284
00:18:00,870 --> 00:18:04,860
feature_placeholdersにも適用します

285
00:18:04,875 --> 00:18:09,689
featuresとfeature_placeholdersの
2つが返り値になります

286
00:18:09,689 --> 00:18:13,235
feature_placeholdersは
エンドユーザーが送信したものです

287
00:18:13,235 --> 00:18:15,520
元はJSONのデータでした

288
00:18:15,520 --> 00:18:20,360
featuresはJSONデータを
取得した結果であり

289
00:18:20,360 --> 00:18:25,120
TF Transformの変換関数を適用します

290
00:18:25,120 --> 00:18:28,695
変換関数のすべての処理は

291
00:18:28,700 --> 00:18:32,000
feature_placeholdersに対して行われ
これが返り値になります

292
00:18:32,000 --> 00:18:35,615
この時点で 入力を処理する
関数が用意されました

293
00:18:35,615 --> 00:18:38,165
では データセットを読み込む場合
何をする必要があるでしょうか

294
00:18:38,165 --> 00:18:40,100
データセットを読み込む場合

295
00:18:40,100 --> 00:18:42,335
それらを変換する必要があります

296
00:18:42,335 --> 00:18:45,607
しかし そのためのコードを
記述する必要はありません

297
00:18:45,615 --> 00:18:50,550
TF Transformには入力関数の
作成機能があるからです

298
00:18:50,550 --> 00:18:54,980
トレーニング用の入力関数を作成し

299
00:18:54,980 --> 00:18:58,195
transformed_metadataで
すべてを適用できます

300
00:18:58,195 --> 00:19:04,395
次にGzipで読み込みます

301
00:19:04,395 --> 00:19:07,940
トレーニング用の入力関数が
組み込まれており

302
00:19:07,940 --> 00:19:10,490
TensorFlowレコードの読み込み方法も
設定されています

303
00:19:10,490 --> 00:19:14,890
通常ではコード全体を記述しますが
その必要はなく

304
00:19:14,890 --> 00:19:18,280
データセットの読み込みも
CSVのデコード処理も不要で

305
00:19:18,290 --> 00:19:20,715
すべて作業しなくてよくなります

306
00:19:20,715 --> 00:19:27,055
トレーニング用入力関数を使用するだけで
それらの作業が完了します

307
00:19:27,060 --> 00:19:30,390
学習と評価については
これまでとまったく同じです

308
00:19:30,390 --> 00:19:33,610
train_specとeval_specを作成し

309
00:19:33,619 --> 00:19:37,120
estimatorに渡します

310
00:19:37,135 --> 00:19:41,485
1つだけ違うのはGzipを
読み込ませることです

311
00:19:41,485 --> 00:19:43,900
Gzipの読み込み用関数に渡します

312
00:19:43,900 --> 00:19:48,106
そしてこの読み込み用関数が
TFレコードの読み込み関数になっています

313
00:19:48,120 --> 00:19:52,880
これで基本的に完了です