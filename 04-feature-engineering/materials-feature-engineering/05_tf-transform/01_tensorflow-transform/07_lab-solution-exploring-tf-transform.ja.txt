このラボでは
TF Transformを試してみましょう TF Transformを使用する理由は Apache Beamで前処理を行えるからです しかし今回はTensorFlowで
前処理を行ってみましょう TensorFlowでは大規模な
前処理用データセットを作成できます そしてトレーニングや評価を
行うことができます さらにその前処理を入力データに適用し 予測を実行できます TensorFlowグラフ自体の
一部としても実行できます では実際に見てみましょう まず TF Transformは
TensorFlowの一部ではありません オープンソースの別個のライブラリです そのため まずはTF Transformの
特定のバージョンをインストールします 使用しているTensorFlowの
バージョンを把握して 次にTF Transformの
対応するバージョンを確認します この動画で使用しているのは
TensorFlow 1.5です 1.5に対応するTF Transformは バージョン0.5です みなさんのバージョンは違うかもしれません ノートブックは常に更新されているので TensorFlowに対応するバージョンが
インストールされているはずです 今回はTF Transform0.5をインストールします また Apache Beam-GCPパッケージ
もインストールします では もう一度すべて正しいか確認します すでにDataflowがあるはずですが
これはアンインストールします 基本的にApache Beam-GCPおよび
Google Cloud Dataflowと同じものだからです 今回はすべてオープンソースのものを
利用したいと思います pip installおよびuninstallを実行します 通常は数分で完了するはずです 完了後 ノートブックのpipパッケージが
最新であるか確認しましょう 最新であるか確認するには
[リセット]をクリックします そして黒丸のボタンが
再度有効になるまで待ちます 有効になると 
特定のセルが処理を完了し pip installが適切に
実行されていることになります では少し待ちましょう 完了したようです 黒丸のボタンが再度有効になりました セルの処理が完了したことになります 実際にセルをみてみましょう 複数の処理が滞りなく行われています 終わりまで進めましょう インストールとアンインストールが
確認できます TF Transform0.5の部分がありますね では 確認してみましょう ここでできることは何でしょうか まず 取り込んだものをみましょう そうするにはリセットする必要があります [リセット]をクリックします セッションが再び開始されます この時点で新しいpipパッケージが
取り込まれています 下にスクロールしてみましょう このセルでpip freezeが行われています ノートブックで実行されている
Dockerコンテナの内容がわかるはずです grepしてflowまたはBeam
を含むパッケージを検索します ここで 縦棒はorを意味しています 実行してみましょう TF TtransformとApache Beamの両方が インストールされているのがわかります TensorFlow自体もインストールされています Tensor BoardとApache airflowもあるようです 今回は必要ありませんが
インストールされました これでTensorFlowをインポートする準備が
ほぼ完了しました transflow_transformも
TFTとしてインポートしています 次にプロジェクトのバケットを変更して
Quick Labsプロジェクトを反映させます すでに反映済みのようです それでは セルを実行して
Bashで取り込まれるのを確認します これはos.environで行われています プロジェクトとコンピュータリージョンが
その通りになっているか確認しましょう 次はBigQueryからデータを取得します ただ前の例とは異なります 緯度や経度などでフィルタリングしません それはApache Beamで行います つまり 予測の最中に誤った
入力があってもホストしません ではいくつかやってみましょう 前処理で運賃などを取得します クエリ自体は以前よりもシンプルです ほとんどの前処理は
Apache Beamで行うからです ここではまず
有効なDataFrameを作成します どうなるかお見せします クエリを動作させて実行し
Pandas DataFrameを作成します Pandas DataFrameが作成されたら
headをコールして最初の数行を表示させます 次にdescribeをコールすると
平均値やその他の数値が表示されます 特定のDataFrameの平均値
標準偏差、数量が表示されます 問題ないですね
それでは 戻りましょう 有効なDataFrameが取得できました fare_amountやhourofdayなどのカラムで
11,181が表示されています このようにクエリに
問題のないことが確認できます ではこのクエリを機械学習の
データセットの作成に使ってみましょう ここでは TF Transformと
Dataflowを使用します これまでのDataflowのジョブとは
異なります Dataflowの実行マシンに別のパッケージを
インストールする必要があります これを行うには
requirements.textを記述します pip installを実行したときのことを
思い出してください TF Transform 0.5.0はpipで
インストールしました TF Transform 0.5.0が
インストールされていますね requirements.textを記述していきます TensorFlow transform 0.5.0の
インストールが必要と記述します そのように記述されていますね 記述できたら次に進みます Dataflowジョブを実行して 要件ファイルとして
このrequirements.textを渡します これによりDataflowにrequirements.text
を参照する必要があることを伝え 必須の全Pythonパッケージを
pip installするように伝えることができます このジョブでは
何が行われるのでしょうか このジョブでは前のジョブと同様に
BigQueryからデータを読み込みます レコードも作成します 前のケースと違うのは
CSVレコードを作成することです TensorFlowの例を作成します より効率的だからです 評価用データセットにトレーニング用
データセットを作る必要もあります では順を追ってやってみましょう まず どのような前処理を行うか
決定します たとえば2種類の前処理を
行うとしましょう 1つ目は入力行が有効かどうか
確認する前処理としましょう これで入力行が確実に有効になります 次に入力用のディクショナリを用意します BigQueryからのデータを
ディクショナリにします またJSONからの予測も
ディクショナリにすることができます 同じコードがBigQueryデータセット
と入力されるJSONデータで動作するのです 実際にやってみましょう 入力を取得します pickuplon、dropofflon、pickuplat dropofflat、hour of the day、
day of the week その他すべてを取得します 取得できないものがあれば
それは有効でないことになります それらは除外しましょう 基本的にすべてを処理します 例外が発生したら
有効ではないとみなします データが取得できたのであれば
有効で条件に一致したということです 運賃の値が2.5以上である 取得した経度が西経78度以上である これらすべての条件にパスしたとします その場合その入力は
有効であるということです では次に前処理に進みましょう データを取得しニューラルネットワーキングの
トレーニングを改善しましょう 具体的に何をするのでしょうか 入力されるfare_amountを
変更せずに渡します そのまま渡すこともできますし
他の関数をコールすることもできます 今回はTF identityをコールして
それを渡してみましょう dayofweekの値は整数型です BigQueryでは1、2、3、4などの
整数型が使われます 前回の機能エンジニアリングラボでは
どうだったでしょうか 基本的にはボキャブラリー内で
ハードコードしていました 今回はTF Transformに
トレーニング用のデータセットから ボキャブラリーを学習させるのです それぞれの数値が何を意味するか
必ずしも知る必要はありません どのような入力データであっても
予測の際に自動的に変換されます ここではdayofweekを
整数型に変換しましょう ボキャブラリーに基づいて文字列を
整数型に変換するのです string_to_intで行っているのは
この処理です hourofdayはすでに整数型ですので
変更せずにそのまま渡します pickuplonは浮動小数点型ですので
これもそのまま渡します しかしニューラルネットワークの学習や 最急降下法は入力値が小さい値 つまり0から1の間などの場合に 優れた性能を発揮します そこでTF Transformに入力値を
調整させます 入力値を0から1の間に調整させます ただこのような調整を行うには TF Transformに最小値と最大値を
認識させる必要があります データセットから学習させるのです そのためのフェーズが2つあり 分析フェーズと変換フェーズが
用意されます 仮にここで0から1の間に値を変換する
変換フェーズがあったとしても 分析フェーズで最小値と最大値が
認識されていなければ この調整を行うことはできません その他の値にも同じ処理を行います passengersの入力値を
浮動小数に型変換します inputs.passengersの値を取得し ones_likeで処理します 同じ数を取得して
文字列に型変換します その結果 すべてのキーが
基本的には文字列の値になります これはあくまでも一例に過ぎません 他にも任意のTensorFlow関数を
コールできます 重要なのは 前処理をすべて
TensorFlow関数で行えることです では前処理が終わったので
エンジニアリングも行いましょう やはりここでも
TensorFlow関数を使用します 経度の取り込みやドロップ
それらの減算 緯度の取り込みやドロップ
それらの減算 さらに経度や緯度の差分の取得など コンピューティングや
スケーリングが可能です つまり 差分やスケーリングを
気にかける必要はないのです それこそがTF Transformの役割であり 適切にスケーリングできるよう
最大値や最小値をみつけてくれます ではもう一度
スケーリングされた値をとって その値からユークリッド距離を
計算しましょう 再びスケーリングする必要はありません 距離の値が0から1の間にあることが
わかっているからです 平方根も0から1の間です すべて平方根内なので問題ありません 実際には少しだけ大きいですね 両方が1である場合1.4になりますが
十分近い値です 小さい数ですので
この時点で調整する必要はありません 前処理の関数はすべて完了しています ですがis_validメソッドと preprocess_tftメソッドを
コールする必要があります これらのメソッドはBeam Transformから
コールする必要があります どのように行うのでしょうか まず最初に行うのは 読み込みを行う生データの
メタデータを設定することです ここでいう生データとは
BigQueryからのデータのことです dayofweekやkeyは文字列です fare_amount、pickuplon、pickuplat
はすべて浮動小数です 新しい生データの処理スキームも
作成します これは基本的にはディクショナリで カラムの名前から文字列、浮動小数、
整数の判断までカバーします hourofdayやpassengersは整数型です これらは生データに存在し
BigQueryからのデータとなります では生データをセルで処理します rawdata_metadataを書き込みます この書き込みを行うのは ユーザーからのJSON入力も
rawdata_metadataにするためです このような形式にすることで 入力用の関数に認識させます 次に 先ほど作成したクエリを使用して BigQueryのデータを読み込みます そしてis_validメソッドを使用して
フィルタリングします is_validメソッドがどのように
入力されるかがわかると思います このメソッドはbeamフィルタの
一部としてコールされます beamフィルタはis_valid関数で指定した
ルールに基づいて実行されます データセットをコールし
分析と変換を行います この場合 変換用の関数を
指定する必要があります 変換用関数はpreprocess_tftです この関数はスケーリングなどの
処理をすべて行います この時点で 変換用データセットと
変換用関数が得られました 次に 変換データを取得し
TFレコードとして書き込みます TFレコードとして書き込むのですが 容量節約のためにGzipで圧縮します 同じことをテストデータにも行います トレーニングデータでは
クエリが1で作成されています テストデータではクエリは
2で作成されています つまり 渡される値が1か2かによって
クエリの設定方法が異なります このphaseの部分です ハッシュバケットの最初か最後の部分の
どちらかを取得しています このようにしてトレーニングデータセットか
評価用データセットのどちらかを取得します 下にスクロールしましょう 完了しました 変換用テストデータを書き込みます 評価用データにも書き込みます そしてこれがとても重要なのですが 変換用のメタデータも
書き込む必要があります これが コールした全TFメソッドが グラフに格納される方法です ここで実際に行われているのは
モデルの書き込みです モデルはトレーニングするものではありません モデルはTensorFlowの処理で構成され 標準のモデルグラフの前に位置します ユーザーからの入力が TensorFlow関数を通じて
標準モデルに渡されるということです これで前処理用データセットを 作成する準備ができました Trueに設定すれば
小規模なデータセットが作成されます しかし今回はFalseに設定して
Dataflowで実行させて データセットを作成します この時点でエラーが発生する場合は Dataflow APIが有効に
なっていないということです Quick Labsプロジェクトで
Dataflow APIを有効にしてください 有効にすれば
Dataflowジョブが起動します これが完了するとpreproc_tftで
ファイルを確認できるはずです そして トレーニングは既存のものと
ほぼ同じものになっているでしょう 実際に違いを確認してみましょう taxifare_tftで
TF Transformを確認します model.pyを確認してみましょう model.py内の違いは何でしょうか input columnsの取得方法は以前と同じです BucketizeやFeature cross
の部分は同じです wide columnsも
deep columnsも作成しています すべて前処理のときと同じです 以前はDataFlowを使用して行っており 追加の関数を作成して
3ヶ所のすべてでコールを行っていました ただ今回は そのようなことをする必要はなく
作成した追加の関数も必要ありません 作成した追加の関数で
以前行っていたことは TF Transformが
グラフの一部として行います したがって 処理関数が与えられた場合 この変換用関数から 実行されたすべての操作が
読み込まれます 入力された生データを取得します ここが生データの部分です TensorFlowのtransform関数で
行われた処理がすべて適用されます これまでのすべての処理です 基本的にはpreprocess_tftで
コールしたコードすべてです すべてをfeaturesに適用します feature_placeholdersにも適用します featuresとfeature_placeholdersの
2つが返り値になります feature_placeholdersは
エンドユーザーが送信したものです 元はJSONのデータでした featuresはJSONデータを
取得した結果であり TF Transformの変換関数を適用します 変換関数のすべての処理は feature_placeholdersに対して行われ
これが返り値になります この時点で 入力を処理する
関数が用意されました では データセットを読み込む場合
何をする必要があるでしょうか データセットを読み込む場合 それらを変換する必要があります しかし そのためのコードを
記述する必要はありません TF Transformには入力関数の
作成機能があるからです トレーニング用の入力関数を作成し transformed_metadataで
すべてを適用できます 次にGzipで読み込みます トレーニング用の入力関数が
組み込まれており TensorFlowレコードの読み込み方法も
設定されています 通常ではコード全体を記述しますが
その必要はなく データセットの読み込みも
CSVのデコード処理も不要で すべて作業しなくてよくなります トレーニング用入力関数を使用するだけで
それらの作業が完了します 学習と評価については
これまでとまったく同じです train_specとeval_specを作成し estimatorに渡します 1つだけ違うのはGzipを
読み込ませることです Gzipの読み込み用関数に渡します そしてこの読み込み用関数が
TFレコードの読み込み関数になっています これで基本的に完了です