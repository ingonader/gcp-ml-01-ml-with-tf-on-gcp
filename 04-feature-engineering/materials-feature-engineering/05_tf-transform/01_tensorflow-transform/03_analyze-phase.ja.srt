1
00:00:00,000 --> 00:00:02,920
分析フェーズについてみていきましょう

2
00:00:02,920 --> 00:00:07,075
トレーニング用データセットの
分析を思い出してください

3
00:00:07,075 --> 00:00:11,260
まず想定されるデータの種類を
Beamに伝える必要があります

4
00:00:11,260 --> 00:00:14,275
これはスキーマの設定で行います

5
00:00:14,275 --> 00:00:20,365
1行目にraw_data_schemaという
ディクショナリを設定します

6
00:00:20,365 --> 00:00:25,005
すべての文字列カラムに対して
エントリを追加します

7
00:00:25,005 --> 00:00:29,315
この文字列はTensorFlowの
データタイプです

8
00:00:29,315 --> 00:00:33,075
そしてtf.float32タイプの全カラムを追加し

9
00:00:33,075 --> 00:00:38,135
raw_data_schemaを更新します

10
00:00:38,135 --> 00:00:42,605
これで raw_data_schemaのデータセットに
すべてのカラムが取り込まれ

11
00:00:42,605 --> 00:00:48,115
DataFlowでBeamによって
処理されるようになります

12
00:00:48,115 --> 00:00:53,130
raw_data_schemaはメタデータ
テンプレートの作成に使われます

13
00:00:53,130 --> 00:00:58,760
次にトレーニング用データセットに対して
analyzeandtransform PTransformを実行し

14
00:00:58,760 --> 00:01:05,705
前処理用学習データと変換関数を取得します

15
00:01:05,705 --> 00:01:11,185
まず beam.io.readで
トレーニング用データを読み込みます

16
00:01:11,185 --> 00:01:17,035
これはBeamの以前のモジュールで見た
すべてのBeamパイプラインと同じです

17
00:01:17,035 --> 00:01:19,765
ここでBigQueryから読み込みます

18
00:01:19,765 --> 00:01:24,375
次にトレーニングで使用したくない
データをフィルタリングします

19
00:01:24,375 --> 00:01:27,532
これはis_valid関数で行います

20
00:01:27,535 --> 00:01:31,385
この関数について今は触れず
後でこのメソッドを説明します

21
00:01:31,395 --> 00:01:37,250
読み込みとフィルタリングを行った生データと

22
00:01:37,250 --> 00:01:40,805
前のスライドで取得した生データのメタデータを

23
00:01:40,805 --> 00:01:45,245
AnalyzeAndTransformDataset PTransform
に渡します

24
00:01:45,245 --> 00:01:49,985
Beamではこの変換が分散的な手法で行われ

25
00:01:49,985 --> 00:01:55,550
命じられたすべての分析が
preprocessメソッドで行われます

26
00:01:55,550 --> 00:01:58,455
このpreprocessメソッド
についても後ほど説明します

27
00:01:58,455 --> 00:02:03,260
これでis_validメソッドと
preprocessメソッドが

28
00:02:03,260 --> 00:02:06,625
Beamでトレーニング用
データセットに対して実行され

29
00:02:06,625 --> 00:02:09,775
フィルタリングと前処理が行われます

30
00:02:09,775 --> 00:02:14,335
前処理されたデータは
PCollectionで戻されます

31
00:02:14,335 --> 00:02:18,805
このparallel Collectionでは
変換されたデータセットをコールします

32
00:02:18,805 --> 00:02:24,020
ただここで注意すべきなのは
前処理で実行した変換結果は

33
00:02:24,020 --> 00:02:27,170
2番目の戻り値に格納されるということです

34
00:02:27,170 --> 00:02:30,770
つまり変換関数です
この点が重要になります

35
00:02:30,770 --> 00:02:34,510
変換データを書き出します

36
00:02:34,510 --> 00:02:38,555
ここではTFレコードとして書き出します

37
00:02:38,555 --> 00:02:41,645
これはTensorFlowで最も有効な
フォーマットです

38
00:02:41,645 --> 00:02:45,425
書き出しはTensorFlow transformの

39
00:02:45,425 --> 00:02:49,545
writetoTFrecord PTransformで行います

40
00:02:49,545 --> 00:02:53,105
ファイルは自動的に終了されます

41
00:02:53,105 --> 00:02:56,640
ただ このときに使用されるスキーマは

42
00:02:56,640 --> 00:03:01,390
生データ用のスキーマではなく
変換されたスキーマです

43
00:03:01,390 --> 00:03:02,681
なぜでしょうか

44
00:03:04,000 --> 00:03:08,165
もちろんそれは書き出したデータが
変換されたデータだからです

45
00:03:08,165 --> 00:03:12,680
生データではなく
前処理済みのデータだからです