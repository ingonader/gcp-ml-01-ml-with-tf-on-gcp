So, in this lab, we tried out TF transform. The reason that we're using TF transform is that it allows us to carry out preprocessing using Apache Beam, but do the preprocessing in TensorFlow. The idea is that we can create preprocess datasets at scale during training and during evaluation. Then we can turn around and apply that preprocessing on data that comes in for predictions, and we can carry that out as part of a TensorFlow graph itself. So, let's look at how that's done. The first thing is that TensorFlow transformer is not part of core TensorFlow, it is an open-source library, but it's a separate library. So, in order to do that, I will go ahead and first install a specific version of transfer transform. So, we have to realize which version of TensorFlow we're using and the corresponding version of TensorFlow transform. Now, when I recorded this video, I was using TensorFlow 1.5 and the corresponding version of TensorFlow transform for TensorFlow 1.5, was transfer transform 0.5. When you are doing it, it might be different. The notebook, we will keep it up to date so that you will have the right version corresponding to the right version of TensorFlow that's installed in the notebooks. So, in this case, I'm basically going to install TensorFlow transform 0.5 and I'm going to install the package Apache Beam-GCP. Again, just to make sure that we get everything correct. That already comes with Dataflow, we'll uninstall it because Apache Beam-GCP and Google Cloud Dataflow, they're essentially the same thing. But in this case, we will just go with all of the open-source things. So, I'll go ahead and do a pip uninstall and a pip install. This is probably going to take a few minutes and once that's done, we want to make sure that the notebook picks up the new pip packages. The way you do this, is to go ahead and click Reset. So, we have to wait for this filled circle to become open again. That means that this particular cell has finished executing and the appropriate pip installs are done. So, let's go ahead and wait. All right. We're back. There it is, this circle that was filled black is now open. That means that this cell has completed. Actually, when you go ahead and look at the cell, you see that it has basically gone ahead and done a bunch of things. Towards the end of it, you should see that it has uninstalled a bunch of things and installed a bunch of things. We do get the tents for transform 0.5. So, but let's make sure. So, what we could do here is, first of all, we want to make sure that it gets picked up. So, in order to do that, we will have to reset. So, I'm clicking the Reset, restarting the session, and at this point, the new pip packages will be picked up. We can go down, and we have a cell that basically does a pip freeze that tells you what's present on the Docker container that's running the notebook and I'm grepping. I'm finding any package that has the word flow or the word Beam in it. So, the vertical bar here is an R. So, let me do that and we should basically see that both TensorFlow transform and Apache Beam are installed, TensorFlow itself is installed. In this case, we seemed to have like Tensor Board and Apache airflow, even though we don't need either of those. But they're there. So, we have that. Now, we are ready to basically import TensorFlow, importance over transformers TFT, and then make sure that you change your bucket in your project to reflect your Quick Labs project. I've already done this. So, I will go ahead and run that cell and makes sure that it can be picked up by Bash. So, that's what a western environment does. Make sure that my project and my compute region reflect this project and this region. The next thing that we want to do, is to basically get our data from BigQuery. But unlike the previous example, we're not doing any more filtering on latitude, longitude, et cetera, we will do that filtering in Apache Beam. That way, we'll make sure if somebody gives us, during predictions, a bad input, we don't get host. Okay? So, we'll basically just go ahead and pick up a few things. We'll do some preprocessing here to get the fare amount, et cetera. But the query itself is much simpler than work as before. Because we'll carry out quite a bit of that preprocessing in Apache Beam. We'll go ahead and this time, I will create a DataFrame valid. Just to show you what happens. I'm running the query, I'm executing it, creating a Pandas DataFrame, and once I have the Pandas DataFrame, I'm calling head which shows me the first few lines. Then I'm calling describe, which will give me the mean and other statistics, mean, standard deviation, and the quantiles of this particular DataFrame. All right. Now we're back. So, we have our DataFrame valid and we see that it basically has, no 11,181 columns of fare amount, hour of day, et cetera. So, all of those and then it basically, we now see that the query is correct. So, let's now use this query to create a machine learning dataset, this time using TF transform and using Dataflow. Unlike every Dataflow job that we've done so far, we now need an extra package to be installed on the machines that run Dataflow. The way we do this, is to basically write a requirements.text. So, remember that when we did a pip install, we basically said, pip install, TensorFlow transform 0.5.0. Well, that's exactly what we do here. We go to, we write a requirements.text. In the requirements.text, we say that we want to install TensorFlow transform 0.5.0. So, let's go ahead and write that. Having written the requirements.text, we can now run our dataflow job passing in this requirements.text as a requirements file. So, this tells Dataflow that it needs to go through requirements.text and pip install all of the Python packages that we need. What is that we're doing in this job? In this job as with the previous jobs, we are going to be basically reading from BigQuery, we are going to be creating records. But unlike the previous case, we created CSV records, in this case, we're going to create TensorFlow examples because they are more efficient. So, how does this work? We also need to create the training dataset in the evaluation dataset. So, let's walk through this piece by piece. The first thing is to decide what kind of preprocessing we want to do. Okay. So, if you want to do two types of preprocessing, one type of preprocessing, is that we want to check if the input row that we get is valid or not so that it is valid. Given a dictionary of inputs, what we get from BigQuery is going to be a dictionary and conveniently, what we get during prediction from JSON is also going to be a dictionary. So, the same code is going to work both on the BigQuery dataset and on the JSON that comes in. So, what are we going to do? We're going to get the inputs, the pickuplon, the dropofflon, the pickuplat, the dropofflat, the hour of the day, the day of the week, all of these things, we're going to try to get them. If any of them we're not able to get, we'll basically say it's not valid, right? So, we're doing a try, except. So, we're going to basically do all of these things. If any of them throws an exception, we basically say this isn't valid. Having gotten them, we then basically say it is valid if all of these conditions are met. If the fare amount is greater than 2.5, and the pickup longitude is greater than negative 78, et cetera. So, all of these tests, if they all pass, then the inputs are valid. Now, for the preprocessing. Now we're going to take our data and we're going to basically do things to make the neural network training better. So what are we going to do? We're going to basically take the inputs fair amount and pass it through unchanged. If I could just say inputs fair amount or I could basically call some other function, like in this case, I'm calling TF identity, just pass it through. The day of the week is an integer. What BigQuery gives us an integer like 1,2,3,4. In the previous lab, for the feature engineering lab, and we did this, what did we do? We essentially hard-coded in the vocabulary. In this case, we're basically going to tell TensorFlow Transform, go learn the vocabulary from the training dataset. So now, we're not going to know necessarily know what this number means, but we know that whatever comes in during prediction, will automatically get converted. So we're going to take the day of the week and we're going to convert that string that we get into an integer based on the vocabulary. That's what the string to int does. The hour of day is already an integer, so we just pass it through unchanged. The pickup lawn is a floating point number. So we could also use it unchanged, but we know that neural network training works a lot better, gradient descent works a lot better if our input values are small numbers, if they're in the range for example, zero to one. So that's what we're asking TensorFlow Transform to do. TensorFlow Transform scale this value from zero to one. But remember, in order to scale it from zero to one, TensorFlow Transform needs to know what the minimum is and what the maximum is. It will learn it from the dataset. That's why we have the two phases. We have the analyze phase and then we have the transform phase. So even though we're just writing that transform here scale zero to one, scale zero to one knows that in order to do this in the analyze phase, it has to find the min and the max. So we do the same thing for all of these things, and then we cast inputs that passengers to be a float, and then we basically take the inputs.passengers and we basically do at once like this. So we basically get an equal number of ones and then cast it to be a string. So in this case, all of our keys are essentially the stringed one. But this is just to give you an example of the fact that you can call arbitrary TensorFlow functions. Key thing is pre-processing is all TensorFlow functions. So having done that we also do engineering. Right? Again, TensorFlow functions, so in this case, I'm taking the pickup lat, drop-off lat, subtracting them, pick-up lawn, drop-off lawn, subtracting them, and then taking the lat def and the lawn def that is computed and also scaling it. So again, we don't need to worry about what the difference, what that scale is. That's TensorFlow Transform's job to figure out what the min and max are to scale it appropriately. Again, we then go ahead and take these scaled values and then compute the Euclidean distance off the scaled values. We don't need to scale this again because we know that if the distances are between zero and one, then this square root will also be between zero and one. So it's okay. It's all within that square. Actually, it could be a slightly more. It will be 1.4 if both of them are one. But close enough. They're small numbers, so we don't need to scale it, and at this point, we basically have all of the pre-processing done, the pre-processing function. So- but we still need to call the is valid method and the pre-process TFT method. We have to call both of these methods from within the beam transform. So how did we do that? The way we do this is we first set up the metadata for the raw data that we are going to be reading it. What's a raw data? This is the data that comes from BigQuery. So we basically say that the day of the week and the key, they're both strings, and the fair amount, the pickup lawn, pickup lat, all of these things are floats, and we basically create a raw data schema which is essentially a dictionary that goes from the name of the column to whether it's a string or it's a float or if it's an integer. Hour of day and passengers, they're both integers. This is in the raw data. This is what comes out of BigQuery. So we take the raw data and we basically sell. Let's go ahead and write the raw data metadata. We write that out so that the JSON input that comes in from the user will also be of this raw data metadata. So we basically is going to be of this form and we want to let our serving input function notice. Then, we basically say, "Go ahead and read the data from BigQuery using the query that we've just created and filter it using the method is valid." So you see how it is valid method comes in. It's being called as part of a beam filter. The beam filter is carried out with the rules that we specified in the is valid function. Then, we basically call, analyze, and transform dataset. When we do that, we have to specify the transformation function. The transformation function is pre-process underscore TFT. This is a one that does all the scaling, etc. So at this point, we basically get back the transform dataset and the transform function, and what we do is that we take the transform data and we write it out as TF records. We write it out as TF records that are Gzipped, compressed to save space. Then we do the same thing for the test data. So in the training data, I created the query at one, and in the test data, I created query at two, and the way I set up my query was that depending on whether one or two was passed in, that was a phase. I either picked up the first few of the hash buckets or the last of the hash buckets. So that's how I'm getting my training dataset or evaluation dataset. Let's scroll down. So having done that, I now write out my transform the test dataset and also write them out to the evaluation stuff and finally, and this is very important, we have to write out the metadata of the transformations. This is how all of the TF methods that we called, they get stored in the graph. So what this does is that it actually writes out a model. A model actually isn't something that you train, but it's a model that consists of TensorFlow operations that are going to get put in front of your normal model graph so that any inputs that come in from the user go through the TensorFlow Transform of TensorFlow functions into your normal model. So with this, we are now ready and we can basically go ahead and create a pre-process dataset. If I set this to be true, I would create a small dataset but I'm setting it to be false. So this is actually going to run in dataflow and it is going to basically go ahead and create it. Okay. So at this point and if again you get an error that says that the Dataflow API is not enabled, go to the quick clubs project and enable the dataflow API and having done that, this dataflow job should get launched and once this is done, you should be able to see files in the pre-process TFT. Once that's done, then the training is very similar to what existed earlier. But let's go and look at it. Let's look at what's actually different. So when we look at TensorFlow Transform under taxifare_tft, let's go look at model.pi, and in the model.pi, what is different? So we basically get our input columns the same way as before. We're bucketizing, we're feature crossing, we are creating a white columns, we're creating a deep columns. All this is identical to the way we did our preprocessing. Earlier, when we did it with DataFlow and we actually had an extra ad engineered function that we remember to call for all three places. In this case though, we don't need to do that, we don't have this ad engineered function. What that add engineered function was doing, TensorFlow Transform is now doing inherently as part of the graph. So what we're basically saying is, when somebody gives me a serving function, I'm basically going to go ahead and read out of this transform function all of these operations that have been performed, take the raw data that comes in, so these are the raw data, and then basically apply everything that happens in TensorFlow Transform function, all of these things that we did. Essentially all of the code that we called in pre-process_tft, we're basically saying apply all of those to my features, to my feature placeholder. So apply them to the feature placeholders, get back the features, and that is now the pair of things that we return. The feature placeholders is what the end-user gives us, the stuff that was in the JSON. The features is a result of taking what was in the JSON and applying all of that TensorFlow Transform transformation function, the transform function. All of those operations do the feature placeholders and that's essentially what gets returned. At this point, we have the serving input function. Now when we are reading the dataset, what do we have to do? When we're reading the dataset, we have to apply these transformations. Luckily though, we don't have to write that code ourselves because TensorFlow Transform comes with an input function maker that you can basically go and say, "Please build me a training input function that applies all of the stuff in the transform metadata.", and now go ahead and read it with Gzip and that's pretty much it. It comes with the built training input function that knows how to read TensorFlow records. So we don't have to write the whole normal code that we would write, where would read a dataset, and we'd apply a decode CSV. All of that stuff completely goes away. We essentially just use the build training input function to essentially do the job for us. The train and evaluate is exactly the same as before. We create a train spec, we create an eval spec, and we pass in the estimator, the train spec and the eval spec. The one difference was that because were you reading Gzip, we passed in a Gzip reader function, and the Gzip reader function is essentially a TF record reader that basically reads Gzip, and that's essentially it.