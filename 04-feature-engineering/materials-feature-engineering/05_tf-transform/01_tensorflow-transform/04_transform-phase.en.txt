The preprocessing function is a function where we transform the input data. In beam, it's called as part of the analyze and transform dataset. In TensorFlow, the things you do in preprocess will get called essentially as part of the serving input function in TensorFlow. In other words, it will get added to the TensorFlow graph, and it could be executed in TensorFlow during serving. Because it'll be executed as part of the TensorFlow graph, the preprocessing function is restricted to functions that you can call from TensorFlow. You cannot call regular Python functions since the preprocess is part of the TensorFlow graph during serving. Let's look at an example. In this example, I'm taking a set of inputs and preprocessing them. What is the data type of inputs? It's a dictionary whose values are tensors. Remember, this is what is returned from the serving input function and represents the raw data as it's read. Input functions return features, labels and thus, is a features. And features is a dict, a dictionary. TF transform will take care of converting the data that comes in via P transform into tensors during the analysis phase. We take the tensors and we use them to create new features and we put these features into the dictionary. The first result, fare amount in my example, is passed through unchanged. We take the input tensor and add it to the result, no changes. The next result we want is a day of the week. We want this to be an integer. However, in the input, it is the string like Thu for Thursday. So what we are doing is that we're asking TensorFlow transform to convert a string that is red, such as Thu into an integer such as three, or five, or whatever that number is. What TF transform will do is to compute the vocabulary of all the possible days of the week in the training data set. It will do this during the analyze phase and use that information to do the string to int mapping in the prediction phase. Next, we want to scale off the drop flat into a number that lies between zero and one. In the analysis phase, TF transform will compute the Min and the Max of the column, and use those values to scale the inputs. We can also invoke other TensorFlow functions. In this case, I'm taking the input number of passengers, which happens to be an integer in JSON and casting it to be a real value number. Once all the features have been created and added, we can return the result. The analyze and transform P transform happens on the training dataset. What should happen on the evaluation dataset? For the evaluation dataset, we carry a pretty much the same beam pipeline that we did on the training dataset. There's one big exception though, we don't analyze the evaluation dataset. If we are scaling the values, the values in the evaluation dataset will be scaled based on the Min and Max found in the training dataset. So on the evaluation dataset, we don't call analyze and transform, we just call transform data set. This will take care of calling all the things that we did in preprocess, pretty cool? Notice however, that the transform dataset needs as input that transform function that was computed on the training data. That's what makes the magic possible. Once we have the transform dataset, we can write it out just like we wrote out the training dataset.