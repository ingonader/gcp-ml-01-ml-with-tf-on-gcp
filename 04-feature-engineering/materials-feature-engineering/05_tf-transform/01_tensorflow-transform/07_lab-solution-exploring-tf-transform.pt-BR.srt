1
00:00:00,000 --> 00:00:01,625
Neste laboratório,

2
00:00:01,625 --> 00:00:04,215
testamos o tf.Transform.

3
00:00:04,215 --> 00:00:08,260
O motivo para usarmos essa
transformação é que ela permite

4
00:00:08,260 --> 00:00:12,210
configurar o pré-processamento
no Apache Beam

5
00:00:12,210 --> 00:00:14,850
e fazê-lo no TensorFlow.

6
00:00:14,850 --> 00:00:16,650
A ideia é que podemos criar

7
00:00:16,650 --> 00:00:19,335
conjuntos de dados de
pré-processamento em escala

8
00:00:19,335 --> 00:00:22,020
durante o treinamento e a avaliação.

9
00:00:22,020 --> 00:00:25,490
Depois, podemos aplicar
esse pré-processamento

10
00:00:25,490 --> 00:00:29,405
em dados coletados nas previsões

11
00:00:29,405 --> 00:00:32,950
e como parte do próprio
gráfico do TensorFlow.

12
00:00:32,950 --> 00:00:34,770
Vamos ver como isso é feito.

13
00:00:34,770 --> 00:00:39,640
Primeiro, o tf.Transform não
faz parte do TensorFlow principal.

14
00:00:39,640 --> 00:00:42,480
É uma biblioteca de
código aberto separada.

15
00:00:42,490 --> 00:00:44,095
Para isso,

16
00:00:44,095 --> 00:00:48,900
preciso instalar uma versão
específica do tf.Transform.

17
00:00:48,900 --> 00:00:52,020
Precisamos ver qual
versão do TensorFlow

18
00:00:52,020 --> 00:00:56,035
nós usamos e a versão
correspondente do tf.Transform.

19
00:00:56,035 --> 00:00:57,980
Quando gravei este vídeo,

20
00:00:57,980 --> 00:01:00,230
eu estava usando o TensorFlow 1.5,

21
00:01:00,230 --> 00:01:05,345
e o tf.Transform
correspondente a essa versão

22
00:01:05,345 --> 00:01:07,835
era a versão 0.5.

23
00:01:07,835 --> 00:01:10,355
Isso pode ser diferente para você.

24
00:01:10,355 --> 00:01:14,270
Manteremos o bloco de notas atualizado
para que você tenha a versão certa

25
00:01:14,270 --> 00:01:19,070
que corresponde à versão
instalada nos blocos de notas.

26
00:01:19,070 --> 00:01:22,580
Neste caso, preciso instalar
a versão 0.5 do tf.Transform

27
00:01:22,580 --> 00:01:27,345
e o pacote Apache Beam-GCP.

28
00:01:27,345 --> 00:01:29,915
Só para garantir que está tudo certo.

29
00:01:29,915 --> 00:01:31,720
Ele já vem com o Dataflow,

30
00:01:31,720 --> 00:01:35,840
vamos desinstalar porque o Apache
Beam-GCP e o Google Cloud Dataflow

31
00:01:35,840 --> 00:01:37,300
são essencialmente iguais.

32
00:01:37,300 --> 00:01:38,775
Mas, neste caso,

33
00:01:38,775 --> 00:01:41,110
usaremos só ferramentas de código aberto.

34
00:01:41,110 --> 00:01:46,310
Vou usar os comandos
pip uninstall e pip install.

35
00:01:46,310 --> 00:01:50,720
Isso levará alguns minutos.
Quando terminar,

36
00:01:50,720 --> 00:01:55,270
precisamos garantir que o bloco de
notas receba os novos pacotes pip.

37
00:01:55,270 --> 00:01:56,720
Para fazer isso,

38
00:01:56,720 --> 00:01:59,140
vamos clicar em "Reset".

39
00:01:59,140 --> 00:02:03,810
Precisamos esperar este
círculo ficar vazio novamente.

40
00:02:03,810 --> 00:02:06,635
Isso significa que
a execução na célula terminou

41
00:02:06,635 --> 00:02:10,250
e as instalações foram feitas.

42
00:02:10,250 --> 00:02:12,410
Vamos esperar.

43
00:02:15,950 --> 00:02:18,970
Certo, voltamos. Aqui está,

44
00:02:18,970 --> 00:02:22,850
este círculo estava todo
preto e agora está vazio.

45
00:02:22,850 --> 00:02:25,370
Isso significa que a célula foi concluída.

46
00:02:25,370 --> 00:02:27,445
Quando você olha a célula,

47
00:02:27,445 --> 00:02:31,700
pode ver que muitas coisas aconteceram.

48
00:02:33,580 --> 00:02:35,155
No final,

49
00:02:35,155 --> 00:02:39,875
você verá que muitas coisas
foram desinstaladas e instaladas.

50
00:02:39,875 --> 00:02:43,760
Nós conseguimos instalar a versão 0.5.

51
00:02:43,760 --> 00:02:45,370
Vamos ter certeza.

52
00:02:45,370 --> 00:02:47,620
O que podemos fazer é,

53
00:02:47,620 --> 00:02:50,510
primeiro, garantir que
os pacotes serão recebidos.

54
00:02:50,510 --> 00:02:51,925
Para fazer isso,

55
00:02:51,925 --> 00:02:53,235
precisamos redefinir.

56
00:02:53,235 --> 00:02:55,010
Então, eu clico em "Reset",

57
00:02:55,010 --> 00:02:57,945
reinicio a sessão e, nesse momento,

58
00:02:57,945 --> 00:03:00,765
os novos pacotes pip serão recebidos.

59
00:03:00,765 --> 00:03:03,865
Podemos descer

60
00:03:03,865 --> 00:03:09,710
e ver uma célula com pip freeze,

61
00:03:09,710 --> 00:03:11,820
que diz o que está presente

62
00:03:11,820 --> 00:03:14,780
no contêiner do Docker
que executa o bloco de notas.

63
00:03:14,780 --> 00:03:21,705
Estou procurando qualquer pacote
com as palavras Flow ou Beam.

64
00:03:21,705 --> 00:03:24,890
A barra vertical aqui é R.

65
00:03:24,890 --> 00:03:29,300
Quando clicamos, vemos que

66
00:03:29,300 --> 00:03:34,485
o tf.Transform e
o Apache Beam estão instalados,

67
00:03:34,485 --> 00:03:36,295
assim como o próprio TensorFlow.

68
00:03:36,295 --> 00:03:39,800
Nesse caso, parece que temos
o TensorBoard e o Apache AirFlow.

69
00:03:39,800 --> 00:03:41,340
Não precisamos de nenhum deles,

70
00:03:41,340 --> 00:03:43,690
mas eles estão lá. Depois dessas etapas,

71
00:03:43,690 --> 00:03:46,815
estamos prontos para importar o TensorFlow

72
00:03:46,815 --> 00:03:49,310
com o comando
import tensorflow_transform as tft.

73
00:03:49,310 --> 00:03:52,010
Lembre-se de alterar

74
00:03:52,010 --> 00:03:55,160
o intervalo do projeto
para refletir o projeto do Qwiklabs.

75
00:03:55,160 --> 00:03:56,620
Eu já fiz isso.

76
00:03:56,620 --> 00:04:02,355
Vou executar a célula e garantir
que ela seja detectada pelo Bash.

77
00:04:02,355 --> 00:04:04,745
É isso que os.environ faz.

78
00:04:04,745 --> 00:04:10,950
E preciso garantir que meu projeto
e minha região correspondam a estes.

79
00:04:10,950 --> 00:04:12,585
Depois, o que precisamos fazer

80
00:04:12,585 --> 00:04:14,760
é coletar os dados do BigQuery.

81
00:04:14,760 --> 00:04:17,875
Ao contrário do exemplo anterior,

82
00:04:17,875 --> 00:04:20,790
não filtraremos mais a latitude,

83
00:04:20,790 --> 00:04:24,320
a longitude etc.
Filtraremos no Apache Beam.

84
00:04:24,320 --> 00:04:26,615
Assim, não há problema se alguém fornecer

85
00:04:26,615 --> 00:04:30,615
uma entrada inválida durante as previsões.

86
00:04:30,615 --> 00:04:34,450
Vamos coletar algumas coisas.

87
00:04:34,450 --> 00:04:37,805
Faremos parte do pré-processamento
para ver o fare_amount etc.

88
00:04:37,805 --> 00:04:41,860
Mas a consulta
está muito mais fácil que antes

89
00:04:41,860 --> 00:04:46,480
porque grande parte do
pré-processamento será no Apache Beam.

90
00:04:46,480 --> 00:04:48,945
Dessa vez,

91
00:04:48,945 --> 00:04:52,210
eu criarei um DataFrame válido

92
00:04:52,210 --> 00:04:54,590
para mostrar o que acontece.

93
00:04:54,590 --> 00:04:56,035
Eu executo a consulta

94
00:04:56,035 --> 00:04:59,065
para criar um DataFrame do Pandas

95
00:04:59,065 --> 00:05:01,300
e, quando tenho o DataFrame,

96
00:05:01,300 --> 00:05:04,970
uso o comando head
para ver as primeiras linhas.

97
00:05:04,970 --> 00:05:07,105
Depois, o comando describe,

98
00:05:07,105 --> 00:05:11,119
que mostra a média e outras estatísticas.

99
00:05:11,119 --> 00:05:17,620
Média, desvio padrão
e os quantis do DataFrame.

100
00:05:20,300 --> 00:05:22,385
Certo, voltamos.

101
00:05:22,385 --> 00:05:28,195
Temos nosso DataFrame
válido e vemos que ele tem

102
00:05:28,195 --> 00:05:33,110
11.181 colunas de fare_amount,

103
00:05:33,110 --> 00:05:34,625
hourofday etc.

104
00:05:34,625 --> 00:05:37,350
E, basicamente,

105
00:05:37,350 --> 00:05:39,705
podemos ver que a consulta está certa.

106
00:05:39,705 --> 00:05:43,430
Vamos usá-la para criar um conjunto
de dados de aprendizado de máquina,

107
00:05:43,430 --> 00:05:46,160
dessa vez com tf.Transform e o Dataflow.

108
00:05:46,160 --> 00:05:49,429
Ao contrário dos jobs
que executamos até agora,

109
00:05:49,429 --> 00:05:54,670
precisamos que um pacote extra seja
instalado nas máquinas com Dataflow.

110
00:05:54,670 --> 00:05:55,900
Para isso,

111
00:05:55,900 --> 00:05:58,975
escrevemos um requirements.txt.

112
00:05:58,975 --> 00:06:02,890
Quando usamos
o comando pip install, escrevemos

113
00:06:02,890 --> 00:06:07,660
pip install TensorFlow
transform 0.5.0.

114
00:06:07,660 --> 00:06:09,565
É isso que faremos aqui.

115
00:06:09,565 --> 00:06:13,465
Escrevemos um arquivo requirements.txt.

116
00:06:13,465 --> 00:06:20,540
No arquivo, dizemos que queremos
instalar o tf.Transform 0.5.0.

117
00:06:20,540 --> 00:06:22,265
Vamos escrever o arquivo.

118
00:06:22,265 --> 00:06:24,730
Depois de escrever,

119
00:06:24,730 --> 00:06:33,055
podemos executar o job do Dataflow
com esse arquivo de requisitos.

120
00:06:33,055 --> 00:06:35,990
Ele diz ao Dataflow que é preciso acessar

121
00:06:35,990 --> 00:06:41,625
o requirements.txt com pip install
de todos os pacotes do Python necessários.

122
00:06:42,535 --> 00:06:44,760
O que estamos fazendo nesse job?

123
00:06:44,760 --> 00:06:47,670
Assim como nos jobs anteriores,

124
00:06:47,670 --> 00:06:50,475
vamos ler o BigQuery

125
00:06:50,475 --> 00:06:54,245
e criar registros.

126
00:06:54,245 --> 00:06:56,140
Mas, ao contrário do caso anterior

127
00:06:56,140 --> 00:06:58,100
em que criamos registros CSV, agora

128
00:06:58,100 --> 00:07:00,740
criaremos exemplos do TensorFlow,

129
00:07:00,740 --> 00:07:03,315
porque eles são mais eficientes.
Como isso funciona?

130
00:07:03,315 --> 00:07:07,165
Também precisamos criar o conjunto
de dados de treinamento e avaliação.

131
00:07:07,165 --> 00:07:10,300
Vamos ver esse processo passo a passo.

132
00:07:10,300 --> 00:07:15,355
Primeiro, decidimos
o tipo de pré-processamento.

133
00:07:15,355 --> 00:07:18,290
Se você quiser usar dois
tipos de pré-processamento,

134
00:07:18,290 --> 00:07:20,065
o primeiro tipo

135
00:07:20,065 --> 00:07:22,520
será verificar

136
00:07:22,520 --> 00:07:27,135
se a linha de entrada é
válida ou não com is_valid.

137
00:07:27,135 --> 00:07:29,240
Em um dicionário de entradas,

138
00:07:29,240 --> 00:07:34,100
recebemos um dicionário
do BigQuery e, convenientemente,

139
00:07:34,100 --> 00:07:39,585
o JSON também fornece
um dicionário durante a previsão.

140
00:07:39,585 --> 00:07:41,420
O mesmo código funcionará

141
00:07:41,420 --> 00:07:47,115
no conjunto de dados do BigQuery
e no JSON recebido. O que faremos?

142
00:07:47,115 --> 00:07:49,520
Vamos coletar as entradas.

143
00:07:49,520 --> 00:07:52,440
Pickuplon, dropofflon, pickuplat,

144
00:07:52,440 --> 00:07:54,790
dropofflat, hourofday,

145
00:07:54,790 --> 00:07:56,385
dayofweek, tudo isso.

146
00:07:56,385 --> 00:07:57,700
Precisamos coletar tudo.

147
00:07:57,700 --> 00:08:00,385
Se não for possível coletar algum valor,

148
00:08:00,385 --> 00:08:02,370
significa que ele não é válido, certo?

149
00:08:02,370 --> 00:08:04,240
Então, usamos try/except.

150
00:08:04,240 --> 00:08:06,330
Precisamos fazer tudo isso.

151
00:08:06,330 --> 00:08:08,230
Se algum elemento retornar uma exceção,

152
00:08:08,230 --> 00:08:10,375
ele não será válido.

153
00:08:10,375 --> 00:08:11,937
Depois de receber os valores,

154
00:08:11,937 --> 00:08:16,500
dizemos que eles são válidos
se as condições forem atendidas.

155
00:08:16,500 --> 00:08:19,010
Fare_amount é maior que 2,5,

156
00:08:19,010 --> 00:08:22,675
e pickup_longitude é maior que -78 etc.

157
00:08:22,675 --> 00:08:24,155
Fazemos todos esses testes.

158
00:08:24,155 --> 00:08:25,485
Se passarem,

159
00:08:25,485 --> 00:08:27,715
as entradas são válidas.

160
00:08:28,685 --> 00:08:30,545
Agora, o pré-processamento.

161
00:08:30,545 --> 00:08:33,830
Vamos usar os dados e fazer algumas coisas

162
00:08:33,830 --> 00:08:37,059
para melhorar
o treinamento da rede neural.

163
00:08:37,059 --> 00:08:38,530
O que faremos?

164
00:08:38,530 --> 00:08:43,890
Vamos enviar
o fare_amount sem modificações.

165
00:08:43,890 --> 00:08:48,265
Eu poderia só dizer fare_amount
ou chamar outra função.

166
00:08:48,265 --> 00:08:51,610
Neste caso, estou usando
tf.Identity para transferir.

167
00:08:52,410 --> 00:08:56,055
Dayofweek é um número inteiro.

168
00:08:56,055 --> 00:08:59,690
O BigQuery mostra um
número inteiro como 1, 2, 3 ,4.

169
00:08:59,690 --> 00:09:02,465
No laboratório anterior,

170
00:09:02,465 --> 00:09:04,020
de engenharia de atributos,

171
00:09:04,020 --> 00:09:05,800
o que fizemos com isso?

172
00:09:05,800 --> 00:09:09,140
Essencialmente, incluímos
o vocabulário no código.

173
00:09:09,140 --> 00:09:12,485
Neste caso, diremos ao
TensorFlow Transform

174
00:09:12,485 --> 00:09:15,255
para aprender o vocabulário
no conjunto de treinamento.

175
00:09:15,255 --> 00:09:20,850
Agora, não sabemos necessariamente
o que esse número significa,

176
00:09:20,850 --> 00:09:23,590
mas sabemos que o que aparecer na previsão

177
00:09:23,590 --> 00:09:25,520
será convertido automaticamente.

178
00:09:25,520 --> 00:09:29,190
Vamos converter a string de dayofweek

179
00:09:29,190 --> 00:09:33,025
que recebemos em um número
inteiro com base no vocabulário.

180
00:09:33,025 --> 00:09:34,860
É isso que string_to_int faz.

181
00:09:34,860 --> 00:09:38,965
Hourofday já é um número inteiro,

182
00:09:38,965 --> 00:09:40,985
então nós transferimos sem mudanças.

183
00:09:40,985 --> 00:09:44,690
Pickuplon é um ponto flutuante.

184
00:09:44,690 --> 00:09:46,810
Também podemos usar sem modificações,

185
00:09:46,810 --> 00:09:51,745
mas sabemos que o treinamento
da rede neural funciona melhor,

186
00:09:51,745 --> 00:09:56,270
o gradiente descendente funciona
melhor com números menores,

187
00:09:56,270 --> 00:09:59,505
por exemplo, de 0 a 1.

188
00:09:59,985 --> 00:10:02,825
É isso que pedimos
para o tf.Transform fazer.

189
00:10:02,825 --> 00:10:08,060
O tf.Transform
escalona esse valor de 0 a 1.

190
00:10:08,060 --> 00:10:10,910
Mas para fazer isso,

191
00:10:10,910 --> 00:10:16,240
ele precisa saber
os valores mínimo e máximo.

192
00:10:16,780 --> 00:10:18,610
Ele aprenderá no conjunto de dados.

193
00:10:18,610 --> 00:10:20,540
Por isso temos duas fases.

194
00:10:20,540 --> 00:10:25,800
Temos a fase de análise
e a de transformação.

195
00:10:25,800 --> 00:10:29,870
Embora estejamos escrevendo
scale_to_0_1 na transformação,

196
00:10:29,870 --> 00:10:34,495
ele sabe que,
para fazer isso na fase de análise,

197
00:10:34,495 --> 00:10:36,690
é preciso encontrar o mínimo e o máximo.

198
00:10:36,690 --> 00:10:39,350
Fazemos o mesmo para tudo isso.

199
00:10:39,350 --> 00:10:43,210
Depois, usamos
cast-inputs-passengers como um float

200
00:10:43,210 --> 00:10:50,780
e mudamos todos de uma vez, desta maneira.

201
00:10:50,780 --> 00:10:55,390
Conseguimos um número igual
de uns e usamos cast para uma string.

202
00:10:55,390 --> 00:10:59,270
Nesse caso, nossas chaves são a string 1.

203
00:10:59,270 --> 00:11:02,450
Mas isso é só um exemplo de que

204
00:11:02,450 --> 00:11:05,685
você pode chamar funções
arbitrárias do TensorFlow.

205
00:11:05,685 --> 00:11:10,065
O principal é que o pré-processamento
é formado por funções do TensorFlow.

206
00:11:10,065 --> 00:11:13,700
Depois, fazemos a engenharia.

207
00:11:13,700 --> 00:11:15,570
Novamente, funções do TensorFlow.

208
00:11:15,570 --> 00:11:18,830
Neste caso, estou subtraindo
pickuplat e dropofflat,

209
00:11:18,830 --> 00:11:21,580
subtraindo pickuplon e dropofflon,

210
00:11:21,580 --> 00:11:23,760
e escalonando latdiff e londiff

211
00:11:23,760 --> 00:11:27,615
que foram calculadas.

212
00:11:27,615 --> 00:11:31,970
Não precisamos nos preocupar em saber

213
00:11:31,970 --> 00:11:33,375
qual é a escala.

214
00:11:33,375 --> 00:11:35,920
É o TensorFlow Transform
que precisa descobrir

215
00:11:35,920 --> 00:11:38,695
os valores mínimo
e máximo para criar a escala.

216
00:11:38,695 --> 00:11:40,720
Nós coletamos

217
00:11:40,720 --> 00:11:46,365
esses valores escalonados
e calculamos a distância euclidiana deles.

218
00:11:46,365 --> 00:11:48,000
Não precisamos escalonar novamente

219
00:11:48,000 --> 00:11:51,250
porque sabemos que
se as distâncias estiverem entre 0 e 1,

220
00:11:51,250 --> 00:11:54,045
então a raiz quadrada
também estará entre 0 e 1.

221
00:11:54,045 --> 00:11:56,345
Certo, está tudo nesse quadrado.

222
00:11:57,495 --> 00:11:59,150
Na verdade,
pode ser um pouco mais.

223
00:11:59,150 --> 00:12:02,770
Seria 1,4 se ambos forem 1,
mas está perto o bastante.

224
00:12:02,770 --> 00:12:05,270
São números pequenos, então
não precisamos escalonar.

225
00:12:05,270 --> 00:12:11,320
Agora, a função de
pré-processamento está pronta.

226
00:12:11,320 --> 00:12:18,240
Mas ainda precisamos chamar
os métodos is_valid e preprocess_tft.

227
00:12:18,240 --> 00:12:23,644
Precisamos chamar esses métodos
na transformação do Beam.

228
00:12:23,644 --> 00:12:24,995
Como fazemos isso?

229
00:12:24,995 --> 00:12:29,070
Para isso, primeiro

230
00:12:29,070 --> 00:12:33,465
configuramos os metadados
para os dados brutos que leremos.

231
00:12:33,465 --> 00:12:34,695
O que são dados brutos?

232
00:12:34,695 --> 00:12:37,520
São dados provenientes do BigQuery.

233
00:12:37,520 --> 00:12:43,850
Dizemos que dayofweek e key são strings,

234
00:12:44,600 --> 00:12:47,350
fare_amount, pickuplon e pickuplat

235
00:12:47,350 --> 00:12:49,490
são floats,

236
00:12:49,490 --> 00:12:52,350
e criamos um esquema de dados brutos

237
00:12:52,350 --> 00:12:54,870
que é um dicionário que abrange

238
00:12:54,870 --> 00:13:00,175
desde o nome da coluna até se o valor
é uma string, float ou número inteiro.

239
00:13:00,175 --> 00:13:03,030
Hourofday e passengers
são números inteiros.

240
00:13:03,030 --> 00:13:04,695
Esses são os dados brutos.

241
00:13:04,695 --> 00:13:06,670
É isso que o BigQuery fornece.

242
00:13:06,670 --> 00:13:10,675
Nós usamos os dados brutos
com o comando cell.

243
00:13:10,675 --> 00:13:15,135
Vamos escrever os metadados brutos.

244
00:13:15,135 --> 00:13:18,040
Nós os escrevemos para que

245
00:13:18,040 --> 00:13:24,005
a entrada JSON recebida do usuário
também seja desses metadados.

246
00:13:24,005 --> 00:13:26,130
Ela terá esse formato,

247
00:13:26,130 --> 00:13:30,340
e queremos que nossa função de
entrada de disponibilização veja isso.

248
00:13:31,060 --> 00:13:32,710
Depois, dizemos:

249
00:13:32,710 --> 00:13:35,850
"Leia os dados do BigQuery com a consulta

250
00:13:35,850 --> 00:13:41,625
que criamos e filtre-os
com o método is_valid".

251
00:13:41,625 --> 00:13:43,810
Você pode ver como
o método is_valid é usado.

252
00:13:43,810 --> 00:13:46,935
Ele é chamado como
parte de um filtro do Beam.

253
00:13:46,935 --> 00:13:53,730
O filtro é executado com as regras
especificadas na função is_valid.

254
00:13:53,730 --> 00:13:56,370
Depois, chamamos

255
00:13:56,370 --> 00:13:58,855
AnalyzeAndTransformDataset.

256
00:13:58,855 --> 00:14:02,890
Depois, precisamos especificar
a função de transformação.

257
00:14:02,890 --> 00:14:06,265
A função é preprocess_tft.

258
00:14:06,265 --> 00:14:08,965
Ela faz todo o escalonamento etc.

259
00:14:08,965 --> 00:14:14,459
Agora, recebemos
transformed_dataset e transform_fn.

260
00:14:14,459 --> 00:14:21,210
Pegamos transformed_data
e escrevemos como TFRecords.

261
00:14:21,210 --> 00:14:24,529
Escrevemos como TFRecords no gz,

262
00:14:24,529 --> 00:14:26,945
compactados para economizar espaço.

263
00:14:26,945 --> 00:14:30,580
Depois, fazemos o mesmo
com os dados de teste.

264
00:14:30,580 --> 00:14:31,930
Nos dados de treinamento,

265
00:14:31,930 --> 00:14:33,490
eu criei a consulta com 1

266
00:14:33,490 --> 00:14:36,285
e, nos dados de teste, com 2.

267
00:14:36,285 --> 00:14:42,365
A consulta é configurada dependendo
de qual foi passado, 1 ou 2,

268
00:14:42,365 --> 00:14:43,725
nessa fase.

269
00:14:43,725 --> 00:14:49,625
Eu uso os primeiros intervalos
de hash ou os últimos.

270
00:14:49,625 --> 00:14:54,380
É assim que recebo meu conjunto
de dados de treinamento ou avaliação.

271
00:14:56,630 --> 00:14:58,075
Vamos descer a tela.

272
00:14:58,075 --> 00:14:59,870
Depois disso,

273
00:14:59,870 --> 00:15:03,710
eu escrevo transformed_test_dataset

274
00:15:03,710 --> 00:15:10,330
e também para os elementos
de avaliação e, finalmente,

275
00:15:10,330 --> 00:15:11,820
e isso é muito importante,

276
00:15:11,820 --> 00:15:15,725
precisamos escrever os
metadados das transformações.

277
00:15:15,725 --> 00:15:19,485
É assim que todos os métodos TF chamados

278
00:15:19,485 --> 00:15:21,375
são armazenados no gráfico.

279
00:15:21,375 --> 00:15:25,840
Esse processo escreve um modelo.

280
00:15:25,840 --> 00:15:28,380
O modelo não é algo que você treina,

281
00:15:28,380 --> 00:15:32,470
ele consiste em operações do TensorFlow

282
00:15:32,470 --> 00:15:37,680
que são colocadas na frente do
seu gráfico de modelo normal

283
00:15:37,680 --> 00:15:41,310
para que as entradas do usuário passem

284
00:15:41,310 --> 00:15:48,225
pelas funções do TensorFlow
e cheguem ao seu modelo normal.

285
00:15:48,225 --> 00:15:50,450
Com isso, está tudo pronto

286
00:15:50,450 --> 00:15:55,270
para criarmos um conjunto
de dados de pré-processamento.

287
00:15:55,270 --> 00:15:56,660
Se eu defino isso como true,

288
00:15:56,660 --> 00:15:59,480
crio um conjunto de dados
pequeno, mas vou usar false.

289
00:15:59,480 --> 00:16:02,690
Isso será executado no Dataflow

290
00:16:02,690 --> 00:16:05,120
para criar o conjunto de dados.

291
00:16:05,750 --> 00:16:09,820
Aqui, se você receber um erro

292
00:16:09,820 --> 00:16:14,445
que diz que a API Dataflow
não está ativada,

293
00:16:14,445 --> 00:16:17,745
acesse o projeto do
Qwiklabs e ative a API.

294
00:16:17,745 --> 00:16:21,440
Depois disso, o job será iniciado,

295
00:16:21,440 --> 00:16:27,650
e você poderá ver arquivos
em preprocess.tft.

296
00:16:27,650 --> 00:16:34,025
Feito isso, o treinamento
será parecido com o anterior.

297
00:16:34,025 --> 00:16:35,150
Vamos dar uma olhada.

298
00:16:35,150 --> 00:16:37,240
Vamos ver as diferenças.

299
00:16:37,240 --> 00:16:42,015
Quando vemos o
tf_Transform em taxifare_tft,

300
00:16:42,015 --> 00:16:46,205
vamos ver model.pi

301
00:16:46,205 --> 00:16:51,605
e ver o que mudou nesse modelo.

302
00:16:51,605 --> 00:16:56,015
Vemos as colunas de entrada
da mesma forma que antes.

303
00:16:56,015 --> 00:16:58,765
Estamos intervalando e fazendo
o cruzamento de atributos,

304
00:16:58,765 --> 00:17:00,280
criando colunas brancas

305
00:17:00,280 --> 00:17:01,780
e colunas profundas.

306
00:17:01,780 --> 00:17:05,819
Isso é idêntico ao pré-processamento.

307
00:17:05,819 --> 00:17:08,679
Antes, quando fizemos isso com o Dataflow,

308
00:17:08,679 --> 00:17:15,280
tínhamos uma ad engineered function
extra que chamamos para os três lugares.

309
00:17:15,280 --> 00:17:16,865
Neste caso,

310
00:17:16,865 --> 00:17:19,750
não precisamos fazer isso,
não temos essa função.

311
00:17:19,750 --> 00:17:22,210
O que essa função estava fazendo,

312
00:17:22,210 --> 00:17:26,095
o tf.Transform agora faz
como parte do gráfico.

313
00:17:26,865 --> 00:17:28,880
Estamos dizendo o seguinte:

314
00:17:28,880 --> 00:17:32,250
Quando alguém oferece
uma função de disponibilização,

315
00:17:32,250 --> 00:17:36,970
eu preciso ler nessa
função de transformação

316
00:17:36,970 --> 00:17:39,625
todas essas operações realizadas,

317
00:17:39,625 --> 00:17:41,805
coletar os dados brutos recebidos,

318
00:17:41,805 --> 00:17:43,470
estes são os dados brutos,

319
00:17:43,470 --> 00:17:49,495
e aplicar tudo que acontece
na transform_fn,

320
00:17:49,495 --> 00:17:51,170
tudo isso que fizemos.

321
00:17:51,170 --> 00:17:56,060
Todo o código chamado em preprocess_tft

322
00:17:56,060 --> 00:18:00,870
será aplicado aos meus atributos,

323
00:18:00,870 --> 00:18:02,620
ao meu feature_placeholders.

324
00:18:02,620 --> 00:18:04,875
Aplique-os a feature_placeholders,

325
00:18:04,875 --> 00:18:06,429
receba os atributos,

326
00:18:06,429 --> 00:18:09,680
e é isso que retornamos.

327
00:18:09,680 --> 00:18:13,235
Feature_placeholders
é o que o usuário final fornece,

328
00:18:13,235 --> 00:18:15,520
o que estava no JSON.

329
00:18:15,520 --> 00:18:20,040
Features é o resultado de
coletar o que estava no JSON

330
00:18:20,040 --> 00:18:25,120
e aplicar a função de
transformação tf.Transform,

331
00:18:25,120 --> 00:18:26,625
o transform_fn.

332
00:18:26,625 --> 00:18:28,480
Todas essas operações

333
00:18:28,480 --> 00:18:32,520
para o feature_placeholders,
que é retornado.

334
00:18:32,520 --> 00:18:35,615
Temos a função de
entrada de disponibilização.

335
00:18:35,615 --> 00:18:38,165
O que precisamos fazer
ao ler o conjunto de dados?

336
00:18:38,165 --> 00:18:40,100
Na leitura,

337
00:18:40,100 --> 00:18:42,335
precisamos aplicar essas transformações.

338
00:18:42,335 --> 00:18:45,755
Felizmente, não precisamos
escrever o código,

339
00:18:45,755 --> 00:18:48,740
porque o tf.Transform inclui

340
00:18:48,740 --> 00:18:52,200
um criador de funções de entrada
para o qual podemos dizer:

341
00:18:52,200 --> 00:18:54,660
"Crie uma função de entrada de treinamento

342
00:18:54,660 --> 00:18:58,425
para aplicar tudo isso nos
metadados de transformação".

343
00:18:58,425 --> 00:19:04,395
Podemos ler com o Gzip e pronto.

344
00:19:04,395 --> 00:19:07,940
Ele tem a função build_training_input

345
00:19:07,940 --> 00:19:10,490
que sabe como ler registros do TensorFlow.

346
00:19:10,490 --> 00:19:14,890
Não precisamos escrever todo o código

347
00:19:14,890 --> 00:19:16,490
para ler um conjunto de dados

348
00:19:16,490 --> 00:19:18,290
e aplicar uma decodificação de CSV.

349
00:19:18,290 --> 00:19:20,715
Tudo isso desaparece.

350
00:19:20,715 --> 00:19:22,715
Essencialmente, só usamos

351
00:19:22,715 --> 00:19:27,060
a função build_training_input
para fazer o trabalho.

352
00:19:27,060 --> 00:19:30,390
O treinamento e a avaliação
são exatamente como antes.

353
00:19:30,390 --> 00:19:31,970
Criamos um train spec,

354
00:19:31,970 --> 00:19:33,619
um eval spec

355
00:19:33,619 --> 00:19:36,880
e passamos os dois para o Estimator.

356
00:19:37,405 --> 00:19:41,485
A única diferença é que,
como você está lendo o Gzip,

357
00:19:41,485 --> 00:19:43,900
passamos uma
função de leitura do Gzip,

358
00:19:43,900 --> 00:19:50,070
que é essencialmente um leitor
de TFRecord que lê o Gzip.

359
00:19:50,510 --> 00:19:52,080
É basicamente isso.