1
00:00:00,000 --> 00:00:04,840
次に Apache Beamの
相補的テクノロジーである

2
00:00:04,840 --> 00:00:08,070
Google Cloud Dataflowについてです

3
00:00:08,070 --> 00:00:12,020
この2つは前処理と
特徴エンジニアリングに役立ちます

4
00:00:12,020 --> 00:00:17,300
まず Cloud Dataflowについてです

5
00:00:17,300 --> 00:00:20,660
特徴量の前処理などのデータ変換は

6
00:00:20,660 --> 00:00:24,250
パイプラインを軸にとらえることができます

7
00:00:24,250 --> 00:00:26,380
パイプラインとは

8
00:00:26,380 --> 00:00:30,940
データを別の形式に変換する
一連のステップです

9
00:00:30,940 --> 00:00:36,070
たとえば BigQueryなどの
データウェアハウスのデータを

10
00:00:36,070 --> 00:00:38,580
パイプラインに入力し

11
00:00:38,580 --> 00:00:41,310
一連のステップで変換し

12
00:00:41,310 --> 00:00:46,720
場合によっては変換中に新しい特徴量を導入し

13
00:00:46,720 --> 00:00:49,540
最後に結果を
Google Cloud Storageなどに保存します

14
00:00:49,550 --> 00:00:53,800
Cloud Dataflowは
データ処理パイプラインを実行できる

15
00:00:53,800 --> 00:00:56,350
プラットフォームです

16
00:00:56,350 --> 00:01:01,520
PythonとJavaで書かれた
パイプラインを実行できます

17
00:01:01,520 --> 00:01:05,980
Dataflowは
スケールに合わせたデータ処理が可能で

18
00:01:05,980 --> 00:01:13,010
サーバーレスなフルマネージド
サービスである点が強みです

19
00:01:13,010 --> 00:01:19,120
デベロッパーはクラスタのサイズを
気にする必要はありません

20
00:01:19,120 --> 00:01:24,060
パイプラインを実行する
リソース量やサーバー数は

21
00:01:24,060 --> 00:01:30,200
処理するデータ量に応じて
Dataflowが柔軟に変更します

22
00:01:30,200 --> 00:01:32,795
Dataflowのコードには

23
00:01:32,795 --> 00:01:37,200
Apache Beamという
オープンソースライブラリを使用し

24
00:01:37,200 --> 00:01:42,122
Apache Beam APIで書いたコードを
Dataflowにデプロイして

25
00:01:42,122 --> 00:01:45,100
パイプラインを実装します

26
00:01:45,100 --> 00:01:51,040
Apache Beamのコードは 私たちが考える
データ処理パイプラインに近く

27
00:01:51,040 --> 00:01:53,684
使いやすいのが特徴です

28
00:01:53,684 --> 00:01:58,400
画面中央のパイプラインのPythonコードは

29
00:01:58,400 --> 00:02:02,300
文書内の行の文字数を分析しています

30
00:02:02,300 --> 00:02:05,460
Google Cloud Storageから

31
00:02:05,460 --> 00:02:10,370
テキストファイルを読み取って
パイプラインに入力し

32
00:02:10,370 --> 00:02:14,590
各行の文字数を出してデータを変換します

33
00:02:14,590 --> 00:02:17,080
この種の変換は

34
00:02:17,080 --> 00:02:21,910
Dataflowが自動的に
スケールして並列処理します

35
00:02:21,910 --> 00:02:26,790
次に グループ分けなどの集計処理によって

36
00:02:26,790 --> 00:02:29,370
行をグループ化します

37
00:02:29,370 --> 00:02:32,530
10文字以下の行は除外するなど

38
00:02:32,530 --> 00:02:34,985
値の絞り込みもできます

39
00:02:34,985 --> 00:02:39,815
変換、グループ分け、フィルター
処理がすべて完了すると

40
00:02:39,815 --> 00:02:43,855
Google Cloud Storageに
結果を書き込みます

41
00:02:43,855 --> 00:02:45,825
この実装では

42
00:02:45,825 --> 00:02:50,165
パイプラインの定義と実行が分れています

43
00:02:50,165 --> 00:02:53,920
p.runメソッドを
呼び出す前のステップはすべて

44
00:02:53,920 --> 00:03:00,560
パイプラインの定義で実行メソッドを呼び出して
初めてパイプラインが実行されます

45
00:03:00,560 --> 00:03:04,950
Apache Beamは
同じパイプラインコードで

46
00:03:04,950 --> 00:03:09,610
バッチデータとストリーミング
データの両方を処理できます

47
00:03:09,610 --> 00:03:15,490
実はBeamという名前も
バッチ＋ストリームから来ています

48
00:03:15,490 --> 00:03:17,600
そのメリットは

49
00:03:17,600 --> 00:03:21,280
Google Cloud Storageのような
バッチデータソースのデータであっても

50
00:03:21,280 --> 00:03:24,400
Pub/Subのような
ストリームデータソースのデータであっても

51
00:03:24,400 --> 00:03:28,290
同じパイプラインロジックを使用でき

52
00:03:28,290 --> 00:03:33,940
出力先もバッチとストリームの
両方に対応している点です

53
00:03:33,940 --> 00:03:37,730
また ロジックを変えることなく

54
00:03:37,730 --> 00:03:41,440
データソースを容易に変更できます

55
00:03:41,440 --> 00:03:43,695
詳しく見ていきます

56
00:03:43,695 --> 00:03:46,820
ここではbeam.ioメソッドで

57
00:03:46,820 --> 00:03:50,130
読み取り/書き込み処理を行っています

58
00:03:50,130 --> 00:03:53,860
メソッドはさまざまなコネクタを使用します
たとえば

59
00:03:53,860 --> 00:03:58,860
Pub/Subコネクタはメッセージや
ストリーミングされたデータの読み取り

60
00:03:58,860 --> 00:04:04,440
他のコネクタはCloud Storageや
ファイルシステムからのテキスト抽出を行います

61
00:04:04,440 --> 00:04:07,460
Apache Beamには

62
00:04:07,460 --> 00:04:12,670
BigQueryなどGoogle Cloud上の
サービスとのコネクタもあれば

63
00:04:12,670 --> 00:04:16,649
独自のコネクタの実装も可能です

64
00:04:16,649 --> 00:04:18,850
詳細に入る前に

65
00:04:18,850 --> 00:04:21,780
用語を確認しておきます

66
00:04:21,780 --> 00:04:28,387
Dataflowではデータ処理パイプラインを
実行できると説明しました

67
00:04:28,388 --> 00:04:31,910
画面右の図がパイプラインです

68
00:04:31,910 --> 00:04:37,150
Apache Beamのパイプラインを
詳しく見ていきます

69
00:04:37,150 --> 00:04:41,962
パイプラインには入力データのソースがあり

70
00:04:41,962 --> 00:04:44,825
一連のステップがあります

71
00:04:44,825 --> 00:04:49,260
Beamの各ステップを「変換」と呼び

72
00:04:49,260 --> 00:04:54,100
変換対象はPCollectionというデータ構造です

73
00:04:54,100 --> 00:04:57,113
詳しくは後で説明しますが

74
00:04:57,113 --> 00:05:02,423
変換は入力としてPCollectionを受け取り

75
00:05:02,423 --> 00:05:07,370
結果を別のPCollectionに出力します

76
00:05:07,370 --> 00:05:10,730
パイプラインの最後の変換結果は

77
00:05:10,730 --> 00:05:14,470
シンクとして出力されるため重要です

78
00:05:14,470 --> 00:05:17,250
パイプラインの実行には

79
00:05:17,250 --> 00:05:20,858
コードを実行するランナーが必要です

80
00:05:20,858 --> 00:05:24,320
プラットフォームごとに異なり

81
00:05:24,320 --> 00:05:27,770
Dataflow用のランナー

82
00:05:27,770 --> 00:05:32,200
Apache Spark用のランナー

83
00:05:32,200 --> 00:05:40,400
パソコンからパイプラインを実行する
ダイレクトランナーなどがあります

84
00:05:40,400 --> 00:05:45,172
独自のカスタムランナーも実装できます

85
00:05:45,712 --> 00:05:49,790
パイプラインの実装方法を説明します

86
00:05:49,790 --> 00:05:53,335
このコードでは mainメソッドで

87
00:05:53,335 --> 00:05:58,340
beam.pipelineにより
パイプラインを作成した後

88
00:05:58,340 --> 00:06:00,450
すべての変換が

89
00:06:00,450 --> 00:06:05,810
パイプラインのメソッドを適用するための
実引数として実装されます

90
00:06:05,810 --> 00:06:10,097
Python版の
Apache Beamライブラリでは

91
00:06:10,097 --> 00:06:15,330
適用メソッドを呼び出す
パイプ演算子は多重定義されます

92
00:06:15,330 --> 00:06:19,800
この型破りな
シンタックスを使うのはそのためです

93
00:06:19,800 --> 00:06:24,540
このread、countwords、writeなどの文字列は

94
00:06:24,540 --> 00:06:28,680
変換に人が読める名前を付けたものです

95
00:06:28,680 --> 00:06:35,310
このパイプラインはGoogle Cloud Storageから
読み取り、書き込みを行います

96
00:06:35,310 --> 00:06:39,560
パイプラインでデータを処理するには

97
00:06:39,560 --> 00:06:46,990
パイプラインインスタンスの
実行メソッドを呼び出して実行します

98
00:06:46,990 --> 00:06:50,410
パイプ演算子を使用するたびに

99
00:06:50,410 --> 00:06:56,730
PCollectionデータ構造を入力して
PCollectionを出力します

100
00:06:56,730 --> 00:07:01,000
PCollectionは多くのデータ構造とは違い

101
00:07:01,000 --> 00:07:05,230
すべてのデータをメモリに保存しません

102
00:07:05,230 --> 00:07:08,624
Dataflowは柔軟に

103
00:07:08,624 --> 00:07:11,632
サーバー群を使用するため

104
00:07:11,632 --> 00:07:17,380
PCollectionはデータ保存場所を示す

105
00:07:17,380 --> 00:07:22,310
ポインター付きのデータ構造と言えます

106
00:07:22,310 --> 00:07:26,580
Google Cloud Storageの
ファイル内の行が

107
00:07:26,580 --> 00:07:29,470
PCollectionだとします

108
00:07:29,470 --> 00:07:32,137
変換方法の一つとして

109
00:07:32,137 --> 00:07:38,997
この行のPCollectionを入力して
整数のPCollectionを返します

110
00:07:38,997 --> 00:07:43,820
この変換ステップで各行の長さを計算します

111
00:07:43,820 --> 00:07:50,980
Apache Beam SDKには
多様なコネクタがあるため

112
00:07:50,980 --> 00:07:55,780
DataflowはCloud Pub/Subや
Kafkaなどのソースから

113
00:07:55,780 --> 00:08:02,806
テキストファイルやストリーミングデータを
読み出すことができます

114
00:08:02,806 --> 00:08:07,780
BigQueryデータウェアハウス用の
コネクタの場合

115
00:08:07,780 --> 00:08:15,160
BigQueryで評価した結果行の表を返す
SQL分を指定します

116
00:08:15,160 --> 00:08:20,614
この表をPCollectionとして
パイプラインに渡し

117
00:08:20,614 --> 00:08:25,200
パイプラインの結果をエクスポートします

118
00:08:25,200 --> 00:08:30,760
結果をファイルシステムに
書き込むこともできます

119
00:08:30,760 --> 00:08:34,159
その際に注意すべきなのは

120
00:08:34,159 --> 00:08:40,920
Dataflowはパイプラインを
分散させて実行できるため

121
00:08:40,920 --> 00:08:43,660
複数のサーバーが同時に

122
00:08:43,660 --> 00:08:47,390
書き込む可能性がある点です

123
00:08:47,390 --> 00:08:55,660
競合を避けるため テキストI/Oコネクタは
デフォルトで出力をシャーディングし

124
00:08:55,660 --> 00:09:01,550
ファイルシステム内の複数のファイルに
結果を書き込みます

125
00:09:01,550 --> 00:09:04,295
たとえば このパイプラインは

126
00:09:04,295 --> 00:09:09,080
データコネクタのoutputが付く
ファイルに結果を書き込みます

127
00:09:09,080 --> 00:09:12,700
合計で10個のファイルに書き込むとすると

128
00:09:12,700 --> 00:09:18,690
Dataflowはテキスト出力0/10
テキスト出力1/10…のように書き込みます

129
00:09:18,690 --> 00:09:20,530
先ほど述べた

130
00:09:20,530 --> 00:09:23,190
競合の問題があるため

131
00:09:23,190 --> 00:09:27,070
書き込みは1台のノードで処理できる

132
00:09:27,070 --> 00:09:31,550
小規模なデータのみに用いるのが合理的です

133
00:09:31,550 --> 00:09:34,730
Pythonのパイプラインでは

134
00:09:34,730 --> 00:09:38,550
コマンドを使って
シェルで直接コードを実行できます

135
00:09:38,550 --> 00:09:46,658
GCP上でパイプラインをDataflowで
実行するジョブとして送信するには

136
00:09:46,658 --> 00:09:49,365
GCPプロジェクト名と

137
00:09:49,365 --> 00:09:56,445
Google Cloud Storageバケットの
場所の実引数を供給し

138
00:09:56,445 --> 00:09:59,815
ランナー名を指定する必要があります