このラボでは ソースコードを GitHubからCloud Shell環境にコピーし スクリプトを実行して ライブラリをダウンロードします 作業に数分ほどかかるため ここではコピーとダウンロードが 終わったところまで早送りします Cloud ShellではテキストベースのNanoなど 各種エディタでパイプラインの
ソースコードを表示できます ここでは Cloud Shellのビルトイン
エディタを使用します エディタの左のメニューから [training-data-analyst]を展開して [courses] > [data_analysis] >
[lab2] > [python]フォルダを開き 「grep.py」ファイルのパイプライン
ソースコードにアクセスします 26行目にあるワイルドカードで指定された Javaファイルを入力として使用します 変換で各ファイルの キーワードを含む行を検索します 検索ワードは「import」です 実装されたパイプラインの
詳細は32～34行目にあります パイプラインのGrepステップで 20行目で定義した
my_grepメソッドを使用します このメソッドは検索ワード「import」と それを含むすべての行を検索し 結果を/tmp/outputディレクトリに
書き込みます Cloud Shellでパイプラインを実行するには pythonコマンドで パイプラインのソースコード名を渡します パイプラインの処理が完了し 出力ファイルを見ると Javaソースコードの
「import」を含むすべての行が 正しく特定できたことが確認できます 次に このパイプラインソースコードを Dataflowプラットフォームで実行します いくつか準備が必要です まず GCPでDataflow APIを検索し [有効にする]ボタンで有効化します 少し時間かがかるので 有効化が終わったところまで早送りします [無効にする]ボタンが表示されているので APIは有効です 次に パイプライン用の Cloud Storageバケットを用意します バケットには固有の名前を付け 必ず「Regional」を選択します ここではus-east4を割り当てました 作成できたら gsutilのcpコマンドで ソースコードファイルを Cloud Shellからバケットにコピーします ソースコードファイルをコピーするのは パイプラインは Cloud Shellファイルシステムに
アクセスできないからです コピーが終わったら ブラウザのバケットのページに戻り ページを更新して正しく
コピーできたことを確認します Dataflowで実行するパイプラインに入力する 4つのJavaファイルです 次に Dataflowでの実行に合わせて修正した パイプラインのソースコードです 「grepc.py」ファイルを見ます このコードではプロジェクト名と
バケット名に定数を使用します プロジェクトとバケットに
同じIDを使用したので 同じ値を入力します このコードでは Dataflowでのパイプラインの実行に必要な ジョブ名やランナーなどの パラメータも指定しています また 入力と出力は Google Cloud Storageバケットの
パスで指定されています その他のコードは同じです 実行するには 同じくpythonコマンドを使って ソースコードのファイル名を
引数として渡します このソースコードは
Dataflowランナーを使用するため Dataflowライブラリとしてパッケージされ ジョブとして送信されます 実行が完了したら GCPに戻って左のハンバーガー
メニューか検索バーから Dataflowを開きます ダッシュボードから パイプラインを確認できます ジョブ名は 「grepc.py」で指定した 「examplejob2」です ジョブは完全に開始されていません オートスケール中と表示されていて 1つの仮想コアのみで実行されています 右側はパイプラインのオプションと ジョブに関する情報です [ログ]を開くと ワーカーを起動中であることがわかります [オートスケール]のグラフでも確認できます このジョブが使用する
ターゲットワーカーは1つです ワーカー数が0から1になったので このパイプラインを実行するために 仮想インスタンスが1つ
プロビジョニングされました ジョブが完了したところまで 早送りします パイプラインの各変換ステップに 緑のチェックマークが付き 完了したことを示しています 右下のグラフは 実行に使用したワーカーの スケールダウンを示しています Google Cloud Storageから
Cloud Shellに出力ファイルをコピーして パイプラインの出力を確認できます コピーできたら Cloud Shellで直接または ブラウザでGoogle Cloud Storageを開いて バケット内の「javahelp」フォルダから
探して確認できます ファイル名は「output」で始まり 00000 of 00004、00001 of 00004
などが続きます ファイルの内容を確認するには [公開リンク]チェックボックスを使います こちらが最初のファイルの内容です