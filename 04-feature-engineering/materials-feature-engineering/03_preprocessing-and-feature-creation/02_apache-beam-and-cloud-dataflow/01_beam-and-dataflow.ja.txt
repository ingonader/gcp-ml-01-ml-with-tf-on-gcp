次に Apache Beamの
相補的テクノロジーである Google Cloud Dataflowについてです この2つは前処理と
特徴エンジニアリングに役立ちます まず Cloud Dataflowについてです 特徴量の前処理などのデータ変換は パイプラインを軸にとらえることができます パイプラインとは データを別の形式に変換する
一連のステップです たとえば BigQueryなどの
データウェアハウスのデータを パイプラインに入力し 一連のステップで変換し 場合によっては変換中に新しい特徴量を導入し 最後に結果を
Google Cloud Storageなどに保存します Cloud Dataflowは
データ処理パイプラインを実行できる プラットフォームです PythonとJavaで書かれた
パイプラインを実行できます Dataflowは
スケールに合わせたデータ処理が可能で サーバーレスなフルマネージド
サービスである点が強みです デベロッパーはクラスタのサイズを
気にする必要はありません パイプラインを実行する
リソース量やサーバー数は 処理するデータ量に応じて
Dataflowが柔軟に変更します Dataflowのコードには Apache Beamという
オープンソースライブラリを使用し Apache Beam APIで書いたコードを
Dataflowにデプロイして パイプラインを実装します Apache Beamのコードは 私たちが考える
データ処理パイプラインに近く 使いやすいのが特徴です 画面中央のパイプラインのPythonコードは 文書内の行の文字数を分析しています Google Cloud Storageから テキストファイルを読み取って
パイプラインに入力し 各行の文字数を出してデータを変換します この種の変換は Dataflowが自動的に
スケールして並列処理します 次に グループ分けなどの集計処理によって 行をグループ化します 10文字以下の行は除外するなど 値の絞り込みもできます 変換、グループ分け、フィルター
処理がすべて完了すると Google Cloud Storageに
結果を書き込みます この実装では パイプラインの定義と実行が分れています p.runメソッドを
呼び出す前のステップはすべて パイプラインの定義で実行メソッドを呼び出して
初めてパイプラインが実行されます Apache Beamは
同じパイプラインコードで バッチデータとストリーミング
データの両方を処理できます 実はBeamという名前も
バッチ＋ストリームから来ています そのメリットは Google Cloud Storageのような
バッチデータソースのデータであっても Pub/Subのような
ストリームデータソースのデータであっても 同じパイプラインロジックを使用でき 出力先もバッチとストリームの
両方に対応している点です また ロジックを変えることなく データソースを容易に変更できます 詳しく見ていきます ここではbeam.ioメソッドで 読み取り/書き込み処理を行っています メソッドはさまざまなコネクタを使用します
たとえば Pub/Subコネクタはメッセージや
ストリーミングされたデータの読み取り 他のコネクタはCloud Storageや
ファイルシステムからのテキスト抽出を行います Apache Beamには BigQueryなどGoogle Cloud上の
サービスとのコネクタもあれば 独自のコネクタの実装も可能です 詳細に入る前に 用語を確認しておきます Dataflowではデータ処理パイプラインを
実行できると説明しました 画面右の図がパイプラインです Apache Beamのパイプラインを
詳しく見ていきます パイプラインには入力データのソースがあり 一連のステップがあります Beamの各ステップを「変換」と呼び 変換対象はPCollectionというデータ構造です 詳しくは後で説明しますが 変換は入力としてPCollectionを受け取り 結果を別のPCollectionに出力します パイプラインの最後の変換結果は シンクとして出力されるため重要です パイプラインの実行には コードを実行するランナーが必要です プラットフォームごとに異なり Dataflow用のランナー Apache Spark用のランナー パソコンからパイプラインを実行する
ダイレクトランナーなどがあります 独自のカスタムランナーも実装できます パイプラインの実装方法を説明します このコードでは mainメソッドで beam.pipelineにより
パイプラインを作成した後 すべての変換が パイプラインのメソッドを適用するための
実引数として実装されます Python版の
Apache Beamライブラリでは 適用メソッドを呼び出す
パイプ演算子は多重定義されます この型破りな
シンタックスを使うのはそのためです このread、countwords、writeなどの文字列は 変換に人が読める名前を付けたものです このパイプラインはGoogle Cloud Storageから
読み取り、書き込みを行います パイプラインでデータを処理するには パイプラインインスタンスの
実行メソッドを呼び出して実行します パイプ演算子を使用するたびに PCollectionデータ構造を入力して
PCollectionを出力します PCollectionは多くのデータ構造とは違い すべてのデータをメモリに保存しません Dataflowは柔軟に サーバー群を使用するため PCollectionはデータ保存場所を示す ポインター付きのデータ構造と言えます Google Cloud Storageの
ファイル内の行が PCollectionだとします 変換方法の一つとして この行のPCollectionを入力して
整数のPCollectionを返します この変換ステップで各行の長さを計算します Apache Beam SDKには
多様なコネクタがあるため DataflowはCloud Pub/Subや
Kafkaなどのソースから テキストファイルやストリーミングデータを
読み出すことができます BigQueryデータウェアハウス用の
コネクタの場合 BigQueryで評価した結果行の表を返す
SQL分を指定します この表をPCollectionとして
パイプラインに渡し パイプラインの結果をエクスポートします 結果をファイルシステムに
書き込むこともできます その際に注意すべきなのは Dataflowはパイプラインを
分散させて実行できるため 複数のサーバーが同時に 書き込む可能性がある点です 競合を避けるため テキストI/Oコネクタは
デフォルトで出力をシャーディングし ファイルシステム内の複数のファイルに
結果を書き込みます たとえば このパイプラインは データコネクタのoutputが付く
ファイルに結果を書き込みます 合計で10個のファイルに書き込むとすると Dataflowはテキスト出力0/10
テキスト出力1/10…のように書き込みます 先ほど述べた 競合の問題があるため 書き込みは1台のノードで処理できる 小規模なデータのみに用いるのが合理的です Pythonのパイプラインでは コマンドを使って
シェルで直接コードを実行できます GCP上でパイプラインをDataflowで
実行するジョブとして送信するには GCPプロジェクト名と Google Cloud Storageバケットの
場所の実引数を供給し ランナー名を指定する必要があります