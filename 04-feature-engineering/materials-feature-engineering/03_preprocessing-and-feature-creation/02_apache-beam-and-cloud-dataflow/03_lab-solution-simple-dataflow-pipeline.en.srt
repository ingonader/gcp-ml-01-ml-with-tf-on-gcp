1
00:00:00,000 --> 00:00:02,160
For this lab, you'll need the source code

2
00:00:02,160 --> 00:00:04,890
copied from GitHub to your Cloud Shell environment.

3
00:00:04,890 --> 00:00:07,290
And you will also need to run a script to

4
00:00:07,290 --> 00:00:10,925
download some libraries that will have the dependencies via pipeline.

5
00:00:10,925 --> 00:00:13,585
These steps take a few minutes to complete.

6
00:00:13,585 --> 00:00:16,900
So right now, you can see the video fast forwarding through

7
00:00:16,900 --> 00:00:22,155
these steps until after the source code is installed and libraries have been downloaded.

8
00:00:22,155 --> 00:00:27,560
From Cloud Shell, you can use different editors to view the source code of the pipeline,

9
00:00:27,560 --> 00:00:30,695
you can use a text based editor like Nano.

10
00:00:30,695 --> 00:00:32,450
But here in this video,

11
00:00:32,450 --> 00:00:36,165
you'll see me use a built-in Cloud Shell graphical editor.

12
00:00:36,165 --> 00:00:37,860
Once this editor loads,

13
00:00:37,860 --> 00:00:40,060
you can see that in the menu on the left,

14
00:00:40,060 --> 00:00:42,385
you can open up the training-data-analyst,

15
00:00:42,385 --> 00:00:46,810
courses, data_analysis, lab2, python folder,

16
00:00:46,810 --> 00:00:50,215
and access to pipeline source code in the grep.py file.

17
00:00:50,215 --> 00:00:57,140
The source code takes as input the various java files highlighted here in line 26.

18
00:00:57,140 --> 00:01:02,045
So, you will use the java file specified was the wildcard statement,

19
00:01:02,045 --> 00:01:04,170
for each one of the files,

20
00:01:04,170 --> 00:01:08,825
the transform is looking for lines of Java Source Code containing the keyword,

21
00:01:08,825 --> 00:01:11,475
the search term is "Import."

22
00:01:11,475 --> 00:01:17,555
You can see the details of the pipeline implementation in the lines 32 to 34.

23
00:01:17,555 --> 00:01:20,630
Notice, that the grep step of the pipeline

24
00:01:20,630 --> 00:01:24,335
is using the My_grep method defined in line 20.

25
00:01:24,335 --> 00:01:27,340
The my_grep method looks for the search term

26
00:01:27,340 --> 00:01:31,540
"Import" and for all the lines that contain the search term,

27
00:01:31,540 --> 00:01:35,675
the result is written out to the /tmp/output directory.

28
00:01:35,675 --> 00:01:38,160
To run the pipeline on Cloud Shell,

29
00:01:38,160 --> 00:01:40,590
you simply use the python command and pass

30
00:01:40,590 --> 00:01:43,890
the name of the source code file with the pipeline implementation.

31
00:01:43,890 --> 00:01:46,810
The pipeline completed successfully and you can confirm

32
00:01:46,810 --> 00:01:50,395
that by looking at the output files that the pipeline created.

33
00:01:50,395 --> 00:01:53,660
The pipeline correctly identified all the lines

34
00:01:53,660 --> 00:01:57,095
of Java source code that contain the key word "Import".

35
00:01:57,095 --> 00:01:59,185
In the remaining part of the lab,

36
00:01:59,185 --> 00:02:01,730
you will take this pipeline source code and

37
00:02:01,730 --> 00:02:05,375
prepare it to run on the Google Cloud Dataflow platform.

38
00:02:05,375 --> 00:02:07,110
But before you can do that,

39
00:02:07,110 --> 00:02:08,750
there are some prerequisites steps.

40
00:02:08,750 --> 00:02:12,210
First, you need to search for dataflow APIs in

41
00:02:12,210 --> 00:02:17,070
GCP and enable the APIs using the enable button you see on the screen.

42
00:02:17,070 --> 00:02:19,115
This is going to take a few moments,

43
00:02:19,115 --> 00:02:23,965
so the video will fast forward until after the APIs are enabled.

44
00:02:23,965 --> 00:02:28,490
Okay, you can confirm that the APIs are enabled,

45
00:02:28,490 --> 00:02:32,555
if you can see the disable button on the dataflow API screen.

46
00:02:32,555 --> 00:02:35,190
Next, you need to make sure that you have

47
00:02:35,190 --> 00:02:38,175
a Cloud Storage Bucket created for your pipeline.

48
00:02:38,175 --> 00:02:39,810
You can create this Bucket,

49
00:02:39,810 --> 00:02:41,940
and it's important that you assign this Bucket

50
00:02:41,940 --> 00:02:45,735
a unique name and make sure it is set up as a original Bucket.

51
00:02:45,735 --> 00:02:50,750
Here, I assigned us-east4 for the Northern Virginia region.

52
00:02:50,750 --> 00:02:53,565
Okay. Once the Bucket is ready,

53
00:02:53,565 --> 00:02:55,850
you will copy the input source code files for

54
00:02:55,850 --> 00:02:59,760
your pipeline from Cloud Shell to the Google Cloud Storage Bucket.

55
00:02:59,760 --> 00:03:02,360
You do this using the GSU copy command.

56
00:03:02,360 --> 00:03:04,250
Remember, that you are copying

57
00:03:04,250 --> 00:03:08,120
these Java source code files for your pipeline because the pipeline does

58
00:03:08,120 --> 00:03:09,630
not have access to

59
00:03:09,630 --> 00:03:14,375
your Cloud Shell file system while it's executing on Google Cloud Dataflow.

60
00:03:14,375 --> 00:03:18,535
After the gsutil copy command finishes copying the files,

61
00:03:18,535 --> 00:03:22,080
you can go back to Google Cloud storage bucket in your browser,

62
00:03:22,080 --> 00:03:27,425
refresh the page, and then you can confirm that the files have been copied successfully.

63
00:03:27,425 --> 00:03:30,440
Here are the four Java files that will be used as

64
00:03:30,440 --> 00:03:33,925
an input to your pipeline running on Google Cloud Dataflow.

65
00:03:33,925 --> 00:03:37,845
Next, take a look at the source code for the pipeline implementation

66
00:03:37,845 --> 00:03:42,045
that was modified to run on the Google Cloud Dataflow platform.

67
00:03:42,045 --> 00:03:45,265
It is in the grepc.py file.

68
00:03:45,265 --> 00:03:50,195
Notice that this one uses constance for project and bucket names,

69
00:03:50,195 --> 00:03:55,430
in my case, I've used the same unique ID for both the project and the Bucket.

70
00:03:55,430 --> 00:03:57,800
So I'm going to put the same value for both.

71
00:03:57,800 --> 00:03:59,860
The code also specify

72
00:03:59,860 --> 00:04:03,860
some parameters that I needed to run this pipeline on Cloud Dataflow.

73
00:04:03,860 --> 00:04:07,330
For example, you need to specify the name of the job running

74
00:04:07,330 --> 00:04:12,520
your pipeline and also the data flow runner to execute the pipeline on dataflow.

75
00:04:12,520 --> 00:04:15,400
Here, the input and the output are

76
00:04:15,400 --> 00:04:19,330
specified as paths to your Google Cloud Storage Bucket.

77
00:04:19,670 --> 00:04:23,535
The rest of the code for the pipeline stays the same.

78
00:04:23,535 --> 00:04:25,830
To run your pipeline on dataflow,

79
00:04:25,830 --> 00:04:29,160
you still use the python command and pass in as arguments,

80
00:04:29,160 --> 00:04:32,520
the file name was the source code of your pipeline implementation.

81
00:04:32,520 --> 00:04:36,710
Here, since the source code used the dataflow runner,

82
00:04:36,710 --> 00:04:40,850
your code is going to be packaged together as dataflow libraries and

83
00:04:40,850 --> 00:04:46,360
submitted as a job to execute a pipeline on top of the Google Cloud Dataflow platform.

84
00:04:46,360 --> 00:04:49,380
When the python command finishes executing,

85
00:04:49,380 --> 00:04:52,470
you're going to go back to the GCP and open up

86
00:04:52,470 --> 00:04:56,925
data flow using the hamburger menu on the left or using the search bar.

87
00:04:56,925 --> 00:04:58,980
And from the data flow dashboard,

88
00:04:58,980 --> 00:05:03,045
you can monitor the pipeline you just submitted as one of the jobs.

89
00:05:03,045 --> 00:05:06,245
Here, the job is called example2,

90
00:05:06,245 --> 00:05:09,415
because that's the name that I've used in the grepc.py file.

91
00:05:09,415 --> 00:05:12,970
First, you'll notice that the job is not yet fully started.

92
00:05:12,970 --> 00:05:15,910
It says it's autoscaling and currently showing

93
00:05:15,910 --> 00:05:19,505
it's only using a single virtual core for execution.

94
00:05:19,505 --> 00:05:21,800
On the right hand side, you can also see

95
00:05:21,800 --> 00:05:25,145
pipeline options and other information about the job.

96
00:05:25,145 --> 00:05:29,360
In the log section, you can find out that the pipeline is not yet running,

97
00:05:29,360 --> 00:05:32,145
because it's still starting up one of the workers,

98
00:05:32,145 --> 00:05:36,395
and you can confirm that by looking at the graph in the autoscalling section.

99
00:05:36,395 --> 00:05:41,455
Here, you'll notice that the job is expecting to use one target worker.

100
00:05:41,455 --> 00:05:45,160
And currently, the number of workers went from zero to one.

101
00:05:45,160 --> 00:05:48,055
So this means that exactly one virtual instance

102
00:05:48,055 --> 00:05:50,835
has been provision to execute this pipeline.

103
00:05:50,835 --> 00:05:54,770
It is going to take a few minutes for this pipeline to finish executing.

104
00:05:54,770 --> 00:05:56,930
So right now, you can see the video fast

105
00:05:56,930 --> 00:06:00,265
forwarding a few minutes until after the job is done.

106
00:06:00,265 --> 00:06:03,930
If you take a closer look at the pipeline you can tell by

107
00:06:03,930 --> 00:06:05,250
the green check marks that

108
00:06:05,250 --> 00:06:07,955
all the individual steps for the transformations have completed.

109
00:06:07,955 --> 00:06:11,430
And by reviewing the graph on the bottom right,

110
00:06:11,430 --> 00:06:13,500
you'll notice that all the workers that have been used

111
00:06:13,500 --> 00:06:15,900
to execute the pipeline have been scaled down.

112
00:06:15,900 --> 00:06:19,460
You can take a look at the output of this pipeline by

113
00:06:19,460 --> 00:06:24,195
copying the output files from Google Cloud Storage to Cloud Shell.

114
00:06:24,195 --> 00:06:26,540
And once the files are copied,

115
00:06:26,540 --> 00:06:30,635
you can review them directly in Cloud Shell or you can also open

116
00:06:30,635 --> 00:06:33,020
Google Cloud Storage in your browser and find

117
00:06:33,020 --> 00:06:35,945
the files in your Bucket under the Java Help folder.

118
00:06:35,945 --> 00:06:39,230
The files will have a prefix of outputs,

119
00:06:39,230 --> 00:06:41,600
so they will be named like 04,

120
00:06:41,600 --> 00:06:44,105
0104, 024 and so on.

121
00:06:44,105 --> 00:06:46,430
To review the content of the files,

122
00:06:46,430 --> 00:06:50,530
it is important that you use a public link checkbox on the right.

123
00:06:50,530 --> 00:06:53,880
Here, you can see the contents of the first file.