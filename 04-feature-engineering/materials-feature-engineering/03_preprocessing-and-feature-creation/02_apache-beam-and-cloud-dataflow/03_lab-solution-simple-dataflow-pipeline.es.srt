1
00:00:00,000 --> 00:00:02,680
Para este lab, deberán
copiar el código fuente

2
00:00:02,680 --> 00:00:04,890
de GitHub a su ambiente de Cloud Shell.

3
00:00:04,890 --> 00:00:07,060
También tendrán que ejecutar un script

4
00:00:07,060 --> 00:00:10,925
para descargar algunas bibliotecas
con las dependencias para la canalización.

5
00:00:11,225 --> 00:00:13,625
Estos pasos tomarán
algunos minutos en completarse.

6
00:00:13,625 --> 00:00:17,580
Ahora, el video avanza rápido
mientras se realizan estos pasos

7
00:00:17,580 --> 00:00:22,155
hasta que se instala el código fuente
y se descargan las bibliotecas.

8
00:00:22,735 --> 00:00:25,497
En Cloud Shell,
podemos usar distintos editores

9
00:00:25,497 --> 00:00:27,780
para ver
el código fuente de la canalización.

10
00:00:27,900 --> 00:00:30,695
Podemos usar un editor
basado en texto, como Nano.

11
00:00:30,695 --> 00:00:32,450
Sin embargo, en este video

12
00:00:32,450 --> 00:00:36,165
me verán utilizar un editor gráfico
que viene incorporado en Cloud Shell.

13
00:00:36,545 --> 00:00:38,160
Una vez se carga este editor

14
00:00:38,160 --> 00:00:40,060
en el menú de la izquierda

15
00:00:40,060 --> 00:00:42,575
podemos abrir la carpeta
training-data-analyst

16
00:00:42,575 --> 00:00:46,810
courses, data_analysis, lab2, python

17
00:00:46,810 --> 00:00:50,215
para acceder al código fuente
de la canalización en el archivo grep.py.

18
00:00:50,215 --> 00:00:57,140
El código fuente toma como entrada
los archivos Java de la línea 26.

19
00:00:58,300 --> 00:01:02,045
Usaremos el archivo Java especificado
con la instrucción de comodín.

20
00:01:02,675 --> 00:01:04,470
Para cada uno de los archivos

21
00:01:04,470 --> 00:01:07,777
la transformación busca líneas
de código fuente de Java

22
00:01:07,777 --> 00:01:11,525
que contengan la palabra clave,
que en este caso es "import".

23
00:01:12,095 --> 00:01:15,105
Podemos ver los detalles de la
implementación de la canalización

24
00:01:15,105 --> 00:01:17,467
en las líneas 32 a la 34.

25
00:01:17,827 --> 00:01:20,630
Fíjense en el paso grep de la canalización

26
00:01:20,630 --> 00:01:24,335
que usa el método my_grep
que se definió en la línea 20.

27
00:01:24,765 --> 00:01:28,510
El método my_grep busca
el término de búsqueda "import"

28
00:01:28,510 --> 00:01:31,540
y, para todas las líneas
que contienen el término de búsqueda

29
00:01:31,540 --> 00:01:35,675
el resultado se escribe
en el directorio /tmp/output.

30
00:01:36,015 --> 00:01:38,160
Para ejecutar
la canalización en Cloud Shell

31
00:01:38,160 --> 00:01:39,990
simplemente usaremos el comando python

32
00:01:39,990 --> 00:01:42,350
y pasaremos el nombre
del archivo de código fuente

33
00:01:42,350 --> 00:01:44,310
con la implementación de la canalización.

34
00:01:44,310 --> 00:01:46,300
La canalización se completó correctamente

35
00:01:46,300 --> 00:01:50,395
y podemos confirmarlo si miramos
los archivos de salida de la canalización.

36
00:01:50,855 --> 00:01:54,900
La canalización identificó correctamente
todas las líneas de código fuente de Java

37
00:01:54,900 --> 00:01:57,275
que contenían la palabra clave "import".

38
00:01:57,555 --> 00:01:59,185
En lo que queda del lab

39
00:01:59,185 --> 00:02:01,730
tomarán este código fuente
de la canalización

40
00:02:01,730 --> 00:02:05,375
y lo prepararán para ejecutarlo
en la plataforma Google Cloud Dataflow.

41
00:02:05,980 --> 00:02:09,060
Pero antes, debemos
seguir algunos pasos previos.

42
00:02:09,060 --> 00:02:12,820
Primero, debemos buscar
las API de Dataflow en GCP

43
00:02:12,820 --> 00:02:17,070
y habilitarlas
con el botón "Enable" que aparece aquí.

44
00:02:17,640 --> 00:02:19,115
Esto tomará unos minutos

45
00:02:19,115 --> 00:02:23,965
por lo que aceleraremos
el video mientras se habilitan las API.

46
00:02:25,125 --> 00:02:28,490
Podemos confirmar
que se habilitaron las API

47
00:02:28,810 --> 00:02:32,555
si vemos el botón "Disable"
en la pantalla "Dataflow API".

48
00:02:33,185 --> 00:02:35,620
Luego, debemos
asegurarnos de haber creado

49
00:02:35,620 --> 00:02:38,365
un depósito en Cloud Storage
para nuestra canalización.

50
00:02:38,555 --> 00:02:40,000
Podemos crear el depósito

51
00:02:40,000 --> 00:02:43,080
y es importante
que le asignemos un nombre único

52
00:02:43,080 --> 00:02:45,735
y que esté configurado
como depósito regional.

53
00:02:46,145 --> 00:02:50,750
Asigné us-east4
en la región de Virginia del Norte.

54
00:02:51,830 --> 00:02:53,735
Con el depósito listo

55
00:02:53,735 --> 00:02:56,370
copiarán los archivos
del código fuente de la entrada

56
00:02:56,370 --> 00:02:59,760
para la canalización de Cloud Shell
al depósito de Google Cloud.

57
00:02:59,990 --> 00:03:02,360
Para ello, usamos el comando gscopy.

58
00:03:02,360 --> 00:03:05,940
Recuerden que deseamos copiar
los archivos de código fuente de Java

59
00:03:05,940 --> 00:03:09,690
para la canalización
porque esta no tiene acceso

60
00:03:09,690 --> 00:03:11,490
al sistema de archivos de Cloud Shell

61
00:03:11,490 --> 00:03:14,375
mientras se ejecuta
en Google Cloud Dataflow.

62
00:03:15,295 --> 00:03:18,535
Una vez que el comando gsutil cp
termina de copiar los archivos

63
00:03:18,535 --> 00:03:22,080
podemos volver al depósito en
Google Cloud Storage en el navegador.

64
00:03:22,080 --> 00:03:27,425
Actualizamos la página y confirmamos
que los archivos se copiaron con éxito.

65
00:03:28,035 --> 00:03:30,470
Estos son los cuatro archivos
de Java que se usarán

66
00:03:30,470 --> 00:03:34,295
como entrada para la canalización
que se ejecuta en Google Cloud Dataflow.

67
00:03:34,505 --> 00:03:37,845
Ahora, veamos el código fuente
de la implementación de la canalización

68
00:03:37,845 --> 00:03:42,045
que se modificó para ejecutarse en
la plataforma de Google Cloud Dataflow.

69
00:03:42,485 --> 00:03:45,265
Está en el archivo grepc.py.

70
00:03:46,135 --> 00:03:50,195
Observen que este usa constantes
para los nombres de proyectos y depósitos.

71
00:03:50,195 --> 00:03:55,430
En mi caso, usé la misma ID única
para el proyecto y el depósito.

72
00:03:55,430 --> 00:03:57,800
Les voy a asignar el mismo valor a ambos.

73
00:03:58,360 --> 00:04:01,050
El código también
especifica algunos parámetros

74
00:04:01,050 --> 00:04:04,130
que necesitaré para ejecutar
esta canalización en Cloud Dataflow.

75
00:04:04,130 --> 00:04:07,330
Por ejemplo, hay que especificar
el nombre del trabajo que ejecuta

76
00:04:07,330 --> 00:04:12,520
la canalización y el runner de Dataflow
que ejecutará la canalización en Dataflow.

77
00:04:12,900 --> 00:04:15,980
Aquí, la entrada
y la salida se especifican

78
00:04:15,980 --> 00:04:19,329
como rutas al depósito
de Google Cloud Storage.

79
00:04:20,570 --> 00:04:23,535
El resto del código
de la canalización no cambia.

80
00:04:23,945 --> 00:04:26,010
Para ejecutar la canalización en Dataflow

81
00:04:26,010 --> 00:04:29,160
seguimos usando el comando python
y pasamos como argumentos

82
00:04:29,160 --> 00:04:32,920
el nombre de archivo del código fuente
de la implementación de la canalización.

83
00:04:33,110 --> 00:04:36,710
Ya que el código fuente
utilizó el runner de Dataflow

84
00:04:36,710 --> 00:04:40,710
el código se empaquetará
como bibliotecas de Dataflow

85
00:04:40,710 --> 00:04:43,670
y se enviará como un trabajo
que ejecutará una canalización

86
00:04:43,670 --> 00:04:46,360
en la plataforma de Google Cloud Dataflow.

87
00:04:46,870 --> 00:04:49,380
Cuando el comando python
termina de ejecutarse

88
00:04:49,380 --> 00:04:53,220
volveremos a GCP y abriremos Dataflow

89
00:04:53,220 --> 00:04:56,925
con el menú de tres líneas,
a la izquierda, o la barra de búsqueda.

90
00:04:57,285 --> 00:04:59,050
Desde el panel de control de Dataflow

91
00:04:59,050 --> 00:05:03,045
podemos supervisar la canalización
que enviamos como uno de los trabajos.

92
00:05:03,765 --> 00:05:06,245
Aquí, el trabajo se llama "example2"

93
00:05:06,245 --> 00:05:09,415
porque ese fue el nombre que utilicé
en el archivo grepc.py.

94
00:05:09,655 --> 00:05:13,010
En primer lugar, observen
que el trabajo no terminó de iniciarse.

95
00:05:13,010 --> 00:05:15,510
Dice que se está ajustando
la escala automáticamente

96
00:05:15,510 --> 00:05:19,505
y muestra que solo está usando
un núcleo virtual para la ejecución.

97
00:05:19,915 --> 00:05:23,070
Al lado derecho, podemos
ver opciones de la canalización

98
00:05:23,070 --> 00:05:25,305
y otra información acerca del trabajo.

99
00:05:25,585 --> 00:05:27,707
En la sección de registro podemos ver

100
00:05:27,707 --> 00:05:29,830
que la canalización
aún no se está ejecutando

101
00:05:29,830 --> 00:05:32,145
porque está iniciando
uno de los trabajadores.

102
00:05:32,145 --> 00:05:36,395
Para confirmarlo, podemos mirar
el gráfico en la sección Autoscaling.

103
00:05:37,075 --> 00:05:41,455
Podemos ver que el trabajo
espera usar un trabajador.

104
00:05:41,455 --> 00:05:45,160
Ahora, la cantidad de trabajadores
aumentó de cero a uno.

105
00:05:45,160 --> 00:05:49,045
Esto significa que se aprovisionó
exactamente una instancia virtual

106
00:05:49,045 --> 00:05:50,835
para ejecutar esta canalización.

107
00:05:51,655 --> 00:05:54,770
Esta canalización tardará
unos minutos en terminar de ejecutarse.

108
00:05:54,770 --> 00:05:58,170
Por eso, el video
se adelantará unos minutos

109
00:05:58,170 --> 00:06:00,265
hasta que se complete el trabajo.

110
00:06:00,915 --> 00:06:03,280
Si vemos la canalización

111
00:06:03,280 --> 00:06:05,250
las marcas verdes indican

112
00:06:05,250 --> 00:06:08,135
que se completaron
todos los pasos de las transformaciones.

113
00:06:08,295 --> 00:06:11,430
Si revisamos el gráfico
de abajo a la derecha

114
00:06:11,430 --> 00:06:15,000
veremos que todos los trabajadores
que usamos para ejecutar la canalización

115
00:06:15,000 --> 00:06:16,220
redujeron su escala.

116
00:06:16,790 --> 00:06:19,460
Para ver la salida de esta canalización

117
00:06:19,460 --> 00:06:24,195
copiamos los archivos de salida de
Google Cloud Storage a Cloud Shell.

118
00:06:24,915 --> 00:06:26,540
Una vez copiados los archivos

119
00:06:26,540 --> 00:06:28,995
podemos revisarlos
directamente en Cloud Shell

120
00:06:28,995 --> 00:06:32,630
o podemos abrir
Google Cloud Storage en el navegador

121
00:06:32,630 --> 00:06:36,475
donde encontraremos los archivos
en el depósito, en la carpeta Java Help.

122
00:06:36,475 --> 00:06:39,230
Los archivos tendrán
un prefijo de salida.

123
00:06:39,230 --> 00:06:43,940
Los nombres tendrán el formato
"0 of 4", "01 of 4", "02 of 4", etcétera.

124
00:06:44,555 --> 00:06:46,750
Para revisar el contenido de los archivos

125
00:06:46,750 --> 00:06:50,530
se debe usar la casilla de verificación
"Public link", que ven a la derecha.

126
00:06:50,860 --> 00:06:53,560
Aquí, observamos
el contenido del primer archivo.