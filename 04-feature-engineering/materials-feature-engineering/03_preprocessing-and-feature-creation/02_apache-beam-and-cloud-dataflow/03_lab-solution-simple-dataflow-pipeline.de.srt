1
00:00:00,000 --> 00:00:02,300
Für dieses Lab
müssen Sie sich den Quellcode

2
00:00:02,300 --> 00:00:04,890
aus GitHub
in Ihre Cloud Shell-Umgebung kopieren.

3
00:00:04,890 --> 00:00:08,720
Außerdem müssen Sie ein Skript ausführen,
um einige Bibliotheken herunterzuladen,

4
00:00:08,720 --> 00:00:11,075
die die Abhängigkeiten
für Ihre Pipeline erfüllen.

5
00:00:11,075 --> 00:00:13,585
Diese Schritte dauern ein paar Minuten.

6
00:00:13,585 --> 00:00:18,820
Daher spulen wir das Video nun vor,
bis der Quellcode installiert

7
00:00:18,820 --> 00:00:22,155
und die Bibliotheken
heruntergeladen wurden.

8
00:00:22,155 --> 00:00:25,230
In Cloud Shell
können Sie den Quellcode der Pipeline

9
00:00:25,230 --> 00:00:27,560
mit verschiedenen Editoren betrachten.

10
00:00:27,560 --> 00:00:30,695
Sie können einen
textbasierten Editor wie Nano benutzen.

11
00:00:30,695 --> 00:00:34,230
Aber in diesem Video
werde ich einen grafischen Editor benutzen

12
00:00:34,230 --> 00:00:36,165
der in Cloud Shell integriert ist.

13
00:00:36,165 --> 00:00:39,620
Wenn dieser Editor geladen ist,
können Sie im linken Menü

14
00:00:39,620 --> 00:00:41,990
die Ordner "training-data-analyst",

15
00:00:41,990 --> 00:00:46,285
"courses", "data_analysis",
"lab2" und "python" öffnen

16
00:00:46,285 --> 00:00:50,180
und in der Datei "grep.py"
auf den Quellcode der Pipeline zugreifen.

17
00:00:50,180 --> 00:00:57,140
Der Quellcode nimmt die hier in Zeile 26
hervorgehobenen Java-Dateien als Eingabe.

18
00:00:57,140 --> 00:01:02,045
Die Platzhalter-Anweisung
definiert also die zu nutzende Java-Datei.

19
00:01:02,045 --> 00:01:04,930
Für jede der Dateien
sucht die Transformation

20
00:01:04,930 --> 00:01:08,825
im Java-Quellcode nach Zeilen,
die den Suchbegriff enthalten.

21
00:01:08,825 --> 00:01:11,475
Dieser Suchbegriff lautet "import".

22
00:01:11,475 --> 00:01:17,555
In den Zeilen 32 bis 34 finden Sie
die Details zur Pipelineimplementierung.

23
00:01:17,555 --> 00:01:20,780
Der Schritt "grep" dieser Pipeline

24
00:01:20,780 --> 00:01:24,335
nutzt die Methode "my_grep",
die in Zeile 20 definiert wird.

25
00:01:24,335 --> 00:01:28,230
Die Methode "my_grep"
sucht nach dem Suchbegriff "import".

26
00:01:28,230 --> 00:01:31,470
Für alle Zeilen,
die diesen Begriff enthalten,

27
00:01:31,470 --> 00:01:35,675
wird das Ergebnis in das Verzeichnis
"/tmp/output" geschrieben.

28
00:01:35,675 --> 00:01:37,750
Zum Ausführen der Pipeline in Cloud Shell

29
00:01:37,750 --> 00:01:39,680
verwenden Sie einfach den Python-Befehl

30
00:01:39,680 --> 00:01:43,890
und übergeben den Namen der Quellcodedatei
mit der Pipelineimplementierung.

31
00:01:43,890 --> 00:01:46,310
Die Pipeline
wurde erfolgreich abgeschlossen.

32
00:01:46,310 --> 00:01:50,395
Das bestätigt Ihnen ein Blick
in die von ihr erstellten Ausgabedateien.

33
00:01:50,395 --> 00:01:54,660
Die Pipeline hat alle Zeilen
des Java-Quellcodes korrekt identifiziert,

34
00:01:54,660 --> 00:01:57,095
die den Suchbegriff "import" enthalten.

35
00:01:57,095 --> 00:02:01,320
Im übrigen Teil des Labs
bereiten Sie diesen Pipelinequellcode

36
00:02:01,320 --> 00:02:05,375
für die Ausführung
in Google Cloud Dataflow vor.

37
00:02:05,375 --> 00:02:06,830
Bevor Sie das aber tun können,

38
00:02:06,830 --> 00:02:08,750
sind ein paar weitere Schritte nötig.

39
00:02:08,750 --> 00:02:12,800
Zunächst müssen Sie
in der GCP nach der Dataflow API suchen

40
00:02:12,800 --> 00:02:17,070
und diese über
die entsprechende Schaltfläche aktivieren.

41
00:02:17,070 --> 00:02:21,235
Dies dauert eine Weile,
daher spulen wir das Video vor,

42
00:02:21,235 --> 00:02:23,925
bis die API aktiviert ist.

43
00:02:23,925 --> 00:02:28,700
Die API ist aktiviert,

44
00:02:28,700 --> 00:02:32,555
wenn Sie neben "Dataflow API"
eine Schaltfläche zum Deaktivieren sehen.

45
00:02:32,555 --> 00:02:34,710
Vergewissern Sie sich als Nächstes,

46
00:02:34,710 --> 00:02:38,175
dass Sie einen Cloud Storage-Bucket
für Ihre Pipeline erstellt haben.

47
00:02:38,175 --> 00:02:40,070
Sie können diesen Bucket erstellen.

48
00:02:40,070 --> 00:02:41,940
Es ist wichtig, dass Sie dem Bucket

49
00:02:41,940 --> 00:02:45,735
einen eindeutigen Namen geben
und ihn als regionalen Bucket einrichten.

50
00:02:45,735 --> 00:02:50,750
Ich habe hier den Speicherort "us-east4",
die Region Northern Virginia, zugewiesen.

51
00:02:50,750 --> 00:02:53,305
Okay. Wenn der Bucket fertig ist,

52
00:02:53,305 --> 00:02:56,530
kopieren Sie die Eingabedateien
mit dem Quellcode für die Pipeline

53
00:02:56,530 --> 00:02:59,760
aus der Cloud Shell
in den Google Cloud Storage-Bucket.

54
00:02:59,760 --> 00:03:02,360
Dafür geben Sie den Befehl "gsutil cp" ein.

55
00:03:02,360 --> 00:03:06,660
Sie kopieren
die Java-Quellcodedateien Ihrer Pipeline,

56
00:03:06,660 --> 00:03:11,210
weil die Pipeline keinen Zugriff
auf Ihr Cloud Shell-Dateisystem hat,

57
00:03:11,210 --> 00:03:14,350
während sie
in Google Cloud Dataflow ausgeführt wird.

58
00:03:14,350 --> 00:03:18,695
Nachdem der Befehl "gsutil cp"
alle Dateien kopiert hat,

59
00:03:18,695 --> 00:03:22,400
können Sie im Browser
zum Cloud Storage-Bucket zurückkehren,

60
00:03:22,400 --> 00:03:27,425
die Seite aktualisieren und überprüfen,
ob die Dateien erfolgreich kopiert wurden.

61
00:03:27,425 --> 00:03:31,290
Hier sind die vier Java-Dateien,
die Ihre Pipeline als Eingabe nimmt,

62
00:03:31,290 --> 00:03:33,925
wenn sie
in Google Cloud Dataflow ausgeführt wird.

63
00:03:33,925 --> 00:03:36,795
Werfen Sie als Nächstes
einen Blick auf den Quellcode

64
00:03:36,795 --> 00:03:38,785
für die Implementierung der Pipeline,

65
00:03:38,785 --> 00:03:42,045
der für die Ausführung
in Cloud Dataflow modifiziert wurde.

66
00:03:42,045 --> 00:03:45,265
Er befindet sich in der Datei "grepc.py".

67
00:03:45,265 --> 00:03:50,195
Beachten Sie, dass Konstanten als Namen
für Projekte und Buckets benutzt werden.

68
00:03:50,195 --> 00:03:55,430
Ich habe hier dieselbe eindeutige ID
für das Projekt und den Bucket gewählt.

69
00:03:55,430 --> 00:03:57,800
Ich setze also
für beide denselben Wert ein.

70
00:03:57,800 --> 00:04:00,430
Im Code werden
auch einige Parameter spezifiziert,

71
00:04:00,430 --> 00:04:03,860
die zum Ausführen dieser Pipeline
in Cloud Dataflow erforderlich sind.

72
00:04:03,860 --> 00:04:09,760
Sie müssen zum Beispiel den Namen des Jobs
und den Dataflow-Runner angeben,

73
00:04:09,760 --> 00:04:12,520
die Ihre Pipeline
in Dataflow ausführen sollen.

74
00:04:12,520 --> 00:04:14,930
Hier werden Eingabe und Ausgabe

75
00:04:14,930 --> 00:04:18,869
als Pfade zu Ihrem
Google Cloud Storage-Bucket angegeben.

76
00:04:20,490 --> 00:04:23,535
Der Rest des Codes
für die Pipeline bleibt unverändert.

77
00:04:23,535 --> 00:04:27,320
In Dataflow führen Sie die Pipeline
wieder mit dem Python-Befehl aus

78
00:04:27,320 --> 00:04:29,020
und übergeben als Argument

79
00:04:29,020 --> 00:04:32,520
den Namen der Datei mit dem Quellcode
Ihrer Pipelineimplementierung.

80
00:04:32,520 --> 00:04:36,830
Da der Quellcode
den Dataflow-Runner benutzt,

81
00:04:36,830 --> 00:04:40,320
wird der Code hier
zu Dataflow-Bibliotheken gepackt

82
00:04:40,320 --> 00:04:42,055
und als Job übergeben,

83
00:04:42,055 --> 00:04:46,360
der eine Pipeline
in Google Cloud Dataflow ausführt.

84
00:04:46,360 --> 00:04:49,380
Nachdem der Python-Befehl ausgeführt wurde,

85
00:04:49,380 --> 00:04:52,990
kehren Sie zur GCP zurück
und öffnen Dataflow

86
00:04:52,990 --> 00:04:56,925
über das Dreistrich-Menü links
oder über die Suchleiste.

87
00:04:56,925 --> 00:04:59,670
Vom Dataflow-Dashboard aus
können Sie die Pipeline,

88
00:04:59,670 --> 00:05:03,045
die Sie gerade gesendet haben,
als einen der Jobs überwachen.

89
00:05:03,045 --> 00:05:06,145
Hier heißt der Job "examplejob2",

90
00:05:06,145 --> 00:05:09,415
weil ich diesen Namen
in der Datei "grepc.py" angegeben habe.

91
00:05:09,415 --> 00:05:12,970
Als Erstes fällt auf, dass der Job
noch nicht vollständig gestartet wurde.

92
00:05:12,970 --> 00:05:15,150
Laut der Anzeige skaliert er automatisch

93
00:05:15,150 --> 00:05:19,505
und nutzt aktuell für die Ausführung
nur einen einzigen virtuellen Kern.

94
00:05:19,505 --> 00:05:22,810
Auf der rechten Seite
können Sie auch Pipelineoptionen

95
00:05:22,810 --> 00:05:25,145
und andere Informationen zum Job sehen.

96
00:05:25,145 --> 00:05:27,190
Im Abschnitt "Logs" sehen Sie,

97
00:05:27,190 --> 00:05:29,547
dass die Pipeline
noch nicht ausgeführt wird,

98
00:05:29,547 --> 00:05:32,285
weil sie noch einen der Worker startet.

99
00:05:32,285 --> 00:05:36,395
Das bestätigt auch ein Blick
auf die Grafik im Abschnitt "Autoscaling".

100
00:05:36,395 --> 00:05:41,215
Hier sehen Sie, dass der Job erwartet,
einen Worker zu nutzen,

101
00:05:41,215 --> 00:05:45,160
und die Zahl der Worker
aktuell von null auf eins gestiegen ist.

102
00:05:45,160 --> 00:05:48,925
Es wurde also genau
eine virtuelle Instanz bereitgestellt,

103
00:05:48,925 --> 00:05:50,835
um diese Pipeline auszuführen.

104
00:05:50,835 --> 00:05:54,770
Es wird ein paar Minuten dauern,
bis diese Pipeline durchlaufen wurde.

105
00:05:54,770 --> 00:05:57,750
Daher spulen wir das Video nun vor,

106
00:05:57,750 --> 00:06:00,265
bis der Job erledigt ist.

107
00:06:00,265 --> 00:06:02,620
Wenn Sie sich die Pipeline genauer ansehen,

108
00:06:02,620 --> 00:06:04,700
können Sie an den grünen Häkchen erkennen,

109
00:06:04,700 --> 00:06:07,955
dass alle Einzelschritte
der Transformationen abgeschlossen wurden.

110
00:06:07,955 --> 00:06:11,390
Die Grafik unten rechts zeigt,

111
00:06:11,390 --> 00:06:16,440
dass alle zur Ausführung der Pipeline
genutzten Worker herunterskaliert wurden.

112
00:06:16,440 --> 00:06:19,210
Sie können sich
die Ausgabe dieser Pipeline ansehen.

113
00:06:19,210 --> 00:06:24,195
Kopieren Sie dazu die Ausgabedateien
von Google Cloud Storage nach Cloud Shell.

114
00:06:24,195 --> 00:06:26,240
Sobald die Dateien kopiert wurden,

115
00:06:26,240 --> 00:06:29,115
können Sie sie
direkt in Cloud Shell überprüfen.

116
00:06:29,115 --> 00:06:32,470
Wahlweise öffnen Sie
Google Cloud Storage in Ihrem Browser

117
00:06:32,470 --> 00:06:35,945
und suchen nach den Dateien
in Ihrem Bucket im Ordner "javahelp".

118
00:06:35,945 --> 00:06:39,030
Die Dateinamen
starten mit dem Präfix "output"

119
00:06:39,030 --> 00:06:44,100
und sind danach nummeriert
mit "00000-of-00004", "00001-of-00004" usw.

120
00:06:44,105 --> 00:06:46,580
Wenn Sie
die Dateiinhalte überprüfen möchten,

121
00:06:46,580 --> 00:06:50,530
müssen Sie rechts
das Kästchen "Öffentlicher Link" anklicken.

122
00:06:50,530 --> 00:06:53,880
Hier sehen Sie den Inhalt der ersten Datei.