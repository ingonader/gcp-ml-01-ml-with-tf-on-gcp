Bonjour, je m'appelle Carl Osipov et je suis chef de projet chez Google. Je travaille avec nos clients Google Cloud pour les aider à déployer
des systèmes de machine learning évolutifs et prêts à passer en production. Cette section du module traite
du prétraitement des données d'entrée et de la création de caractéristiques, deux techniques qui peuvent vous aider à préparer un ensemble de caractéristiques
pour un système de machine learning. Pour commencer, vous verrez des exemples de prétraitement
et de création de caractéristiques, puis vous découvrirez les défis qu'impliquent ces techniques
pour l'extraction de caractéristiques. Dans les deux parties
restantes de cette section, vous verrez comment
des outils comme Cloud Dataflow et Cloud Dataprep peuvent
vous aider à relever ces défis. Voyons d'abord quelques exemples
qui vous permettront de comprendre quand procéder au prétraitement
et à la création de caractéristiques. Certaines valeurs d'un ensemble de
caractéristiques doivent être normalisées ou mises à l'échelle avant
d'être utilisées par le modèle de ML. Le scaling revient ici à remplacer
une caractéristique à valeurs réelles, comme un prix, par une plage comprise
entre 0 et 1 via la formule présentée ici. Le scaling peut être nécessaire
pour de nombreuses raisons, mais, la plupart du temps,
il sert à améliorer les performances de l'entraînement ML, en particulier
celles de la descente de gradient. Pour calculer la formule de scaling, vous devez connaître les valeurs minimale
et maximale d'une caractéristique. Si vous ne connaissez pas ces valeurs, vous devrez peut-être
prétraiter la totalité de l'ensemble de données pour les obtenir. Le prétraitement peut aussi être
utile pour les valeurs catégoriques de vos ensembles de données,
telles que les noms de villes, comme le montre cet extrait de code. Par exemple, pour utiliser dans
TensorFlow l'encodage en mode one-hot, qui permet de représenter des villes sous forme
de caractéristiques à valeurs binaires dans votre ensemble de caractéristiques, vous pouvez utiliser la méthode
"categorical_column_with_vocabulary_list" de l'API Layers. Pour utiliser cette méthode, vous devez
lui transmettre une liste de valeurs. Dans cet exemple, des noms de villes. Si vous n'avez pas ce dictionnaire
de valeurs pour une clé, vous pouvez le créer. C'est une étape de prétraitement
sur l'ensemble de données complet. Dans ce module, vous allez découvrir les technologies libres qui permettent
de mettre en œuvre le prétraitement. BigQuery et Apache Beam servent à traiter l'ensemble
de données complet avant l'entraînement, ce qui inclut des opérations telles que l'exclusion de certains points
de données de l'ensemble d'entraînement ainsi que le calcul
de statistiques récapitulatives et de vocabulaires sur l'ensemble complet. N'oubliez pas :
pour certaines caractéristiques, vous aurez besoin
de statistiques sur une période limitée. Vous voudrez par exemple
connaître le nombre moyen de produits vendus par un site Web
au cours de l'heure précédente. Pour ces caractéristiques
portant sur une période, vous utiliserez les pipelines de données
par lots et par flux de Beam. D'autres caractéristiques permettant
le prétraitement d'un point de données à la fois peuvent être mises en œuvre
directement dans TensorFlow ou avec Beam. Comme vous pouvez le voir, Apache Beam et la technologie complémentaire
de Google Cloud, appelée Cloud Dataflow, sont importants
pour cette partie du module. Je décrirai d'abord
les limitations de l'utilisation exclusive de BigQuery et de TensorFlow
pour extraire des caractéristiques. J'expliquerai ensuite l'intérêt de Beam. BigQuery est un entrepôt
de données ultra-évolutif, très rapide et entièrement géré disponible
sous forme de service Google Cloud. Il facilite l'extraction
de caractéristiques, car il permet d'utiliser le SQL standard pour
des tâches de prétraitement courantes. Par exemple, si vous prétraitez un ensemble de données contenant
10 milliards d'enregistrements sur des courses en taxi à New York, vous pouvez obtenir des données
fausses, comme des courses chères sur une distance nulle. Vous pouvez écrire une instruction SQL
pour exclure ces données de votre ensemble d'entraînement et l'exécuter
dans BigQuery en quelques secondes. Vous pouvez écrire d'autres instructions avec des fonctions mathématiques
et de traitement de données SQL standards. Elles peuvent être utiles pour effectuer
des calculs simples, comme des additions sur des données sources, et la conversion
dans des formats de données courants, par exemple pour extraire l'heure
d'enregistrements avec horodatage. Si vous utilisez SQL pour prétraiter
des exemples d'entraînement, il est primordial de mettre en œuvre exactement la même logique
de prétraitement dans TensorFlow. Vous verrez ensuite deux approches d'écriture de ce code
de prétraitement dans TensorFlow. Dans la pratique, vous utiliserez l'une ou l'autre, et parfois les deux. N'oubliez pas que de nombreuses étapes
de prétraitement courantes peuvent être écrites avec les méthodes existantes
de l'API Feature Columns de TensorFlow. Par exemple, si vous devez discrétiser
une caractéristique à valeur réelle, vous pouvez utiliser
la méthode "bucketized_column". Si l'étape de prétraitement nécessaire est
indisponible dans les API TensorFlow, vous pouvez modifier
les fonctions utilisées dans les paramètres
de la fonction "input_fn" pendant l'entraînement,
la validation et le test. Nous verrons ceci plus en détail
dans les prochaines diapositives. Avec la première option, vous mettez en
œuvre votre propre code de prétraitement. Dans cet exemple, il est empaqueté dans la méthode "add_engineered" et la mise en œuvre n'implique pas de statistiques globales
de l'ensemble de données source. Pour calculer la distance euclidienne depuis les coordonnées existantes
pour le point de données, le code renvoie uniquement le dictionnaire
de caractéristiques d'origine et la nouvelle valeur calculée
avec la formule de distance. Pour que la distance euclidienne
soit incluse dans les étapes d'entraînement,
d'évaluation et de diffusion, toutes les fonctions "input_fn"
correspondantes encapsulent l'appel à la méthode "add_engineered" autour de l'ensemble
de caractéristiques non prétraité. Si l'étape de prétraitement nécessaire
existe déjà dans l'API TensorFlow, il vous suffit d'appeler
la méthode d'aide appropriée lors de la définition de la liste
des colonnes de caractéristiques. Dans cet exemple,
la méthode "bucketized_column" permet de récupérer la latitude
à partir des données sources et de vérifier
que les valeurs se situent entre 38 et 42. Les valeurs d'origine
de la latitude sont ensuite placées dans l'un des buckets mutuellement
exclusifs. Ainsi, le nombre de buckets de la plage est contrôlé
par le paramètre du bucket de fin. Continuer d'utiliser du SQL
pour le code de prétraitement dans BigQuery et TensorFlow peut
devenir complexe et difficile à gérer. Comme vous l'avez vu, utiliser Apache Beam pour prétraiter les caractéristiques permet de conserver le même code pendant
l'entraînement et la diffusion du modèle. Cependant, Apache Beam ne permet pas d'accéder aux méthodes
d'aide pratiques de TensorFlow. Comme dans cet exemple, vous devrez donc mettre en œuvre
votre propre code de prétraitement. Dans cette partie du module, vous avez vu des exemples spécifiques dans lesquels
Apache Beam facilite le prétraitement.