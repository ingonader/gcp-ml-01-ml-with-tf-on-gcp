1
00:00:00,000 --> 00:00:02,130
Hi. My name is Carl Osipov,

2
00:00:02,130 --> 00:00:03,960
and I'm a program manager at Google.

3
00:00:03,960 --> 00:00:06,480
I work with our customers that use Google Cloud,

4
00:00:06,480 --> 00:00:08,940
and I help them succeed with deploying machine learning

5
00:00:08,940 --> 00:00:12,390
systems that are scalable and production ready.

6
00:00:12,390 --> 00:00:17,985
This section of the module covers input data preprocessing and feature creation,

7
00:00:17,985 --> 00:00:19,910
which are two techniques that can help you

8
00:00:19,910 --> 00:00:22,900
prepare a feature set for a machine learning system.

9
00:00:22,900 --> 00:00:27,860
To get started, you'll take a look at examples of pre-processing and feature creation,

10
00:00:27,860 --> 00:00:30,020
and learn about the challenges involved in

11
00:00:30,020 --> 00:00:32,800
applying these techniques as part of feature engineering.

12
00:00:32,800 --> 00:00:36,300
Then, in the remaining two parts of the section,

13
00:00:36,300 --> 00:00:39,050
you will see how tools like Google Cloud Dataflow

14
00:00:39,050 --> 00:00:42,585
and Cloud Dataprep can help you with these challenges.

15
00:00:42,585 --> 00:00:46,790
Okay. First, here are a few examples that will give you

16
00:00:46,790 --> 00:00:51,230
some intuition as to when you should use preprocessing and feature creation.

17
00:00:51,230 --> 00:00:54,560
Some values in a feature set need to be normalized or

18
00:00:54,560 --> 00:00:58,865
rescaled before they should be used by machine learning by the AML.

19
00:00:58,865 --> 00:01:02,600
Here,a scaling means changing a real valued feature like

20
00:01:02,600 --> 00:01:07,275
a price to a range from zero to one using the formula shown.

21
00:01:07,275 --> 00:01:10,175
Rescaling can be done for many reasons.

22
00:01:10,175 --> 00:01:14,045
But most of the time, it's done to improve the performance of ML training,

23
00:01:14,045 --> 00:01:17,075
specifically, the performance of gradient descent.

24
00:01:17,075 --> 00:01:20,715
Notice that to compute the rescaling formula,

25
00:01:20,715 --> 00:01:24,570
you need to know both the minimum and maximum values for a feature.

26
00:01:24,570 --> 00:01:26,415
If you don't know these values,

27
00:01:26,415 --> 00:01:30,435
you may need to preprocess your entire dataset to find.

28
00:01:30,435 --> 00:01:34,450
Preprocessing can also be useful for categorical values in

29
00:01:34,450 --> 00:01:39,135
the datasets like names of cities as shown in the code snippet on the slide.

30
00:01:39,135 --> 00:01:43,555
For example, to use a one hadan coding technique in TensorFlow,

31
00:01:43,555 --> 00:01:45,925
which will help you represent different cities

32
00:01:45,925 --> 00:01:48,915
as binary valued features in your feature set,

33
00:01:48,915 --> 00:01:54,840
you can use the categorical_column_with_vocabulary_list method from the layers API.

34
00:01:54,840 --> 00:01:58,255
To use this method, you need to pass out a list of values,

35
00:01:58,255 --> 00:02:00,880
which in this example are different city names.

36
00:02:00,880 --> 00:02:03,985
If you don't have this dictionary of values for a key,

37
00:02:03,985 --> 00:02:05,505
you may also want to create it,

38
00:02:05,505 --> 00:02:08,785
was a preprocessing step over the entire dataset.

39
00:02:08,785 --> 00:02:11,025
In this module, you'll learn about

40
00:02:11,025 --> 00:02:14,375
free technologies that will help you implement pre-processing.

41
00:02:14,375 --> 00:02:17,210
BigQuery and Apache Beam will be used to

42
00:02:17,210 --> 00:02:20,750
process the full input dataset prior to training.

43
00:02:20,750 --> 00:02:26,210
This covers operation like excluding some data points from the training dataset and also,

44
00:02:26,210 --> 00:02:30,725
computing summary statistics and vocabularies over the entire input dataset.

45
00:02:30,725 --> 00:02:32,915
Keep in mind that for some features,

46
00:02:32,915 --> 00:02:35,780
you will need statistics over a limited time window.

47
00:02:35,780 --> 00:02:38,210
For example, if you need to know the average number of

48
00:02:38,210 --> 00:02:41,310
products sold by a website over the past hour,

49
00:02:41,310 --> 00:02:43,660
for these types of time-windowed features,

50
00:02:43,660 --> 00:02:47,075
you will use Beam's batch and streaming data pipelines.

51
00:02:47,075 --> 00:02:50,195
Other features that can be pre-processed one data point at

52
00:02:50,195 --> 00:02:54,855
a time can be implemented either in TensorFlow directly or using Beam.

53
00:02:54,855 --> 00:02:57,230
So, as you can see, Apache Beam and

54
00:02:57,230 --> 00:03:00,040
the complementary Google Cloud technology called

55
00:03:00,040 --> 00:03:03,590
Cloud Dataflow will be important to this part of the module.

56
00:03:03,590 --> 00:03:06,635
So, first, I will describe some limitations of using

57
00:03:06,635 --> 00:03:09,850
only BigQuery and TensorFlow for feature engineering,

58
00:03:09,850 --> 00:03:12,330
and then, explain how Beam can help.

59
00:03:12,330 --> 00:03:15,605
BigQuery is a massively scalable, very fast,

60
00:03:15,605 --> 00:03:19,950
and a fully managed data warehouse available as a service from Google Cloud.

61
00:03:19,950 --> 00:03:23,590
BigQuery can help you as feature engineering because it lets

62
00:03:23,590 --> 00:03:27,160
you use standard sequel to implement common preprocessing tasks.

63
00:03:27,160 --> 00:03:29,670
For example, if you are preprocessing

64
00:03:29,670 --> 00:03:33,620
a dataset was 10 billion records of text he writes in New York City,

65
00:03:33,620 --> 00:03:37,750
some of the records may happen to have bogus data like expensive rides,

66
00:03:37,750 --> 00:03:39,730
showing a distance of zero miles.

67
00:03:39,730 --> 00:03:43,975
You can write a sequel statement to filter out the bogus data from your training examples

68
00:03:43,975 --> 00:03:48,300
dataset and run the sequel on BigQuery in seconds.

69
00:03:48,300 --> 00:03:51,230
Of course, you can also write other statements

70
00:03:51,230 --> 00:03:54,575
using standard sequel math and data processing functions.

71
00:03:54,575 --> 00:03:59,630
These can be valuable for simple calculations like additions over source data and also,

72
00:03:59,630 --> 00:04:02,510
for parsing common data formats, for instance,

73
00:04:02,510 --> 00:04:06,655
to extract details about the time of date from records with timestamps.

74
00:04:06,655 --> 00:04:10,600
If you do decide to use sequel to pre-process training examples,

75
00:04:10,600 --> 00:04:13,590
it is absolutely critical that you take care to

76
00:04:13,590 --> 00:04:17,325
implement exactly the same preprocessing logic in TensorFlow.

77
00:04:17,325 --> 00:04:20,035
Next, you will see two approaches for

78
00:04:20,035 --> 00:04:23,125
how to write this pre-processing code in TensorFlow.

79
00:04:23,125 --> 00:04:26,430
In practice, you may find yourself using the first,

80
00:04:26,430 --> 00:04:29,435
or the second approach, and sometimes you may use both.

81
00:04:29,435 --> 00:04:33,530
Keep in mind that many common preprocessing steps can be

82
00:04:33,530 --> 00:04:38,350
written using one of the existing methods from the TensorFlow of feature columns API.

83
00:04:38,350 --> 00:04:42,760
For example, if you need to change a real value feature into a discrete one,

84
00:04:42,760 --> 00:04:45,350
you can use the bucket dice column method.

85
00:04:45,350 --> 00:04:50,490
If the feature pre-processing step that you need is not available in the TensorFlow APIs,

86
00:04:50,490 --> 00:04:52,460
you can modify the functions used in

87
00:04:52,460 --> 00:04:56,360
the input parameters during training, validation, and test.

88
00:04:56,360 --> 00:05:00,600
The upcoming slides will explain this in more detail.

89
00:05:01,620 --> 00:05:06,670
Was the first option you implement your own pre-processing code.

90
00:05:06,670 --> 00:05:09,505
In this example, the pre-processing code is

91
00:05:09,505 --> 00:05:12,640
packaged in the add engineered method and

92
00:05:12,640 --> 00:05:16,945
the implementation does not need any global statistics from the source dataset.

93
00:05:16,945 --> 00:05:19,510
To compute the euclidean distance feature from

94
00:05:19,510 --> 00:05:22,135
the existing latlong coordinates for datapoint,

95
00:05:22,135 --> 00:05:25,220
the code just returns the original features dictionary

96
00:05:25,220 --> 00:05:29,380
along with the new feature value computed using the distance formula.

97
00:05:29,380 --> 00:05:33,370
To ensure that the euclidean distance feature gets included during training,

98
00:05:33,370 --> 00:05:35,675
evaluation, and, serving steps,

99
00:05:35,675 --> 00:05:39,525
all of the corresponding input_fn functions wrap the call to

100
00:05:39,525 --> 00:05:43,780
add_engineered method around the an pre-processed feature set.

101
00:05:43,780 --> 00:05:48,990
If the pre-processing step that you need already exists in the TensorFlow API,

102
00:05:48,990 --> 00:05:51,040
you're in luck because you can simply call

103
00:05:51,040 --> 00:05:54,710
the appropriate helper methods when defining your feature columns list.

104
00:05:54,710 --> 00:05:57,880
In this example, the bucketized_column method

105
00:05:57,880 --> 00:06:01,030
is used to take the latitude coordinates from the source data,

106
00:06:01,030 --> 00:06:05,485
and make sure that the values are in the range from 38 and 42.

107
00:06:05,485 --> 00:06:09,760
Next, the original values for the latitude are placed into one of

108
00:06:09,760 --> 00:06:13,140
the several mutually exclusive buckets such at the number

109
00:06:13,140 --> 00:06:16,885
of the buckets in the range is controlled by the end bucket's parameter.

110
00:06:16,885 --> 00:06:20,080
Maintaining pre-processing code in sequel for

111
00:06:20,080 --> 00:06:24,460
BigQuery and in TensorFlow can get complex and difficult to manage.

112
00:06:24,460 --> 00:06:26,100
As you can see on earlier,

113
00:06:26,100 --> 00:06:28,840
one of the advantages of using Apache Beam to

114
00:06:28,840 --> 00:06:30,910
pre-process features is that

115
00:06:30,910 --> 00:06:35,460
the same code can be used during both training and serving of a model.

116
00:06:35,460 --> 00:06:38,225
However, when using Apache Beam,

117
00:06:38,225 --> 00:06:42,445
you will not have access to the convenient helper methods from TensorFlow.

118
00:06:42,445 --> 00:06:45,370
This means as shown in this example that you

119
00:06:45,370 --> 00:06:48,140
will need to implement your own pre-processing code.

120
00:06:48,140 --> 00:06:49,470
In this part of the module,

121
00:06:49,470 --> 00:06:55,520
you have reviewed specific examples where Apache Beam can help you was pre-processing.