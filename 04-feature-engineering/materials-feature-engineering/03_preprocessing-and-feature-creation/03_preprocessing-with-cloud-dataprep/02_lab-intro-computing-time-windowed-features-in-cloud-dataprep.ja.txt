このラボでは BigQueryのタクシー料金のデータセットを Dataprepで前処理します データ値の分散を調査し ヒストグラムで可視化し Dataprepフローで 1時間あたりの平均利用数に基づく 移動時間枠による特徴量を作成します 最後にフローをGCPにデプロイして実行し Dataflowでジョブの実行状況を
モニタリングします ラボを始める前の 準備からです GCPダッシュボードを開きます まず Google Cloud Storageの
バケットを作成します ハンバーガーアイコンをクリックして [Storage] > [ブラウザ]をクリックして [バケットを作成]を選択します バケット名はグローバルに 重複しない名前にする必要があります 固有のバケット名を入力し
ロケーションはus-east4としました [作成]をクリックするとすぐに バケットの準備ができました 次に BigQueryデータセットの準備です [Products & Services]メニューの [ビッグデータ]セクションの [BigQuery]をクリックします ブラウザで新しいタブが開きます プロジェクト名の右にある 下向きの矢印をクリックして
[Create new dataset]を選択します データセットIDを「taxi_cab_reporting」として
[OK]をクリックして作成します データセットができたら GCPダッシュボードに戻って [Dataprep]をクリックします Dataprepはパートナーが
提供するサービスなので [ACCEPT]をクリックして
利用規約に同意する必要があります さらに [Allow]をクリックして Dataprepの提供元に
データアクセスを許可します Dataprepが有効になるまで
しばらくかかるため 早送りで先に進めます 次に Dataprepに使用する
アカウントを選択し [ALLOW]をクリックします プロジェクトに初めて
Dataprepを設定する場合 データを保存するバケットを指定します ラボの初めに作成したバケットを選択し [Use this folder]をクリックします バケットを選択して
[Continue]をクリックします 設定が完了したら [Don't show me any helpers]をクリックして
チュートリアルをスキップします 次に Dataprepでフローを作成します 名前は「NYCタクシーレポート」 内容は「タクシー利用に関するデータの 取り込み、変換、分析」です [Create]をクリックします まず フローが処理する データセットを追加します ここでは 事前にパブリッククラウド
ストレージに保存してある データセットを使用します asl-ml-immersion/nyctaxicab
ディレクトリにアクセスし ディレクトリ内の 2015年と2016年のタクシー料金
データを使用します このカンマ区切りのCSVファイルを [Import & Add to Flow]で フローに追加します データセットの処理や操作を行うには [Add new Recipe]をクリックして レシピにステップを追加します データセットの読み込みが終わると プレビューが表示されます データセットにはタクシーの 乗車日時、降車日時、乗客数などの 情報が含まれているのがわかります 乗車距離のヒストグラムからは ほとんどが8km未満だとわかります 次に2015年と2016年の
データセットを結合します 2016年分を選択し [Add and Align by Name]をクリックして 各データセットの列見出しと 結合後の見出しを一致させます [Add to Recipe]をクリックすると 2015年と2016年のデータを含む
データセットのサンプルが表示されます 乗車日と乗車時間は個別の列になっています このラボではタクシー料金の
移動平均を計算するので まず入力データをSQLの日付/時間
フォーマットに変換します レシピにMergeを追加して 複数の列の値を連結します pickup_dayとpickup_timeの列を連結して pickup_datetimeという新しい列を作ります 値の区切り文字はスペースにします 画面左側に プレビューが表示されます 次に乗車時間をSQLの日付/時間
フォーマットに変換して 新しい列を作成します 新しい列ができたら分と秒を除く 年月日と時間の情報を抽出します hour_pickup_datetimeの列には
分と秒の値がないので SQLの日付/時間
フォーマットに変換できません 変換を可能にするためには 新しいステップを追加し 再びMergeを使って hour_pickup_datetimeの列の値と 分と秒の値「0000」を連結します 新しい列には 自動的に名前が付けられますが 簡単に変更できます ここでは「pickup_hour」に変更します 次に 乗車時間ごとの統計情報を計算します sumやaverageなど
標準SQLの集計関数を使用できます このWranglerで 乗車人数、乗車距離、料金それぞれの 合計と平均を計算します 最後に乗車時間ごとの最大料金を計算します ここでも画面左側に 統計情報とヒストグラムの
プレビューが表示されます 平均料金のヒストグラムを見ると ほとんどが乗車1回あたり
18～19ドルだとわかります 次に乗車時間ごとに
過去3時間分のデータに基づく 料金の移動平均を計算します Dataprepの
ROLLINGAVERAGE関数を使います 移動平均の値を入力し
乗車時間で分類します 最後に列の名前を
「average_3hr_rolling_fare」とします レシピができたら Dataflowのジョブとしてデプロイします [Run Job]をクリックして ジョブの結果を保存する場所を指定します デフォルトはGoogle Cloud Storageの
CSVファイルですが 保存先をBigQueryにして ジョブが実行されるたびに
新しいテーブルを作成できます 画面右側で 
[Create new table every run]を選択し 名前を「tlc_yellow_trips_reporting」
に変更すると このラボの初めに作成したデータセットに 新しいテーブルが作成されます [Run Job]をクリックすると ジョブが変換中と表示され DataprepからDataflowにデプロイされます メニューの [JOBS]で進捗を確認できます 右側の省略記号をクリックすると ジョブをデプロイした直後には
表示されませんが しばらく待ってページを更新すると Dataflowジョブにアクセスする
ためのリンクが表示されます クリックするとDataflow UIが開き 変換ステップをDataflowで確認できます Dataflow UIの右側には ジョブの詳細が表示されます 開始したばかりで ジョブを実行するクラスタのスケール中ですが すでにジョブ構成の結果を確認できます ほとんどの変換ステップは開始前ですが BigQueryのテーブルの準備と CSV入力ファイルのデータの
取り込みは始まっています Dataflowでのジョブの確認に加えて BigQueryでは データセットのジョブの出力を確認できます ジョブを実行すると tlc_yellow_trips_reportingに
値が挿入されます テーブルの作成にしばらくかかるので 少し待ってからページを更新します テーブルができたら SQL文を入力して
結果を取り出します 実行前に必ずSQL Dialectの
構成を確認してください ジョブを実行すると 乗車時間、平均乗車距離、平均料金など 約192KBのデータが生成されます 今回のラボは以上です