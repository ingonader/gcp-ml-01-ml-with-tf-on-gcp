1
00:00:00,000 --> 00:00:01,890
No início deste módulo,

2
00:00:01,890 --> 00:00:04,860
você aprendeu técnicas para
implementar o código

3
00:00:04,860 --> 00:00:07,290
para fazer pré-processamento
e criação de atributos.

4
00:00:07,290 --> 00:00:08,880
Para usar essas técnicas,

5
00:00:08,880 --> 00:00:12,495
você precisa ter um bom entendimento
do domínio do problema

6
00:00:12,495 --> 00:00:16,200
e também precisa saber um pouco sobre
seus dados brutos de entrada.

7
00:00:16,200 --> 00:00:19,850
Na prática, você nem sempre tem
esse conhecimento e compreensão.

8
00:00:19,850 --> 00:00:22,055
Talvez precise fazer
engenharia de atributos

9
00:00:22,055 --> 00:00:24,225
e ciência de dados
em domínios desconhecidos,

10
00:00:24,225 --> 00:00:28,125
e talvez saiba pouco ou nada
sobre seus dados de entrada brutos.

11
00:00:28,125 --> 00:00:30,570
Assim, no restante deste módulo,

12
00:00:30,570 --> 00:00:33,530
você examinará as ferramentas
e técnicas que podem ajudá-lo,

13
00:00:33,530 --> 00:00:35,980
se estiver começando com
a ciência de dados do zero.

14
00:00:35,980 --> 00:00:40,205
Antes, você usou ferramentas
como o Apache Beam e o Cloud Dataflow.

15
00:00:40,205 --> 00:00:43,950
Em seguida, você aprenderá sobre uma
ferramenta chamada Cloud Dataprep,

16
00:00:43,950 --> 00:00:47,254
que permite usar uma
interface de usuário gráfica e interativa

17
00:00:47,254 --> 00:00:49,170
para melhor entender,

18
00:00:49,170 --> 00:00:51,390
visualizar e pré-processar os dados.

19
00:00:51,390 --> 00:00:53,860
Quando feita corretamente,
a engenharia de atributos

20
00:00:53,860 --> 00:00:57,355
pode melhorar bastante o desempenho
do sistema de aprendizado de máquina.

21
00:00:57,355 --> 00:00:59,355
E para ter sucesso com a engenharia,

22
00:00:59,355 --> 00:01:02,355
é importante ter conhecimento de domínio
para o seu sistema.

23
00:01:02,355 --> 00:01:05,810
E, especificamente, entender os dados
de entrada brutos.

24
00:01:05,810 --> 00:01:07,260
Então, o que isso significa?

25
00:01:07,260 --> 00:01:10,435
Como você pode começar a entender
um conjunto de dados

26
00:01:10,435 --> 00:01:13,010
com milhões ou bilhões de registros?

27
00:01:13,010 --> 00:01:16,750
Ao trabalhar com um conjunto de dados
que você nunca viu antes,

28
00:01:16,750 --> 00:01:19,300
comece com uma análise exploratória.

29
00:01:19,300 --> 00:01:22,025
Visualize os valores do conjunto de dados,

30
00:01:22,025 --> 00:01:25,870
entenda quais valores ocorreram
com frequência e com pouca frequência,

31
00:01:25,870 --> 00:01:28,765
localize valores atípicos
e procure valores ausentes.

32
00:01:28,765 --> 00:01:32,560
Você definitivamente quer conhecer as
estatísticas do conjunto de dados, médias,

33
00:01:32,560 --> 00:01:35,240
desvio padrão para diferentes
variáveis ​​em seus dados,

34
00:01:35,240 --> 00:01:37,715
seus valores mínimo e máximo,

35
00:01:37,715 --> 00:01:41,550
e você quer explorar as
distribuições desses valores.

36
00:01:41,550 --> 00:01:44,945
Além disso, ao trabalhar com
aprendizado de máquina,

37
00:01:44,945 --> 00:01:48,789
é provável que você trabalhe com uma
equipe que inclui cientistas de dados,

38
00:01:48,789 --> 00:01:51,355
desenvolvedores de software
e analistas de negócios.

39
00:01:51,355 --> 00:01:54,430
Isso significa que você precisa
compartilhar os resultados

40
00:01:54,430 --> 00:01:57,640
dos aprendizados sobre o conjunto
de dados com outras pessoas

41
00:01:57,640 --> 00:02:01,165
e também aproveitar o conhecimento
de sua equipe para ter insights.

42
00:02:01,165 --> 00:02:04,960
No restante deste módulo, veremos
duas abordagens complementares.

43
00:02:04,960 --> 00:02:07,440
Vamos começar explorando
um conjunto de dados

44
00:02:07,440 --> 00:02:10,310
e passar para o pré-processamento
e criação de atributos.

45
00:02:10,310 --> 00:02:14,030
Na primeira abordagem, usaremos as
ferramentas que você já viu,

46
00:02:14,030 --> 00:02:17,305
incluindo BigQuery, Cloud Dataflow
e Tensorflow.

47
00:02:17,305 --> 00:02:20,800
Na segunda abordagem, apresentaremos
o Cloud Dataprep

48
00:02:20,800 --> 00:02:26,275
e mostraremos como ele ajuda na análise
exploratória e no processamento de dados.

49
00:02:26,275 --> 00:02:28,120
Vamos começar com
a primeira abordagem,

50
00:02:28,120 --> 00:02:30,870
usando as ferramentas já
conhecidas para explorar dados.

51
00:02:30,870 --> 00:02:32,195
No início deste curso,

52
00:02:32,195 --> 00:02:34,735
você viu exemplos do uso
de bibliotecas gráficas,

53
00:02:34,735 --> 00:02:37,875
como o Seabourn, para visualizar
dados e o Cloud Datalab.

54
00:02:37,875 --> 00:02:41,080
O exemplo na tela mostra um gráfico
de dados, do conjunto de dados

55
00:02:41,080 --> 00:02:44,710
da tarifa de táxi de Nova York
disponível no BigQuery.

56
00:02:44,710 --> 00:02:46,810
Neste caso, o diagrama representa

57
00:02:46,810 --> 00:02:50,450
a distância da viagem de táxi em relação
a uma quantia justa para as viagens.

58
00:02:50,450 --> 00:02:55,300
Usar um bloco de notas no Datalab para
explorar e visualizar o conjunto de dados

59
00:02:55,300 --> 00:02:57,190
pode parecer uma abordagem prática.

60
00:02:57,190 --> 00:03:01,045
No entanto, lembre-se de que
o ambiente padrão do Datalab

61
00:03:01,045 --> 00:03:05,190
está sendo executado em um único servidor
virtual com memória limitada.

62
00:03:05,190 --> 00:03:07,640
No caso do conjunto de dados
de tarifa de táxi,

63
00:03:07,640 --> 00:03:09,460
há bilhões de pontos de dados.

64
00:03:09,460 --> 00:03:12,235
Então, será impraticável ou muito caro

65
00:03:12,235 --> 00:03:17,285
plotar e analisar todos usando apenas
um único ambiente sem Datalab.

66
00:03:17,285 --> 00:03:20,770
Em vez de carregar os bilhões de registros
de todo o conjunto de dados

67
00:03:20,770 --> 00:03:23,755
de tarifa de táxi no ambiente
de laboratório de dados,

68
00:03:23,755 --> 00:03:28,415
você pode usar o SQL e calcular
estatísticas resumidas usando o BigQuery.

69
00:03:28,415 --> 00:03:30,390
Como mostrado neste diagrama,

70
00:03:30,390 --> 00:03:33,705
você ainda pode usar o Datalab
para gravar o código SQL.

71
00:03:33,705 --> 00:03:35,315
Assim que ele estiver pronto,

72
00:03:35,315 --> 00:03:40,645
envie a instrução SQL para o BigQuery
por meio das APIs e terá o resultado.

73
00:03:40,645 --> 00:03:44,350
Como as estatísticas resumidas
são apenas algumas linhas de dados,

74
00:03:44,350 --> 00:03:47,620
você pode plotá-las facilmente no
Datalab usando o Seaborne

75
00:03:47,620 --> 00:03:50,210
ou outras bibliotecas Python
para visualização.

76
00:03:50,210 --> 00:03:53,860
Além disso, como você aprendeu
nas seções anteriores deste módulo,

77
00:03:53,860 --> 00:03:57,760
é possível usar as APIs Apache Beam
e o Cloud Dataflow para implementar

78
00:03:57,760 --> 00:04:01,825
cálculos de estatísticas resumidas e
outros jobs de pré-processamento de dados.

79
00:04:01,825 --> 00:04:06,460
Use Python ou Java para gravar o código
para o canal de processamento de dados.

80
00:04:06,460 --> 00:04:10,275
Em seguida, vamos ver a segunda
abordagem, em que você usará

81
00:04:10,275 --> 00:04:14,170
o Cloud Dataprep para ter um
melhor entendimento dos dados de entrada

82
00:04:14,170 --> 00:04:18,410
e para fazer engenharia de atributos
com uma interface visual interativa,

83
00:04:18,410 --> 00:04:20,375
em vez de gravar um
código de baixo nível.

84
00:04:20,375 --> 00:04:22,810
Então, o que é o Cloud Dataprep?

85
00:04:22,810 --> 00:04:26,260
É um serviço totalmente gerenciado
disponível no GCP,

86
00:04:26,260 --> 00:04:28,615
que permite explorar
e transformar seus dados

87
00:04:28,615 --> 00:04:33,020
de modo interativo usando um navegador da
Web com uma quantidade mínima de código.

88
00:04:33,880 --> 00:04:36,970
O Dataprep pode conseguir dados
de várias fontes,

89
00:04:36,970 --> 00:04:39,855
incluindo o Google Cloud Storage
e o BigQuery.

90
00:04:39,855 --> 00:04:42,840
Você também pode enviar seus
próprios dados para o Dataprep.

91
00:04:42,840 --> 00:04:45,930
Depois que o Dataprep souber
de onde conseguir os dados,

92
00:04:45,930 --> 00:04:49,595
você poderá usar essa IU gráfica
para explorar dados

93
00:04:49,595 --> 00:04:51,635
e criar visualizações deles.

94
00:04:51,635 --> 00:04:55,085
Por exemplo, você pode usar
histogramas de valores de dados

95
00:04:55,085 --> 00:04:59,600
e conseguir resumos estatísticos
como médias e valores percentuais.

96
00:04:59,600 --> 00:05:03,055
Depois de ter explorado
e entendido o conjunto de dados,

97
00:05:03,055 --> 00:05:07,555
você pode usar o Dataprep para calcular
os fluxos de transformações de dados.

98
00:05:08,965 --> 00:05:12,655
Os fluxos são semelhantes aos canais
que você viu no Dataflow.

99
00:05:12,655 --> 00:05:15,870
Na verdade, os fluxos são compatíveis
com o Dataflow.

100
00:05:15,870 --> 00:05:17,900
Você pode pegar um fluxo do Dataprep

101
00:05:17,900 --> 00:05:21,085
e executá-lo como um canal
na plataforma do Dataflow.

102
00:05:21,085 --> 00:05:25,880
No Dataprep, os fluxos são implementados
como uma sequência de receitas.

103
00:05:25,880 --> 00:05:28,815
Elas são etapas
de processamento de dados

104
00:05:28,815 --> 00:05:31,375
criadas de uma biblioteca
dos chamados Wranglers.

105
00:05:31,375 --> 00:05:34,250
O Dataprep possui Wranglers
para muitas tarefas comuns

106
00:05:34,250 --> 00:05:36,570
de processamento de dados,
mostradas à esquerda.

107
00:05:36,570 --> 00:05:39,890
Você verá exemplos
específicos deles em breve.

108
00:05:39,890 --> 00:05:42,920
Lembre-se de que, em vez de
você mesmo precisar

109
00:05:42,920 --> 00:05:46,210
implementar essas etapas de processamento
de dados e codificar,

110
00:05:46,210 --> 00:05:47,835
se usar os Wranglers,

111
00:05:47,835 --> 00:05:50,450
o Dataprep poderá conseguir
o fluxo e as receitas

112
00:05:50,450 --> 00:05:53,240
e convertê-los em um 
canal do Dataflow.

113
00:05:53,240 --> 00:05:56,495
Em seguida, usando a
mesma interface do Dataprep,

114
00:05:56,495 --> 00:05:58,010
você pode pegar o fluxo,

115
00:05:58,010 --> 00:06:01,950
executá-lo como um job no Dataflow
e monitorar o andamento do job.

116
00:06:01,950 --> 00:06:04,740
A biblioteca Dataprep
tem muitos Wranglers prontos

117
00:06:04,740 --> 00:06:06,780
para tarefas de processamento de dados.

118
00:06:06,780 --> 00:06:09,990
Você pode limpar os dados
usando a remoção de duplicação

119
00:06:09,990 --> 00:06:11,850
ou filtrar valores ausentes e atípicos,

120
00:06:11,850 --> 00:06:16,170
ou fazer agregações comuns,
como contar ou somar valores,

121
00:06:16,170 --> 00:06:19,560
ou pode unir uma união
de tabelas de dados diferentes

122
00:06:19,560 --> 00:06:23,760
e transformar dados em tipos diferentes,
como strings ou números inteiros.

123
00:06:23,760 --> 00:06:25,710
Enquanto o fluxo está em execução,

124
00:06:25,710 --> 00:06:29,715
use a interface do Dataflow para monitorar
os detalhes do andamento dos jobs,

125
00:06:29,715 --> 00:06:31,475
e, depois que
o job estiver concluído,

126
00:06:31,475 --> 00:06:34,230
você poderá ter um resumo
do status do job no Dataprep.

127
00:06:34,230 --> 00:06:37,270
Como você pode ver na captura
de tela do job concluído,

128
00:06:37,270 --> 00:06:40,760
o resumo inclui as
estatísticas e visualizações

129
00:06:40,740 --> 00:06:43,930
que você pode conseguir para qualquer
conjunto de dados no Dataprep.