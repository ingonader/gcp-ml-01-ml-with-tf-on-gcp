Welcome back. In this lab, you will take the taxi fare data set from BigQuerry and pre-process it using the Cloud Dataprep tool. In the tool, you will explore the distribution of data values, visualize the distributions using histogram plots, and then, implement a data prep flow to create a new feature based on an average number of taxi rides per hour, in a rolling time window. Finally, you will deploy and run the Dataprep flow on GCP and monitor the job execution using Dataflow. Let's take a closer look. Okay, to get started with this lab, you need to prepare some prerequisites. You begin at the Google Cloud Platform dashboard. First, you will need Google Cloud Storage bucket. You can create one if you go into the products and services menu, which you can access by clicking on the hamburger icon. Scroll down to storage, browser, and click on create bucket. As you can see from the guidance on the screen, the name of the storage bucket must be globally unique. Here, I have set up a unique bucket name in the us-east4 location. Shortly after I click create, I can see that the bucket with the unique name is ready. The next thing that you need to prepare for this lab is the BigQuery dataset. You can find BigQuery back in the products and services menu, under the Big Data section. When you click on BigQuery, you should see a new tab opening a browser. To the right of your project name, click on the down arrow and choose create new dataset. Use the name taxi cab reporting for the data set and click okay to create it. Once the data set is ready, you need to go back to the Google Cloud Platform dashboard. From there, navigate to the Dataproc link in the Products and Services menu. Since Cloud Dataprep is a service from a Google partner, you need to accept a new set of terms and conditions. Click accept to do that. Also you need to click allow to let Trifacta, which is Google's partner that develop Dataprep, to access your data. When you click allow, it'll take a few moments to enable Dataprep for your project. So right now, you can see the video fast forwarding for the wait. Next, you need to choose the account to use for Cloud Dataprep and allow Dataprep to access your project. When you're setting up Dataprep on your project for the first time, you need to specify the storage bucket that will hold your data. Here you can see that the bucket created in the beginning of this lab, is used to set up Dataprep. Once the bucket is selected, click continue. Once Dataprep is set up, you can dismiss the help tutorial by clicking don't show any helpers. Next, you'll use Dataprep to create a new flow. Let's call this flow NYC Taxi reporting. The flow is going to show a process for ingesting, transforming, and analyzing taxi data. Go ahead click create. The first thing you need to do in order to create a flow is to add some datasets for the flow to process. In this case, you will import some predefined datasets that our team already saved to the public cloud storage bucket and you can access the storage bucket, using the name asl-ml-immersion under the NYC taxicab directory. The directory has a few files. You will use the files with the 2015 and 2016 taxi fare data. Notice, that these are comma separated values CSB files. Click import and shortly, you will see the two files added to your flow. To implement data processing or wrangling for these datasets, you will need to add in your recipe. Next, you will add steps to this recipe. Once the dataset is loaded, you'll see a preview of a sample of the data from the dataset. Here for example, you can see that the dataset includes information about taxi rides such as pickup date time, drop of date time and the number of passengers in the taxi. Also, notice from the trip distance histogram that most of the trips were under five miles of distance. Next, you go ahead in union 2015 and 2016 datasets so you can work was more rows of data. Once you select the 2016 dataset, you need to click on add and align by name which will make sure that the names that have the corresponding column headers, are aligned to the union version of the dataset. Add the union step to the recipe and after Dataprep previews the union, you'll see a sample of the datasets that includes taxi rides for 2015 and 2016. Notice, that the data about the pickup date and pickup day time is in different counts. Since this lab will show you how to compute the rolling averages for taxi fare amounts, first you need to convert the input data to the SQL date, time format. For that, you can add and merge to the recipe, which will concatenate values from multiple columns. In this case, the columns are called the pickup date and the pickup time. Use pickup date time as the new column name. Also, go ahead and use a single space as a delimiter between values. Notice that on the left, you now have a preview of the new column. Next, create a new derived column that will convert pickup time into a SQL date, time format. Once a new date time stand field is available, you will extract out just a year, month, date, and the hour information with all the details of the minutes and seconds. Since the hour pickup date time column is missing the values for minutes and seconds, it's not parseable as SQL date, time format. So, you need to create a new column that can be converted into a valid SQL date, time value. To do that, you will create a new merge operation and you will use the merge wrangler again. This wrangler will concatenate values of the hour, pickup date, and time column with a string that contains four zero characters for the values of the minutes and seconds. Notice that when you add a new column, it will get an automatically generated name like column one. You can easily rename that. In this case, you can rename it to pickup hour. Next, you will compute some statistics based on the pickup hour values. You can use standard SQL statistical aggregation functions like sum or average. You can see that this wrangler will compute the sums and averages for passenger counts and the same combination of the sum and average for the trip distance and fare amount. Lastly, it will compute the maximum fare amounts for each pickup hour. Just as earlier, notice that you get a preview of the results for the computed statistics in the histograms on the left hand side of the screen. Also, if you observe the average fare amount in the histograms, most of the average fares are on the range from $18 to $19 per trip. Next, you will go ahead and compute the rolling average for the fare amount by looking at the trailing free hours worth of data for each pickup hour. You can compute this using the Rolling Average Function in Cloud Dataprep. Here are the values for the rolling average, assorted by the pickup hour. Finally, name this column average free hour rolling fare. All right, once the recipe is ready, you can deploy the recipe as a Google Cloud Dataflow job. To do that, you need to click on run job and specify where the results of the jobs are going to be published or other words, store. By default, the results of the job is saved as CSB file in Google Cloud storage. Instead of doing that, you can change the destination to be BigQuery and you can create a new table in BigQuery every time the job is executed. So, if you change your selection on the right to create a new table at every run and rename the table to TLC yellow trips reporting, you will get a new table in the NYC Taxi reporting dataset. Remember, this is the dataset that you created in the beginning of this lab.Go ahead and run the job. Once the job is shown as transforming, Dataprep will begin to deploy the job to data flow. This usually takes a few moments. You can monitor the progress of the job from the job session of the Dataprep menu. If you click on the ellipsis symbol on the right. The ellipsis menu will not have the link to the Dataflow job right away after you deploy the job, but if you wait a few moments and refresh the page, you'll find that the menu gets updated and you'll see a link to access the Dataflow job. If you click the link, you'll be automatically taken to the Dataflow user interface, where you can monitor the detailed transformation steps in Dataflow as created by Dataprep. On the right hand side of the Dataflow UI, you can get details about this job execution. Here, you can see that since the job has just started, the Dataflow cluster to run the job still needs to be scaled. However, you can already monitor the results of the job configuration. Here, none of the individual transformation steps of the job have started except the few that are preparing the table in BigQuery and are just starting out to fetch data from the input CSB files from Google Cloud Storage. In addition to monitoring this job from Dataflow, you can navigate to BigQuery and monitor the output of the job in your taxicab reporting dataset. As you recall, once the job starts running, it will insert values into a new TLC yellow trip reporting table. Since it takes a while for the table to be created, you may need to wait and refresh the page to see the update. Once the table is in place, you can enter a SQL statement to fetch results from the table. However, make sure you have your SQL dialect configured properly before you run it. Here, you can see that running the job generate roughly 192 kilobytes of data, including information about the pickup hours, average trip distances, average fare and the other information computed by Dataflow. Okay, this is it for this lab.