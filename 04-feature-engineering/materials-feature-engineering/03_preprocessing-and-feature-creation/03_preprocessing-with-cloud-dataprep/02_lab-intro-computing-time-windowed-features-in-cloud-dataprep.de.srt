1
00:00:00,000 --> 00:00:01,275
Willkommen zurück.

2
00:00:01,275 --> 00:00:02,325
In diesem Lab

3
00:00:02,325 --> 00:00:06,340
erledigen Sie die Vorverarbeitung
für das Taxikosten-Dataset aus BigQuery

4
00:00:06,340 --> 00:00:08,185
mithilfe von Cloud Dataprep.

5
00:00:08,185 --> 00:00:12,210
In diesem Tool untersuchen Sie
die Verteilung von Datenwerten,

6
00:00:12,210 --> 00:00:15,320
stellen sie als Histogramm dar

7
00:00:15,320 --> 00:00:17,880
und implementieren
dann einen Dataprep-Ablauf,

8
00:00:17,880 --> 00:00:22,280
um ein neues Merkmal auf Basis
der mittleren Taxifahrten pro Stunde

9
00:00:22,280 --> 00:00:24,555
in einem gleitenden Zeitfenster
zu erstellen.

10
00:00:24,555 --> 00:00:28,100
Schließlich implementieren Sie
den Dataprep-Ablauf auf der GCP

11
00:00:28,100 --> 00:00:29,265
und führen ihn aus

12
00:00:29,265 --> 00:00:32,285
Sie überwachen die Jobausführung
mit Dataflow.

13
00:00:32,285 --> 00:00:34,545
Sehen wir uns das an.

14
00:00:34,545 --> 00:00:37,110
Bevor Sie mir dem Lab beginnen,

15
00:00:37,110 --> 00:00:39,370
müssen Sie ein paar Vorbereitungen treffen.

16
00:00:39,370 --> 00:00:42,990
Beginnen Sie auf dem
Google Cloud Platform-Dashboard.

17
00:00:42,990 --> 00:00:46,260
Zunächst benötigen Sie
einen Google Cloud Storage-Bucket.

18
00:00:46,260 --> 00:00:50,510
Klicken Sie dazu auf das Dreistrich-Menü

19
00:00:50,510 --> 00:00:53,550
und öffnen Sie
das Menü für Produkte und Dienste.

20
00:00:53,550 --> 00:00:55,785
Scrollen Sie nach unten zu "Storage",

21
00:00:55,785 --> 00:00:59,205
dann "Browser"
und klicken Sie auf "Bucket erstellen".

22
00:00:59,205 --> 00:01:01,755
Wie Sie den Hinweisen entnehmen können,

23
00:01:01,755 --> 00:01:05,355
muss der Name des
Storage-Buckets global eindeutig sein.

24
00:01:05,355 --> 00:01:08,360
Hier habe ich
einen eindeutigen Bucket-Namen

25
00:01:08,360 --> 00:01:11,200
im Speicherort "us-east4" erstellt.

26
00:01:11,200 --> 00:01:13,665
Ich klicke auf "Erstellen"

27
00:01:13,665 --> 00:01:17,220
und kurz darauf ist der Bucket
mit dem eindeutigen Namen bereit.

28
00:01:17,220 --> 00:01:21,885
Als Nächstes müssen Sie
das BigQuery-Dataset vorbereiten.

29
00:01:21,885 --> 00:01:28,025
Sie finden BigQuery im Menü für Produkte
und Dienste im Abschnitt "Big Data".

30
00:01:28,025 --> 00:01:33,030
Wenn Sie auf "BigQuery" klicken,
öffnet sich im Browser ein neuer Tab.

31
00:01:33,035 --> 00:01:37,060
Klicken Sie auf den Abwärtspfeil
rechts neben dem Namen Ihres Projekts

32
00:01:37,060 --> 00:01:40,180
und wählen Sie
"Neues Dataset erstellen" aus.

33
00:01:40,180 --> 00:01:43,910
Nennen Sie das Dataset "taxi_cab_reporting"

34
00:01:43,910 --> 00:01:46,350
und klicken Sie
auf "OK", um es zu erstellen.

35
00:01:46,350 --> 00:01:48,800
Wenn das Dataset bereit ist,

36
00:01:48,800 --> 00:01:52,115
kehren Sie
zum Google Cloud Platform-Dashboard zurück.

37
00:01:52,115 --> 00:01:57,630
Öffnen Sie dort den Link "Dataprep"
im Menü für Produkte und Dienste.

38
00:01:57,630 --> 00:02:01,160
Da der Dienst Cloud Dataprep
von einem Google-Partner angeboten wird,

39
00:02:01,160 --> 00:02:04,205
müssen Sie noch
dessen Nutzungsbedingungen akzeptieren.

40
00:02:04,205 --> 00:02:06,125
Klicken Sie dafür auf "Akzeptieren".

41
00:02:06,125 --> 00:02:08,500
Außerdem müssen Sie
noch auf "Zulassen" klicken,

42
00:02:08,500 --> 00:02:11,890
damit Trifacta,
Google-Partner und Entwickler von Dataprep,

43
00:02:11,890 --> 00:02:13,640
auf Ihre Daten zugreifen kann.

44
00:02:13,640 --> 00:02:18,350
Danach dauert es nicht lange,
bis Dataprep für Ihr Projekt aktiviert ist.

45
00:02:18,350 --> 00:02:22,590
Diese Wartezeit
überspringen wir jetzt in unserem Video.

46
00:02:22,590 --> 00:02:26,550
Wählen Sie als Nächstes das Konto aus,
das Sie für Dataprep verwenden möchten,

47
00:02:26,550 --> 00:02:29,680
und gewähren Sie
Cloud Dataprep Zugriff auf Ihr Projekt.

48
00:02:29,680 --> 00:02:33,430
Wenn Sie Dataprep
zum ersten Mal für ein Projekt einrichten,

49
00:02:33,430 --> 00:02:36,775
müssen Sie
den Storage-Bucket für Ihre Daten angeben.

50
00:02:36,775 --> 00:02:40,545
Wie Sie sehen, benutzen wir
den Bucket vom Anfang dieses Labs

51
00:02:40,545 --> 00:02:42,720
zur Einrichtung von Dataprep.

52
00:02:42,720 --> 00:02:46,020
Klicken Sie
nach Auswahl des Buckets auf "Weiter".

53
00:02:46,020 --> 00:02:49,930
Sobald Dataprep eingerichtet ist,
können Sie die Anleitung schließen,

54
00:02:49,930 --> 00:02:52,805
indem Sie die Hilfe,
wie hier gezeigt, deaktivieren.

55
00:02:52,805 --> 00:02:56,940
Als Nächstes erstellen Sie
mit Dataprep einen neuen Ablauf.

56
00:02:56,940 --> 00:03:00,290
Nennen wir diesen Ablauf
"NYC Taxi Cab Reporting".

57
00:03:00,290 --> 00:03:04,000
Der Ablauf wird
ein Verfahren für die Aufnahme,

58
00:03:04,000 --> 00:03:07,165
Transformation und
Analyse der Taxidaten zeigen.

59
00:03:07,165 --> 00:03:09,340
Klicken Sie nun auf "Erstellen".

60
00:03:09,340 --> 00:03:12,480
Um einen Ablauf zu erstellen,

61
00:03:12,480 --> 00:03:15,760
müssen Sie zuerst ein paar Datasets
hinzufügen, die er verarbeiten soll.

62
00:03:15,760 --> 00:03:19,335
In diesem Fall importieren Sie
ein paar vordefinierte Datasets,

63
00:03:19,335 --> 00:03:23,555
die unser Team in einem öffentlichen
Cloud Storage-Bucket gespeichert hat.

64
00:03:23,555 --> 00:03:25,185
Auf den Storage-Bucket

65
00:03:25,185 --> 00:03:30,980
können Sie im Verzeichnis
"asl-ml-immersion/nyctaxicab" zugreifen.

66
00:03:30,980 --> 00:03:33,430
Das Verzeichnis enthält einige Dateien.

67
00:03:33,430 --> 00:03:39,315
Sie verwenden die Dateien
mit den Taxigebühren von 2015 und 2016.

68
00:03:39,315 --> 00:03:43,400
Beachten Sie, dass dies CSV-Dateien
mit durch Komma getrennten Werten sind.

69
00:03:43,400 --> 00:03:45,130
Klicken Sie auf "Importieren".

70
00:03:45,130 --> 00:03:48,620
Die beiden Dateien
werden dann Ihrem Ablauf hinzugefügt.

71
00:03:48,620 --> 00:03:52,590
Um für diese Datasets Datenverarbeitung
oder Wrangling zu implementieren,

72
00:03:52,590 --> 00:03:54,570
müssen Sie ein neues Schema hinzufügen.

73
00:03:54,570 --> 00:03:57,930
Als Nächstes fügen Sie
diesem Schema Schritte hinzu.

74
00:03:57,930 --> 00:03:59,750
Sobald das Dataset geladen ist,

75
00:03:59,750 --> 00:04:03,190
sehen Sie eine Vorschau
mit Beispieldaten aus dem Dataset.

76
00:04:03,190 --> 00:04:07,810
Dieses Dataset enthält
zum Beispiel Informationen zu Taxifahrten

77
00:04:07,810 --> 00:04:13,855
wie Tag und Zeitpunkt von Abholung
und Ankunft sowie die Zahl der Fahrgäste.

78
00:04:13,855 --> 00:04:16,670
Beachten Sie im Fahrtdistanzhistogramm,

79
00:04:16,670 --> 00:04:21,090
dass die meisten Fahrten über eine
Distanz von unter acht Kilometern gingen.

80
00:04:21,090 --> 00:04:25,685
Als Nächstes erzeugen Sie
eine Union der Datasets für 2015 und 2016,

81
00:04:25,685 --> 00:04:28,325
damit Sie mit
mehr Datenzeilen arbeiten können.

82
00:04:28,325 --> 00:04:31,125
Wenn Sie das Dataset für 2016 auswählen,

83
00:04:31,125 --> 00:04:34,170
müssen Sie auf "Hinzufügen
und nach Name abgleichen" klicken,

84
00:04:34,170 --> 00:04:37,250
wodurch die Namen mit
den entsprechenden Spaltenüberschriften

85
00:04:37,250 --> 00:04:40,095
an der Union-Version
des Datasets ausgerichtet werden.

86
00:04:40,095 --> 00:04:44,865
Fügen Sie dem Schema
den Union-Schritt hinzu.

87
00:04:44,865 --> 00:04:50,030
Dataprep zeigt dann eine Vorschau
mit Daten der Fahrten von 2015 und 2016.

88
00:04:50,030 --> 00:04:56,355
Beachten Sie, dass Tag und Tageszeit der
Abholung in verschiedenen Spalten stehen.

89
00:04:56,355 --> 00:04:58,240
Da Sie in diesem Lab lernen,

90
00:04:58,240 --> 00:05:01,750
wie man gleitende Durchschnitte
für Taxigebühren berechnet,

91
00:05:01,750 --> 00:05:06,440
müssen Sie die Eingabedaten vorher
in das SQL-Format DATETIME konvertieren.

92
00:05:06,440 --> 00:05:09,810
Dazu können Sie
ein "Merge" zum Schema hinzufügen,

93
00:05:09,810 --> 00:05:12,950
das die Werte
mehrerer Spalten zusammenführt.

94
00:05:12,950 --> 00:05:17,405
In diesem Fall heißen die Spalten
"pickup_day" und "pickup_time".

95
00:05:17,405 --> 00:05:20,905
Nennen Sie
die neue Spalte "pickup_datetime".

96
00:05:20,905 --> 00:05:25,375
Nehmen Sie als Trennzeichen
ein Leerzeichen zwischen den Werten.

97
00:05:25,385 --> 00:05:28,880
Beachten Sie die Vorschau
der neuen Spalte auf der linken Seite.

98
00:05:28,880 --> 00:05:31,530
Erstellen Sie
als Nächstes eine neue abgeleitete Spalte

99
00:05:31,530 --> 00:05:35,020
für die Abholzeit im SQL DATETIME-Format.

100
00:05:35,020 --> 00:05:38,255
Sobald das neue Feld mit Datums-
und Zeitstempel zur Verfügung steht.

101
00:05:38,255 --> 00:05:41,120
extrahieren Sie Jahr, Monat, Tag

102
00:05:41,120 --> 00:05:44,980
und die Stunde
ohne die Minuten und Sekunden.

103
00:05:44,980 --> 00:05:50,345
Da in der Spalte "hour_pickup_datetime"
die Werte für Minuten und Sekunden fehlen,

104
00:05:50,345 --> 00:05:53,540
lässt sie sich nicht
als SQL DATETIME parsen.

105
00:05:53,540 --> 00:05:55,670
Daher müssen Sie eine neue Spalte erzeugen,

106
00:05:55,670 --> 00:05:59,090
die in einen gültigen SQL DATETIME-Wert
konvertierbar ist.

107
00:05:59,090 --> 00:06:01,470
Dafür erstellen Sie

108
00:06:01,470 --> 00:06:05,530
einen neuen Merge-Vorgang
und benutzen wieder den Merge-Wrangler.

109
00:06:05,530 --> 00:06:10,150
Dieser Wrangler verkettet Werte
aus der Spalte "hour_pickup_datetime"

110
00:06:10,150 --> 00:06:15,130
in einem String mit vier Nullen
für die Minuten- und Sekundenwerte.

111
00:06:15,130 --> 00:06:17,860
Beachten Sie,
dass jede neu hinzugefügte Spalte

112
00:06:17,860 --> 00:06:21,195
einen automatisch
generierten Namen wie "column1" erhält.

113
00:06:21,195 --> 00:06:23,040
Sie können sie einfach umbenennen.

114
00:06:23,040 --> 00:06:26,880
In diesem Fall
können Sie sie "pickup_hour" nennen.

115
00:06:28,080 --> 00:06:32,830
Berechnen Sie nun ein paar Statistiken
basierend auf den "pickup_hour"-Werten.

116
00:06:32,830 --> 00:06:35,745
Sie können
Standard-SQL-Aggregationsfunktionen

117
00:06:35,745 --> 00:06:37,505
wie Summe oder Durchschnitt benutzen.

118
00:06:37,505 --> 00:06:42,030
Wie Sie sehen, berechnet
der Wrangler die Summen und Durchschnitte

119
00:06:42,030 --> 00:06:47,750
sowohl für die Fahrgastanzahl als auch
für die Fahrtdistanz und die Taxigebühr.

120
00:06:47,750 --> 00:06:53,080
Er berechnet auch die maximalen
Taxigebühren für jede Abholstunde.

121
00:06:56,180 --> 00:07:00,490
Auch hier wird eine Vorschau
der Ergebnisse der berechneten Statistiken

122
00:07:00,490 --> 00:07:03,515
links in den Histogrammen angezeigt.

123
00:07:03,515 --> 00:07:07,475
Im Histogramm
unter "average_fare_amount" fällt auf,

124
00:07:07,475 --> 00:07:12,490
dass die meisten Durchschnittsgebühren
zwischen 18 $ und 19 $ pro Fahrt liegen.

125
00:07:12,490 --> 00:07:17,150
Als Nächstes berechnen Sie den
gleitenden Durchschnitt der Taxigebühren.

126
00:07:17,150 --> 00:07:21,995
Dafür betrachten Sie die Daten zu
den drei Stunden nach jeder Abholstunde.

127
00:07:21,995 --> 00:07:26,930
In Cloud Dataprep können Sie diese mit
der Funktion "ROLLINGAVERAGE" berechnen.

128
00:07:26,930 --> 00:07:29,335
Hier sind die Werte
für den gleitenden Durchschnitt,

129
00:07:29,335 --> 00:07:31,300
sortiert nach der Stunde der Abholung.

130
00:07:31,300 --> 00:07:36,440
Zum Abschluss nennen Sie
diese Spalte "average_3hr_rolling_fare".

131
00:07:36,440 --> 00:07:39,750
Wenn das Schema fertig ist,

132
00:07:39,750 --> 00:07:43,395
können Sie es als
Google Cloud Dataflow-Job implementieren.

133
00:07:43,395 --> 00:07:47,355
Klicken Sie dafür
auf "Job ausführen" und geben Sie an,

134
00:07:47,355 --> 00:07:52,035
wo die Ergebnisse veröffentlicht
bzw. gespeichert werden sollen.

135
00:07:52,035 --> 00:07:54,655
Standardmäßig werden die Ergebnisse

136
00:07:54,655 --> 00:07:57,765
als CSV-Datei
in Google Cloud Storage gespeichert.

137
00:07:57,765 --> 00:08:02,000
Stattdessen
können Sie auch BigQuery als Ziel angeben

138
00:08:02,000 --> 00:08:06,640
und bei jeder Jobausführung
eine neue Tabelle in BigQuery erstellen.

139
00:08:06,645 --> 00:08:12,185
Wenn Sie rechts "Neue Tabelle
bei jeder Ausführung erstellen" auswählen

140
00:08:12,195 --> 00:08:15,715
und die Tabelle umbenennen
in "tlc_yellow_trips_reporting",

141
00:08:15,715 --> 00:08:20,210
erhalten Sie eine neue Tabelle
im Dataset "taxi_cab_reporting".

142
00:08:20,210 --> 00:08:24,465
Dieses Dataset haben Sie
am Anfang dieses Labs erstellt.

143
00:08:25,435 --> 00:08:28,210
Führen Sie den Job nun also aus.

144
00:08:28,210 --> 00:08:30,835
Sobald für den Job
die Transformation angezeigt wird,

145
00:08:30,835 --> 00:08:33,559
stellt Dataprep
den Job für Dataflow bereit.

146
00:08:33,559 --> 00:08:35,760
Das dauert in der Regel nicht lange.

147
00:08:35,760 --> 00:08:40,414
Sie können den Jobfortschritt
im Dataprep-Menü unter "Jobs" überwachen,

148
00:08:40,414 --> 00:08:43,904
indem Sie auf der rechten Seite
auf das Dreipunkt-Menü klicken.

149
00:08:43,904 --> 00:08:47,380
Das Dreipunkt-Menü
enthält den Link zum Dataflow-Job

150
00:08:47,380 --> 00:08:49,590
nicht gleich nach dessen Bereitstellung.

151
00:08:49,590 --> 00:08:52,659
Wenn Sie aber
kurz warten und die Seite aktualisieren,

152
00:08:52,659 --> 00:08:57,635
erscheint im aktualisierten Menü
ein Link zum Dataflow-Job.

153
00:08:57,635 --> 00:09:02,480
Der Link öffnet automatisch
die Benutzeroberfläche von Dataflow,

154
00:09:02,480 --> 00:09:04,905
wo Sie
die detaillierten Transformationsschritte,

155
00:09:04,905 --> 00:09:08,205
die Sie mit Dataprep
erstellt haben, überwachen können.

156
00:09:08,205 --> 00:09:10,645
Auf der rechten Seite der Dataflow-UI

157
00:09:10,645 --> 00:09:13,615
sehen Sie Details zu dieser Jobausführung.

158
00:09:13,615 --> 00:09:17,160
Hier können Sie sehen, dass
– da der Job gerade erst begonnen hat –

159
00:09:17,160 --> 00:09:21,080
der Dataflow-Cluster für die Jobausführung
erst noch skaliert werden muss.

160
00:09:21,080 --> 00:09:25,225
Sie können aber schon die Ergebnisse
der Jobkonfiguration überwachen.

161
00:09:25,225 --> 00:09:30,530
Hier hat kein Transformationsschritt
bisher begonnen, außer den wenigen,

162
00:09:30,530 --> 00:09:34,300
die die Tabelle in BigQuery
vorbereiten und gerade damit beginnen,

163
00:09:34,300 --> 00:09:38,370
Daten aus den CSV-Eingabedateien
in Google Cloud Storage abzurufen.

164
00:09:38,370 --> 00:09:41,730
Zusätzlich zur Jobüberwachung in Dataflow

165
00:09:41,730 --> 00:09:43,300
können Sie auch BigQuery öffnen

166
00:09:43,300 --> 00:09:47,575
und im Dataset "taxi_cab_reporting"
die Ausgabe des Jobs überwachen.

167
00:09:47,575 --> 00:09:50,525
Wie Sie wissen,
fügt der Job, sobald er ausgeführt wird,

168
00:09:50,525 --> 00:09:55,135
Werte in die neue Tabelle
"tlc_yellow_trips_reporting" ein.

169
00:09:55,135 --> 00:09:57,600
Da es etwas dauert,
bis die Tabelle erstellt wurde,

170
00:09:57,600 --> 00:10:00,135
müssen Sie eventuell warten
und die Seite neu laden,

171
00:10:00,135 --> 00:10:01,955
um die Aktualisierung zu sehen.

172
00:10:01,955 --> 00:10:03,710
Wenn die Tabelle erstellt wurde,

173
00:10:03,710 --> 00:10:07,185
können Sie mit einer
SQL-Anweisung Ergebnisse daraus abrufen.

174
00:10:07,185 --> 00:10:08,890
Vergewissern Sie sich aber,

175
00:10:08,890 --> 00:10:11,365
dass Ihr SQL-Dialekt
korrekt konfiguriert ist,

176
00:10:11,365 --> 00:10:13,025
bevor Sie sie ausführen.

177
00:10:13,025 --> 00:10:18,560
Hier sehen Sie, dass die Jobausführung
etwa 192 Kilobyte Daten generiert,

178
00:10:18,560 --> 00:10:20,924
darunter
Informationen über die Abholstunden,

179
00:10:20,924 --> 00:10:23,120
durchschnittliche
Fahrtdistanzen und Gebühren

180
00:10:23,120 --> 00:10:26,120
sowie die anderen Informationen,
die Dataflow berechnet.

181
00:10:26,120 --> 00:10:29,000
Okay. So weit für dieses Lab.