1
00:00:00,730 --> 00:00:02,090
Anteriormente en este módulo

2
00:00:02,090 --> 00:00:04,680
conocieron técnicas
para implementar el código

3
00:00:04,690 --> 00:00:07,650
que se encarga del preprocesamiento
y la creación de atributos.

4
00:00:07,650 --> 00:00:12,320
Para usar estas técnicas debemos
conocer bien el dominio del problema

5
00:00:12,320 --> 00:00:16,775
además de saber bastante
sobre los datos sin procesar de entrada.

6
00:00:16,800 --> 00:00:19,910
En la práctica, no siempre
contaremos con la información necesaria

7
00:00:19,910 --> 00:00:21,827
y habrá que hacer
ingeniería de atributos

8
00:00:21,827 --> 00:00:24,025
con datos de dominios
de los que sabemos poco

9
00:00:24,025 --> 00:00:28,125
sabiendo poco o nada
acerca de los datos sin procesar.

10
00:00:29,125 --> 00:00:30,570
En el resto de este módulo

11
00:00:30,570 --> 00:00:33,650
veremos las herramientas
y técnicas que nos pueden ayudar

12
00:00:33,660 --> 00:00:36,000
si comenzamos
con conjuntos de datos desde cero.

13
00:00:36,000 --> 00:00:40,205
Anteriormente, usaron herramientas
como Apache Beam y Cloud Dataflow.

14
00:00:40,595 --> 00:00:43,950
Ahora, conocerán
una herramienta llamada Cloud Dataprep

15
00:00:43,950 --> 00:00:48,024
que ofrece una interfaz gráfica
de usuario interactiva

16
00:00:48,024 --> 00:00:51,270
que nos ayuda a comprender,
visualizar y preprocesar los datos.

17
00:00:51,760 --> 00:00:54,940
Si se hace bien, la ingeniería
de atributos puede mejorar mucho

18
00:00:54,940 --> 00:00:57,325
el rendimiento de un sistema de AA.

19
00:00:57,325 --> 00:00:59,555
Para hacer bien
la ingeniería de atributos

20
00:00:59,555 --> 00:01:02,355
es importante conocer
el dominio del sistema que usamos.

21
00:01:02,355 --> 00:01:06,230
Específicamente, debemos conocer
los datos sin procesar de entrada.

22
00:01:06,230 --> 00:01:07,532
¿Qué significa esto?

23
00:01:07,532 --> 00:01:12,815
¿Cómo abordamos un conjunto de datos
con miles de millones de registros?

24
00:01:13,480 --> 00:01:16,750
Cuando trabajamos con un conjunto
de datos totalmente desconocido

25
00:01:16,750 --> 00:01:19,300
el primer paso
es realizar un análisis exploratorio.

26
00:01:19,300 --> 00:01:22,025
Tenemos que visualizar
los valores del conjunto de datos

27
00:01:22,025 --> 00:01:25,870
comprender los valores
más y menos comunes

28
00:01:25,870 --> 00:01:28,765
identificar valores anómalos
y buscar valores faltantes.

29
00:01:29,375 --> 00:01:32,800
Es fundamental tener estadísticas
del conjunto de datos, como promedios

30
00:01:32,800 --> 00:01:35,240
la desviación estándar
de las variables de los datos

31
00:01:35,240 --> 00:01:37,715
sus valores mínimos y máximos

32
00:01:37,715 --> 00:01:41,550
y queremos explorar
las distribuciones de esos valores.

33
00:01:42,250 --> 00:01:44,945
Cuando trabajemos
con aprendizaje automático

34
00:01:44,945 --> 00:01:48,789
probablemente estemos en un equipo
que incluirá científicos de datos

35
00:01:48,789 --> 00:01:51,735
desarrolladores de software
y analistas de negocios.

36
00:01:51,735 --> 00:01:55,060
Por eso, necesitaremos
una manera de compartir

37
00:01:55,060 --> 00:01:58,170
con otras personas
lo que aprendimos del conjunto de datos.

38
00:01:58,170 --> 00:02:01,485
Eso también nos permitirá
beneficiarnos del conocimiento del equipo.

39
00:02:01,485 --> 00:02:04,960
En el resto del módulo, veremos
dos enfoques complementarios.

40
00:02:04,960 --> 00:02:07,180
Empezaremos por explorar
un conjunto de datos

41
00:02:07,180 --> 00:02:10,040
y pasaremos al preprocesamiento
y la creación de atributos.

42
00:02:10,040 --> 00:02:13,640
Primero, usaremos
herramientas que ya conocen

43
00:02:13,640 --> 00:02:17,335
como BigQuery,
Cloud Dataflow y TensorFlow.

44
00:02:17,735 --> 00:02:20,800
Para el segundo enfoque,
presentaremos Cloud Dataprep

45
00:02:20,800 --> 00:02:24,977
y veremos cómo puede ayudarnos
con el análisis exploratorio

46
00:02:24,977 --> 00:02:26,525
y el procesamiento de datos.

47
00:02:26,525 --> 00:02:28,120
Comencemos con el primer enfoque

48
00:02:28,120 --> 00:02:30,990
usando herramientas que
conocemos para explorar los datos.

49
00:02:30,990 --> 00:02:32,195
En este curso

50
00:02:32,195 --> 00:02:34,735
ya vieron ejemplos
de algunas bibliotecas de gráficos

51
00:02:34,735 --> 00:02:37,875
como Seaborn para visualizar
los datos en Cloud Datalab.

52
00:02:38,155 --> 00:02:41,080
Este ejemplo muestra
un gráfico de datos

53
00:02:41,080 --> 00:02:43,265
del conjunto de tarifas
de taxis de Nueva York

54
00:02:43,265 --> 00:02:44,870
que está disponible en BigQuery.

55
00:02:44,980 --> 00:02:50,340
El diagrama muestra la distancia
de los viajes en relación con la tarifa.

56
00:02:50,450 --> 00:02:55,040
Usar un notebook en Datalab
para explorar y visualizar los datos

57
00:02:55,040 --> 00:02:57,190
podría parecer un enfoque práctico.

58
00:02:57,190 --> 00:03:01,045
Sin embargo, recordemos
que el entorno predeterminado de Datalab

59
00:03:01,045 --> 00:03:05,190
se ejecuta en un solo servidor virtual
con una cantidad limitada de memoria.

60
00:03:05,500 --> 00:03:09,820
En el conjunto de datos de tarifas
de taxi, hay miles de millones de datos.

61
00:03:09,950 --> 00:03:14,445
Sería poco práctico o demasiado caro
trazar y analizar todos esos datos

62
00:03:14,445 --> 00:03:17,285
con un entorno
de Datalab de un solo nodo.

63
00:03:17,285 --> 00:03:23,620
En vez de cargar millones de registros
de viajes en taxi en el entorno de Datalab

64
00:03:23,875 --> 00:03:28,415
podemos usar SQL y calcular
estadísticas de resumen con BigQuery.

65
00:03:28,865 --> 00:03:30,390
Como se ve en este diagrama

66
00:03:30,390 --> 00:03:33,705
podemos usar Datalab
para escribir el código de SQL.

67
00:03:33,705 --> 00:03:36,905
Cuando el código está listo,
enviamos la instrucción de SQL

68
00:03:36,905 --> 00:03:40,645
a BigQuery a través de las API
y obtenemos el resultado.

69
00:03:41,125 --> 00:03:44,350
Como las estadísticas de resumen
son solo algunas filas de datos

70
00:03:44,350 --> 00:03:47,620
podemos trazarlas fácilmente
en Datalab con Seaborn

71
00:03:47,620 --> 00:03:50,210
o alguna otra biblioteca
de visualización de Python.

72
00:03:50,210 --> 00:03:53,860
Además, como aprendimos
anteriormente en el módulo

73
00:03:53,860 --> 00:03:57,870
podemos usar las API de Apache Beam
y Cloud Dataflow para implementar

74
00:03:57,870 --> 00:04:01,825
cálculos de estadísticas de resumen
y otros preprocesamientos de datos.

75
00:04:02,085 --> 00:04:04,612
Podemos usar Python o Java
para escribir el código

76
00:04:04,612 --> 00:04:06,460
de la canalización de procesamiento.

77
00:04:06,800 --> 00:04:09,265
Ahora, veamos el segundo enfoque

78
00:04:09,265 --> 00:04:14,170
en el que usaremos Cloud Dataprep
para comprender mejor los datos de entrada

79
00:04:14,170 --> 00:04:18,190
y hacer ingeniería de atributos
con una interfaz visual interactiva

80
00:04:18,190 --> 00:04:20,095
en vez de escribir código de bajo nivel.

81
00:04:21,005 --> 00:04:22,810
¿Qué es Cloud Dataprep?

82
00:04:23,070 --> 00:04:26,260
Es un servicio completamente administrado
que forma parte de GCP.

83
00:04:26,260 --> 00:04:29,845
Permite explorar y transformar
los datos de manera interactiva

84
00:04:29,845 --> 00:04:32,980
a través de un navegador web
y con muy poco código.

85
00:04:34,040 --> 00:04:37,300
Dataprep puede obtener datos
de una variedad de fuentes

86
00:04:37,300 --> 00:04:39,855
como Google Cloud Storage y BigQuery.

87
00:04:39,855 --> 00:04:42,840
También podemos subir
nuestros propios datos a Dataprep.

88
00:04:43,410 --> 00:04:46,055
Una vez que Dataprep sabe
dónde debe obtener los datos

89
00:04:46,240 --> 00:04:51,565
podemos usar esta IU gráfica
para explorarlos y crear visualizaciones.

90
00:04:51,635 --> 00:04:55,085
Por ejemplo, podemos ver
histogramas de valores de datos

91
00:04:55,085 --> 00:04:59,600
y obtener resúmenes estadísticos,
como promedios y valores de percentiles.

92
00:05:00,300 --> 00:05:03,055
Tras explorar y comprender
el conjunto de datos

93
00:05:03,055 --> 00:05:07,555
podemos usar Dataprep para calcular
flujos de las transformaciones de datos.

94
00:05:09,035 --> 00:05:12,655
Los flujos son similares
a las canalizaciones de Dataflow.

95
00:05:12,655 --> 00:05:15,870
De hecho, los flujos
son compatibles con Dataflow.

96
00:05:15,870 --> 00:05:17,900
Podemos tomar un flujo de Dataprep

97
00:05:17,900 --> 00:05:21,085
y ejecutarlo como canalización
en la plataforma de Dataflow.

98
00:05:21,385 --> 00:05:25,880
En Dataprep, los flujos se implementan
como una secuencia de recetas

99
00:05:25,880 --> 00:05:31,375
que son pasos de procesamiento de datos
creados con una biblioteca de "wranglers".

100
00:05:31,645 --> 00:05:35,342
Dataprep tiene wranglers
para muchas tareas de procesamiento

101
00:05:35,342 --> 00:05:36,910
como se ve a la izquierda.

102
00:05:37,170 --> 00:05:39,890
Pronto veremos
ejemplos específicos de wranglers.

103
00:05:40,270 --> 00:05:43,900
Tengan en mente que no tendrán
que implementar estos pasos

104
00:05:43,900 --> 00:05:46,070
de procesamiento de datos en el código.

105
00:05:46,070 --> 00:05:50,455
Si usamos los wranglers, Dataprep
puede tomar el flujo y las recetas

106
00:05:50,455 --> 00:05:53,240
y convertirlos
en una canalización de Dataflow.

107
00:05:53,640 --> 00:05:56,495
Luego, con la misma interfaz de Dataprep

108
00:05:56,495 --> 00:05:59,950
podemos tomar el flujo,
ejecutarlo como trabajo en Dataflow

109
00:05:59,950 --> 00:06:01,950
y supervisar el progreso de ese trabajo.

110
00:06:01,950 --> 00:06:06,780
La biblioteca de Dataprep tiene una gama
de wranglers listos para tareas comunes.

111
00:06:06,780 --> 00:06:11,920
Para limpiar datos, podemos deduplicarlos
o filtrar los valores faltantes o anómalos

112
00:06:11,920 --> 00:06:14,045
y también podemos
hacer agregaciones comunes

113
00:06:14,045 --> 00:06:16,170
como contar o sumar valores.

114
00:06:16,170 --> 00:06:19,820
También podemos combinar tablas
de datos con operaciones Join o Union

115
00:06:19,820 --> 00:06:23,760
y transformar los datos en diversos tipos,
como strings o números enteros.

116
00:06:23,760 --> 00:06:25,710
Mientras se ejecuta el flujo

117
00:06:25,710 --> 00:06:29,715
podemos usar la interfaz de Dataflow para
supervisar los detalles del progreso.

118
00:06:29,715 --> 00:06:34,335
Cuando el trabajo termina, podemos ver
un resumen de su estado en Dataprep.

119
00:06:34,700 --> 00:06:37,650
Como se puede ver en esta captura,
una vez terminado el trabajo

120
00:06:37,650 --> 00:06:41,680
el resumen incluye estadísticas
y visualizaciones que pueden obtenerse

121
00:06:41,680 --> 00:06:44,120
sobre cualquier conjunto
de datos en Dataprep.