1
00:00:00,400 --> 00:00:01,215
Bienvenidos.

2
00:00:01,215 --> 00:00:05,320
En este lab, tomarán el conjunto de datos
de los taxis de Nueva York de BigQuery

3
00:00:05,320 --> 00:00:08,445
y lo preprocesarán
con la herramienta Cloud Dataprep.

4
00:00:08,445 --> 00:00:12,210
Con ella, explorarán la distribución
de los valores de los datos

5
00:00:12,210 --> 00:00:15,470
visualizarán las distribuciones
con gráficos de histogramas

6
00:00:15,470 --> 00:00:19,710
y luego implementarán un flujo
en Dataprep para crear un nuevo atributo

7
00:00:19,710 --> 00:00:24,390
basado en el promedio de viajes
en taxi por hora en un periodo móvil.

8
00:00:24,555 --> 00:00:29,265
Finalmente, implementarán
y ejecutarán el flujo de Dataprep en GCP

9
00:00:29,265 --> 00:00:32,275
y supervisarán la ejecución con Dataflow.

10
00:00:32,275 --> 00:00:34,065
Veamos.

11
00:00:34,065 --> 00:00:39,330
Para comenzar con este lab
debemos preparar algunos requisitos.

12
00:00:39,370 --> 00:00:42,990
Comenzamos en el panel de control
de Google Cloud Platform.

13
00:00:42,990 --> 00:00:46,260
Primero, necesitamos
un depósito de Google Cloud Storage.

14
00:00:46,260 --> 00:00:50,510
Para crear uno, vamos
al menú Products & services

15
00:00:50,510 --> 00:00:53,550
al que pueden acceder
mediante el menú de tres líneas.

16
00:00:53,550 --> 00:00:55,785
Bajen hasta Storage

17
00:00:55,785 --> 00:00:59,205
Browser, y hagan clic en Create Bucket.

18
00:00:59,205 --> 00:01:01,755
Como pueden ver en la pantalla

19
00:01:01,755 --> 00:01:05,355
el nombre del depósito
debe ser único a nivel global.

20
00:01:05,355 --> 00:01:11,200
Tengo un nombre único
para mi depósito en la ubicación us-east4.

21
00:01:11,200 --> 00:01:13,665
Después de hacer clic en Create

22
00:01:13,665 --> 00:01:17,220
puedo ver que el depósito
con su nombre único ya está listo.

23
00:01:17,220 --> 00:01:21,885
Lo siguiente que hay que preparar
es el conjunto de datos de BigQuery.

24
00:01:21,885 --> 00:01:25,965
Podemos encontrar BigQuery
en el menú Products & services

25
00:01:25,965 --> 00:01:28,020
en la sección Big Data.

26
00:01:28,020 --> 00:01:30,220
Cuando hacemos clic en BigQuery

27
00:01:30,220 --> 00:01:33,035
veremos que se abre una pestaña nueva.

28
00:01:33,035 --> 00:01:35,330
A la derecha del nombre del proyecto

29
00:01:35,330 --> 00:01:40,180
hagan clic en la flecha hacia abajo
y seleccionen Create new dataset.

30
00:01:40,180 --> 00:01:44,185
Usen el nombre taxi_cab_reporting
para el conjunto de datos

31
00:01:44,185 --> 00:01:45,880
y hagan clic en OK.

32
00:01:46,870 --> 00:01:48,800
Cuando el conjunto de datos está listo

33
00:01:48,800 --> 00:01:52,115
regresamos al panel de control
de Google Cloud Platform.

34
00:01:52,115 --> 00:01:57,630
Ahora, vamos al vínculo de Dataprep
en el menú Products & services.

35
00:01:57,630 --> 00:02:01,160
Como Cloud Dataprep
es un servicio de un socio de Google

36
00:02:01,160 --> 00:02:04,205
hay que aceptar
sus términos y condiciones.

37
00:02:04,205 --> 00:02:06,125
Hagan clic en Accept.

38
00:02:06,125 --> 00:02:09,680
También hay que hacer clic
en Allow, para que Trifacta

39
00:02:09,680 --> 00:02:13,640
el socio de Google que desarrolla
Dataprep, pueda acceder a los datos.

40
00:02:13,640 --> 00:02:15,435
Cuando hagan clic en Allow,

41
00:02:15,435 --> 00:02:18,560
Dataprep demorará unos minutos
en habilitarse para el proyecto.

42
00:02:18,560 --> 00:02:22,590
Por eso, el video avanza rápido
para que no tengamos que esperar.

43
00:02:22,590 --> 00:02:25,630
Ahora, deben seleccionar
la cuenta que desean usar

44
00:02:25,630 --> 00:02:29,680
para Cloud Dataprep y permitir
que Dataprep acceda al proyecto.

45
00:02:29,680 --> 00:02:33,430
Cuando configuren Dataprep
en el proyecto por primera vez

46
00:02:33,430 --> 00:02:37,265
tendrán que especificar el depósito de
almacenamiento que contendrá los datos.

47
00:02:37,265 --> 00:02:40,545
Aquí pueden ver que el depósito
que creamos al inicio del lab

48
00:02:40,545 --> 00:02:42,720
se usa para configurar Dataprep.

49
00:02:42,720 --> 00:02:46,020
Una vez seleccionado el depósito,
hagan clic en Continue.

50
00:02:46,020 --> 00:02:47,910
Tras configurar Dataprep

51
00:02:47,910 --> 00:02:49,997
pueden descartar el instructivo de ayuda

52
00:02:49,997 --> 00:02:52,805
con un clic
en "Don't show me any helpers".

53
00:02:53,365 --> 00:02:56,940
Ahora, usaremos Dataprep
para crear un flujo nuevo.

54
00:02:56,940 --> 00:03:00,290
Llamémoslo "NYC Taxi reporting".

55
00:03:00,290 --> 00:03:05,640
El flujo mostrará un proceso
para transferir, transformar y analizar

56
00:03:05,640 --> 00:03:07,165
los datos de los viajes en taxi.

57
00:03:07,635 --> 00:03:09,340
Hagan clic en Create.

58
00:03:09,340 --> 00:03:11,980
Lo primero que harán para crear un flujo

59
00:03:11,980 --> 00:03:15,760
es agregar conjuntos de datos
para que el flujo los procese.

60
00:03:15,760 --> 00:03:19,685
En este caso, importaremos
conjuntos de datos predefinidos

61
00:03:19,685 --> 00:03:23,195
que nuestro equipo guardó
en un depósito público de Cloud Storage.

62
00:03:25,445 --> 00:03:28,782
Para acceder a él,
usen el nombre asl-ml-immersion

63
00:03:28,782 --> 00:03:31,300
en el directorio /nyctaxicab.

64
00:03:31,300 --> 00:03:33,430
El directorio contiene varios archivos.

65
00:03:33,430 --> 00:03:39,315
Usarán los archivos con los datos
de viajes en taxi de 2015 y 2016.

66
00:03:39,315 --> 00:03:43,400
Fíjense en que son archivos .csv,
de valores separados por comas.

67
00:03:43,400 --> 00:03:45,220
Hagan clic en Import.

68
00:03:45,220 --> 00:03:48,620
En breve, verán
que los dos archivos se agregan al flujo.

69
00:03:48,620 --> 00:03:52,590
Para implementar el procesamiento de datos
o wrangling para estos conjuntos de datos

70
00:03:52,590 --> 00:03:54,570
tendremos que agregar una receta.

71
00:03:54,570 --> 00:03:57,640
Ahora, agregaremos pasos a la receta.

72
00:03:57,670 --> 00:03:59,800
Una vez se haya cargado
el conjunto de datos

73
00:03:59,810 --> 00:04:03,650
tendrán una vista previa
de una muestra de datos del conjunto.

74
00:04:03,650 --> 00:04:06,930
Por ejemplo, podemos ver que el
conjunto de datos incluye información

75
00:04:06,930 --> 00:04:10,035
sobre los viajes en taxi,
como la fecha y hora de inicio

76
00:04:10,035 --> 00:04:13,945
la fecha y hora de fin,
y la cantidad de pasajeros del taxi.

77
00:04:14,355 --> 00:04:17,670
También podemos ver
en el histograma de las distancias

78
00:04:17,670 --> 00:04:21,089
que la mayoría de los viajes
fueron de menos de cinco millas.

79
00:04:21,089 --> 00:04:25,927
Ahora, aplicaremos la operación Union
a los conjuntos de datos de 2015 y 2016

80
00:04:25,927 --> 00:04:28,325
para trabajar con más filas de datos.

81
00:04:28,325 --> 00:04:33,685
Seleccionamos el conjunto de 2016
y hacemos clic en Add and Align by Name

82
00:04:33,685 --> 00:04:37,030
para que los nombres
con los encabezados de columnas adecuados

83
00:04:37,030 --> 00:04:40,095
se alineen con la versión
de los datos resultante de Union.

84
00:04:40,095 --> 00:04:42,480
Una vez que agregamos
el paso de unión a la receta

85
00:04:42,480 --> 00:04:44,865
Dataprep hace una vista previa de la unión

86
00:04:44,865 --> 00:04:49,880
y presenta una muestra de los conjuntos
de datos con los viajes de 2015 y 2016.

87
00:04:50,330 --> 00:04:54,575
Observen que los datos
de fecha y hora de inicio del viaje

88
00:04:54,575 --> 00:04:56,222
están en columnas diferentes.

89
00:04:56,862 --> 00:05:01,460
Ya que este lab muestra cómo calcular
promedios móviles del costo de los viajes

90
00:05:01,460 --> 00:05:05,980
convertiremos los datos de entrada
al formato de fecha y hora de SQL.

91
00:05:07,000 --> 00:05:10,060
Para eso, agregaremos
una operación Merge a la receta.

92
00:05:10,060 --> 00:05:12,950
Eso concatenará
los valores de varias columnas.

93
00:05:13,150 --> 00:05:17,405
En este caso, las columnas
se llaman pickup_date y pickup_time.

94
00:05:17,405 --> 00:05:20,665
Usaremos pickup_datetime
como nombre de la nueva columna.

95
00:05:20,905 --> 00:05:25,385
Además, usaremos un espacio como
delimitador de valores.

96
00:05:25,385 --> 00:05:29,110
Observen que, a la izquierda, aparece
una vista previa de la nueva columna.

97
00:05:29,110 --> 00:05:31,820
Ahora, crearemos
una nueva columna derivada

98
00:05:31,820 --> 00:05:35,020
que convertirá pickup_time
al formato de fecha y hora de SQL.

99
00:05:35,020 --> 00:05:38,005
Una vez que un nuevo campo
de fecha y hora esté disponible

100
00:05:38,005 --> 00:05:42,562
extraeremos solo la información
del año, el mes, la fecha y la hora

101
00:05:42,562 --> 00:05:44,960
sin los detalles de minutos y segundos.

102
00:05:44,990 --> 00:05:50,345
Ya que la columna hour_pickup_datetime
no tiene valores de minutos y segundos

103
00:05:50,345 --> 00:05:53,540
no se puede convertir
al formato de fecha y hora de SQL.

104
00:05:53,540 --> 00:05:56,820
Por ende, debemos crear una nueva
columna que sí pueda convertirse

105
00:05:56,820 --> 00:05:59,800
en un valor de fecha
y hora válido para SQL.

106
00:05:59,800 --> 00:06:03,130
Para hacerlo, crearemos
una nueva operación Merge

107
00:06:03,130 --> 00:06:05,530
y volveremos a usar
el wrangler de Merge.

108
00:06:05,530 --> 00:06:10,260
Este wrangler concatenará los valores
de la columna hour_pickup_datetime

109
00:06:10,260 --> 00:06:14,890
con una string que contiene cuatro ceros,
para el valor de los minutos y segundos.

110
00:06:15,130 --> 00:06:17,860
Observen que cuando agregamos una columna

111
00:06:17,860 --> 00:06:21,475
se le asigna automáticamente
un nombre como column1.

112
00:06:21,475 --> 00:06:23,420
Podemos cambiarle el nombre.

113
00:06:23,420 --> 00:06:27,090
En este caso, le pondremos pickup_hour.

114
00:06:28,060 --> 00:06:32,830
Ahora, calcularemos algunas estadísticas
con los valores de pickup_hour.

115
00:06:32,830 --> 00:06:36,347
Podemos usar las funciones estándar
de agregación estadística de SQL

116
00:06:36,347 --> 00:06:37,925
como sum o average.

117
00:06:38,455 --> 00:06:42,040
Podemos ver que este wrangler
calculará las sumas y los promedios

118
00:06:42,040 --> 00:06:43,525
de la cantidad de pasajeros

119
00:06:43,525 --> 00:06:45,830
y la misma combinación
de la suma y el promedio

120
00:06:45,830 --> 00:06:47,750
para la distancia
del viaje y la tarifa.

121
00:06:47,750 --> 00:06:53,300
Por último, calculará
los montos máximos de cada hora de inicio.

122
00:06:56,120 --> 00:06:59,440
Como antes, observen que tenemos
una vista previa de los resultados

123
00:06:59,440 --> 00:07:03,515
de las estadísticas calculadas
en los histogramas de la izquierda.

124
00:07:03,515 --> 00:07:07,475
Además, si ven
los montos promedio en los histogramas

125
00:07:07,475 --> 00:07:12,490
la mayoría de los valores
oscilan entre USD 18 y 19 por viaje.

126
00:07:12,490 --> 00:07:17,150
Ahora, calcularemos
el promedio móvil de las tarifas.

127
00:07:17,150 --> 00:07:20,142
Para ello, usaremos los datos
de las 3 horas siguientes

128
00:07:20,142 --> 00:07:21,995
a cada hora de inicio.

129
00:07:21,995 --> 00:07:26,930
Para este cálculo, usaremos 
la función ROLLINGAVERAGE de Dataprep.

130
00:07:26,930 --> 00:07:29,335
Estos son los valores
del promedio móvil

131
00:07:29,335 --> 00:07:31,300
ordenados por hora de inicio.

132
00:07:31,300 --> 00:07:36,440
Por último, le ponemos
"average_3hour_rolling_fare" a la columna.

133
00:07:37,480 --> 00:07:39,750
Una vez que la receta está lista

134
00:07:39,750 --> 00:07:43,395
podemos implementarla
como trabajo de Google Cloud Dataflow.

135
00:07:43,395 --> 00:07:47,355
Para hacerlo, debemos hacer clic
en Run job y especificar la ubicación

136
00:07:47,355 --> 00:07:52,035
en la que almacenaremos
los resultados del trabajo.

137
00:07:52,035 --> 00:07:57,765
De manera predeterminada, se guardan
como archivo .csv en Google Cloud Storage.

138
00:07:57,765 --> 00:08:02,120
Como alternativa, podemos
cambiar el destino a BigQuery

139
00:08:02,120 --> 00:08:06,640
y crear una nueva tabla en BigQuery
cada vez que se ejecuta el trabajo.

140
00:08:06,640 --> 00:08:11,795
Si cambiamos la selección de la derecha
a "Create new table every run"

141
00:08:11,795 --> 00:08:16,300
y renombramos la tabla
a tlc_yellow_trips_reporting

142
00:08:16,300 --> 00:08:20,225
obtendremos una nueva tabla
en el conjunto de datos de los taxis.

143
00:08:20,225 --> 00:08:24,335
Recuerden que este es el conjunto
de datos que crearon al comienzo del lab.

144
00:08:25,585 --> 00:08:27,810
Ahora, ejecuten el trabajo.

145
00:08:28,050 --> 00:08:30,525
Cuando el trabajo
entre en la etapa "Transforming"

146
00:08:30,525 --> 00:08:33,559
Dataprep comenzará
a implementarlo en Dataflow.

147
00:08:33,559 --> 00:08:35,760
Esto suele tomar un tiempo.

148
00:08:35,760 --> 00:08:40,414
Podemos supervisar el progreso del trabajo
en la sesión de trabajo, en Dataprep.

149
00:08:40,414 --> 00:08:43,554
Para ello, hacemos clic
en los puntos suspensivos de la derecha.

150
00:08:43,554 --> 00:08:48,340
Este menú no tendrá el vínculo
al trabajo de Dataflow inmediatamente

151
00:08:48,340 --> 00:08:49,590
después de implementarlo

152
00:08:49,590 --> 00:08:52,659
pero si esperan un poco
y actualizan la página

153
00:08:52,659 --> 00:08:57,635
verán que se actualiza el menú
y aparece un vínculo al trabajo.

154
00:08:57,635 --> 00:09:02,110
Si hacen clic en el vínculo, irán
automáticamente a la interfaz de Dataflow

155
00:09:02,110 --> 00:09:05,045
donde podrán supervisar
los pasos de transformación detallados

156
00:09:05,045 --> 00:09:07,660
en Dataflow, creados por Dataprep.

157
00:09:07,660 --> 00:09:10,645
En el lado derecho de la IU de Dataflow

158
00:09:10,645 --> 00:09:13,615
pueden ver detalles
acerca de la ejecución del trabajo.

159
00:09:13,615 --> 00:09:17,160
Aquí, pueden ver que,
como el trabajo acaba de comenzar

160
00:09:17,160 --> 00:09:21,440
el clúster de Dataflow que ejecutará
el trabajo aún no ajustó su escala.

161
00:09:21,440 --> 00:09:25,225
Sin embargo, ya pueden supervisar
los resultados de la configuración.

162
00:09:25,225 --> 00:09:29,720
Aquí, no ha comenzado ninguno
de los pasos de transformación

163
00:09:29,720 --> 00:09:32,830
excepto los pocos que están
preparando la tabla en BigQuery

164
00:09:32,830 --> 00:09:37,950
y que recién comienzan a obtener datos
de los archivos .csv de Cloud Storage.

165
00:09:38,370 --> 00:09:41,730
Además de supervisar
este trabajo desde Dataflow

166
00:09:41,730 --> 00:09:45,420
pueden navegar a BigQuery
y supervisar el resultado del trabajo

167
00:09:45,420 --> 00:09:47,575
en el conjunto de datos
taxi_cab_reporting.

168
00:09:47,575 --> 00:09:50,525
Como recordarán,
cuando el trabajo comienza a ejecutarse

169
00:09:50,525 --> 00:09:55,135
insertará valores en una nueva tabla
llamada tlc_yellow_trips_reporting.

170
00:09:55,135 --> 00:09:57,600
Ya que la tabla tarda un tiempo en crearse

171
00:09:57,600 --> 00:10:01,545
tal vez deban esperar y actualizar
la página para ver los avances.

172
00:10:02,145 --> 00:10:05,390
Una vez que aparece la tabla,
podemos ingresar una instrucción de SQL

173
00:10:05,390 --> 00:10:07,655
para obtener resultados de la tabla.

174
00:10:07,655 --> 00:10:12,255
Asegúrense de haber configurado
el dialecto de SQL correctamente antes.

175
00:10:13,005 --> 00:10:18,350
Podemos ver que la ejecución
del trabajo generó unos 192 kb de datos

176
00:10:18,350 --> 00:10:20,920
lo que incluye
información sobre horas de inicio

177
00:10:20,920 --> 00:10:25,870
distancias promedio, tarifas promedio
y otra información calculada por Dataflow.

178
00:10:26,710 --> 00:10:29,030
Con esto, concluimos este lab.