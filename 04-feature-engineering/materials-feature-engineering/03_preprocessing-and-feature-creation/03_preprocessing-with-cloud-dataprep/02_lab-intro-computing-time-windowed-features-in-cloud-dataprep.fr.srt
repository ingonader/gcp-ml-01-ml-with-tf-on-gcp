1
00:00:00,000 --> 00:00:00,716
Bienvenue.

2
00:00:00,716 --> 00:00:03,712
Dans cet atelier, vous allez
récupérer l'ensemble de données

3
00:00:03,712 --> 00:00:07,020
sur les tarifs des courses en taxi
créé dans BigQuery et le prétraiter

4
00:00:07,020 --> 00:00:09,055
à l'aide de Cloud Dataprep.

5
00:00:09,055 --> 00:00:12,710
Dans cet outil, vous allez explorer
la distribution des valeurs des données,

6
00:00:12,710 --> 00:00:15,870
visualiser les distributions
avec des nuages d'histogramme

7
00:00:15,870 --> 00:00:18,570
et mettre en œuvre un flux Dataprep

8
00:00:18,570 --> 00:00:20,665
pour créer
une caractéristique correspondant

9
00:00:20,665 --> 00:00:22,760
au nombre moyen
de courses en taxi par heure

10
00:00:22,760 --> 00:00:24,555
dans une période mobile.

11
00:00:24,555 --> 00:00:29,265
Enfin, vous allez déployer
et exécuter le flux Dataprep sur GCP,

12
00:00:29,265 --> 00:00:32,505
puis surveiller l'exécution
de la tâche avec Dataflow.

13
00:00:32,505 --> 00:00:34,065
Voyons cela de plus près.

14
00:00:34,065 --> 00:00:37,110
Pour commencer,

15
00:00:37,110 --> 00:00:39,700
vous devez réaliser
quelques configurations préalables.

16
00:00:39,700 --> 00:00:42,990
Démarrez sur le tableau
de bord Google Cloud Platform.

17
00:00:42,990 --> 00:00:46,260
Il vous faut d'abord
un bucket Google Cloud Storage.

18
00:00:46,260 --> 00:00:50,510
Pour en créer un, accédez
au menu "Produits et services",

19
00:00:50,510 --> 00:00:53,550
disponible depuis l'icône
en forme de hamburger.

20
00:00:53,550 --> 00:00:55,785
Faites défiler jusqu'à "Stockage",

21
00:00:55,785 --> 00:00:59,205
"Navigateur", puis cliquez
sur "Créer un bucket".

22
00:00:59,205 --> 00:01:01,755
Comme le précisent
les indications à l'écran,

23
00:01:01,755 --> 00:01:05,355
le nom du bucket
de stockage doit être unique.

24
00:01:05,355 --> 00:01:11,200
J'ai configuré ici un nom
de bucket unique dans la zone "us-east4".

25
00:01:11,200 --> 00:01:13,665
Après avoir cliqué sur "Créer",

26
00:01:13,665 --> 00:01:17,220
je peux voir que le bucket
avec le nom unique est prêt.

27
00:01:17,220 --> 00:01:21,885
Vous devez ensuite préparer
l'ensemble de données BigQuery.

28
00:01:21,885 --> 00:01:25,965
Vous trouverez BigQuery
dans le menu "Produits et services",

29
00:01:25,965 --> 00:01:28,020
section "Big Data".

30
00:01:28,020 --> 00:01:30,220
Lorsque vous cliquez sur BigQuery,

31
00:01:30,220 --> 00:01:33,035
un nouvel onglet s'ouvre
dans le navigateur.

32
00:01:33,035 --> 00:01:35,330
À droite du nom de votre projet,

33
00:01:35,330 --> 00:01:37,755
cliquez sur la flèche vers le bas

34
00:01:37,755 --> 00:01:40,180
et sélectionnez
"Créer un ensemble de données".

35
00:01:40,180 --> 00:01:46,350
Saisissez le nom
"taxi_cab_reporting" et cliquez sur "OK".

36
00:01:46,350 --> 00:01:48,800
Une fois que
l'ensemble de données est prêt,

37
00:01:48,800 --> 00:01:52,115
revenez au tableau
de bord Google Cloud Platform.

38
00:01:52,115 --> 00:01:57,630
Accédez au lien "Dataprep"
du menu "Produits et services".

39
00:01:57,630 --> 00:02:01,160
Cloud Dataprep étant
un service d'un partenaire Google,

40
00:02:01,160 --> 00:02:04,205
vous devez accepter
de nouvelles conditions d'utilisation.

41
00:02:04,205 --> 00:02:06,125
Cliquez sur "Accepter".

42
00:02:06,125 --> 00:02:09,680
Vous devez également autoriser Trifacta,

43
00:02:09,680 --> 00:02:13,640
le partenaire Google qui développe
Dataprep, à accéder à vos données.

44
00:02:13,640 --> 00:02:18,350
Cliquez sur "Autoriser". L'activation
du projet prend quelques minutes.

45
00:02:18,350 --> 00:02:22,590
La vidéo passe en avance rapide.

46
00:02:22,590 --> 00:02:26,300
Vous devez ensuite choisir le compte
à utiliser pour Cloud Dataprep

47
00:02:26,300 --> 00:02:29,680
et autoriser Dataprep
à accéder à votre projet.

48
00:02:29,680 --> 00:02:33,430
Lorsque vous configurez Dataprep
sur votre projet pour la première fois,

49
00:02:33,430 --> 00:02:36,775
vous devez indiquer le bucket
de stockage qui contiendra vos données.

50
00:02:36,775 --> 00:02:40,545
Vous pouvez voir ici
que le bucket créé au début de l'atelier

51
00:02:40,545 --> 00:02:42,720
est utilisé pour configurer Dataprep.

52
00:02:42,720 --> 00:02:46,020
Une fois le bucket sélectionné,
cliquez sur "Continuer".

53
00:02:46,020 --> 00:02:48,020
Une fois Dataprep configuré, cliquez sur

54
00:02:48,020 --> 00:02:52,805
"Ne pas afficher les assistants"
pour désactiver le tutoriel.

55
00:02:52,805 --> 00:02:56,940
Vous allez maintenant utiliser
Dataprep pour créer un flux.

56
00:02:56,940 --> 00:03:00,290
Appelons-le "NYC Taxi Cab Reporting".

57
00:03:00,290 --> 00:03:03,805
Le flux correspondra
à un processus d'ingestion,

58
00:03:03,820 --> 00:03:07,165
de transformation et d'analyse
des données sur les taxis.

59
00:03:07,165 --> 00:03:09,340
Cliquez sur "Créer".

60
00:03:09,340 --> 00:03:12,580
Pour créer un flux, vous devez d'abord

61
00:03:12,580 --> 00:03:15,760
ajouter des ensembles
de données à traiter.

62
00:03:15,760 --> 00:03:20,345
Dans ce cas, vous importerez des ensembles
de données prédéfinis que notre équipe

63
00:03:20,345 --> 00:03:23,155
a déjà enregistrés
dans le bucket de stockage public.

64
00:03:23,155 --> 00:03:25,445
Pour accéder à ce bucket,

65
00:03:25,445 --> 00:03:30,980
saisissez le nom "asl-ml-immersion"
dans le répertoire "nyctaxicab".

66
00:03:30,980 --> 00:03:33,430
Le répertoire contient plusieurs fichiers,

67
00:03:33,430 --> 00:03:39,315
que vous utiliserez avec les données
des courses en taxi de 2015 et 2016.

68
00:03:39,315 --> 00:03:43,400
Ce sont des fichiers CSV
(valeurs séparées par une virgule).

69
00:03:43,400 --> 00:03:45,910
Cliquez sur "Importer".

70
00:03:45,910 --> 00:03:48,620
Les deux fichiers sont
rapidement ajoutés à votre flux.

71
00:03:48,620 --> 00:03:51,345
Pour mettre en œuvre
le traitement ou la préparation

72
00:03:51,345 --> 00:03:53,540
des données
de ces ensembles de données,

73
00:03:53,540 --> 00:03:55,660
vous devez ajouter une combinaison,

74
00:03:55,660 --> 00:03:57,930
puis lui ajouter des étapes.

75
00:03:57,930 --> 00:03:59,750
Une fois l'ensemble de données chargé,

76
00:03:59,750 --> 00:04:03,190
vous obtenez un aperçu
des données qu'il contient.

77
00:04:03,190 --> 00:04:06,380
Ici, vous pouvez par exemple voir
que l'ensemble de données contient

78
00:04:06,380 --> 00:04:08,335
des informations sur les courses en taxi,

79
00:04:08,335 --> 00:04:11,765
comme le jour et l'heure de prise
en charge, le jour et l'heure de dépôt,

80
00:04:11,765 --> 00:04:13,855
et le nombre de passagers.

81
00:04:13,855 --> 00:04:17,670
Aussi, dans l'histogramme
"trip_distance", vous pouvez voir

82
00:04:17,670 --> 00:04:21,089
que la plupart des courses
sont inférieures à cinq miles.

83
00:04:21,089 --> 00:04:28,325
Réunissez les ensembles de données
de 2015 et 2016 pour avoir plus de lignes.

84
00:04:28,325 --> 00:04:31,125
Sélectionnez l'ensemble de 2016,

85
00:04:31,125 --> 00:04:34,050
puis cliquez sur
"Ajouter et trier par nom"

86
00:04:34,050 --> 00:04:37,250
pour que les noms
correspondant aux en-têtes de colonne

87
00:04:37,250 --> 00:04:40,095
soient alignés
avec l'ensemble de données d'union.

88
00:04:40,095 --> 00:04:42,480
Ajoutez l'étape
d'unification à la combinaison.

89
00:04:42,480 --> 00:04:44,865
Lorsque Dataprep affiche un aperçu,

90
00:04:44,865 --> 00:04:50,030
vous voyez un échantillon qui comprend
les courses en taxi de 2015 et 2016.

91
00:04:50,030 --> 00:04:56,355
Notez que la date et l'heure de prise
en charge sont des données séparées.

92
00:04:56,355 --> 00:04:59,520
Cet atelier va vous montrer
comment calculer

93
00:04:59,520 --> 00:05:01,750
des moyennes mobiles du prix des courses.

94
00:05:01,750 --> 00:05:06,430
Vous devez d'abord convertir les données
d'entrée au format SQL datetime.

95
00:05:06,430 --> 00:05:09,810
Pour ce faire, vous pouvez ajouter
à la combinaison une étape de fusion

96
00:05:09,810 --> 00:05:12,950
qui concatène les valeurs
de plusieurs colonnes,

97
00:05:12,950 --> 00:05:17,405
dans ce cas, "pickup_day"
et "pickup_time".

98
00:05:17,405 --> 00:05:20,905
Nommez la nouvelle colonne
"pickup_datetime".

99
00:05:20,905 --> 00:05:25,385
Utilisez une espace unique
comme délimiteur entre les valeurs.

100
00:05:25,385 --> 00:05:26,960
Sur la gauche,

101
00:05:26,960 --> 00:05:29,005
un aperçu
de la nouvelle colonne s'affiche.

102
00:05:29,005 --> 00:05:31,820
Créez ensuite une colonne dérivée

103
00:05:31,820 --> 00:05:35,020
qui convertira "pickup_time"
au format SQL datetime.

104
00:05:35,020 --> 00:05:38,005
Dès qu'un nouveau champ
"datetime" est disponible,

105
00:05:38,005 --> 00:05:41,120
extrayez l'année, le mois, la date

106
00:05:41,120 --> 00:05:44,980
et l'heure avec
les minutes et les secondes.

107
00:05:44,980 --> 00:05:47,662
Comme la colonne
"hour_pickup_datetime" ne contient pas

108
00:05:47,662 --> 00:05:50,345
les minutes et les secondes,

109
00:05:50,345 --> 00:05:53,540
elle ne peut pas être convertie
au format SQL datetime.

110
00:05:53,540 --> 00:05:56,300
Vous devez donc créer une colonne qui peut

111
00:05:56,300 --> 00:05:59,650
être convertie
en valeur SQL datetime valide.

112
00:05:59,650 --> 00:06:01,470
Pour ce faire, créez

113
00:06:01,470 --> 00:06:05,530
une opération de fusion et utilisez
à nouveau l'outil de fusion.

114
00:06:05,530 --> 00:06:09,200
Celui-ci concatène
les valeurs de la colonne

115
00:06:09,200 --> 00:06:11,660
"hour_pickup_datetime" avec une chaîne

116
00:06:11,660 --> 00:06:15,130
contenant quatre zéros pour
les valeurs des minutes et des secondes.

117
00:06:15,130 --> 00:06:17,860
Lorsque vous ajoutez une colonne,

118
00:06:17,860 --> 00:06:21,195
elle obtient un nom
généré automatiquement comme "column1".

119
00:06:21,195 --> 00:06:23,040
Vous pouvez la renommer facilement.

120
00:06:23,040 --> 00:06:28,700
Dans ce cas, vous pouvez
la renommer "pickup_hour".

121
00:06:28,700 --> 00:06:32,830
Calculez ensuite des statistiques
basées sur les valeurs de "pickup_hour".

122
00:06:32,830 --> 00:06:36,505
Vous pouvez utiliser des fonctions
d'agrégation statistique SQL standards

123
00:06:36,505 --> 00:06:38,595
comme "sum" ou "average".

124
00:06:38,595 --> 00:06:41,680
Vous pouvez voir que cet outil
calcule les sommes et les moyennes

125
00:06:41,680 --> 00:06:44,130
pour le nombre de passagers,
et la même combinaison

126
00:06:44,130 --> 00:06:46,105
de la somme et de la moyenne pour

127
00:06:46,105 --> 00:06:48,610
la distance
et le montant équitable des courses.

128
00:06:48,610 --> 00:06:56,710
Enfin, il calcule les montants maximaux
pour chaque heure de prise en charge.

129
00:06:56,710 --> 00:06:59,440
Comme précédemment,
vous obtenez un aperçu des résultats

130
00:06:59,440 --> 00:07:03,515
pour les statistiques calculées
dans les histogrammes sur la gauche.

131
00:07:03,515 --> 00:07:07,475
Si vous observez les montants
équitables moyens dans les histogrammes,

132
00:07:07,475 --> 00:07:12,490
vous voyez que
la plupart se situent entre 18 $ et 19 $.

133
00:07:12,490 --> 00:07:17,150
Calculez ensuite
la moyenne mobile du montant équitable

134
00:07:17,150 --> 00:07:21,995
d'après les heures de données disponibles
pour chaque heure de prise en charge.

135
00:07:21,995 --> 00:07:26,930
Vous pouvez la calculer avec la fonction
ROLLINGAVERAGE de Cloud Dataprep.

136
00:07:26,930 --> 00:07:29,335
Voici les valeurs de la moyenne mobile

137
00:07:29,335 --> 00:07:31,300
triées par heure de prise en charge.

138
00:07:31,300 --> 00:07:37,010
Enfin, nommez cette colonne
"average_3h_rolling_fare".

139
00:07:37,010 --> 00:07:39,750
Une fois la combinaison prête,

140
00:07:39,750 --> 00:07:43,395
vous pouvez la déployer
en tant que tâche Google Cloud Dataflow.

141
00:07:43,395 --> 00:07:47,355
Pour ce faire, cliquez
sur "Exécuter la tâche" et indiquez

142
00:07:47,355 --> 00:07:52,035
où les résultats de la tâche
seront publiés, c'est-à-dire stockés.

143
00:07:52,035 --> 00:07:57,765
Par défaut, ils sont enregistrés dans
un fichier CSV sur Google Cloud Storage.

144
00:07:57,765 --> 00:08:02,120
Mais, vous pouvez
indiquer BigQuery comme destination

145
00:08:02,120 --> 00:08:06,640
et y créer une table à chaque fois
que la tâche et exécutée.

146
00:08:06,640 --> 00:08:10,945
Si vous modifiez votre sélection
à droite pour créer une table

147
00:08:10,945 --> 00:08:16,300
à chaque exécution et renommez la table
"tlc_yellow_trips_reporting",

148
00:08:16,300 --> 00:08:17,785
vous obtenez une nouvelle table

149
00:08:17,785 --> 00:08:20,225
dans l'ensemble de données
"NYC Taxi Cab Reporting".

150
00:08:20,225 --> 00:08:24,885
C'est celui que vous avez créé
au début de cet atelier.

151
00:08:24,885 --> 00:08:28,210
Exécutez la tâche.

152
00:08:28,210 --> 00:08:30,345
Une fois la tâche en cours
de transformation,

153
00:08:30,345 --> 00:08:33,559
Dataprep commence
à la déployer sur Dataflow.

154
00:08:33,559 --> 00:08:36,030
Cette opération prend
généralement quelques instants.

155
00:08:36,030 --> 00:08:38,132
Vous pouvez suivre
la progression de la tâche

156
00:08:38,132 --> 00:08:40,174
dans la section "Tâches" du menu Dataprep.

157
00:08:40,174 --> 00:08:43,934
Si vous cliquez sur l'icône représentant
des points de suspension sur la droite,

158
00:08:43,934 --> 00:08:46,510
le menu ne contiendra pas le lien

159
00:08:46,510 --> 00:08:49,590
vers la tâche Dataflow
tout de suite après son déploiement.

160
00:08:49,590 --> 00:08:52,659
Attendez quelques instants
et actualisez la page

161
00:08:52,659 --> 00:08:57,635
pour que le menu soit mis à jour
et que le lien apparaisse.

162
00:08:57,635 --> 00:09:02,110
Si vous cliquez sur le lien, vous serez
redirigé vers l'interface de Dataflow,

163
00:09:02,110 --> 00:09:05,825
où vous pouvez surveiller dans Dataflow
les étapes de transformation détaillées

164
00:09:05,825 --> 00:09:07,660
créées dans Dataprep.

165
00:09:07,660 --> 00:09:10,645
À droite de l'interface de Dataflow,

166
00:09:10,645 --> 00:09:13,615
des informations sur l'exécution
de la tâche s'affichent.

167
00:09:13,615 --> 00:09:17,160
Vous pouvez voir ici
que la tâche vient de démarrer.

168
00:09:17,160 --> 00:09:21,080
Le cluster Dataflow qui va exécuter
la tâche doit encore être mis à l'échelle.

169
00:09:21,080 --> 00:09:23,695
Cependant, vous pouvez
déjà surveiller les résultats

170
00:09:23,695 --> 00:09:25,225
de la configuration de la tâche.

171
00:09:25,225 --> 00:09:29,290
Aucune des étapes de transformation
individuelles de la tâche n'a démarré,

172
00:09:29,290 --> 00:09:32,820
sauf celles qui préparent
la table dans BigQuery

173
00:09:32,820 --> 00:09:35,230
et qui commencent
à peine à récupérer les données

174
00:09:35,230 --> 00:09:38,370
depuis les fichiers CSV d'entrée
dans Google Cloud Storage.

175
00:09:38,370 --> 00:09:41,730
En plus de surveiller
cette tâche dans Dataflow,

176
00:09:41,730 --> 00:09:44,140
vous pouvez accéder à BigQuery

177
00:09:44,140 --> 00:09:47,575
et surveiller le résultat de la tâche
dans votre ensemble de données.

178
00:09:47,575 --> 00:09:50,525
Pour rappel, une fois que la tâche
commence à s'exécuter,

179
00:09:50,525 --> 00:09:55,135
elle insère des valeurs dans une table
nommée "tlc_yellow_trip_reporting".

180
00:09:55,135 --> 00:09:57,610
Comme la création de la table
peut prendre un moment,

181
00:09:57,610 --> 00:10:01,545
patientez et actualisez si besoin
la page pour voir la mise à jour.

182
00:10:01,545 --> 00:10:03,710
Une fois la table en place,

183
00:10:03,710 --> 00:10:07,475
vous pouvez saisir une instruction SQL
pour récupérer les résultats de la table.

184
00:10:07,475 --> 00:10:10,020
Cependant, vérifiez que votre dialecte SQL

185
00:10:10,020 --> 00:10:12,565
est configuré correctement
avant de l'exécuter.

186
00:10:12,565 --> 00:10:18,560
Vous pouvez voir ici que l'exécution de
la tâche génère environ 192 Ko de données,

187
00:10:18,560 --> 00:10:21,040
dont les informations
sur l'heure de prise en charge,

188
00:10:21,040 --> 00:10:23,880
la distance moyenne
des courses, le montant moyen,

189
00:10:23,880 --> 00:10:26,120
et d'autres informations
calculées par Dataflow.

190
00:10:26,120 --> 00:10:29,030
Voilà, cet atelier est terminé.