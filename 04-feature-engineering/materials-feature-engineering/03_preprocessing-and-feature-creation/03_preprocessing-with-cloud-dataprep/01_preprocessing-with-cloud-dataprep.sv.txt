Early in this module, you learned about techniques for implementing the code to do Preprocessing and feature creation. To use these techniques, you need to have a pretty good understanding of the domain of your problem, and you also need to know quite a bit about your raw input data. In practice, you may not always have that knowledge and understanding, you may need to do feature engineering and data science in unfamiliar domains, and you may need to know little or nothing about your raw input data. So, in the rest of this module, you will take a look at tools and techniques that can help you if you're starting with data science from scratch. Previously, you can use tools like Apache beam and Cloud Dataflow. Next, you will learn about a tool called Cloud Dataprep, which lets you use an interactive, graphical user interface to better understand, visualize and preprocess your data. When done right, feature engineering can significantly improve the performance of your machine learning system. And to succeed with feature engineering, It is important to have domain knowledge for your system. And specifically, to understand your raw input data. So, what does this mean? How can you even start to understand a data set with millions, or billions of records. When working with a data set that you have never seen before, you should start with an exploratory analysis, you should visualize the values of the data set, understand which values happened frequently and infrequently, find outliers and look for missing values. You definitely want to know the statistics of the data set, averages, standard deviation for different variables in your data, there are minimum and maximum values, and you want to explore the distributions of these values. Also, when working on machine learning, chances are you working with a team that can include data scientists, software developers, and business analysts. This means that you should have a way to share the results of your learnings about the data set with others, and also tap into the knowledge of your team for insights. The rest of this module we'll cover two complementary approaches. Let's start with exploring a data set and move on to preprocessing and feature creation. The first approach, we'll use the tools that you have already seen including BigQuery, Cloud Dataflow and Tenserflow. The second approach, we'll introduce Cloud Dataprep, and show you how Dataprep can help with both exploratory analysis and data processing. Let's start with the first approach, where you will use the tools you already know to explore your data. Early in this course, you have seen examples of using graphing libraries like Seabourn to visualize data and Cloud Datalab. The example in your screen shows a plot of data from the New York City taxi fare data set available in BigQuery. In this case, the diagram graphs the taxi trip distance against a fair amount for the trips. Now, using a notebook in Datalab to explore and visualize your data set, may seem like a practical approach. However, remember that the default Datalab environment is running in a single virtual server with a limited amount of memory. In case of the taxi fare dataset, there are billions of data points. So, it will be impractical or too expensive, to plot and analyze all of them using just a single no datalab environment. Instead of loading the billions of records of the entire taxi fare data set in the data lab environment, you can use SQL and calculate summary statistics using BigQuery. As shown in this diagram, you can still use datalab to write you SQL code, once the code is ready, you submit the SQL statement to BigQuery via the APIs and get back the result. Since the summary statistics are just a few rows of data, you can easily plot them in datalab using Seaborne, or other Python visualization libraries. Also, as you learned from the earlier sections of this module, you can use Apache beam APIs and Cloud Dataflow to implement calculations of summary statistics and other data preprocessing jobs, you can use Python or Java to write the code for your data processing pipeline. Next, let's take a look at the second approach, where you will use Cloud Dataprep to develop a better understanding of your input data, and to do feature engineering using an interactive visual interface, instead of writing low level code. So, what is Cloud Dataprep? It is a fully managed service available from GCP, and it lets you explore and transform your data interactively using a web browser with a minimal amount of code, Dataprep can get data from a variety of sources including Google Cloud storage, and BigQuery. You can also upload your own data to Dataprep. Once Dataprep knows where to get your data, you can use this graphical UI to explore your data, and create data visualizations. For example, you can view histograms of data values and get statistical summaries like averages, percentile values. After you have explored and understood your dataset, you can use Dataprep to compute flows of data transformations. The flows are similar to the pipelines that you have seen in dataflow. In fact, the flows are compatible with dataflow. You can take a Dataprep flow, and run it as a pipeline on the data flow platform. In Dataprep, the flows are implemented as a sequence of recipes, the recipes are data processing steps built from a library of so called wranglers. Dataprep has Wranglers for many common data processing tasks shown on the left. You will see specific examples of Wranglers shortly. Keep in mind that instead of you having to implement these data processing steps and code yourself, if you use the wranglers, Dataprep can take your flow and its recipes, and convert them to a dataflow pipeline. Then, using the same Dataprep interface, you can take the flow, run it as a job on Dataflow and monitor the progress of the job. Dataprep library has a variety of pre-built Wranglers for common data processing tasks. You can clean up data using duplication of filter out missing an outlier values, or you can do common aggregations like counting or summing up values, or you can join a union different data tables together, and you can transform data into different types like strings or integers. While the flow is executing, you can use the Dataflow interface to monitor the details of the jobs progress, and once the job is done, you can get a summary of the job status in Dataprep. As you can see from the screenshot of the completed job, the summary includes the statistics and visualizations, that you can get for any dataset in Dataprep.