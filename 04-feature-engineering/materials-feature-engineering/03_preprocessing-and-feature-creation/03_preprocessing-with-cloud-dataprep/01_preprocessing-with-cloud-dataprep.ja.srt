1
00:00:00,000 --> 00:00:01,470
これまで

2
00:00:01,470 --> 00:00:04,370
前処理と特徴量作成のための

3
00:00:04,370 --> 00:00:07,680
コードの実装手法を学習しました

4
00:00:07,680 --> 00:00:10,330
こうした手法を使うには

5
00:00:10,330 --> 00:00:12,665
対象となる分野と

6
00:00:12,665 --> 00:00:16,200
入力する生データの理解が必要です

7
00:00:16,200 --> 00:00:17,980
実際には

8
00:00:17,980 --> 00:00:23,135
不慣れな分野のデータセットの
特徴エンジニアリングで

9
00:00:23,135 --> 00:00:28,125
生データに関して知識がないこともあります

10
00:00:28,125 --> 00:00:30,070
そこで 次は

11
00:00:30,070 --> 00:00:33,830
ゼロからのデータセットの処理に役立つ

12
00:00:33,830 --> 00:00:36,180
ツールと手法です

13
00:00:36,180 --> 00:00:41,295
次に取り上げるCloud Dataprepは

14
00:00:41,295 --> 00:00:44,860
対話型のGUIを使った

15
00:00:44,860 --> 00:00:51,044
データの理解、可視化、前処理に
特化したツールです

16
00:00:51,044 --> 00:00:54,130
特徴エンジニアリングを適切に行えば

17
00:00:54,130 --> 00:00:59,365
MLシステムのパフォーマンスが
大幅に向上します

18
00:00:59,365 --> 00:01:03,445
そのためには システムの対象分野の知識

19
00:01:03,445 --> 00:01:07,140
具体的には生データの理解が重要です

20
00:01:07,140 --> 00:01:10,185
では 膨大なデータセットの

21
00:01:10,185 --> 00:01:13,340
どこから始めるのでしょうか

22
00:01:13,340 --> 00:01:16,860
初めて見るデータセットの場合

23
00:01:16,860 --> 00:01:19,300
調査分析から始めます

24
00:01:19,300 --> 00:01:21,105
値を可視化し

25
00:01:21,105 --> 00:01:25,870
頻繁に発生する値とそうでない値を把握し

26
00:01:25,870 --> 00:01:28,765
外れ値や欠損値を探します

27
00:01:28,765 --> 00:01:32,130
データセットの平均値、標準偏差

28
00:01:32,130 --> 00:01:35,580
最小値、最大値

29
00:01:35,580 --> 00:01:41,485
値の分布などの統計情報が必要です

30
00:01:41,485 --> 00:01:44,560
MLに携わっていると

31
00:01:44,560 --> 00:01:48,425
データサイエンティストやソフトウェア開発者

32
00:01:48,425 --> 00:01:51,335
ビジネスアナリストと連携することも多く

33
00:01:51,335 --> 00:01:54,310
自分の調査の結果をチームと共有すると同時に

34
00:01:54,310 --> 00:01:58,950
チームメンバーの知識を活用する方法を

35
00:01:58,950 --> 00:02:02,225
身に付ける必要があります

36
00:02:02,225 --> 00:02:04,690
ここでは相補的な2つのアプローチで

37
00:02:04,690 --> 00:02:10,090
データセットの調査から始めて
前処理と特徴量作成に進みます

38
00:02:10,090 --> 00:02:12,650
1つ目のアプローチでは

39
00:02:12,650 --> 00:02:17,305
BigQuery、Dataflow、TenserFlow
などのツールを使用します

40
00:02:17,305 --> 00:02:22,880
2つ目のアプローチでは
調査分析とデータ処理の両方に役立つ

41
00:02:22,880 --> 00:02:26,755
Cloud Dataprepを取り上げます

42
00:02:26,755 --> 00:02:30,500
では 1つ目のアプローチからです

43
00:02:30,500 --> 00:02:32,195
これまでに

44
00:02:32,195 --> 00:02:34,735
Seabornなどを使った

45
00:02:34,735 --> 00:02:39,155
Datalabのデータ可視化の例を見てきました

46
00:02:39,155 --> 00:02:43,100
画面の例は BigQueryで利用できる

47
00:02:43,100 --> 00:02:47,570
ニューヨークのタクシー料金データセットの

48
00:02:47,570 --> 00:02:50,530
乗車距離と料金のグラフです

49
00:02:50,530 --> 00:02:54,300
デフォルトのDatalab環境での

50
00:02:54,300 --> 00:02:57,540
データセットの調査と可視化は

51
00:02:57,540 --> 00:03:02,610
メモリが限られた1つの
仮想サーバーで実行されます

52
00:03:02,610 --> 00:03:05,980
タクシー料金データセットには

53
00:03:05,980 --> 00:03:08,610
数十億のデータがあるため

54
00:03:08,610 --> 00:03:14,245
単一ノードのDatalab環境で
すべてをプロットし分析するのは

55
00:03:14,245 --> 00:03:17,285
非実用的でコストが高すぎます

56
00:03:17,285 --> 00:03:22,000
そこで データセット全体を読み込むのではなく

57
00:03:22,000 --> 00:03:28,425
SQLを使ってBigQueryで
基本統計量を計算します

58
00:03:28,425 --> 00:03:30,390
図のように

59
00:03:30,390 --> 00:03:34,235
DatalabでSQLコードを書いて

60
00:03:34,235 --> 00:03:38,385
APIを通じてBigQueryに送信し

61
00:03:38,385 --> 00:03:40,645
結果を取得します

62
00:03:40,645 --> 00:03:43,950
基本統計量は数行のデータなので

63
00:03:43,950 --> 00:03:47,770
SeabornなどPythonの
可視化ライブラリで

64
00:03:47,770 --> 00:03:50,210
容易にプロットできます

65
00:03:50,210 --> 00:03:53,300
また 前に説明したように

66
00:03:53,300 --> 00:03:56,920
Apache Beam APIとDataflowで

67
00:03:56,920 --> 00:04:01,985
基本統計量の計算などの
データ処理ジョブを実装できます

68
00:04:01,985 --> 00:04:06,460
コードにはPythonかJavaを使用できます

69
00:04:06,460 --> 00:04:09,375
次は 2つ目のアプローチです

70
00:04:09,375 --> 00:04:13,570
Dataprepを使って
入力データの理解を深め

71
00:04:13,570 --> 00:04:16,610
ローレベルコードを書く代わりに

72
00:04:16,610 --> 00:04:20,065
対話型GUIで特徴エンジニアリングを行います

73
00:04:20,065 --> 00:04:22,810
Cloud Dataprepは

74
00:04:22,810 --> 00:04:26,270
GCPのフルマネージドサービスです

75
00:04:26,270 --> 00:04:29,525
ウェブブラウザを使った対話型で

76
00:04:29,525 --> 00:04:34,050
最低限のコード作成で
データを調査、変換できます

77
00:04:34,050 --> 00:04:39,455
Dataprepはさまざまなソースから
データを取得でき

78
00:04:39,455 --> 00:04:42,840
独自のデータもアップロードできます

79
00:04:42,840 --> 00:04:45,930
ソースを指定したら

80
00:04:45,930 --> 00:04:50,835
GUIでデータを調査し可視化できます

81
00:04:50,835 --> 00:04:55,325
たとえば 値のヒストグラム表示や

82
00:04:55,325 --> 00:04:59,600
平均値などの統計量の計算ができます

83
00:04:59,600 --> 00:05:03,525
データセットを調査し理解したら

84
00:05:03,525 --> 00:05:07,295
データ変換のフローを作成します

85
00:05:07,295 --> 00:05:11,680
このフローはパイプラインに近いもので

86
00:05:11,680 --> 00:05:15,060
Dataprepのフローを

87
00:05:15,060 --> 00:05:21,085
Dataflowのパイプラインとして実行できます

88
00:05:21,085 --> 00:05:26,410
Dataprepのフローは一連のレシピで

89
00:05:26,410 --> 00:05:34,625
レシピはWranglerを使って
構築されたデータ処理ステップです

90
00:05:34,625 --> 00:05:37,270
後ほど説明しますが

91
00:05:37,270 --> 00:05:40,200
Wranglerを使えば

92
00:05:40,200 --> 00:05:45,155
データ処理ステップやコードを
実装することなく

93
00:05:45,155 --> 00:05:48,930
フローとフローに含まれるレシピを

94
00:05:48,930 --> 00:05:53,240
Dataflowパイプラインに変換できます

95
00:05:53,245 --> 00:05:58,600
さらに フローをDataflowのジョブとして実行し

96
00:05:58,600 --> 00:06:01,950
ジョブの進捗を確認できます

97
00:06:01,950 --> 00:06:07,810
一般的なデータ処理を行う
さまざまなWranglerがあり

98
00:06:07,810 --> 00:06:13,390
重複排除やフィルターによる
データのクリーンアップ

99
00:06:13,390 --> 00:06:16,810
カウントや合計などの集計

100
00:06:16,810 --> 00:06:19,860
複数のテーブルの統合

101
00:06:19,860 --> 00:06:23,760
データタイプの変換などが可能です

102
00:06:23,760 --> 00:06:25,530
フロー実行中は

103
00:06:25,530 --> 00:06:29,355
Dataflow管理画面でジョブの進捗を確認し

104
00:06:29,355 --> 00:06:30,705
完了後は

105
00:06:30,705 --> 00:06:34,230
Dataprepでステータスを確認できます

106
00:06:34,230 --> 00:06:37,270
画面は完了したジョブの概要で

107
00:06:37,270 --> 00:06:40,760
Dataperpのデータセットの

108
00:06:40,760 --> 00:06:44,120
統計情報とグラフが含まれます