1
00:00:00,000 --> 00:00:01,890
Au début de ce module,

2
00:00:01,890 --> 00:00:04,140
vous avez appris
les techniques de mise en œuvre

3
00:00:04,140 --> 00:00:07,290
du code de prétraitement
et de création des caractéristiques.

4
00:00:07,290 --> 00:00:08,880
Pour utiliser ces techniques,

5
00:00:08,880 --> 00:00:12,495
vous devez bien connaître
le domaine de votre problème

6
00:00:12,495 --> 00:00:15,770
et vos données d'entrée brutes.

7
00:00:15,770 --> 00:00:19,900
Dans la pratique, vous n'avez pas
forcément toujours ces connaissances.

8
00:00:19,900 --> 00:00:22,572
Vous pouvez avoir besoin
d'extraire des caractéristiques

9
00:00:22,572 --> 00:00:25,835
d'ensembles de données dans
des domaines que vous ne maîtrisez pas,

10
00:00:25,835 --> 00:00:28,785
en ne sachant rien, ou presque,
des données d'entrée brutes.

11
00:00:28,785 --> 00:00:30,570
Dans le reste de ce module,

12
00:00:30,570 --> 00:00:33,060
nous verrons des outils et techniques

13
00:00:33,060 --> 00:00:36,590
qui vous aideront si vous travaillez
sur des ensembles de données inconnus.

14
00:00:36,590 --> 00:00:40,365
Précédemment, vous avez utilisé des outils
comme Apache Beam et Cloud Dataflow.

15
00:00:40,365 --> 00:00:43,950
Vous allez maintenant
découvrir Cloud Dataprep,

16
00:00:43,950 --> 00:00:45,334
un outil qui offre

17
00:00:45,334 --> 00:00:48,180
une interface utilisateur
graphique interactive

18
00:00:48,180 --> 00:00:52,140
permettant de comprendre, de visualiser
et de prétraiter les données.

19
00:00:52,140 --> 00:00:54,810
Si elle est bien faite,
l'extraction de caractéristiques

20
00:00:54,810 --> 00:00:58,435
peut nettement améliorer les performances
d'un système de machine learning.

21
00:00:58,435 --> 00:00:59,355
Pour la réussir,

22
00:00:59,355 --> 00:01:02,355
il est important de connaître
le domaine de ce système,

23
00:01:02,355 --> 00:01:05,750
en particulier les données
d'entrée brutes.

24
00:01:05,750 --> 00:01:07,140
Qu'est-ce que cela signifie ?

25
00:01:07,140 --> 00:01:11,055
Comment se familiariser avec un ensemble
de données comportant des millions,

26
00:01:11,055 --> 00:01:13,010
voire des milliards d'enregistrements ?

27
00:01:13,010 --> 00:01:16,750
Lorsque vous travaillez avec un ensemble
de données que vous n'aviez jamais vu,

28
00:01:16,750 --> 00:01:19,300
vous devez commencer
par une analyse exploratoire.

29
00:01:19,300 --> 00:01:22,075
Vous devez visualiser les valeurs
de l'ensemble de données,

30
00:01:22,075 --> 00:01:25,870
comprendre quelles valeurs sont
fréquentes ou rares,

31
00:01:25,870 --> 00:01:28,765
et rechercher les valeurs
aberrantes et manquantes.

32
00:01:28,765 --> 00:01:32,580
Il est bon de connaître les statistiques
de l'ensemble de données, les moyennes,

33
00:01:32,580 --> 00:01:35,240
l'écart type des différentes variables
dans vos données,

34
00:01:35,240 --> 00:01:37,715
les valeurs minimale et maximale,

35
00:01:37,715 --> 00:01:41,550
ainsi que d'explorer
la distribution de ces valeurs.

36
00:01:41,550 --> 00:01:44,945
Lorsque vous faites du machine learning,

37
00:01:44,945 --> 00:01:48,789
vous travaillez probablement avec
une équipe comprenant des data scientists,

38
00:01:48,789 --> 00:01:52,075
des développeurs de logiciels
et des analystes de veille stratégique.

39
00:01:52,075 --> 00:01:54,430
Vous avez donc besoin
de partager vos découvertes

40
00:01:54,430 --> 00:01:57,640
relatives à l'ensemble de données
avec d'autres personnes

41
00:01:57,640 --> 00:02:01,165
et d'exploiter les connaissances
de votre équipe pour obtenir des insights.

42
00:02:01,165 --> 00:02:04,970
Dans le reste de ce module, nous verrons
deux approches complémentaires.

43
00:02:04,970 --> 00:02:07,160
Commençons par explorer
un ensemble de données,

44
00:02:07,160 --> 00:02:10,340
puis passons au prétraitement
et à la création de caractéristiques.

45
00:02:10,340 --> 00:02:14,330
Avec la première approche,
nous utiliserons des outils déjà explorés,

46
00:02:14,330 --> 00:02:17,305
comme BigQuery,
Cloud Dataflow et TensorFlow.

47
00:02:17,305 --> 00:02:20,800
Avec la seconde approche,
nous introduirons Cloud Dataprep,

48
00:02:20,800 --> 00:02:26,275
pour voir comment il permet l'analyse
exploratoire et le traitement des données.

49
00:02:26,275 --> 00:02:28,120
Commençons par la première approche,

50
00:02:28,120 --> 00:02:31,030
et explorons les données
avec les outils que nous connaissons.

51
00:02:31,030 --> 00:02:32,195
Au début de ce cours,

52
00:02:32,195 --> 00:02:34,675
vous avez vu des exemples
de bibliothèques graphiques

53
00:02:34,675 --> 00:02:38,055
comme Seaborn, utilisées pour visualiser
les données dans Cloud Datalab.

54
00:02:38,055 --> 00:02:40,267
L'exemple à l'écran
est un graphique de données

55
00:02:40,267 --> 00:02:43,830
issues de l'ensemble de données sur
le prix des courses en taxi à New York

56
00:02:43,830 --> 00:02:45,700
disponible dans BigQuery.

57
00:02:45,700 --> 00:02:47,230
Dans ce cas, le schéma illustre

58
00:02:47,230 --> 00:02:50,450
le rapport entre la distance
et le prix équitable des courses.

59
00:02:50,450 --> 00:02:54,815
L'exploration et la visualisation
de votre ensemble de données

60
00:02:54,815 --> 00:02:57,190
dans un bloc-notes Datalab
peut sembler pratique.

61
00:02:57,190 --> 00:03:01,045
Cependant, souvenez-vous
que l'environnement Datalab par défaut

62
00:03:01,045 --> 00:03:05,190
s'exécute dans un seul serveur virtuel
disposant d'une mémoire limitée.

63
00:03:05,190 --> 00:03:07,640
Or, l'ensemble de données
sur les taxis contient

64
00:03:07,640 --> 00:03:09,460
des millions de points de données.

65
00:03:09,460 --> 00:03:12,235
Il serait donc compliqué et trop cher

66
00:03:12,235 --> 00:03:17,285
de les tracer et de les analyser tous
dans un environnement Datalab à un nœud.

67
00:03:17,285 --> 00:03:20,230
Au lieu de charger
les millions d'enregistrements

68
00:03:20,230 --> 00:03:23,875
de l'ensemble de données sur les taxis
dans l'environnement Datalab,

69
00:03:23,875 --> 00:03:25,375
vous pouvez utiliser SQL

70
00:03:25,375 --> 00:03:28,415
et calculer des statistiques
récapitulatives avec BigQuery.

71
00:03:28,415 --> 00:03:30,390
Comme le montre ce schéma,

72
00:03:30,390 --> 00:03:33,705
vous pouvez quand même utiliser
Datalab pour écrire votre code SQL.

73
00:03:33,705 --> 00:03:35,315
Une fois le code prêt,

74
00:03:35,315 --> 00:03:37,980
vous envoyez l'instruction SQL à BigQuery

75
00:03:37,980 --> 00:03:40,145
par le biais des API
et obtenez le résultat.

76
00:03:40,145 --> 00:03:42,572
Comme les statistiques
récapitulatives ne comptent

77
00:03:42,572 --> 00:03:44,500
que quelques lignes de données,

78
00:03:44,500 --> 00:03:47,620
vous pouvez facilement
les tracer dans Datalab avec Seaborn

79
00:03:47,620 --> 00:03:50,210
ou d'autres bibliothèques
de visualisation Python.

80
00:03:50,210 --> 00:03:53,860
Comme vous l'avez appris
précédemment dans ce module,

81
00:03:53,860 --> 00:03:57,760
vous pouvez utiliser les API Apache Beam
et Cloud Dataflow pour mettre en œuvre

82
00:03:57,760 --> 00:03:59,792
des calculs
de statistiques récapitulatives

83
00:03:59,792 --> 00:04:01,825
et d'autres tâches de prétraitement.

84
00:04:01,825 --> 00:04:03,500
Vous pouvez utiliser Python ou Java

85
00:04:03,500 --> 00:04:07,070
pour écrire le code de votre pipeline
de traitement des données.

86
00:04:07,070 --> 00:04:09,265
Voyons maintenant
la seconde approche,

87
00:04:09,265 --> 00:04:14,170
qui consiste à utiliser Cloud Dataprep
pour mieux comprendre vos données d'entrée

88
00:04:14,170 --> 00:04:18,190
et à extraire des caractéristiques
avec une interface visuelle interactive

89
00:04:18,190 --> 00:04:20,065
au lieu d'écrire du code de bas niveau.

90
00:04:20,065 --> 00:04:22,810
Qu'est-ce que Cloud Dataprep ?

91
00:04:22,810 --> 00:04:26,260
C'est un service entièrement géré de GCP

92
00:04:26,260 --> 00:04:28,825
qui permet d'explorer
et de transformer des données

93
00:04:28,825 --> 00:04:30,892
de façon interactive
avec un navigateur Web

94
00:04:30,892 --> 00:04:33,160
en utilisant un minimum de code.

95
00:04:33,160 --> 00:04:35,880
Dataprep peut récupérer les données

96
00:04:35,880 --> 00:04:39,855
depuis différentes sources
telles que Cloud Storage et BigQuery.

97
00:04:39,855 --> 00:04:42,840
Vous pouvez aussi importer
vos propres données dans Dataprep.

98
00:04:42,840 --> 00:04:45,930
Une fois que Dataprep sait
où récupérer les données,

99
00:04:45,930 --> 00:04:49,595
vous pouvez utiliser
l'interface graphique pour les explorer

100
00:04:49,595 --> 00:04:51,635
et créer des visualisations.

101
00:04:51,635 --> 00:04:55,085
Par exemple, vous pouvez
afficher des histogrammes de valeurs

102
00:04:55,085 --> 00:04:57,342
et obtenir
des statistiques récapitulatives,

103
00:04:57,342 --> 00:04:59,600
comme des moyennes ou des centiles.

104
00:04:59,600 --> 00:05:03,055
Une fois que vous avez exploré
et compris votre ensemble de données,

105
00:05:03,055 --> 00:05:08,115
vous pouvez utiliser Dataprep pour créer
des flux de transformation des données.

106
00:05:08,115 --> 00:05:12,655
Les flux sont semblables aux pipelines
que vous avez vus dans Dataflow.

107
00:05:12,655 --> 00:05:15,870
En fait, les flux sont
compatibles avec Dataflow.

108
00:05:15,870 --> 00:05:17,900
Vous pouvez exécuter un flux Dataprep

109
00:05:17,900 --> 00:05:21,085
en tant que pipeline
sur la plate-forme Dataflow.

110
00:05:21,085 --> 00:05:25,880
Dans Dataprep, les flux sont mis en œuvre
sous la forme d'une suite de combinaisons,

111
00:05:25,880 --> 00:05:28,795
qui sont des étapes
de traitement de données

112
00:05:28,795 --> 00:05:31,725
créées à partir d'une bibliothèque
d'outils de préparation.

113
00:05:31,725 --> 00:05:34,351
Dataprep offre ce type d'outil
pour de nombreuses tâches

114
00:05:34,351 --> 00:05:37,760
de traitement des données courantes,
illustrées à gauche.

115
00:05:37,760 --> 00:05:40,660
Nous verrons sous peu
des exemples d'outils de préparation.

116
00:05:40,660 --> 00:05:43,070
Si vous utilisez ces outils,

117
00:05:43,070 --> 00:05:45,120
vous n'avez plus besoin de mettre en œuvre

118
00:05:45,120 --> 00:05:47,835
les étapes de traitement
de données et le code vous-même.

119
00:05:47,835 --> 00:05:50,450
Dataprep peut convertir
votre flux et ses combinaisons

120
00:05:50,450 --> 00:05:53,240
en pipeline Dataflow.

121
00:05:53,240 --> 00:05:56,445
Toujours avec l'interface Dataprep,

122
00:05:56,445 --> 00:05:57,790
vous pouvez prendre le flux,

123
00:05:57,790 --> 00:06:01,580
l'exécuter en tant que tâche
sur Dataflow et suivre sa progression.

124
00:06:01,580 --> 00:06:05,285
La bibliothèque Dataprep dispose de
plusieurs outils de préparation prédéfinis

125
00:06:05,285 --> 00:06:07,230
pour les tâches
de traitement courantes :

126
00:06:07,230 --> 00:06:09,460
nettoyer les données
avec la déduplication,

127
00:06:09,460 --> 00:06:11,850
exclure les valeurs
aberrantes et manquantes,

128
00:06:11,850 --> 00:06:14,360
réaliser des opérations
d'agrégation courantes,

129
00:06:14,360 --> 00:06:16,170
comme les décomptes ou les additions,

130
00:06:16,170 --> 00:06:19,560
joindre ou unir différentes tables

131
00:06:19,560 --> 00:06:21,660
et convertir
des données en différents types,

132
00:06:21,660 --> 00:06:23,760
comme des chaînes ou des nombres entiers.

133
00:06:23,760 --> 00:06:25,710
Pendant l'exécution du flux,

134
00:06:25,710 --> 00:06:29,715
vous pouvez utiliser l'interface Dataflow
pour suivre la progression de la tâche.

135
00:06:29,715 --> 00:06:31,275
Une fois la tâche terminée,

136
00:06:31,275 --> 00:06:34,230
vous pouvez obtenir un résumé
de son état dans Dataprep.

137
00:06:34,230 --> 00:06:37,550
Comme le montre cette capture
d'écran de la tâche terminée,

138
00:06:37,550 --> 00:06:40,760
le résumé inclut
les statistiques et les visualisations

139
00:06:40,760 --> 00:06:44,120
que vous pouvez obtenir pour
tout ensemble de données dans Dataprep.