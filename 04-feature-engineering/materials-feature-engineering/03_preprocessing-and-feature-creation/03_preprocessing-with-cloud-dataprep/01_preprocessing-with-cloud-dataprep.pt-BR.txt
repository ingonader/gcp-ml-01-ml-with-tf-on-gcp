No início deste módulo, você aprendeu técnicas para
implementar o código para fazer pré-processamento
e criação de atributos. Para usar essas técnicas, você precisa ter um bom entendimento
do domínio do problema e também precisa saber um pouco sobre
seus dados brutos de entrada. Na prática, você nem sempre tem
esse conhecimento e compreensão. Talvez precise fazer
engenharia de atributos e ciência de dados
em domínios desconhecidos, e talvez saiba pouco ou nada
sobre seus dados de entrada brutos. Assim, no restante deste módulo, você examinará as ferramentas
e técnicas que podem ajudá-lo, se estiver começando com
a ciência de dados do zero. Antes, você usou ferramentas
como o Apache Beam e o Cloud Dataflow. Em seguida, você aprenderá sobre uma
ferramenta chamada Cloud Dataprep, que permite usar uma
interface de usuário gráfica e interativa para melhor entender, visualizar e pré-processar os dados. Quando feita corretamente,
a engenharia de atributos pode melhorar bastante o desempenho
do sistema de aprendizado de máquina. E para ter sucesso com a engenharia, é importante ter conhecimento de domínio
para o seu sistema. E, especificamente, entender os dados
de entrada brutos. Então, o que isso significa? Como você pode começar a entender
um conjunto de dados com milhões ou bilhões de registros? Ao trabalhar com um conjunto de dados
que você nunca viu antes, comece com uma análise exploratória. Visualize os valores do conjunto de dados, entenda quais valores ocorreram
com frequência e com pouca frequência, localize valores atípicos
e procure valores ausentes. Você definitivamente quer conhecer as
estatísticas do conjunto de dados, médias, desvio padrão para diferentes
variáveis ​​em seus dados, seus valores mínimo e máximo, e você quer explorar as
distribuições desses valores. Além disso, ao trabalhar com
aprendizado de máquina, é provável que você trabalhe com uma
equipe que inclui cientistas de dados, desenvolvedores de software
e analistas de negócios. Isso significa que você precisa
compartilhar os resultados dos aprendizados sobre o conjunto
de dados com outras pessoas e também aproveitar o conhecimento
de sua equipe para ter insights. No restante deste módulo, veremos
duas abordagens complementares. Vamos começar explorando
um conjunto de dados e passar para o pré-processamento
e criação de atributos. Na primeira abordagem, usaremos as
ferramentas que você já viu, incluindo BigQuery, Cloud Dataflow
e Tensorflow. Na segunda abordagem, apresentaremos
o Cloud Dataprep e mostraremos como ele ajuda na análise
exploratória e no processamento de dados. Vamos começar com
a primeira abordagem, usando as ferramentas já
conhecidas para explorar dados. No início deste curso, você viu exemplos do uso
de bibliotecas gráficas, como o Seabourn, para visualizar
dados e o Cloud Datalab. O exemplo na tela mostra um gráfico
de dados, do conjunto de dados da tarifa de táxi de Nova York
disponível no BigQuery. Neste caso, o diagrama representa a distância da viagem de táxi em relação
a uma quantia justa para as viagens. Usar um bloco de notas no Datalab para
explorar e visualizar o conjunto de dados pode parecer uma abordagem prática. No entanto, lembre-se de que
o ambiente padrão do Datalab está sendo executado em um único servidor
virtual com memória limitada. No caso do conjunto de dados
de tarifa de táxi, há bilhões de pontos de dados. Então, será impraticável ou muito caro plotar e analisar todos usando apenas
um único ambiente sem Datalab. Em vez de carregar os bilhões de registros
de todo o conjunto de dados de tarifa de táxi no ambiente
de laboratório de dados, você pode usar o SQL e calcular
estatísticas resumidas usando o BigQuery. Como mostrado neste diagrama, você ainda pode usar o Datalab
para gravar o código SQL. Assim que ele estiver pronto, envie a instrução SQL para o BigQuery
por meio das APIs e terá o resultado. Como as estatísticas resumidas
são apenas algumas linhas de dados, você pode plotá-las facilmente no
Datalab usando o Seaborne ou outras bibliotecas Python
para visualização. Além disso, como você aprendeu
nas seções anteriores deste módulo, é possível usar as APIs Apache Beam
e o Cloud Dataflow para implementar cálculos de estatísticas resumidas e
outros jobs de pré-processamento de dados. Use Python ou Java para gravar o código
para o canal de processamento de dados. Em seguida, vamos ver a segunda
abordagem, em que você usará o Cloud Dataprep para ter um
melhor entendimento dos dados de entrada e para fazer engenharia de atributos
com uma interface visual interativa, em vez de gravar um
código de baixo nível. Então, o que é o Cloud Dataprep? É um serviço totalmente gerenciado
disponível no GCP, que permite explorar
e transformar seus dados de modo interativo usando um navegador da
Web com uma quantidade mínima de código. O Dataprep pode conseguir dados
de várias fontes, incluindo o Google Cloud Storage
e o BigQuery. Você também pode enviar seus
próprios dados para o Dataprep. Depois que o Dataprep souber
de onde conseguir os dados, você poderá usar essa IU gráfica
para explorar dados e criar visualizações deles. Por exemplo, você pode usar
histogramas de valores de dados e conseguir resumos estatísticos
como médias e valores percentuais. Depois de ter explorado
e entendido o conjunto de dados, você pode usar o Dataprep para calcular
os fluxos de transformações de dados. Os fluxos são semelhantes aos canais
que você viu no Dataflow. Na verdade, os fluxos são compatíveis
com o Dataflow. Você pode pegar um fluxo do Dataprep e executá-lo como um canal
na plataforma do Dataflow. No Dataprep, os fluxos são implementados
como uma sequência de receitas. Elas são etapas
de processamento de dados criadas de uma biblioteca
dos chamados Wranglers. O Dataprep possui Wranglers
para muitas tarefas comuns de processamento de dados,
mostradas à esquerda. Você verá exemplos
específicos deles em breve. Lembre-se de que, em vez de
você mesmo precisar implementar essas etapas de processamento
de dados e codificar, se usar os Wranglers, o Dataprep poderá conseguir
o fluxo e as receitas e convertê-los em um 
canal do Dataflow. Em seguida, usando a
mesma interface do Dataprep, você pode pegar o fluxo, executá-lo como um job no Dataflow
e monitorar o andamento do job. A biblioteca Dataprep
tem muitos Wranglers prontos para tarefas de processamento de dados. Você pode limpar os dados
usando a remoção de duplicação ou filtrar valores ausentes e atípicos, ou fazer agregações comuns,
como contar ou somar valores, ou pode unir uma união
de tabelas de dados diferentes e transformar dados em tipos diferentes,
como strings ou números inteiros. Enquanto o fluxo está em execução, use a interface do Dataflow para monitorar
os detalhes do andamento dos jobs, e, depois que
o job estiver concluído, você poderá ter um resumo
do status do job no Dataprep. Como você pode ver na captura
de tela do job concluído, o resumo inclui as
estatísticas e visualizações que você pode conseguir para qualquer
conjunto de dados no Dataprep.