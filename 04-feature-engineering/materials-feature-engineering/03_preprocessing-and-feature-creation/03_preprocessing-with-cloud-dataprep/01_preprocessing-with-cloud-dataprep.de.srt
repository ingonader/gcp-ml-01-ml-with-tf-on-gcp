1
00:00:00,000 --> 00:00:01,890
In diesem Modul haben Sie bereits

2
00:00:01,890 --> 00:00:04,590
Techniken
für die Implementierung des Codes

3
00:00:04,590 --> 00:00:07,290
zur Vorverarbeitung
und Merkmalerstellung kennengelernt.

4
00:00:07,290 --> 00:00:09,070
Um diese Techniken anzuwenden,

5
00:00:09,070 --> 00:00:12,645
brauchen Sie ein recht gutes Verständnis
Ihres Problembereichs

6
00:00:12,645 --> 00:00:16,200
und einiges Wissen
über Ihre Roheingabedaten.

7
00:00:16,200 --> 00:00:19,130
In der Praxis
können diese Kenntnisse fehlen.

8
00:00:19,130 --> 00:00:21,375
Bei Feature Engineering und Data Science

9
00:00:21,375 --> 00:00:23,955
muss man sich manchmal
auf unbekanntes Terrain begeben.

10
00:00:23,955 --> 00:00:28,125
Manchmal weiß man auch
kaum etwas über seine Roheingabedaten.

11
00:00:28,125 --> 00:00:32,540
Im Rest dieses Moduls
behandeln wir daher Tools und Techniken,

12
00:00:32,540 --> 00:00:35,820
die Ihnen helfen, wenn Sie
mit Data Science bei null anfangen.

13
00:00:35,820 --> 00:00:40,205
Sie haben bereits Tools wie
Apache Beam und Cloud Dataflow verwendet.

14
00:00:40,205 --> 00:00:43,330
Als Nächstes lernen Sie
ein Tool namens Cloud Dataprep kennen,

15
00:00:43,330 --> 00:00:47,254
mit dem Sie Zugriff auf eine interaktive,
grafische Benutzeroberfläche erhalten,

16
00:00:47,254 --> 00:00:51,150
in der Sie Ihre Daten besser verstehen,
visualisieren und vorverarbeiten können.

17
00:00:51,150 --> 00:00:53,560
Gutes Feature Engineering

18
00:00:53,560 --> 00:00:57,045
kann die Leistung Ihres Systems
für maschinelles Lernen deutlich steigern.

19
00:00:57,045 --> 00:01:02,165
Für erfolgreiches Feature Engineering
müssen Sie Ihr System gut verstehen.

20
00:01:02,165 --> 00:01:05,685
Insbesondere müssen Sie
Ihre Roheingabedaten verstehen.

21
00:01:05,685 --> 00:01:07,140
Was genau bedeutet das?

22
00:01:07,140 --> 00:01:10,125
Wo fängt man an,
wenn man ein Dataset mit Millionen

23
00:01:10,125 --> 00:01:13,010
oder gar Milliarden
von Datensätzen verstehen will?

24
00:01:13,010 --> 00:01:16,710
Wenn Sie mit einem Dataset arbeiten,
das Sie zum ersten Mal sehen,

25
00:01:16,710 --> 00:01:19,690
sollten Sie
mit einer explorativen Analyse beginnen.

26
00:01:19,690 --> 00:01:22,535
Sie sollten
die Werte des Datasets visualisieren

27
00:01:22,535 --> 00:01:25,960
und untersuchen, welche Werte
regelmäßig oder unregelmäßig auftauchen.

28
00:01:25,960 --> 00:01:28,765
Suchen Sie
nach Ausreißern und fehlenden Werten.

29
00:01:28,765 --> 00:01:31,830
Auf jeden Fall sind die Statistiken
des Datasets interessant

30
00:01:31,830 --> 00:01:32,850
wie Durchschnitte,

31
00:01:32,850 --> 00:01:35,365
die Standardabweichung
für verschiedene Variablen,

32
00:01:35,365 --> 00:01:41,550
Mindest- und Höchstwerte
und die Verteilung dieser Werte.

33
00:01:41,550 --> 00:01:44,925
Außerdem arbeiten Sie
bei maschinellem Lernen

34
00:01:44,925 --> 00:01:47,082
wahrscheinlich mit einem Team zusammen,

35
00:01:47,082 --> 00:01:48,339
zu dem Data Scientists,

36
00:01:48,339 --> 00:01:51,355
Softwareentwickler,
und Businessanalysten gehören können.

37
00:01:51,355 --> 00:01:54,030
Das bedeutet,
Sie sollten eine Möglichkeit haben,

38
00:01:54,030 --> 00:01:57,120
Ihre Erkenntnisse über das Dataset
mit anderen zu teilen

39
00:01:57,120 --> 00:02:01,165
und das Wissen Ihres Teams anzuzapfen,
um weitere Informationen zu erhalten.

40
00:02:01,165 --> 00:02:04,960
Im Rest dieses Moduls geht es
um zwei einander ergänzende Ansätze.

41
00:02:04,960 --> 00:02:06,970
Untersuchen wir nun ein Dataset

42
00:02:06,970 --> 00:02:10,090
und sehen uns dann
Vorverarbeitung und Merkmalserstellung an.

43
00:02:10,090 --> 00:02:14,270
Der erste Ansatz nutzt uns bekannte Tools

44
00:02:14,270 --> 00:02:17,305
wie BigQuery,
Cloud Dataflow und TensorFlow.

45
00:02:17,305 --> 00:02:21,300
Beim zweiten Ansatz lernen Sie
Cloud Dataprep kennen und erfahren,

46
00:02:21,300 --> 00:02:26,275
wie Sie damit eine explorative Analyse
und die Datenverarbeitung durchführen.

47
00:02:26,275 --> 00:02:28,010
Beginnen wir mit dem ersten Ansatz,

48
00:02:28,010 --> 00:02:30,990
bei dem Sie sich die Daten
mit bekannten Tools erschließen.

49
00:02:30,990 --> 00:02:33,575
Sie haben
in diesem Kurs bereits Beispiele gesehen,

50
00:02:33,575 --> 00:02:35,695
wie Sie mit Grafikbibliotheken wie Seaborn

51
00:02:35,695 --> 00:02:37,875
Daten
in Cloud Datalab visualisieren können.

52
00:02:37,875 --> 00:02:41,080
Dieses Beispiel stellt Daten
aus dem Dataset grafisch dar,

53
00:02:41,080 --> 00:02:44,710
das Taxigebühren in New York City enthält
und in BigQuery verfügbar ist.

54
00:02:44,710 --> 00:02:47,980
Das Diagramm
stellt in diesem Fall die Fahrtdistanz

55
00:02:47,980 --> 00:02:50,450
gegenüber dem Preis der Taxifahrt dar.

56
00:02:50,450 --> 00:02:54,930
Das Dataset in einem Datalab-Notebook
zu erschließen und zu visualisieren,

57
00:02:54,930 --> 00:02:57,190
mag nun praktisch erscheinen.

58
00:02:57,190 --> 00:03:01,045
Denken Sie aber daran,
dass die standardmäßige Datalab-Umgebung

59
00:03:01,045 --> 00:03:05,190
auf einem einzelnen, virtuellen Server
mit begrenztem Arbeitsspeicher läuft.

60
00:03:05,190 --> 00:03:09,450
Das Taxigebühren-Dataset enthält
zum Beispiel Milliarden von Datenpunkten.

61
00:03:09,460 --> 00:03:14,125
Es wäre also unpraktisch oder zu teuer,
sie alle mithilfe einer Datalab-Umgebung

62
00:03:14,125 --> 00:03:17,285
mit nur einem Knoten
darzustellen und zu analysieren.

63
00:03:17,285 --> 00:03:21,730
Anstatt die Milliarden von Einträgen
des ganzen Taxigebühren-Datasets

64
00:03:21,730 --> 00:03:23,805
in die Datalab-Umgebung zu laden,

65
00:03:23,805 --> 00:03:28,415
können Sie mit SQL und BigQuery
zusammenfassende Statistiken erstellen.

66
00:03:28,415 --> 00:03:30,390
Wie dieses Diagramm zeigt,

67
00:03:30,390 --> 00:03:33,705
können Sie trotzdem
Ihren SQL-Code in Datalab schreiben.

68
00:03:33,705 --> 00:03:35,175
Wenn der Code fertig ist,

69
00:03:35,175 --> 00:03:38,970
übergeben Sie die SQL-Anweisung
über die APIs an BigQuery

70
00:03:38,970 --> 00:03:40,645
und erhalten das Ergebnis.

71
00:03:40,645 --> 00:03:44,490
Da die zusammenfassenden Statistiken
nur aus ein paar Zeilen Daten bestehen,

72
00:03:44,490 --> 00:03:47,150
können Sie sie
in Datalab einfach mit Seaborn

73
00:03:47,150 --> 00:03:50,210
oder anderen Darstellungsbibliotheken
für Python visualisieren.

74
00:03:50,210 --> 00:03:53,510
Wie Sie bereits erfahren haben,

75
00:03:53,510 --> 00:03:56,817
können Sie Apache Beam-APIs
und Cloud Dataflow verwenden,

76
00:03:56,817 --> 00:03:59,230
um Berechnungen
zusammenfassender Statistiken

77
00:03:59,230 --> 00:04:01,825
und andere Datenvorverarbeitungsjobs
zu implementieren.

78
00:04:01,825 --> 00:04:06,460
Ihre Datenverarbeitungspipeline können Sie
mit Python oder Java programmieren.

79
00:04:06,460 --> 00:04:09,015
Sehen wir uns nun den zweiten Ansatz an,

80
00:04:09,015 --> 00:04:13,090
bei dem Sie sich die Eingabedaten
mit Cloud Dataprep erschließen

81
00:04:13,090 --> 00:04:18,250
und für das Feature Engineering dessen
interaktive, visuelle Schnittstelle nutzen,

82
00:04:18,250 --> 00:04:20,065
statt Code zu schreiben.

83
00:04:20,065 --> 00:04:22,810
Was ist Cloud Dataprep eigentlich?

84
00:04:22,810 --> 00:04:26,260
Es ist ein vollständig verwalteter Dienst,
der über die GCP verfügbar ist.

85
00:04:26,260 --> 00:04:29,495
Damit können Sie Ihre Daten
interaktiv und mit sehr wenig Code

86
00:04:29,495 --> 00:04:32,680
in einem Webbrowser untersuchen
und die Daten auch transformieren.

87
00:04:32,680 --> 00:04:36,430
Dataprep kann Daten
aus vielen verschiedenen Quellen

88
00:04:36,430 --> 00:04:39,855
wie Google Cloud Storage
und BigQuery abrufen.

89
00:04:39,855 --> 00:04:42,840
Sie können zudem
eigene Daten zu Dataprep hochladen.

90
00:04:42,840 --> 00:04:45,930
Sobald Dataprep weiß,
wo es die Daten abrufen soll,

91
00:04:45,930 --> 00:04:49,335
können Sie diese
über die grafische UI untersuchen

92
00:04:49,335 --> 00:04:51,635
und Datenvisualisierungen erstellen.

93
00:04:51,635 --> 00:04:55,045
Sie können sich zum Beispiel
Histogramme der Datenwerte ansehen

94
00:04:55,045 --> 00:04:56,980
und statistische Zusammenfassungen

95
00:04:56,980 --> 00:04:59,600
wie Durchschnitte
oder Perzentilwerte berechnen.

96
00:04:59,600 --> 00:05:03,055
Nachdem Sie Ihr Dataset verstanden haben,

97
00:05:03,055 --> 00:05:07,555
können Sie mit Dataprep
Datentransformationsabläufe berechnen.

98
00:05:07,555 --> 00:05:12,655
Diese Abläufe ähneln den Pipelines,
die Sie in Dataflow kennengelernt haben.

99
00:05:12,655 --> 00:05:15,870
Tatsächlich sind die Abläufe
sogar mit Dataflow kompatibel.

100
00:05:15,870 --> 00:05:18,160
Sie können einen Dataprep-Ablauf

101
00:05:18,160 --> 00:05:21,085
einfach
als Pipeline in Dataflow ausführen.

102
00:05:21,085 --> 00:05:25,880
In Dataprep wird ein Ablauf
als Abfolge von Schemas implementiert.

103
00:05:25,880 --> 00:05:27,885
Schemas sind Datenverarbeitungsschritte,

104
00:05:27,885 --> 00:05:31,375
die aus einer Bibliothek sogenannter
Wrangler zusammengestellt werden.

105
00:05:31,375 --> 00:05:33,780
Wie Sie links sehen, hat Dataprep Wrangler

106
00:05:33,780 --> 00:05:36,260
für viele gängige
Datenverarbeitungsaufgaben.

107
00:05:36,260 --> 00:05:39,890
Sie sehen gleich noch
konkrete Beispiele für Wrangler.

108
00:05:39,890 --> 00:05:45,960
Sie müssen die Datenverarbeitungsschritte
und den Code nicht selbst implementieren.

109
00:05:45,960 --> 00:05:50,360
Über Wranglers kann Dataprep
einfach den Ablauf und die Schemas

110
00:05:50,360 --> 00:05:53,345
in eine Dataflow-Pipeline konvertieren.

111
00:05:53,345 --> 00:05:57,805
Danach können Sie den Ablauf
in derselben Dataprep-Benutzeroberfläche

112
00:05:57,805 --> 00:05:59,810
als Job in Dataflow ausführen

113
00:05:59,810 --> 00:06:01,950
und den Fortschritt überwachen.

114
00:06:01,950 --> 00:06:04,830
Die Dataprep-Bibliothek bietet
viele vorgefertigte Wrangler

115
00:06:04,830 --> 00:06:06,780
für gängige Datenverarbeitungsaufgaben.

116
00:06:06,780 --> 00:06:09,550
Sie können Daten bereinigen,
indem Sie Duplikate entfernen

117
00:06:09,550 --> 00:06:12,030
oder fehlende Werte
und Ausreißer herausfiltern.

118
00:06:12,030 --> 00:06:16,180
Sie können Werte zusammenführen,
indem Sie sie zählen oder aufsummieren

119
00:06:16,180 --> 00:06:19,560
oder Sie können
verschiedene Tabellen zusammenführen.

120
00:06:19,560 --> 00:06:23,760
Sie können Daten in verschiedene Typen
wie Strings oder Ganzzahlen umwandeln.

121
00:06:23,760 --> 00:06:25,570
Während der Ablauf ausgeführt wird,

122
00:06:25,570 --> 00:06:29,055
können Sie die Jobdetails
über die Dataflow-Oberfläche überwachen,

123
00:06:29,055 --> 00:06:31,055
und wenn der Job abgeschlossen ist,

124
00:06:31,055 --> 00:06:34,230
erhalten Sie in Dataprep
eine Zusammenfassung des Jobstatus.

125
00:06:34,230 --> 00:06:37,610
Wie Sie in diesem Screenshot
eines abgeschlossenen Jobs sehen,

126
00:06:37,610 --> 00:06:40,690
enthält die Zusammenfassung
Statistiken und Visualisierungen,

127
00:06:40,690 --> 00:06:44,090
die Sie in Dataprep
für jedes Dataset erhalten können.