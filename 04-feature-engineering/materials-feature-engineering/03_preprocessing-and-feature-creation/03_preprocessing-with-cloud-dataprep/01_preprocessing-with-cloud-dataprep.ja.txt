これまで 前処理と特徴量作成のための コードの実装手法を学習しました こうした手法を使うには 対象となる分野と 入力する生データの理解が必要です 実際には 不慣れな分野のデータセットの
特徴エンジニアリングで 生データに関して知識がないこともあります そこで 次は ゼロからのデータセットの処理に役立つ ツールと手法です 次に取り上げるCloud Dataprepは 対話型のGUIを使った データの理解、可視化、前処理に
特化したツールです 特徴エンジニアリングを適切に行えば MLシステムのパフォーマンスが
大幅に向上します そのためには システムの対象分野の知識 具体的には生データの理解が重要です では 膨大なデータセットの どこから始めるのでしょうか 初めて見るデータセットの場合 調査分析から始めます 値を可視化し 頻繁に発生する値とそうでない値を把握し 外れ値や欠損値を探します データセットの平均値、標準偏差 最小値、最大値 値の分布などの統計情報が必要です MLに携わっていると データサイエンティストやソフトウェア開発者 ビジネスアナリストと連携することも多く 自分の調査の結果をチームと共有すると同時に チームメンバーの知識を活用する方法を 身に付ける必要があります ここでは相補的な2つのアプローチで データセットの調査から始めて
前処理と特徴量作成に進みます 1つ目のアプローチでは BigQuery、Dataflow、TenserFlow
などのツールを使用します 2つ目のアプローチでは
調査分析とデータ処理の両方に役立つ Cloud Dataprepを取り上げます では 1つ目のアプローチからです これまでに Seabornなどを使った Datalabのデータ可視化の例を見てきました 画面の例は BigQueryで利用できる ニューヨークのタクシー料金データセットの 乗車距離と料金のグラフです デフォルトのDatalab環境での データセットの調査と可視化は メモリが限られた1つの
仮想サーバーで実行されます タクシー料金データセットには 数十億のデータがあるため 単一ノードのDatalab環境で
すべてをプロットし分析するのは 非実用的でコストが高すぎます そこで データセット全体を読み込むのではなく SQLを使ってBigQueryで
基本統計量を計算します 図のように DatalabでSQLコードを書いて APIを通じてBigQueryに送信し 結果を取得します 基本統計量は数行のデータなので SeabornなどPythonの
可視化ライブラリで 容易にプロットできます また 前に説明したように Apache Beam APIとDataflowで 基本統計量の計算などの
データ処理ジョブを実装できます コードにはPythonかJavaを使用できます 次は 2つ目のアプローチです Dataprepを使って
入力データの理解を深め ローレベルコードを書く代わりに 対話型GUIで特徴エンジニアリングを行います Cloud Dataprepは GCPのフルマネージドサービスです ウェブブラウザを使った対話型で 最低限のコード作成で
データを調査、変換できます Dataprepはさまざまなソースから
データを取得でき 独自のデータもアップロードできます ソースを指定したら GUIでデータを調査し可視化できます たとえば 値のヒストグラム表示や 平均値などの統計量の計算ができます データセットを調査し理解したら データ変換のフローを作成します このフローはパイプラインに近いもので Dataprepのフローを Dataflowのパイプラインとして実行できます Dataprepのフローは一連のレシピで レシピはWranglerを使って
構築されたデータ処理ステップです 後ほど説明しますが Wranglerを使えば データ処理ステップやコードを
実装することなく フローとフローに含まれるレシピを Dataflowパイプラインに変換できます さらに フローをDataflowのジョブとして実行し ジョブの進捗を確認できます 一般的なデータ処理を行う
さまざまなWranglerがあり 重複排除やフィルターによる
データのクリーンアップ カウントや合計などの集計 複数のテーブルの統合 データタイプの変換などが可能です フロー実行中は Dataflow管理画面でジョブの進捗を確認し 完了後は Dataprepでステータスを確認できます 画面は完了したジョブの概要で Dataperpのデータセットの 統計情報とグラフが含まれます