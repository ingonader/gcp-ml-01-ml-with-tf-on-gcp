1
00:00:00,000 --> 00:00:02,325
Welcome back. In this lab,

2
00:00:02,325 --> 00:00:04,500
you will take the taxi fare data set from

3
00:00:04,500 --> 00:00:08,445
BigQuerry and pre-process it using the Cloud Dataprep tool.

4
00:00:08,445 --> 00:00:12,210
In the tool, you will explore the distribution of data values,

5
00:00:12,210 --> 00:00:15,870
visualize the distributions using histogram plots, and then,

6
00:00:15,870 --> 00:00:18,570
implement a data prep flow to create

7
00:00:18,570 --> 00:00:22,740
a new feature based on an average number of taxi rides per hour,

8
00:00:22,740 --> 00:00:24,555
in a rolling time window.

9
00:00:24,555 --> 00:00:29,265
Finally, you will deploy and run the Dataprep flow on GCP

10
00:00:29,265 --> 00:00:34,065
and monitor the job execution using Dataflow. Let's take a closer look.

11
00:00:34,065 --> 00:00:37,110
Okay, to get started with this lab,

12
00:00:37,110 --> 00:00:39,370
you need to prepare some prerequisites.

13
00:00:39,370 --> 00:00:42,990
You begin at the Google Cloud Platform dashboard.

14
00:00:42,990 --> 00:00:46,260
First, you will need Google Cloud Storage bucket.

15
00:00:46,260 --> 00:00:50,510
You can create one if you go into the products and services menu,

16
00:00:50,510 --> 00:00:53,550
which you can access by clicking on the hamburger icon.

17
00:00:53,550 --> 00:00:55,785
Scroll down to storage,

18
00:00:55,785 --> 00:00:59,205
browser, and click on create bucket.

19
00:00:59,205 --> 00:01:01,755
As you can see from the guidance on the screen,

20
00:01:01,755 --> 00:01:05,355
the name of the storage bucket must be globally unique.

21
00:01:05,355 --> 00:01:11,200
Here, I have set up a unique bucket name in the us-east4 location.

22
00:01:11,200 --> 00:01:13,665
Shortly after I click create,

23
00:01:13,665 --> 00:01:17,220
I can see that the bucket with the unique name is ready.

24
00:01:17,220 --> 00:01:21,885
The next thing that you need to prepare for this lab is the BigQuery dataset.

25
00:01:21,885 --> 00:01:25,965
You can find BigQuery back in the products and services menu,

26
00:01:25,965 --> 00:01:28,020
under the Big Data section.

27
00:01:28,020 --> 00:01:30,220
When you click on BigQuery,

28
00:01:30,220 --> 00:01:33,035
you should see a new tab opening a browser.

29
00:01:33,035 --> 00:01:35,330
To the right of your project name,

30
00:01:35,330 --> 00:01:40,180
click on the down arrow and choose create new dataset.

31
00:01:40,180 --> 00:01:46,350
Use the name taxi cab reporting for the data set and click okay to create it.

32
00:01:46,350 --> 00:01:48,800
Once the data set is ready,

33
00:01:48,800 --> 00:01:52,115
you need to go back to the Google Cloud Platform dashboard.

34
00:01:52,115 --> 00:01:57,630
From there, navigate to the Dataproc link in the Products and Services menu.

35
00:01:57,630 --> 00:02:01,160
Since Cloud Dataprep is a service from a Google partner,

36
00:02:01,160 --> 00:02:04,205
you need to accept a new set of terms and conditions.

37
00:02:04,205 --> 00:02:06,125
Click accept to do that.

38
00:02:06,125 --> 00:02:09,680
Also you need to click allow to let Trifacta,

39
00:02:09,680 --> 00:02:13,640
which is Google's partner that develop Dataprep, to access your data.

40
00:02:13,640 --> 00:02:18,350
When you click allow, it'll take a few moments to enable Dataprep for your project.

41
00:02:18,350 --> 00:02:22,590
So right now, you can see the video fast forwarding for the wait.

42
00:02:22,590 --> 00:02:25,630
Next, you need to choose the account to use for

43
00:02:25,630 --> 00:02:29,680
Cloud Dataprep and allow Dataprep to access your project.

44
00:02:29,680 --> 00:02:33,430
When you're setting up Dataprep on your project for the first time,

45
00:02:33,430 --> 00:02:36,775
you need to specify the storage bucket that will hold your data.

46
00:02:36,775 --> 00:02:40,545
Here you can see that the bucket created in the beginning of this lab,

47
00:02:40,545 --> 00:02:42,720
is used to set up Dataprep.

48
00:02:42,720 --> 00:02:46,020
Once the bucket is selected, click continue.

49
00:02:46,020 --> 00:02:47,910
Once Dataprep is set up,

50
00:02:47,910 --> 00:02:52,805
you can dismiss the help tutorial by clicking don't show any helpers.

51
00:02:52,805 --> 00:02:56,940
Next, you'll use Dataprep to create a new flow.

52
00:02:56,940 --> 00:03:00,290
Let's call this flow NYC Taxi reporting.

53
00:03:00,290 --> 00:03:04,000
The flow is going to show a process for ingesting,

54
00:03:04,000 --> 00:03:07,165
transforming, and analyzing taxi data.

55
00:03:07,165 --> 00:03:09,340
Go ahead click create.

56
00:03:09,340 --> 00:03:11,980
The first thing you need to do in order to create

57
00:03:11,980 --> 00:03:15,760
a flow is to add some datasets for the flow to process.

58
00:03:15,760 --> 00:03:20,345
In this case, you will import some predefined datasets that our team

59
00:03:20,345 --> 00:03:25,445
already saved to the public cloud storage bucket and you can access the storage bucket,

60
00:03:25,445 --> 00:03:30,980
using the name asl-ml-immersion under the NYC taxicab directory.

61
00:03:30,980 --> 00:03:33,430
The directory has a few files.

62
00:03:33,430 --> 00:03:39,315
You will use the files with the 2015 and 2016 taxi fare data.

63
00:03:39,315 --> 00:03:43,400
Notice, that these are comma separated values CSB files.

64
00:03:43,400 --> 00:03:45,910
Click import and shortly,

65
00:03:45,910 --> 00:03:48,620
you will see the two files added to your flow.

66
00:03:48,620 --> 00:03:52,590
To implement data processing or wrangling for these datasets,

67
00:03:52,590 --> 00:03:54,570
you will need to add in your recipe.

68
00:03:54,570 --> 00:03:57,930
Next, you will add steps to this recipe.

69
00:03:57,930 --> 00:03:59,750
Once the dataset is loaded,

70
00:03:59,750 --> 00:04:03,190
you'll see a preview of a sample of the data from the dataset.

71
00:04:03,190 --> 00:04:06,310
Here for example, you can see that the dataset includes

72
00:04:06,310 --> 00:04:09,875
information about taxi rides such as pickup date time,

73
00:04:09,875 --> 00:04:13,855
drop of date time and the number of passengers in the taxi.

74
00:04:13,855 --> 00:04:17,670
Also, notice from the trip distance histogram that most of

75
00:04:17,670 --> 00:04:21,090
the trips were under five miles of distance.

76
00:04:21,090 --> 00:04:28,325
Next, you go ahead in union 2015 and 2016 datasets so you can work was more rows of data.

77
00:04:28,325 --> 00:04:31,125
Once you select the 2016 dataset,

78
00:04:31,125 --> 00:04:34,170
you need to click on add and align by name which will

79
00:04:34,170 --> 00:04:37,250
make sure that the names that have the corresponding column headers,

80
00:04:37,250 --> 00:04:40,095
are aligned to the union version of the dataset.

81
00:04:40,095 --> 00:04:44,865
Add the union step to the recipe and after Dataprep previews the union,

82
00:04:44,865 --> 00:04:50,030
you'll see a sample of the datasets that includes taxi rides for 2015 and 2016.

83
00:04:50,030 --> 00:04:56,355
Notice, that the data about the pickup date and pickup day time is in different counts.

84
00:04:56,355 --> 00:05:01,750
Since this lab will show you how to compute the rolling averages for taxi fare amounts,

85
00:05:01,750 --> 00:05:06,430
first you need to convert the input data to the SQL date, time format.

86
00:05:06,430 --> 00:05:09,810
For that, you can add and merge to the recipe,

87
00:05:09,810 --> 00:05:12,950
which will concatenate values from multiple columns.

88
00:05:12,950 --> 00:05:17,405
In this case, the columns are called the pickup date and the pickup time.

89
00:05:17,405 --> 00:05:20,905
Use pickup date time as the new column name.

90
00:05:20,905 --> 00:05:25,385
Also, go ahead and use a single space as a delimiter between values.

91
00:05:25,385 --> 00:05:26,960
Notice that on the left,

92
00:05:26,960 --> 00:05:28,855
you now have a preview of the new column.

93
00:05:28,855 --> 00:05:31,820
Next, create a new derived column that will

94
00:05:31,820 --> 00:05:35,020
convert pickup time into a SQL date, time format.

95
00:05:35,020 --> 00:05:38,005
Once a new date time stand field is available,

96
00:05:38,005 --> 00:05:41,120
you will extract out just a year, month, date,

97
00:05:41,120 --> 00:05:44,980
and the hour information with all the details of the minutes and seconds.

98
00:05:44,980 --> 00:05:50,345
Since the hour pickup date time column is missing the values for minutes and seconds,

99
00:05:50,345 --> 00:05:53,540
it's not parseable as SQL date, time format.

100
00:05:53,540 --> 00:05:56,300
So, you need to create a new column that can be

101
00:05:56,300 --> 00:05:59,650
converted into a valid SQL date, time value.

102
00:05:59,650 --> 00:06:01,470
To do that, you will create

103
00:06:01,470 --> 00:06:05,530
a new merge operation and you will use the merge wrangler again.

104
00:06:05,530 --> 00:06:09,200
This wrangler will concatenate values of the hour, pickup date,

105
00:06:09,200 --> 00:06:11,660
and time column with a string that contains

106
00:06:11,660 --> 00:06:15,130
four zero characters for the values of the minutes and seconds.

107
00:06:15,130 --> 00:06:17,860
Notice that when you add a new column,

108
00:06:17,860 --> 00:06:21,195
it will get an automatically generated name like column one.

109
00:06:21,195 --> 00:06:23,040
You can easily rename that.

110
00:06:23,040 --> 00:06:27,340
In this case, you can rename it to pickup hour.

111
00:06:27,500 --> 00:06:32,830
Next, you will compute some statistics based on the pickup hour values.

112
00:06:32,830 --> 00:06:37,505
You can use standard SQL statistical aggregation functions like sum or average.

113
00:06:37,505 --> 00:06:42,040
You can see that this wrangler will compute the sums and averages for

114
00:06:42,040 --> 00:06:44,530
passenger counts and the same combination of

115
00:06:44,530 --> 00:06:47,750
the sum and average for the trip distance and fare amount.

116
00:06:47,750 --> 00:06:53,300
Lastly, it will compute the maximum fare amounts for each pickup hour.

117
00:06:55,340 --> 00:06:59,440
Just as earlier, notice that you get a preview of the results for

118
00:06:59,440 --> 00:07:03,515
the computed statistics in the histograms on the left hand side of the screen.

119
00:07:03,515 --> 00:07:07,475
Also, if you observe the average fare amount in the histograms,

120
00:07:07,475 --> 00:07:12,490
most of the average fares are on the range from $18 to $19 per trip.

121
00:07:12,490 --> 00:07:17,150
Next, you will go ahead and compute the rolling average for the fare amount by

122
00:07:17,150 --> 00:07:21,995
looking at the trailing free hours worth of data for each pickup hour.

123
00:07:21,995 --> 00:07:26,930
You can compute this using the Rolling Average Function in Cloud Dataprep.

124
00:07:26,930 --> 00:07:29,335
Here are the values for the rolling average,

125
00:07:29,335 --> 00:07:31,300
assorted by the pickup hour.

126
00:07:31,300 --> 00:07:36,440
Finally, name this column average free hour rolling fare.

127
00:07:36,440 --> 00:07:39,750
All right, once the recipe is ready,

128
00:07:39,750 --> 00:07:43,395
you can deploy the recipe as a Google Cloud Dataflow job.

129
00:07:43,395 --> 00:07:47,355
To do that, you need to click on run job and specify

130
00:07:47,355 --> 00:07:52,035
where the results of the jobs are going to be published or other words, store.

131
00:07:52,035 --> 00:07:57,765
By default, the results of the job is saved as CSB file in Google Cloud storage.

132
00:07:57,765 --> 00:08:02,120
Instead of doing that, you can change the destination to be BigQuery and you

133
00:08:02,120 --> 00:08:06,640
can create a new table in BigQuery every time the job is executed.

134
00:08:06,640 --> 00:08:10,945
So, if you change your selection on the right to create a new table at

135
00:08:10,945 --> 00:08:16,300
every run and rename the table to TLC yellow trips reporting,

136
00:08:16,300 --> 00:08:20,225
you will get a new table in the NYC Taxi reporting dataset.

137
00:08:20,225 --> 00:08:23,705
Remember, this is the dataset that you created in the beginning of this

138
00:08:23,705 --> 00:08:28,210
lab.Go ahead and run the job.

139
00:08:28,210 --> 00:08:30,345
Once the job is shown as transforming,

140
00:08:30,345 --> 00:08:33,560
Dataprep will begin to deploy the job to data flow.

141
00:08:33,560 --> 00:08:35,760
This usually takes a few moments.

142
00:08:35,760 --> 00:08:40,415
You can monitor the progress of the job from the job session of the Dataprep menu.

143
00:08:40,415 --> 00:08:43,555
If you click on the ellipsis symbol on the right.

144
00:08:43,555 --> 00:08:46,510
The ellipsis menu will not have the link to

145
00:08:46,510 --> 00:08:49,590
the Dataflow job right away after you deploy the job,

146
00:08:49,590 --> 00:08:52,659
but if you wait a few moments and refresh the page,

147
00:08:52,659 --> 00:08:57,635
you'll find that the menu gets updated and you'll see a link to access the Dataflow job.

148
00:08:57,635 --> 00:09:02,570
If you click the link, you'll be automatically taken to the Dataflow user interface,

149
00:09:02,570 --> 00:09:05,045
where you can monitor the detailed transformation steps

150
00:09:05,045 --> 00:09:07,660
in Dataflow as created by Dataprep.

151
00:09:07,660 --> 00:09:10,645
On the right hand side of the Dataflow UI,

152
00:09:10,645 --> 00:09:13,615
you can get details about this job execution.

153
00:09:13,615 --> 00:09:17,160
Here, you can see that since the job has just started,

154
00:09:17,160 --> 00:09:21,080
the Dataflow cluster to run the job still needs to be scaled.

155
00:09:21,080 --> 00:09:25,225
However, you can already monitor the results of the job configuration.

156
00:09:25,225 --> 00:09:28,340
Here, none of the individual transformation steps of

157
00:09:28,340 --> 00:09:31,280
the job have started except the few that are preparing

158
00:09:31,280 --> 00:09:34,130
the table in BigQuery and are just starting out to

159
00:09:34,130 --> 00:09:38,370
fetch data from the input CSB files from Google Cloud Storage.

160
00:09:38,370 --> 00:09:41,730
In addition to monitoring this job from Dataflow,

161
00:09:41,730 --> 00:09:44,140
you can navigate to BigQuery and monitor

162
00:09:44,140 --> 00:09:47,575
the output of the job in your taxicab reporting dataset.

163
00:09:47,575 --> 00:09:50,525
As you recall, once the job starts running,

164
00:09:50,525 --> 00:09:55,135
it will insert values into a new TLC yellow trip reporting table.

165
00:09:55,135 --> 00:09:57,600
Since it takes a while for the table to be created,

166
00:09:57,600 --> 00:10:01,545
you may need to wait and refresh the page to see the update.

167
00:10:01,545 --> 00:10:03,710
Once the table is in place,

168
00:10:03,710 --> 00:10:07,185
you can enter a SQL statement to fetch results from the table.

169
00:10:07,185 --> 00:10:12,565
However, make sure you have your SQL dialect configured properly before you run it.

170
00:10:12,565 --> 00:10:18,560
Here, you can see that running the job generate roughly 192 kilobytes of data,

171
00:10:18,560 --> 00:10:20,920
including information about the pickup hours,

172
00:10:20,920 --> 00:10:26,120
average trip distances, average fare and the other information computed by Dataflow.

173
00:10:26,120 --> 00:10:29,030
Okay, this is it for this lab.