In diesem Modul haben Sie bereits Techniken
für die Implementierung des Codes zur Vorverarbeitung
und Merkmalerstellung kennengelernt. Um diese Techniken anzuwenden, brauchen Sie ein recht gutes Verständnis
Ihres Problembereichs und einiges Wissen
über Ihre Roheingabedaten. In der Praxis
können diese Kenntnisse fehlen. Bei Feature Engineering und Data Science muss man sich manchmal
auf unbekanntes Terrain begeben. Manchmal weiß man auch
kaum etwas über seine Roheingabedaten. Im Rest dieses Moduls
behandeln wir daher Tools und Techniken, die Ihnen helfen, wenn Sie
mit Data Science bei null anfangen. Sie haben bereits Tools wie
Apache Beam und Cloud Dataflow verwendet. Als Nächstes lernen Sie
ein Tool namens Cloud Dataprep kennen, mit dem Sie Zugriff auf eine interaktive,
grafische Benutzeroberfläche erhalten, in der Sie Ihre Daten besser verstehen,
visualisieren und vorverarbeiten können. Gutes Feature Engineering kann die Leistung Ihres Systems
für maschinelles Lernen deutlich steigern. Für erfolgreiches Feature Engineering
müssen Sie Ihr System gut verstehen. Insbesondere müssen Sie
Ihre Roheingabedaten verstehen. Was genau bedeutet das? Wo fängt man an,
wenn man ein Dataset mit Millionen oder gar Milliarden
von Datensätzen verstehen will? Wenn Sie mit einem Dataset arbeiten,
das Sie zum ersten Mal sehen, sollten Sie
mit einer explorativen Analyse beginnen. Sie sollten
die Werte des Datasets visualisieren und untersuchen, welche Werte
regelmäßig oder unregelmäßig auftauchen. Suchen Sie
nach Ausreißern und fehlenden Werten. Auf jeden Fall sind die Statistiken
des Datasets interessant wie Durchschnitte, die Standardabweichung
für verschiedene Variablen, Mindest- und Höchstwerte
und die Verteilung dieser Werte. Außerdem arbeiten Sie
bei maschinellem Lernen wahrscheinlich mit einem Team zusammen, zu dem Data Scientists, Softwareentwickler,
und Businessanalysten gehören können. Das bedeutet,
Sie sollten eine Möglichkeit haben, Ihre Erkenntnisse über das Dataset
mit anderen zu teilen und das Wissen Ihres Teams anzuzapfen,
um weitere Informationen zu erhalten. Im Rest dieses Moduls geht es
um zwei einander ergänzende Ansätze. Untersuchen wir nun ein Dataset und sehen uns dann
Vorverarbeitung und Merkmalserstellung an. Der erste Ansatz nutzt uns bekannte Tools wie BigQuery,
Cloud Dataflow und TensorFlow. Beim zweiten Ansatz lernen Sie
Cloud Dataprep kennen und erfahren, wie Sie damit eine explorative Analyse
und die Datenverarbeitung durchführen. Beginnen wir mit dem ersten Ansatz, bei dem Sie sich die Daten
mit bekannten Tools erschließen. Sie haben
in diesem Kurs bereits Beispiele gesehen, wie Sie mit Grafikbibliotheken wie Seaborn Daten
in Cloud Datalab visualisieren können. Dieses Beispiel stellt Daten
aus dem Dataset grafisch dar, das Taxigebühren in New York City enthält
und in BigQuery verfügbar ist. Das Diagramm
stellt in diesem Fall die Fahrtdistanz gegenüber dem Preis der Taxifahrt dar. Das Dataset in einem Datalab-Notebook
zu erschließen und zu visualisieren, mag nun praktisch erscheinen. Denken Sie aber daran,
dass die standardmäßige Datalab-Umgebung auf einem einzelnen, virtuellen Server
mit begrenztem Arbeitsspeicher läuft. Das Taxigebühren-Dataset enthält
zum Beispiel Milliarden von Datenpunkten. Es wäre also unpraktisch oder zu teuer,
sie alle mithilfe einer Datalab-Umgebung mit nur einem Knoten
darzustellen und zu analysieren. Anstatt die Milliarden von Einträgen
des ganzen Taxigebühren-Datasets in die Datalab-Umgebung zu laden, können Sie mit SQL und BigQuery
zusammenfassende Statistiken erstellen. Wie dieses Diagramm zeigt, können Sie trotzdem
Ihren SQL-Code in Datalab schreiben. Wenn der Code fertig ist, übergeben Sie die SQL-Anweisung
über die APIs an BigQuery und erhalten das Ergebnis. Da die zusammenfassenden Statistiken
nur aus ein paar Zeilen Daten bestehen, können Sie sie
in Datalab einfach mit Seaborn oder anderen Darstellungsbibliotheken
für Python visualisieren. Wie Sie bereits erfahren haben, können Sie Apache Beam-APIs
und Cloud Dataflow verwenden, um Berechnungen
zusammenfassender Statistiken und andere Datenvorverarbeitungsjobs
zu implementieren. Ihre Datenverarbeitungspipeline können Sie
mit Python oder Java programmieren. Sehen wir uns nun den zweiten Ansatz an, bei dem Sie sich die Eingabedaten
mit Cloud Dataprep erschließen und für das Feature Engineering dessen
interaktive, visuelle Schnittstelle nutzen, statt Code zu schreiben. Was ist Cloud Dataprep eigentlich? Es ist ein vollständig verwalteter Dienst,
der über die GCP verfügbar ist. Damit können Sie Ihre Daten
interaktiv und mit sehr wenig Code in einem Webbrowser untersuchen
und die Daten auch transformieren. Dataprep kann Daten
aus vielen verschiedenen Quellen wie Google Cloud Storage
und BigQuery abrufen. Sie können zudem
eigene Daten zu Dataprep hochladen. Sobald Dataprep weiß,
wo es die Daten abrufen soll, können Sie diese
über die grafische UI untersuchen und Datenvisualisierungen erstellen. Sie können sich zum Beispiel
Histogramme der Datenwerte ansehen und statistische Zusammenfassungen wie Durchschnitte
oder Perzentilwerte berechnen. Nachdem Sie Ihr Dataset verstanden haben, können Sie mit Dataprep
Datentransformationsabläufe berechnen. Diese Abläufe ähneln den Pipelines,
die Sie in Dataflow kennengelernt haben. Tatsächlich sind die Abläufe
sogar mit Dataflow kompatibel. Sie können einen Dataprep-Ablauf einfach
als Pipeline in Dataflow ausführen. In Dataprep wird ein Ablauf
als Abfolge von Schemas implementiert. Schemas sind Datenverarbeitungsschritte, die aus einer Bibliothek sogenannter
Wrangler zusammengestellt werden. Wie Sie links sehen, hat Dataprep Wrangler für viele gängige
Datenverarbeitungsaufgaben. Sie sehen gleich noch
konkrete Beispiele für Wrangler. Sie müssen die Datenverarbeitungsschritte
und den Code nicht selbst implementieren. Über Wranglers kann Dataprep
einfach den Ablauf und die Schemas in eine Dataflow-Pipeline konvertieren. Danach können Sie den Ablauf
in derselben Dataprep-Benutzeroberfläche als Job in Dataflow ausführen und den Fortschritt überwachen. Die Dataprep-Bibliothek bietet
viele vorgefertigte Wrangler für gängige Datenverarbeitungsaufgaben. Sie können Daten bereinigen,
indem Sie Duplikate entfernen oder fehlende Werte
und Ausreißer herausfiltern. Sie können Werte zusammenführen,
indem Sie sie zählen oder aufsummieren oder Sie können
verschiedene Tabellen zusammenführen. Sie können Daten in verschiedene Typen
wie Strings oder Ganzzahlen umwandeln. Während der Ablauf ausgeführt wird, können Sie die Jobdetails
über die Dataflow-Oberfläche überwachen, und wenn der Job abgeschlossen ist, erhalten Sie in Dataprep
eine Zusammenfassung des Jobstatus. Wie Sie in diesem Screenshot
eines abgeschlossenen Jobs sehen, enthält die Zusammenfassung
Statistiken und Visualisierungen, die Sie in Dataprep
für jedes Dataset erhalten können.