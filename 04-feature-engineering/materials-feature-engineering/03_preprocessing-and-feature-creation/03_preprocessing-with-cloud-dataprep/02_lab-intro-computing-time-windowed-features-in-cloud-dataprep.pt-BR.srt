1
00:00:00,000 --> 00:00:02,325
Bem-vindo de volta.
Neste laboratório,

2
00:00:02,325 --> 00:00:04,650
você pegará o conjunto de
dados de tarifa de táxi

3
00:00:04,650 --> 00:00:08,445
do BigQuery e o pré-processará usando
a ferramenta Cloud Dataprep.

4
00:00:08,445 --> 00:00:12,210
Na ferramenta, você explorará a
distribuição dos valores de dados,

5
00:00:12,210 --> 00:00:15,870
visualizará as distribuições usando
gráficos de histograma e, depois,

6
00:00:15,870 --> 00:00:19,550
implementará um fluxo do
Dataprep para criar um novo atributo,

7
00:00:19,550 --> 00:00:22,740
baseado no número médio
de viagens de táxi por hora,

8
00:00:22,740 --> 00:00:24,555
em uma janela de tempo contínua.

9
00:00:24,555 --> 00:00:29,265
Finalmente, você implantará e executará
o fluxo do Dataprep no GCP

10
00:00:29,265 --> 00:00:32,035
e monitorará a execução
da tarefa usando o Dataflow.

11
00:00:32,035 --> 00:00:34,065
Vamos olhar com mais detalhes.

12
00:00:34,065 --> 00:00:37,110
Certo, para começar com este laboratório,

13
00:00:37,110 --> 00:00:39,370
você precisa preparar
alguns pré-requisitos.

14
00:00:39,370 --> 00:00:42,990
Você começa no painel do
Google Cloud Platform.

15
00:00:42,990 --> 00:00:46,260
Primeiro, você precisará do intervalo
do Google Cloud Storage.

16
00:00:46,260 --> 00:00:50,510
Você pode criar um no menu
"Produtos e serviços",

17
00:00:50,510 --> 00:00:53,570
que pode ser acessado clicando
no ícone de três traços.

18
00:00:53,570 --> 00:00:55,785
Role para baixo até "Armazenamento",

19
00:00:55,785 --> 00:00:59,205
"Navegador" e clique em "Criar intervalo".

20
00:00:59,205 --> 00:01:01,755
Como você pode ver na orientação na tela,

21
00:01:01,755 --> 00:01:05,355
o nome do intervalo de armazenamento
precisa ser único e global.

22
00:01:05,355 --> 00:01:11,200
Aqui, configurei um nome de intervalo
único na localização us-east4.

23
00:01:11,200 --> 00:01:13,665
Logo após clicar em "Criar",

24
00:01:13,665 --> 00:01:17,220
vejo que o intervalo com o
nome único está pronto.

25
00:01:17,220 --> 00:01:21,885
O próximo passo é preparar
o conjunto de dados do BigQuery.

26
00:01:21,885 --> 00:01:25,965
Você pode encontrar o BigQuery voltando
no menu "Produtos e serviços",

27
00:01:25,965 --> 00:01:28,020
na seção "Big Data".

28
00:01:28,020 --> 00:01:30,220
Ao clicar em "BigQuery",

29
00:01:30,220 --> 00:01:33,035
você verá uma nova guia
abrindo no navegador.

30
00:01:33,035 --> 00:01:35,330
À direita do nome do seu projeto,

31
00:01:35,330 --> 00:01:40,180
clique na seta para baixo e escolha
"Criar novo conjunto de dados".

32
00:01:40,180 --> 00:01:46,350
Use o nome "taxi_cab_reporting" para o
conjunto de dados e clique em "OK".

33
00:01:47,000 --> 00:01:48,800
Quando estiver pronto,

34
00:01:48,800 --> 00:01:52,115
você precisará voltar ao painel
do Google Cloud Platform.

35
00:01:52,115 --> 00:01:57,630
Daí, navegue até o link do Dataprep
no menu "Produtos e serviços".

36
00:01:57,630 --> 00:02:01,160
Como o Cloud Dataprep é um serviço
de um parceiro do Google,

37
00:02:01,160 --> 00:02:04,205
você precisa aceitar um novo
conjunto de termos e condições.

38
00:02:04,205 --> 00:02:06,125
Clique em "Aceitar" para fazer isso.

39
00:02:06,125 --> 00:02:09,680
Além disso, você precisa clicar
em "Permitir" para que a Trifacta,

40
00:02:09,680 --> 00:02:13,640
parceira do Google que desenvolve
o Dataprep, acesse seus dados.

41
00:02:13,640 --> 00:02:18,350
Depois de clicar, levará alguns instantes
para ativar o Dataprep para o projeto.

42
00:02:18,350 --> 00:02:22,590
Então, você pode ver o vídeo avançar
para a espera.

43
00:02:22,590 --> 00:02:25,630
Em seguida, você precisa escolher
a conta a ser usada

44
00:02:25,630 --> 00:02:29,680
para o Cloud Dataprep e permitir que o
Dataprep acesse o projeto.

45
00:02:29,680 --> 00:02:33,050
Quando você está configurando o Dataprep
no projeto pela primeira vez,

46
00:02:33,050 --> 00:02:36,775
é necessário especificar o intervalo de
armazenamento que conterá seus dados.

47
00:02:36,775 --> 00:02:40,545
Aqui você pode ver que o intervalo
criado no início deste laboratório

48
00:02:40,545 --> 00:02:42,720
é usado para configurar o Dataprep.

49
00:02:42,720 --> 00:02:46,020
Depois que o intervalo for selecionado,
clique em "Continuar".

50
00:02:46,020 --> 00:02:48,780
Depois que o Dataprep
estiver configurado, você poderá

51
00:02:48,780 --> 00:02:52,805
dispensar o tutorial de ajuda clicando
em "Não mostrar nenhum assistente".

52
00:02:52,805 --> 00:02:56,940
Em seguida, você usará o Dataprep
para criar um novo fluxo.

53
00:02:56,940 --> 00:03:00,290
Vamos chamar esse fluxo
de "NYC Taxi reporting".

54
00:03:00,290 --> 00:03:04,000
O fluxo mostrará
um processo para ingestão,

55
00:03:04,000 --> 00:03:07,165
transformação e análise de dados de táxi.

56
00:03:07,165 --> 00:03:09,340
Vá em frente clique em "Criar".

57
00:03:09,340 --> 00:03:11,980
O primeiro passo para criar um fluxo

58
00:03:11,980 --> 00:03:15,760
é adicionar alguns conjuntos
de dados para o fluxo processar.

59
00:03:15,760 --> 00:03:20,345
Nesse caso, você importará alguns
conjuntos predefinidos que nossa equipe

60
00:03:20,345 --> 00:03:24,605
já salvou para o intervalo público do
Cloud Storage, e poderá acessar

61
00:03:24,605 --> 00:03:30,980
o intervalo de armazenamento usando o nome
asl-ml-immersion no diretório NYC taxicab.

62
00:03:30,980 --> 00:03:33,430
O diretório tem alguns arquivos.

63
00:03:33,430 --> 00:03:39,315
Você usará os arquivos com os dados
de tarifa de táxi de 2015 e 2016.

64
00:03:39,315 --> 00:03:43,400
Observe que esses são valores separados
por vírgulas, arquivos CSV.

65
00:03:43,400 --> 00:03:45,910
Clique em "Importar" e, logo,

66
00:03:45,910 --> 00:03:48,620
você verá os dois arquivos
adicionados ao fluxo.

67
00:03:48,620 --> 00:03:52,590
Para implementar o processamento ou a
conversão para esses conjuntos de dados,

68
00:03:52,590 --> 00:03:54,570
você precisará "Adicionar nova receita".

69
00:03:54,570 --> 00:03:57,930
Em seguida, você adicionará
etapas a essa receita.

70
00:03:57,930 --> 00:04:00,060
Depois que o conjunto
de dados for carregado,

71
00:04:00,060 --> 00:04:03,190
você verá uma visualização
de uma amostra dos dados do conjunto.

72
00:04:03,190 --> 00:04:06,310
Aqui, por exemplo, você pode ver que o
conjunto de dados inclui

73
00:04:06,310 --> 00:04:09,875
informações sobre as corridas de táxi,
como data/hora do embarque,

74
00:04:09,875 --> 00:04:13,855
data/hora da chegada
e o número de passageiros no táxi.

75
00:04:13,855 --> 00:04:17,410
Além disso, observe a partir do histograma
de distância da viagem

76
00:04:17,410 --> 00:04:21,089
que a maioria das viagens estava abaixo
de cinco milhas de distância.

77
00:04:21,089 --> 00:04:25,675
Em seguida, una conjuntos de dados
de 2015 e 2016

78
00:04:25,675 --> 00:04:28,325
para trabalhar com linhas
de dados pequenas.

79
00:04:28,325 --> 00:04:31,125
Depois de selecionar o conjunto
de dados de 2016,

80
00:04:31,125 --> 00:04:33,780
você precisa clicar em "Adicionar"
e alinhar pelo nome,

81
00:04:33,780 --> 00:04:37,710
o que fará com que os nomes que tiverem
os cabeçalhos das colunas correspondentes

82
00:04:37,710 --> 00:04:40,435
estejam alinhados à versão da
união do conjunto de dados.

83
00:04:40,435 --> 00:04:44,865
Adicione a etapa de união à receita e,
depois que o Dataprep visualizar a união,

84
00:04:44,865 --> 00:04:50,030
você verá uma amostra dos conjuntos, que
inclui viagens de táxi para 2015 e 2016.

85
00:04:50,030 --> 00:04:56,355
Os dados de data e hora
de embarque estão em colunas diferentes.

86
00:04:56,355 --> 00:04:58,970
Como este laboratório
mostrará como calcular

87
00:04:58,970 --> 00:05:01,750
as médias contínuas
dos valores de tarifa de táxi,

88
00:05:01,750 --> 00:05:06,430
primeiro, converta os dados de entrada
para o formato de data/hora SQL.

89
00:05:06,430 --> 00:05:09,810
Para isso, você pode adicionar
uma mescla à receita,

90
00:05:09,810 --> 00:05:12,950
que concatenará valores
de várias colunas.

91
00:05:12,950 --> 00:05:17,405
Neste caso, as colunas são chamadas de
data de embarque e horário de embarque.

92
00:05:17,405 --> 00:05:20,905
Use "pickup_datetime"
como o novo nome da coluna.

93
00:05:20,905 --> 00:05:25,115
Além disso, use um único espaço como
um delimitador entre os valores.

94
00:05:25,115 --> 00:05:26,960
Observe que, à esquerda,

95
00:05:26,960 --> 00:05:29,075
você agora tem uma prévia
da nova coluna.

96
00:05:29,075 --> 00:05:31,820
Em seguida, crie uma nova coluna derivada

97
00:05:31,820 --> 00:05:35,010
que converterá o tempo de embarque
em um formato de data/hora SQL.

98
00:05:35,010 --> 00:05:38,115
Quando o novo campo de carimbo 
de data/hora estiver disponível,

99
00:05:38,115 --> 00:05:41,120
você extrairá apenas informações
do ano, mês, data

100
00:05:41,120 --> 00:05:44,980
e hora, sem os detalhes
dos minutos e segundos.

101
00:05:44,980 --> 00:05:50,345
Como a coluna de data/hora do embarque
não tem valores para minutos e segundos,

102
00:05:50,345 --> 00:05:53,540
ela não pode ser usada como
formato de data/hora SQL.

103
00:05:53,540 --> 00:05:56,300
Então, você precisa criar uma
nova coluna que possa ser

104
00:05:56,300 --> 00:05:59,650
convertida em um valor válido
de data/hora SQL.

105
00:05:59,650 --> 00:06:01,470
Para fazer isso, você criará

106
00:06:01,470 --> 00:06:05,530
uma nova operação de mesclagem e usará
o wrangler de mesclagem novamente.

107
00:06:05,530 --> 00:06:08,670
Ele concatenará valores
das colunas de hora e

108
00:06:08,670 --> 00:06:11,260
data/hora de embarque
com uma string

109
00:06:11,260 --> 00:06:15,130
com quatro caracteres zero
para os valores dos minutos e segundos.

110
00:06:15,130 --> 00:06:17,860
Observe que, quando você
adiciona uma nova coluna,

111
00:06:17,860 --> 00:06:21,195
ela recebe um nome gerado automaticamente,
como a coluna um.

112
00:06:21,195 --> 00:06:23,040
Você pode facilmente renomear isso.

113
00:06:23,040 --> 00:06:26,540
Nesse caso, você pode renomeá-la como
"pickup_hour".

114
00:06:28,470 --> 00:06:32,830
Depois, você calculará estatísticas com
base nos valores das horas de embarque.

115
00:06:32,830 --> 00:06:37,505
Você pode usar funções padrão de agregação
estatística SQL, como soma ou média.

116
00:06:37,505 --> 00:06:42,040
Você pode ver que esse Wrangler calculará
as somas e as médias das contagens

117
00:06:42,040 --> 00:06:44,530
de passageiros e a mesma combinação

118
00:06:44,530 --> 00:06:47,750
da soma e da média da distância
da viagem e do valor da tarifa.

119
00:06:47,750 --> 00:06:52,650
Por fim, calculará os valores máximos
da tarifa para cada hora de embarque.

120
00:06:56,220 --> 00:06:59,440
Assim como anteriormente,
você tem uma visualização dos resultados

121
00:06:59,440 --> 00:07:03,515
das estatísticas calculadas nos
histogramas no lado esquerdo da tela.

122
00:07:03,515 --> 00:07:07,475
Além disso, se você observar o valor médio
da tarifa nos histogramas,

123
00:07:07,475 --> 00:07:12,490
a maioria das tarifas médias está na faixa
de US$ 18 a US$ 19 por viagem.

124
00:07:12,490 --> 00:07:17,150
Em seguida, você calculará a média
contínua para o valor da tarifa,

125
00:07:17,150 --> 00:07:21,995
observando as horas livres de dados
acumulados para cada hora de embarque.

126
00:07:21,995 --> 00:07:26,930
Você pode calcular isso usando a função de
média contínua no Cloud Dataprep.

127
00:07:26,930 --> 00:07:29,335
Aqui estão os valores
para a média contínua,

128
00:07:29,335 --> 00:07:31,300
classificados pela hora de embarque.

129
00:07:31,300 --> 00:07:36,440
Por fim, nomeie essa coluna como
"average free hour rolling fare".

130
00:07:36,440 --> 00:07:39,750
Tudo bem, quando a receita estiver pronta,

131
00:07:39,750 --> 00:07:43,395
você poderá implantá-la como um job
do Google Cloud Dataflow.

132
00:07:43,395 --> 00:07:47,355
Para fazer isso, você precisa clicar em
"Executar o job" e especificar

133
00:07:47,355 --> 00:07:52,035
onde os resultados dos jobs serão
publicados ou armazenados.

134
00:07:52,035 --> 00:07:54,905
Por padrão, os resultados
do job são salvos

135
00:07:54,905 --> 00:07:57,765
como um arquivo CSV
no Google Cloud Storage.

136
00:07:57,765 --> 00:08:02,120
Em vez de fazer isso, você pode alterar
o destino para BigQuery

137
00:08:02,120 --> 00:08:06,640
e criar uma nova tabela no BigQuery
toda vez que o job for executado.

138
00:08:06,640 --> 00:08:11,025
Portanto, se você alterar a seleção à
direita para criar uma nova tabela

139
00:08:11,025 --> 00:08:16,300
em cada execução e renomear a tabela para
"TLC_yellow_trips_reporting",

140
00:08:16,300 --> 00:08:20,225
você receberá uma nova tabela no conjunto
de dados "NYC Taxi reporting".

141
00:08:20,225 --> 00:08:24,265
Lembre-se, este é o conjunto de dados
criado no início deste laboratório.

142
00:08:25,535 --> 00:08:28,210
Vá em frente e execute o job.

143
00:08:28,210 --> 00:08:30,345
Depois que o job aparecer
como transformação,

144
00:08:30,345 --> 00:08:33,559
o Dataprep começará a implantar o job
no Dataflow.

145
00:08:33,559 --> 00:08:35,760
Isso geralmente leva alguns meses.

146
00:08:35,760 --> 00:08:40,414
Você pode monitorar o progresso do job na
seção do job no menu Dataprep.

147
00:08:40,414 --> 00:08:43,554
Se você clicar no símbolo
de reticências à direita,

148
00:08:43,554 --> 00:08:46,510
o menu de reticências não terá o link

149
00:08:46,510 --> 00:08:49,590
para o job do Dataflow imediatamente após
você implantar o job,

150
00:08:49,590 --> 00:08:52,659
mas se você esperar um pouco
e atualizar a página,

151
00:08:52,659 --> 00:08:57,635
verá que o menu é atualizado e verá um
link para acessar o job do Dataflow.

152
00:08:57,635 --> 00:09:02,570
Se você clicar nele, será levado
para a interface do usuário do Dataflow,

153
00:09:02,570 --> 00:09:05,905
onde poderá monitorar as etapas
de transformação no Dataflow,

154
00:09:05,905 --> 00:09:07,660
conforme criado pelo Dataprep.

155
00:09:07,660 --> 00:09:10,645
No lado direito da IU do Dataflow,

156
00:09:10,645 --> 00:09:13,615
você tem detalhes sobre
essa execução do job.

157
00:09:13,615 --> 00:09:17,160
Aqui, você pode ver que, como o job
acabou de ser iniciado,

158
00:09:17,160 --> 00:09:21,080
o cluster do Dataflow, para executar
o job, ainda precisa ser escalonado.

159
00:09:21,080 --> 00:09:25,225
No entanto, você já pode monitorar os
resultados da configuração do job.

160
00:09:25,225 --> 00:09:29,490
Aqui, nenhuma das etapas de transformação
individuais do job foi iniciada,

161
00:09:29,490 --> 00:09:31,340
exceto as poucas que preparam

162
00:09:31,340 --> 00:09:34,800
a tabela no BigQuery, e estão
apenas começando a buscar dados

163
00:09:34,800 --> 00:09:38,370
dos arquivos CSV de entrada
do Google Cloud Storage.

164
00:09:38,370 --> 00:09:41,730
Além de monitorar esse job no Dataflow,

165
00:09:41,730 --> 00:09:44,140
você pode navegar para
o BigQuery e monitorar

166
00:09:44,140 --> 00:09:47,575
a saída do job no conjunto de dados
do relatório do táxi.

167
00:09:47,575 --> 00:09:50,525
Como você se lembra, assim que o job
começar a ser executado,

168
00:09:50,525 --> 00:09:55,135
ele inserirá valores em uma nova tabela de
relatório de viagem amarela do TLC.

169
00:09:55,135 --> 00:09:57,600
Como demora um pouco
para a tabela ser criada,

170
00:09:57,600 --> 00:10:01,545
talvez seja necessário aguardar e
atualizar a página.

171
00:10:01,545 --> 00:10:03,710
Depois que a tabela estiver no lugar,

172
00:10:03,710 --> 00:10:07,185
você poderá inserir uma instrução SQL
para buscar os resultados da tabela.

173
00:10:07,185 --> 00:10:12,565
Tenha o dialeto SQL configurado
corretamente antes de executá-lo.

174
00:10:12,565 --> 00:10:18,560
Aqui, você pode ver que a execução do job
gera cerca de 192 kilobytes de dados,

175
00:10:18,560 --> 00:10:20,920
incluindo informações sobre
as horas de embarque,

176
00:10:20,920 --> 00:10:23,150
as distâncias médias da viagem,
a tarifa média

177
00:10:23,150 --> 00:10:26,120
e outras informações
calculadas pelo Dataflow.

178
00:10:26,120 --> 00:10:28,790
Certo.
Isso é tudo para este laboratório.