1
00:00:00,000 --> 00:00:03,510
In this course, we looked at feature engineering as

2
00:00:03,510 --> 00:00:07,500
a way to improve the performance of your machine learning models.

3
00:00:07,500 --> 00:00:13,315
In this course, you'll learn how to convert raw data into features,

4
00:00:13,315 --> 00:00:20,070
pre-process data in such a way that the pre-processing is also done during serving,

5
00:00:20,070 --> 00:00:24,390
choose among the various feature columns in TensorFlow,

6
00:00:24,390 --> 00:00:30,930
memorize large data sets using feature crosses and simple models,

7
00:00:30,930 --> 00:00:38,265
and finally, to simplify preprocessing pipelines using tensor for transfer.

8
00:00:38,265 --> 00:00:42,540
We started out by saying that feature engineering was

9
00:00:42,540 --> 00:00:47,730
a necessary thing because all our raw data won't be numeric.

10
00:00:47,730 --> 00:00:51,270
We will have to create features from raw data anyway,

11
00:00:51,270 --> 00:00:57,780
so why not do it in a way that makes a machine learning model learn better?

12
00:00:57,780 --> 00:01:03,070
We then looked at the kinds of things that you do in pre-processing,

13
00:01:03,070 --> 00:01:06,620
everything from filtering data and computing

14
00:01:06,620 --> 00:01:12,540
vocabularies to resizing images and normalizing volume levels.

15
00:01:12,540 --> 00:01:18,660
We then considered where we would do these kinds of operations and

16
00:01:18,660 --> 00:01:24,660
realize that Apache beam was ideal for this because it lets you do everything.

17
00:01:24,660 --> 00:01:31,680
We learned how beam worked and how to execute beam pipelines and cloud dataflow.

18
00:01:31,680 --> 00:01:39,150
We then looked at two interesting ways to create new features from your raw data.

19
00:01:39,150 --> 00:01:45,180
We looked at feature crosses and the embedding columns and talked about how we

20
00:01:45,180 --> 00:01:50,670
would trade off between memorization on one hand and generalization on the other.

21
00:01:50,670 --> 00:01:58,710
Finally, we put it together by showing you how to implement pre-processing methods

22
00:01:58,710 --> 00:02:02,790
using tensor for transfer in such a way that

23
00:02:02,790 --> 00:02:07,965
the pre-processed datasets are created in a distributed way using beam,

24
00:02:07,965 --> 00:02:14,780
but also computed efficiently as part of the model graph using TensorFlow.

25
00:02:14,780 --> 00:02:21,660
And that brings us to the end of the fourth course in this specialization.

26
00:02:21,660 --> 00:02:23,580
In the first course,

27
00:02:23,580 --> 00:02:27,120
we talked about how Googled SML and what it means to

28
00:02:27,120 --> 00:02:31,065
be AI fast and how to frame a machine learning problem.

29
00:02:31,065 --> 00:02:33,270
In the second course,

30
00:02:33,270 --> 00:02:39,705
you learned how to create datasets and how optimization of machine learning models works.

31
00:02:39,705 --> 00:02:41,730
In the third course,

32
00:02:41,730 --> 00:02:47,650
you'll start to write TensorFlow models using the estimator API.

33
00:02:47,650 --> 00:02:49,500
And in this course,

34
00:02:49,500 --> 00:02:54,600
you learned how to improve those models using feature engineering.

35
00:02:54,600 --> 00:02:58,140
Stick around for the next course which

36
00:02:58,140 --> 00:03:01,590
is going to be about the art and science of machine learning,

37
00:03:01,590 --> 00:03:08,010
practical tips to squeeze performance out of your machine learning models,

38
00:03:08,010 --> 00:03:15,510
and be sure to join us for the next specialization on advanced machine learning topics.

39
00:03:15,510 --> 00:03:18,685
This will be about machine learning at scale,

40
00:03:18,685 --> 00:03:22,850
and on specialized machine learning models for images,

41
00:03:22,850 --> 00:03:28,630
sequencers and recommendations. See you around.