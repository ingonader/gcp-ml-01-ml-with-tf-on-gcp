統計を取って
欠損値があることに気づいた場合 通常は列の平均値などを
仮に使います ここがMLと統計学の分かれ道です MLでは データがある場合とない場合で
別々のモデルを作ります データがある場合とない場合で
別の処理ができます なぜならMLでは十分なデータがあり できる限り細かい粒度で
構築したいからです 一方 統計学では 手元のデータから最善の結果を
得られるようにデータを保持します 両者の考え方が異なるので 異常値の扱い異なり MLでは十分な数の異常値を見つけて トレーニングに使います 「5個のサンプル」ルールを
覚えていますね 統計学では 収集できる全データが集まったら 異常値をすべて取り除きます MLと統計学ではシナリオが異なるので
考え方も異なります 統計学はしばしば限定的な
データ領域で使われ MLは大量のデータで運用されます 欠損データの有無を示す列を
追加することは MLでは標準的な作業です データが不十分な場合は
平均値を代わりに使います この例では
家の価値を予測します データセットには緯度と
このような2つのピークがあり 1つはSFO、もう1つはLAS サンフランシスコと
ロサンゼルスです このモデルでは 緯度を浮動小数点の特徴として
表しても意味がありません 緯度と家の価値の間に
線形関係がないからです たとえば緯度35度の家は
34度の家より 35/34倍の値段ではありません とはいえ緯度は家の価格の
良い指標になるでしょう 大小のある値を たとえばこのように扱えます 浮動小数点の1つの特徴の代わりに 11個の異なるブール値の特徴つまり yes-no latitudeBin1、latitudeBin2 ...
latitudeBin11までの yes-noブール値にして 固定値のビン境界を使います データサイエンティストが
よく使うのは四分位の境界で 各ビン内の
値の数が一定になります 他の回帰の問題で
よく見かけるでしょう トレーニングサイクルの大半は 異常なインスタンスの修正に費やされます MLではロングテールを分解しますが
通常の統計では取り除きます ある家に50部屋もある場合
範囲の上限である「4部屋」に設定します 家の価格が
数十万ドルするのに比べて 部屋数などは小さな数値です オプティマイザは従来
これをうまく扱うことができず 価格が勾配を支配します さて最近のMLアーキテクチャでは 変数の大小をいわゆる
バッチ正規化によって考慮します サンプルからなる1つのバッチに
すべて異常値が含まれる場合は 問題が発生するかもしれませんが 以前ほど重要ではありません