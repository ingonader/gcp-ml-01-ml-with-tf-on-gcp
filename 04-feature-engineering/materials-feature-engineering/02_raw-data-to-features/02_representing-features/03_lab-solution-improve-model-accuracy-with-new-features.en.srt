1
00:00:00,000 --> 00:00:02,070
So in this lab,

2
00:00:02,070 --> 00:00:04,755
we're going to be trying out different features.

3
00:00:04,755 --> 00:00:08,100
So what we did was that we went to A underscore features.

4
00:00:08,100 --> 00:00:11,240
And let me just go ahead and clear all the cells.

5
00:00:11,240 --> 00:00:14,725
So I'm sure that everything that I'm running I'm actually running.

6
00:00:14,725 --> 00:00:18,450
And the first thing is to go ahead and do a bunch of imports.

7
00:00:18,450 --> 00:00:19,970
We are importing TensorFlow,

8
00:00:19,970 --> 00:00:21,260
we are importing Pandas,

9
00:00:21,260 --> 00:00:23,245
we are importing NumPy, et cetera.

10
00:00:23,245 --> 00:00:27,630
So, let's go ahead and import those and then load

11
00:00:27,630 --> 00:00:31,920
up the dataset which is from the California housing dataset.

12
00:00:31,920 --> 00:00:33,330
So this is what we're going to be doing.

13
00:00:33,330 --> 00:00:37,985
We're going to be trying to predict the price of houses in California from this dataset.

14
00:00:37,985 --> 00:00:40,050
And then we just load up the dataset,

15
00:00:40,050 --> 00:00:42,680
it's kind of good to know what's in this dataset.

16
00:00:42,680 --> 00:00:44,825
So let's go ahead and do df.head.

17
00:00:44,825 --> 00:00:47,050
This shows us the first few lines,

18
00:00:47,050 --> 00:00:50,730
and we learn that there is a longitude and latitude,

19
00:00:50,730 --> 00:00:52,590
the housing median age.

20
00:00:52,590 --> 00:00:57,155
So this dataset is not actually individual houses, it's actually aggregated.

21
00:00:57,155 --> 00:01:01,410
So you have the total number of rooms, it's 5,612.

22
00:01:01,410 --> 00:01:03,120
Obviously, this is not one house,

23
00:01:03,120 --> 00:01:08,510
this is all of the rooms in that aggregation which is either a zip code,

24
00:01:08,510 --> 00:01:10,980
or a county, or whatever that aggregation is.

25
00:01:10,980 --> 00:01:13,440
So we have the total number of rooms in that aggregation,

26
00:01:13,440 --> 00:01:14,985
total number of bedrooms,

27
00:01:14,985 --> 00:01:17,020
the population, the number of people.

28
00:01:17,020 --> 00:01:19,140
So it's about a thousand people, it looks like.

29
00:01:19,140 --> 00:01:23,150
And that is in 472 households.

30
00:01:23,150 --> 00:01:27,190
The median income is 1.5 in some units.

31
00:01:27,190 --> 00:01:34,005
And the median housing value is 66,900 again and some units.

32
00:01:34,005 --> 00:01:36,720
So that is essentially the thing that we're trying

33
00:01:36,720 --> 00:01:39,700
to learn from the dataset that we're going to learn from.

34
00:01:39,700 --> 00:01:43,335
Now, this is the first few lines of this dataset.

35
00:01:43,335 --> 00:01:47,030
That is good to basically get an idea of what these numbers look like.

36
00:01:47,030 --> 00:01:49,605
But df.describe() is extremely useful.

37
00:01:49,605 --> 00:01:51,840
What df.describe() does in Pandas.

38
00:01:51,840 --> 00:01:55,740
Is that it shows you statistics of the numeric columns.

39
00:01:55,740 --> 00:01:57,600
So if there is any categorical columns,

40
00:01:57,600 --> 00:01:59,390
it's not going to show us anything about it,

41
00:01:59,390 --> 00:02:01,830
but every numeric column here,

42
00:02:01,830 --> 00:02:03,345
everything is a numeric column.

43
00:02:03,345 --> 00:02:05,700
It's going to show us, for example,

44
00:02:05,700 --> 00:02:09,630
that there are 17,000 longitudes in the dataset,

45
00:02:09,630 --> 00:02:11,940
17,000 latitudes in the dataset.

46
00:02:11,940 --> 00:02:14,550
So this is the number of rows in the dataset and

47
00:02:14,550 --> 00:02:17,490
this is a good ideas to basically go ahead and check,

48
00:02:17,490 --> 00:02:20,130
that all of them are actually 17,000.

49
00:02:20,130 --> 00:02:22,740
If any of them is not 17,000,

50
00:02:22,740 --> 00:02:27,750
that indicates that one or more of the values for that role are missing.

51
00:02:27,750 --> 00:02:29,340
So in this case,

52
00:02:29,340 --> 00:02:31,589
number one sanity check,

53
00:02:31,589 --> 00:02:32,685
no values are missing.

54
00:02:32,685 --> 00:02:36,120
We have 17,000 rows for all of the values.

55
00:02:36,120 --> 00:02:39,475
The mean longitude is minus 119.

56
00:02:39,475 --> 00:02:41,640
The mean latitude is 35.

57
00:02:41,640 --> 00:02:44,000
This makes sense because this is California.

58
00:02:44,000 --> 00:02:47,820
The mean housing age is 28.6.

59
00:02:47,820 --> 00:02:50,340
This happens to be years. So about 30 years old.

60
00:02:50,340 --> 00:02:53,685
The total number of rooms is 2,643.

61
00:02:53,685 --> 00:02:56,355
That is not an individual house, is it.

62
00:02:56,355 --> 00:03:00,240
Right? So this is probably a total number of rooms in that aggregation unit.

63
00:03:00,240 --> 00:03:03,090
We have to do something with it.

64
00:03:03,090 --> 00:03:09,900
And the total number of bedrooms again looks odd 539, population 1,429,

65
00:03:09,900 --> 00:03:13,245
number of households is 501,

66
00:03:13,245 --> 00:03:17,925
and median income is 3.9 and,

67
00:03:17,925 --> 00:03:23,055
let's say median housing value is 27,000 in this case.

68
00:03:23,055 --> 00:03:26,015
Right? So that is the mean of all of those.

69
00:03:26,015 --> 00:03:27,600
And then you have the standard deviation,

70
00:03:27,600 --> 00:03:29,610
the minimum value that exists,

71
00:03:29,610 --> 00:03:31,770
the minimum number of rooms is two.

72
00:03:31,770 --> 00:03:36,025
The maximum number of rooms is 37,937.

73
00:03:36,025 --> 00:03:39,225
So that gives us an idea of what this data looks like.

74
00:03:39,225 --> 00:03:42,300
And what we're going to do is, we basically going to split this data

75
00:03:42,300 --> 00:03:45,390
into two parts and here for experimentation,

76
00:03:45,390 --> 00:03:47,295
we're not going actually save it anywhere,

77
00:03:47,295 --> 00:03:49,485
the random splitting is fine enough.

78
00:03:49,485 --> 00:03:53,880
So, I'm basically creating a mask and the mask is basically

79
00:03:53,880 --> 00:03:59,760
creating an array of the length of the dfs so that's 17,000.

80
00:03:59,760 --> 00:04:03,135
And checking if the random is less than 0.8.

81
00:04:03,135 --> 00:04:05,880
So which means 80 percent of the values will be

82
00:04:05,880 --> 00:04:10,920
one and 20 percent of the value would be zero, approximately.

83
00:04:10,920 --> 00:04:14,340
So the training df is all of those values for which it is

84
00:04:14,340 --> 00:04:19,110
one and the evaluation df is all of the values for which the mask is zero.

85
00:04:19,110 --> 00:04:20,609
So at this point,

86
00:04:20,609 --> 00:04:25,740
we will get two data frames, traindf and evaldf.

87
00:04:25,740 --> 00:04:27,850
And I can add a new thing.

88
00:04:27,850 --> 00:04:33,390
And I can print the length of traindf and that is about 13,000.

89
00:04:33,390 --> 00:04:37,035
And I can also print a length of evaldf,

90
00:04:37,035 --> 00:04:41,235
and that is about 20 percent, about 3,400.

91
00:04:41,235 --> 00:04:44,265
So at this point, we now have our training dataset.

92
00:04:44,265 --> 00:04:46,065
Our evaluation dataset.

93
00:04:46,065 --> 00:04:48,855
Let's go ahead and build our model.

94
00:04:48,855 --> 00:04:50,100
And to build our model,

95
00:04:50,100 --> 00:04:53,025
the first thing is we need to read our data.

96
00:04:53,025 --> 00:04:55,720
So I'm going to do make_input function.

97
00:04:55,720 --> 00:04:57,515
Given a data frame,

98
00:04:57,515 --> 00:05:00,370
number of epochs we want to read it,

99
00:05:00,370 --> 00:05:03,140
and I'll use the pandas_input function,

100
00:05:03,140 --> 00:05:07,025
to take the data frame but not just the data frame,

101
00:05:07,025 --> 00:05:09,515
I will add extra features to it.

102
00:05:09,515 --> 00:05:11,535
And to get you started,

103
00:05:11,535 --> 00:05:13,770
we said look, the number of room,

104
00:05:13,770 --> 00:05:15,585
the total number of rooms here.

105
00:05:15,585 --> 00:05:17,540
This is kind of ridiculous, right?

106
00:05:17,540 --> 00:05:22,335
We don't have a house with 2,643 rooms. That's not right.

107
00:05:22,335 --> 00:05:23,910
So what this actually is,

108
00:05:23,910 --> 00:05:27,120
is the total number of rooms in that aggregation,

109
00:05:27,120 --> 00:05:28,425
in that zip code.

110
00:05:28,425 --> 00:05:30,990
So what do we have to normalize this by.

111
00:05:30,990 --> 00:05:33,675
We have to bring it down to a single house value.

112
00:05:33,675 --> 00:05:37,050
So what we're doing is that we are taking the total number of rooms,

113
00:05:37,050 --> 00:05:39,285
and dividing it by the number of households.

114
00:05:39,285 --> 00:05:42,480
And that tells us a number of rooms in that house.

115
00:05:42,480 --> 00:05:45,750
In a typical house, in that zip code.

116
00:05:45,750 --> 00:05:49,755
So what else do we have to normalize for the number of zip code.

117
00:05:49,755 --> 00:05:51,120
Well, let's look at this.

118
00:05:51,120 --> 00:05:54,090
The latitudes and longitudes seem to be fine as they are.

119
00:05:54,090 --> 00:05:57,375
The total number of rooms we have to normalize.

120
00:05:57,375 --> 00:05:59,895
We also have a normalize the total number of bedrooms.

121
00:05:59,895 --> 00:06:01,365
So let's just do this.

122
00:06:01,365 --> 00:06:05,685
So instead of doing a number of rooms in addition,

123
00:06:05,685 --> 00:06:10,590
let's do the number of bedrooms is that what's called.

124
00:06:10,590 --> 00:06:13,360
No it's called total bedrooms.

125
00:06:15,140 --> 00:06:17,880
Total bed rooms.

126
00:06:17,880 --> 00:06:24,075
And this could be the number of bedrooms.

127
00:06:24,075 --> 00:06:28,170
So that are our two extra features.

128
00:06:28,170 --> 00:06:32,670
We created our input function and now our feature columns,

129
00:06:32,670 --> 00:06:36,225
the housing_median_age, right, it's a numeric column.

130
00:06:36,225 --> 00:06:38,430
It could be used as this, right?

131
00:06:38,430 --> 00:06:41,025
The median age when we look at it.

132
00:06:41,025 --> 00:06:42,375
These numbers make sense.

133
00:06:42,375 --> 00:06:43,980
These seem to be years.

134
00:06:43,980 --> 00:06:46,650
So we can use them as they are.

135
00:06:46,650 --> 00:06:48,750
So let's use the median age.

136
00:06:48,750 --> 00:06:55,275
Then we will go ahead and take the latitude and bucketize them between 32 and 42.

137
00:06:55,275 --> 00:06:57,045
Why 32 and 42?

138
00:06:57,045 --> 00:07:04,645
Because we go back here and we see that the latitude varies between 32 and 42.

139
00:07:04,645 --> 00:07:11,790
So we can bucketize the latitude between 32 and 42. What else should we use?

140
00:07:11,790 --> 00:07:13,184
If you're using latitude,

141
00:07:13,184 --> 00:07:15,570
we might as well also use longitude.

142
00:07:15,570 --> 00:07:18,520
So let's go ahead and take this,

143
00:07:21,280 --> 00:07:25,240
and also do the longitude.

144
00:07:25,240 --> 00:07:28,755
So we will do the longitude here.

145
00:07:28,755 --> 00:07:38,270
But the longitude boundaries need to be between negative 124 and negative 114.

146
00:07:38,270 --> 00:07:42,835
So let's go down here and change this to the negative

147
00:07:42,835 --> 00:07:49,970
124 and negative 114 and one degree longitude is probably reasonable.

148
00:07:49,970 --> 00:07:53,660
One degree is essentially about 100 kilometers.

149
00:07:53,660 --> 00:07:54,935
So that's about right.

150
00:07:54,935 --> 00:07:56,360
So we can do this.

151
00:07:56,360 --> 00:07:58,355
The number of rooms.

152
00:07:58,355 --> 00:08:00,625
Remember that we added the number of bedrooms.

153
00:08:00,625 --> 00:08:03,410
So let's go ahead and do that as well.

154
00:08:03,410 --> 00:08:06,560
Number of rooms, number of bedrooms.

155
00:08:06,560 --> 00:08:08,795
And then we have the median income.

156
00:08:08,795 --> 00:08:12,595
So that is our set of feature columns.

157
00:08:12,595 --> 00:08:17,294
And then you go ahead and train and evaluate,

158
00:08:17,294 --> 00:08:20,410
using train_and_evaluate passing in the train spec,

159
00:08:20,410 --> 00:08:22,195
the eval spec, et cetera.

160
00:08:22,195 --> 00:08:24,160
And at this point,

161
00:08:24,160 --> 00:08:26,215
we can call train_and_evaluate,

162
00:08:26,215 --> 00:08:28,315
and write out a trained model.

163
00:08:28,315 --> 00:08:30,340
And when we run this,

164
00:08:30,340 --> 00:08:33,670
we should get an evaluation output.

165
00:08:33,670 --> 00:08:35,065
So we are running it.

166
00:08:35,065 --> 00:08:39,605
We asked it to run for 5,000 steps.

167
00:08:39,605 --> 00:08:40,875
So at this point,

168
00:08:40,875 --> 00:08:42,660
I'm on step 1,650.

169
00:08:42,660 --> 00:08:44,455
So let's just wait a little bit.

170
00:08:44,455 --> 00:08:46,180
And once it's done,

171
00:08:46,180 --> 00:08:47,950
notice that every once in a while,

172
00:08:47,950 --> 00:08:49,555
it's saving a dictionary,

173
00:08:49,555 --> 00:08:52,270
specifying what the average loss is.

174
00:08:52,270 --> 00:08:56,790
The average loss is not all that useful because the losses computed on a batch.

175
00:08:56,790 --> 00:08:58,215
So not that great.

176
00:08:58,215 --> 00:08:59,565
But this loss.

177
00:08:59,565 --> 00:09:02,875
This is the loss on the evaluation dataset and that makes more sense.

178
00:09:02,875 --> 00:09:05,465
So let's go ahead and actually the loss,

179
00:09:05,465 --> 00:09:10,570
and the average loss is computer in the evaluation dataset that makes more sense.

180
00:09:10,570 --> 00:09:12,640
The loss itself is on just a batch.

181
00:09:12,640 --> 00:09:14,110
So we don't need that.

182
00:09:14,110 --> 00:09:15,585
So let's go down.

183
00:09:15,585 --> 00:09:17,350
Let's wait for it to be done,

184
00:09:17,350 --> 00:09:21,205
4,000, 5,000. And there it is.

185
00:09:21,205 --> 00:09:27,130
Our average loss over the entire dataset the RMSC is 0.59.

186
00:09:27,130 --> 00:09:29,200
Now, let's try something else.

187
00:09:29,200 --> 00:09:38,480
Let's go ahead and see what happens if I don't have the number of rooms.

188
00:09:38,480 --> 00:09:42,685
So let's now add these extra things, and what we could do,

189
00:09:42,685 --> 00:09:47,305
is when we are creating our input columns.

190
00:09:47,305 --> 00:09:50,410
We'll just decide not to use the number of rooms,

191
00:09:50,410 --> 00:09:52,225
the number of bedrooms,

192
00:09:52,225 --> 00:09:55,450
or the latitude, or the longitude,

193
00:09:55,450 --> 00:09:56,965
or the median income.

194
00:09:56,965 --> 00:09:59,080
So all we have is the age of the house.

195
00:09:59,080 --> 00:10:02,215
So if we do that and that's our only feature column.

196
00:10:02,215 --> 00:10:04,145
And what happens when we do that?

197
00:10:04,145 --> 00:10:06,570
So we'll go down here.

198
00:10:08,320 --> 00:10:11,525
Remember that we got 0.59 earlier.

199
00:10:11,525 --> 00:10:13,205
So I'll clear the cell,

200
00:10:13,205 --> 00:10:16,425
and run it again, this time it's just with one input.

201
00:10:16,425 --> 00:10:19,810
And at this point, what do we get.

202
00:10:20,210 --> 00:10:23,310
At this point, our loss is 1.87.

203
00:10:23,310 --> 00:10:26,930
So obviously all those input features were useful.

204
00:10:26,930 --> 00:10:30,870
Right? Because again the loss went up, when we didn't have them.

205
00:10:30,870 --> 00:10:33,180
So this gives you an idea of what kind of

206
00:10:33,180 --> 00:10:36,180
features are useful and you could do this manually,

207
00:10:36,180 --> 00:10:38,070
you could basically go ahead and say,

208
00:10:38,070 --> 00:10:40,710
what happens if I just use the median income.

209
00:10:40,710 --> 00:10:43,590
And the median income is a pretty good indicator.

210
00:10:43,590 --> 00:10:47,720
So if we use the median income and you try this again,

211
00:10:47,720 --> 00:10:50,495
you will find that the loss goes down dramatically.

212
00:10:50,495 --> 00:10:53,025
And the other features are not that useful.

213
00:10:53,025 --> 00:10:54,450
They're useful, not that useful.

214
00:10:54,450 --> 00:10:59,250
The median income is particularly useful because it goes into what people can

215
00:10:59,250 --> 00:11:05,445
afford and the market tends to match what people can afford.

216
00:11:05,445 --> 00:11:09,810
So notice that now we've got an average loss of 0.69.

217
00:11:09,810 --> 00:11:16,410
So essentially, we went from 1.38 to 0.69 simply by adding one feature the median income.

218
00:11:16,410 --> 00:11:19,230
The median income was an extremely useful feature.

219
00:11:19,230 --> 00:11:21,345
Now, how does this matter in the real-world,

220
00:11:21,345 --> 00:11:24,150
they way this matters in the real-world is,

221
00:11:24,150 --> 00:11:26,625
it really matters what data you collect.

222
00:11:26,625 --> 00:11:28,680
Imagine that you had this dataset,

223
00:11:28,680 --> 00:11:33,240
and you did not collect the median income of people who lived in that neighborhood.

224
00:11:33,240 --> 00:11:36,180
Now, your model is pretty bad.

225
00:11:36,180 --> 00:11:39,630
So this is what we mean by when we say that,

226
00:11:39,630 --> 00:11:44,220
what matters for a machine learning model is not the model itself,

227
00:11:44,220 --> 00:11:47,325
but the data that put into the model.

228
00:11:47,325 --> 00:11:51,045
And it really, really matters for this specific model

229
00:11:51,045 --> 00:11:55,200
that you have the median income of the people who live in a neighborhood,

230
00:11:55,200 --> 00:11:58,350
to be able to predict the house prices in that neighborhood.

231
00:11:58,350 --> 00:12:00,240
This is a very important feature.

232
00:12:00,240 --> 00:12:02,640
And in order to have this feature,

233
00:12:02,640 --> 00:12:06,880
you need to have the data engineering pipeline to bring this data in.