So in this lab, we're going to be trying out different features. So what we did was that we went to A underscore features. And let me just go ahead and clear all the cells. So I'm sure that everything that I'm running I'm actually running. And the first thing is to go ahead and do a bunch of imports. We are importing TensorFlow, we are importing Pandas, we are importing NumPy, et cetera. So, let's go ahead and import those and then load up the dataset which is from the California housing dataset. So this is what we're going to be doing. We're going to be trying to predict the price of houses in California from this dataset. And then we just load up the dataset, it's kind of good to know what's in this dataset. So let's go ahead and do df.head. This shows us the first few lines, and we learn that there is a longitude and latitude, the housing median age. So this dataset is not actually individual houses, it's actually aggregated. So you have the total number of rooms, it's 5,612. Obviously, this is not one house, this is all of the rooms in that aggregation which is either a zip code, or a county, or whatever that aggregation is. So we have the total number of rooms in that aggregation, total number of bedrooms, the population, the number of people. So it's about a thousand people, it looks like. And that is in 472 households. The median income is 1.5 in some units. And the median housing value is 66,900 again and some units. So that is essentially the thing that we're trying to learn from the dataset that we're going to learn from. Now, this is the first few lines of this dataset. That is good to basically get an idea of what these numbers look like. But df.describe() is extremely useful. What df.describe() does in Pandas. Is that it shows you statistics of the numeric columns. So if there is any categorical columns, it's not going to show us anything about it, but every numeric column here, everything is a numeric column. It's going to show us, for example, that there are 17,000 longitudes in the dataset, 17,000 latitudes in the dataset. So this is the number of rows in the dataset and this is a good ideas to basically go ahead and check, that all of them are actually 17,000. If any of them is not 17,000, that indicates that one or more of the values for that role are missing. So in this case, number one sanity check, no values are missing. We have 17,000 rows for all of the values. The mean longitude is minus 119. The mean latitude is 35. This makes sense because this is California. The mean housing age is 28.6. This happens to be years. So about 30 years old. The total number of rooms is 2,643. That is not an individual house, is it. Right? So this is probably a total number of rooms in that aggregation unit. We have to do something with it. And the total number of bedrooms again looks odd 539, population 1,429, number of households is 501, and median income is 3.9 and, let's say median housing value is 27,000 in this case. Right? So that is the mean of all of those. And then you have the standard deviation, the minimum value that exists, the minimum number of rooms is two. The maximum number of rooms is 37,937. So that gives us an idea of what this data looks like. And what we're going to do is, we basically going to split this data into two parts and here for experimentation, we're not going actually save it anywhere, the random splitting is fine enough. So, I'm basically creating a mask and the mask is basically creating an array of the length of the dfs so that's 17,000. And checking if the random is less than 0.8. So which means 80 percent of the values will be one and 20 percent of the value would be zero, approximately. So the training df is all of those values for which it is one and the evaluation df is all of the values for which the mask is zero. So at this point, we will get two data frames, traindf and evaldf. And I can add a new thing. And I can print the length of traindf and that is about 13,000. And I can also print a length of evaldf, and that is about 20 percent, about 3,400. So at this point, we now have our training dataset. Our evaluation dataset. Let's go ahead and build our model. And to build our model, the first thing is we need to read our data. So I'm going to do make_input function. Given a data frame, number of epochs we want to read it, and I'll use the pandas_input function, to take the data frame but not just the data frame, I will add extra features to it. And to get you started, we said look, the number of room, the total number of rooms here. This is kind of ridiculous, right? We don't have a house with 2,643 rooms. That's not right. So what this actually is, is the total number of rooms in that aggregation, in that zip code. So what do we have to normalize this by. We have to bring it down to a single house value. So what we're doing is that we are taking the total number of rooms, and dividing it by the number of households. And that tells us a number of rooms in that house. In a typical house, in that zip code. So what else do we have to normalize for the number of zip code. Well, let's look at this. The latitudes and longitudes seem to be fine as they are. The total number of rooms we have to normalize. We also have a normalize the total number of bedrooms. So let's just do this. So instead of doing a number of rooms in addition, let's do the number of bedrooms is that what's called. No it's called total bedrooms. Total bed rooms. And this could be the number of bedrooms. So that are our two extra features. We created our input function and now our feature columns, the housing_median_age, right, it's a numeric column. It could be used as this, right? The median age when we look at it. These numbers make sense. These seem to be years. So we can use them as they are. So let's use the median age. Then we will go ahead and take the latitude and bucketize them between 32 and 42. Why 32 and 42? Because we go back here and we see that the latitude varies between 32 and 42. So we can bucketize the latitude between 32 and 42. What else should we use? If you're using latitude, we might as well also use longitude. So let's go ahead and take this, and also do the longitude. So we will do the longitude here. But the longitude boundaries need to be between negative 124 and negative 114. So let's go down here and change this to the negative 124 and negative 114 and one degree longitude is probably reasonable. One degree is essentially about 100 kilometers. So that's about right. So we can do this. The number of rooms. Remember that we added the number of bedrooms. So let's go ahead and do that as well. Number of rooms, number of bedrooms. And then we have the median income. So that is our set of feature columns. And then you go ahead and train and evaluate, using train_and_evaluate passing in the train spec, the eval spec, et cetera. And at this point, we can call train_and_evaluate, and write out a trained model. And when we run this, we should get an evaluation output. So we are running it. We asked it to run for 5,000 steps. So at this point, I'm on step 1,650. So let's just wait a little bit. And once it's done, notice that every once in a while, it's saving a dictionary, specifying what the average loss is. The average loss is not all that useful because the losses computed on a batch. So not that great. But this loss. This is the loss on the evaluation dataset and that makes more sense. So let's go ahead and actually the loss, and the average loss is computer in the evaluation dataset that makes more sense. The loss itself is on just a batch. So we don't need that. So let's go down. Let's wait for it to be done, 4,000, 5,000. And there it is. Our average loss over the entire dataset the RMSC is 0.59. Now, let's try something else. Let's go ahead and see what happens if I don't have the number of rooms. So let's now add these extra things, and what we could do, is when we are creating our input columns. We'll just decide not to use the number of rooms, the number of bedrooms, or the latitude, or the longitude, or the median income. So all we have is the age of the house. So if we do that and that's our only feature column. And what happens when we do that? So we'll go down here. Remember that we got 0.59 earlier. So I'll clear the cell, and run it again, this time it's just with one input. And at this point, what do we get. At this point, our loss is 1.87. So obviously all those input features were useful. Right? Because again the loss went up, when we didn't have them. So this gives you an idea of what kind of features are useful and you could do this manually, you could basically go ahead and say, what happens if I just use the median income. And the median income is a pretty good indicator. So if we use the median income and you try this again, you will find that the loss goes down dramatically. And the other features are not that useful. They're useful, not that useful. The median income is particularly useful because it goes into what people can afford and the market tends to match what people can afford. So notice that now we've got an average loss of 0.69. So essentially, we went from 1.38 to 0.69 simply by adding one feature the median income. The median income was an extremely useful feature. Now, how does this matter in the real-world, they way this matters in the real-world is, it really matters what data you collect. Imagine that you had this dataset, and you did not collect the median income of people who lived in that neighborhood. Now, your model is pretty bad. So this is what we mean by when we say that, what matters for a machine learning model is not the model itself, but the data that put into the model. And it really, really matters for this specific model that you have the median income of the people who live in a neighborhood, to be able to predict the house prices in that neighborhood. This is a very important feature. And in order to have this feature, you need to have the data engineering pipeline to bring this data in.