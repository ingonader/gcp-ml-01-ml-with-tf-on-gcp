But if you take any statistics you might see if there was missing values, you would normally impute a value like the average for that column. So, that's where philosophically ML and statistics start to diverge. In ML the idea is that you build the separate model for this situation where you have the data versus when you don't. We can afford to do this and ML where we actually have the data and where we don't have the data, because in ML we have enough data where we want to build something as fine grain as we can. Statistics on the other hand is about keeping the data that you have in getting the best results out of the data that you have. The difference in philosophy affects how you treat outliers. In ML you go out and find enough outliers that becomes something that you can actually train with. Remember that five sample rule that we had? With statistics you say, "I've got all the data I'll ever be able to collect." So, you throw out outliers. It's a philosophical difference because of the scenarios where ML and statistics are used. Statistics is often used in a limited data regime or ML operates with lots of data. So having an extra column to flag whether on you're missing data is what you would normally do in ML. When you don't have enough data and you imputed to replace it by an average. Now, this example here is of predicting a house value. The data set includes latitude and two peaks that you see here, one for SFO and the other for LAS, that's San Francisco and Los Angeles. It doesn't make sense to represent latitude as a floating point feature in our model. It's because there's no linear relationship exists between latitude and the housing values. For example, houses in latitude 35 and not 35, 34 times more expensive than houses at latitude 34. And yet individual latitudes are probably a pretty good indicator of housing values. So, what do we do with a magnitude piece? Well, what if we did this, instead of having one floating point feature let's take a look and have 11 distinct boolean features. Yes-no latitudeBin1, latitudeBin2 all the way to latitudeBin11 with yes-no boolean values. And here, we've just use fixed bin boundaries. And other options that you see commonly used between data scientist that have quantile boundaries so that the number of values in each bin is constant. You'll see this a lot in other regression problems. Quite a few training cycles will be spent trying to get the unusual instances correct. So, you're collapsing the long tail on ML versus removing them from their set in normal statistics. If the house is 50 rooms we said it to have four rooms which is the top of our range. The idea is that the price of a home in the hundreds of thousands while things like the number of rooms are small numbers. And optimizers have traditionally a hard time dealing with this. The price ends up dominating your gradient. Now, modern architectures for ML end up taking a variable magnitudes into account because of what's called batch normalization. Although you may run into issues if a batch of examples happens to have all unusual values. So, this is not as important as it used to be.