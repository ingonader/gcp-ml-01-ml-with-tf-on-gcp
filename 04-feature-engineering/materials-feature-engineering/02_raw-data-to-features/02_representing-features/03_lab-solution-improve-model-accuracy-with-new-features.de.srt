1
00:00:00,000 --> 00:00:01,850
In diesem Lab

2
00:00:01,850 --> 00:00:04,755
probieren wir verschiedene Funktionen aus.

3
00:00:04,755 --> 00:00:08,100
Wir rufen also wieder "A_features" auf.

4
00:00:08,100 --> 00:00:11,240
Ich muss nur kurz alle Zellen löschen,

5
00:00:11,240 --> 00:00:14,725
um sicherzustellen, dass alle aktiven
Befehle von mir ausgeführt wurden.

6
00:00:14,725 --> 00:00:18,450
Zuerst führen wir
einige Importvorgänge durch.

7
00:00:18,450 --> 00:00:20,000
Wir importieren TensorFlow,

8
00:00:20,000 --> 00:00:21,470
wir importieren Pandas,

9
00:00:21,470 --> 00:00:23,245
wir importieren NumPy usw.

10
00:00:23,245 --> 00:00:26,880
Wir führen diese Importbefehle aus

11
00:00:26,880 --> 00:00:31,920
und laden das Dataset aus den
Daten für kalifornische Immobilienpreise.

12
00:00:31,920 --> 00:00:37,565
Wir versuchen, mit diesem Dataset die
Häuserpreise in Kalifornien vorherzusagen.

13
00:00:37,565 --> 00:00:40,170
Nachdem das Dataset geladen wurde,

14
00:00:40,170 --> 00:00:42,680
sollten wir
einen Blick auf die Daten werfen.

15
00:00:42,680 --> 00:00:47,065
Mit der Anweisung "df.head"
werden die ersten Zeilen angezeigt.

16
00:00:47,065 --> 00:00:50,730
Wir sehen Längen- und Breitengrade,

17
00:00:50,730 --> 00:00:53,000
das Durchschnittsalter der Immobilien.

18
00:00:53,000 --> 00:00:57,155
Es sind also keine Daten zu einzelnen
Häusern, sondern aggregierte Daten.

19
00:00:57,155 --> 00:01:01,410
Die Zahl der Zimmer insgesamt: 5612.

20
00:01:01,410 --> 00:01:03,520
Das ist offensichtlich nicht ein Haus,

21
00:01:03,520 --> 00:01:06,940
sondern die Zimmer
einer aggregierten Datenmenge.

22
00:01:06,940 --> 00:01:10,980
für eine Postleitzahl oder einen Landkreis.

23
00:01:10,980 --> 00:01:13,440
Wir sehen die Gesamtzahl der Zimmer,

24
00:01:13,440 --> 00:01:15,345
die Gesamtzahl der Schlafzimmer,

25
00:01:15,345 --> 00:01:17,140
die Bevölkerungszahl.

26
00:01:17,140 --> 00:01:19,790
Es sind etwa 1.000 Menschen,

27
00:01:19,790 --> 00:01:22,910
aufgeteilt auf 472 Haushalte.

28
00:01:22,910 --> 00:01:27,190
Das Durchschnittseinkommen
ist 1,5 in einer unbekannten Einheit.

29
00:01:27,190 --> 00:01:34,005
Der mittlere Immobilienwert
beträgt 66.900 in irgendeiner Einheit.

30
00:01:34,005 --> 00:01:36,720
Diese Art von Informationen des Datasets

31
00:01:36,720 --> 00:01:39,700
versuchen wir erst einmal herauszufinden.

32
00:01:41,610 --> 00:01:46,970
Diese ersten Zeilen geben uns
einen Eindruck der enthaltenen Werte.

33
00:01:46,970 --> 00:01:49,605
Der Befehl
"df.describe()" ist sehr nützlich.

34
00:01:49,605 --> 00:01:52,880
Sie können damit in Pandas Statistiken

35
00:01:52,880 --> 00:01:55,740
zu numerischen Spalten anzeigen lassen.

36
00:01:55,740 --> 00:01:59,070
Der Befehl liefert keine
Informationen zu kategorischen Spalten,

37
00:01:59,070 --> 00:02:01,830
aber zu allen numerischen Spalten.

38
00:02:01,830 --> 00:02:03,845
Alle Spalten hier sind numerisch.

39
00:02:03,845 --> 00:02:05,700
Wir sehen zum Beispiel,

40
00:02:05,700 --> 00:02:09,170
dass das Dataset 17.000 Längengrade

41
00:02:09,170 --> 00:02:11,640
und 17.000 Breitengrade enthält.

42
00:02:11,640 --> 00:02:14,770
Das ist die Anzahl der Zeilen im Dataset.

43
00:02:14,770 --> 00:02:19,870
Wir können jetzt prüfen,
ob es tatsächlich immer 17.000 sind.

44
00:02:19,870 --> 00:02:22,740
Wenn die Zahl nicht immer 17.000 ist,

45
00:02:22,740 --> 00:02:27,750
fehlt mindestens ein Wert für diese Rolle.

46
00:02:27,750 --> 00:02:32,680
In diesem Fall zeigt die
Qualitätsprüfung, dass keine Werte fehlen.

47
00:02:32,685 --> 00:02:36,120
Wir haben 17.000 Zeilen für alle Werte.

48
00:02:36,120 --> 00:02:39,475
Der durchschnittliche Längengrad ist -119.

49
00:02:39,475 --> 00:02:41,640
Der durchschnittliche Breitengrad ist 35.

50
00:02:41,640 --> 00:02:44,000
Das sollte für Kalifornien stimmen.

51
00:02:44,000 --> 00:02:47,350
Das mittlere Immobilienalter ist 28,6.

52
00:02:47,350 --> 00:02:50,340
Die Angabe ist in Jahren,
also sind es etwa 30 Jahre.

53
00:02:50,340 --> 00:02:53,685
Die Gesamtzahl der Zimmer ist 2643.

54
00:02:53,685 --> 00:02:56,185
Das kann kein einzelnes Haus sein,

55
00:02:56,185 --> 00:03:00,240
also ist die Zahl
wahrscheinlich ein aggregierter Wert,

56
00:03:00,240 --> 00:03:03,090
den wir noch bearbeiten müssen.

57
00:03:03,090 --> 00:03:07,600
Die Anzahl
der Schlafzimmer ist merkwürdig: 539.

58
00:03:07,600 --> 00:03:09,820
Bevölkerung: 1.429.

59
00:03:09,820 --> 00:03:13,245
Die Anzahl der Haushälte ist 501,

60
00:03:13,245 --> 00:03:17,775
das durchschnittliche Einkommen ist 3,9

61
00:03:17,775 --> 00:03:22,645
und der durchschnittliche
Immobilienwert lautet 207.000.

62
00:03:22,645 --> 00:03:25,815
Das ist der Durchschnitt all dieser Werte.

63
00:03:25,815 --> 00:03:27,780
Dann haben wir die Standardabweichung,

64
00:03:27,780 --> 00:03:29,780
den kleinsten vorhandenen Wert.

65
00:03:29,780 --> 00:03:32,240
Die geringste Anzahl von Zimmern ist 2.

66
00:03:32,240 --> 00:03:36,025
Die höchste Anzahl von Zimmern ist 37.937.

67
00:03:36,025 --> 00:03:39,225
Wir wissen jetzt
ungefähr, wie diese Daten aussehen.

68
00:03:39,225 --> 00:03:42,300
Als Nächstes
teilen wir die Daten in zwei Teile auf

69
00:03:42,300 --> 00:03:46,845
und lassen Sie
zu Versuchszwecken ungespeichert.

70
00:03:46,845 --> 00:03:49,485
Die zufällige Aufteilung ist ausreichend.

71
00:03:51,065 --> 00:03:58,310
Ich erstelle eine Maske, die
ein Array mit der Länge von df erstellt,

72
00:03:58,310 --> 00:03:59,760
also 17.000.

73
00:03:59,760 --> 00:04:03,135
Die Zufälligkeit sollte unter 0,8 liegen.

74
00:04:03,135 --> 00:04:06,540
Das bedeutet,
dass etwa 80 Prozent der Werte Eins

75
00:04:06,540 --> 00:04:10,920
und 20 Prozent der Werte Null sind.

76
00:04:10,920 --> 00:04:15,070
Der Training-DF steht für
alle Werte, für die die Maske Eins ist,

77
00:04:15,070 --> 00:04:19,110
und der Evaluation-DF steht
für alle Werte, für die sie Null ist.

78
00:04:19,110 --> 00:04:25,758
Damit erhalten wir
zwei Dataframes: traindf und evaldf.

79
00:04:25,758 --> 00:04:28,230
Ich kann etwas hinzufügen

80
00:04:28,230 --> 00:04:33,390
und die Länge von "traindf"
drucken, was sich auf etwa 13.000 beläuft.

81
00:04:33,390 --> 00:04:37,035
Ebenso kann ich
die Länge von "evaldf" drucken,

82
00:04:37,035 --> 00:04:41,235
was ungefähr
20 Prozent sind, also 3.400.

83
00:04:41,235 --> 00:04:43,975
Jetzt haben wir unser Training-Dataset

84
00:04:43,975 --> 00:04:46,065
und unser Evaluation-Dataset.

85
00:04:46,065 --> 00:04:48,855
Nun können wir unser Modell erstellen.

86
00:04:48,855 --> 00:04:52,930
Dafür müssen wir
zuerst unsere Daten einlesen.

87
00:04:52,930 --> 00:04:55,550
Ich erstelle die Funktion "make_input_fn",

88
00:04:55,550 --> 00:04:57,515
gebe einen Dataframe an,

89
00:04:57,515 --> 00:05:00,370
die Anzahl der Epochen,
die gelesen werden sollen,

90
00:05:00,370 --> 00:05:03,440
und wende die Funktion "pandas_input" an,

91
00:05:03,440 --> 00:05:06,375
um den Dataframe zu übernehmen.

92
00:05:06,375 --> 00:05:09,515
Vorher werde ich den Dataframe
noch um weitere Funktionen ergänzen.

93
00:05:09,515 --> 00:05:13,035
Doch zuerst gehen wir noch einmal zurück

94
00:05:13,035 --> 00:05:15,335
zur Gesamtzahl der Zimmer.

95
00:05:15,335 --> 00:05:17,540
Die Zahl ist unrealistisch,

96
00:05:17,540 --> 00:05:22,165
denn kein Haus hat 2.643 Zimmer.

97
00:05:22,165 --> 00:05:26,120
Es handelt sich also um die Anzahl
der Zimmer in der gesamten Aggregation,

98
00:05:26,120 --> 00:05:28,425
für die eine Postleitzahl.

99
00:05:28,425 --> 00:05:30,990
Diese Zahl muss auf einen Wert

100
00:05:30,990 --> 00:05:34,015
für ein einziges Haus normalisiert werden.

101
00:05:34,015 --> 00:05:37,050
Dazu teilen wir die Gesamtzahl der Zimmer

102
00:05:37,050 --> 00:05:39,285
durch die Anzahl der Haushalte.

103
00:05:39,285 --> 00:05:42,480
Das Ergebnis ist
die durchschnittliche Anzahl der Zimmer

104
00:05:42,480 --> 00:05:45,750
in einem Haus für diese Postleitzahl.

105
00:05:45,750 --> 00:05:49,755
Müssen wir noch andere Werte
für diese Postleitzahl normalisieren?

106
00:05:51,120 --> 00:05:54,090
Die Längen- und Breitengrade sehen gut aus.

107
00:05:54,090 --> 00:05:57,375
Die Gesamtzahl
der Zimmer muss normalisiert werden

108
00:05:57,375 --> 00:06:00,515
und die Gesamtzahl der Schlafzimmer.

109
00:06:01,365 --> 00:06:05,685
Neben der Anzahl der Zimmer nehmen wir uns

110
00:06:05,685 --> 00:06:09,770
die Anzahl der Schlafzimmer vor,

111
00:06:09,770 --> 00:06:13,360
genau genommen
die Gesamtzahl der Schlafzimmer.

112
00:06:24,075 --> 00:06:28,170
Das sind unsere zwei Zusatzfunktionen.

113
00:06:28,170 --> 00:06:30,940
Wir haben unsere Eingabefunktion erstellt.

114
00:06:30,940 --> 00:06:34,080
Unsere Funktionsspalte "housing_median_age"

115
00:06:34,080 --> 00:06:36,225
ist eine numerische Spalte.

116
00:06:36,225 --> 00:06:38,730
So könnte sie verwendet werden.

117
00:06:38,730 --> 00:06:40,835
Wenn wir uns
das Durchschnittsalter ansehen,

118
00:06:40,835 --> 00:06:42,625
ergeben diese Zahlen Sinn.

119
00:06:42,625 --> 00:06:44,370
Es scheinen Jahre zu sein,

120
00:06:44,370 --> 00:06:47,660
also können wir sie unverändert nutzen.

121
00:06:48,760 --> 00:06:55,275
Dann fassen wir die Breitengrade in
einem Bereich zwischen 32 und 42 zusammen.

122
00:06:55,275 --> 00:06:57,045
Warum 32 und 42?

123
00:06:57,045 --> 00:07:04,645
Weil die Breitengrade
im Bereich zwischen 32 und 42 variieren.

124
00:07:04,645 --> 00:07:08,180
Wir können Sie deshalb zusammenfassen.

125
00:07:11,790 --> 00:07:14,064
Wenn wir Breitengrade verwenden,

126
00:07:14,064 --> 00:07:16,430
können wir auch Längengrade nutzen.

127
00:07:16,430 --> 00:07:19,230
Dafür kopieren wir diese Zeile.

128
00:07:21,280 --> 00:07:25,240
Und wir passen sie für Längengrade an.

129
00:07:25,240 --> 00:07:28,755
Wir fügen die Zeile hier ein.

130
00:07:30,605 --> 00:07:38,270
Die Grenzwerte für die Längengrade
liegen jedoch zwischen -124 und -114.

131
00:07:38,270 --> 00:07:42,805
Diese Werte müssen also angepasst werden.

132
00:07:42,805 --> 00:07:46,825
-124 und -114

133
00:07:46,825 --> 00:07:50,170
und 1 Längengrad sollte realistisch sein.

134
00:07:50,170 --> 00:07:53,660
1 Grad ist ungefähr 100 Kilometer.

135
00:07:53,660 --> 00:07:55,525
Das sollte ungefähr stimmen.

136
00:07:55,525 --> 00:07:58,360
Als Nächstes kommt die Anzahl der Zimmer.

137
00:07:58,360 --> 00:08:01,075
Wir haben schon
die Anzahl der Schlafzimmer hinzugefügt,

138
00:08:01,075 --> 00:08:03,410
jetzt kommen noch die Zimmer hinzu.

139
00:08:03,410 --> 00:08:06,560
Anzahl der Zimmer, Anzahl der Schlafzimmer

140
00:08:06,560 --> 00:08:09,575
und das durchschnittliche Einkommen.

141
00:08:09,575 --> 00:08:12,595
Das sind unsere Funktionsspalten.

142
00:08:12,595 --> 00:08:17,294
Jetzt können wir mit "train_and_evaluate"
trainieren und evaluieren.

143
00:08:17,294 --> 00:08:21,995
Dabei werden die Trainings- und
Evaluierungsangaben usw. übergeben.

144
00:08:23,955 --> 00:08:26,090
Wir rufen "train_and_evaluate" auf,

145
00:08:26,090 --> 00:08:28,815
um ein trainiertes Modell auszugeben.

146
00:08:29,605 --> 00:08:33,690
Beim Ausführen sollte
eine Evaluierungsausgabe geliefert werden.

147
00:08:33,690 --> 00:08:35,855
Wir starten die Ausführung

148
00:08:35,855 --> 00:08:39,605
und haben 5.000 Schritte angegeben.

149
00:08:39,605 --> 00:08:42,675
Jetzt sind wir bei Schritt 1.650.

150
00:08:42,675 --> 00:08:45,485
Wir müssen noch etwas warten.

151
00:08:46,660 --> 00:08:52,270
Ab und zu wird ein Wörterbuch mit
dem durchschnittlichen Verlust gespeichert.

152
00:08:52,270 --> 00:08:55,430
Der durchschnittliche
Verlust ist nicht besonders hilfreich,

153
00:08:55,430 --> 00:08:58,215
da er für einen Batch berechnet wird.

154
00:08:59,095 --> 00:09:02,875
Dieser Verlust im
Evaluation-Dataset ist jedoch hilfreich.

155
00:09:05,365 --> 00:09:07,395
Der durchschnittliche Verlust

156
00:09:07,395 --> 00:09:10,570
wird im Evaluation-Dataset
berechnet – das ergibt mehr Sinn.

157
00:09:10,570 --> 00:09:12,640
Der Verlust selbst
gilt nur für einen Batch.

158
00:09:12,640 --> 00:09:14,720
Das ist für uns nicht wichtig.

159
00:09:14,720 --> 00:09:17,555
Wir warten weiter darauf,
dass die Ausführung beendet wird.

160
00:09:17,555 --> 00:09:21,205
4.000, 5.000 – und fertig.

161
00:09:21,205 --> 00:09:27,130
Der durchschnittliche Verlust für das
gesamte Dataset, das RMSC, beträgt 0,59.

162
00:09:27,130 --> 00:09:29,750
Jetzt probieren wir etwas anderes.

163
00:09:29,750 --> 00:09:38,480
Wir testen, was passiert,
wenn ich die Anzahl der Zimmer nicht habe.

164
00:09:38,480 --> 00:09:42,685
Wir fügen die Zusatzfunktionen hinzu

165
00:09:42,685 --> 00:09:47,305
und verzichten
beim Erstellen der Eingabespalten

166
00:09:47,305 --> 00:09:50,040
auf die Anzahl der Zimmer,

167
00:09:50,040 --> 00:09:52,225
die Anzahl der Schlafzimmer,

168
00:09:52,225 --> 00:09:55,040
den Breiten- und Längengrad

169
00:09:55,040 --> 00:09:57,275
und das Durchschnittseinkommen.

170
00:09:57,275 --> 00:09:59,940
Nur das Alter des Hauses bleibt übrig

171
00:09:59,940 --> 00:10:02,505
und bildet unsere einzige Funktionsspalte.

172
00:10:02,505 --> 00:10:04,685
Welche Auswirkung hat das?

173
00:10:04,685 --> 00:10:06,960
Wir scrollen nach unten.

174
00:10:08,320 --> 00:10:11,525
Der vorherige Verlustwert war 0,59.

175
00:10:11,525 --> 00:10:13,395
Jetzt lösche ich die Zelle

176
00:10:13,395 --> 00:10:16,425
und führe die Operation
nur mit einem Eingabewert aus.

177
00:10:16,425 --> 00:10:19,280
Was ist das Ergebnis?

178
00:10:20,210 --> 00:10:23,310
Unser Verlust ist 1,87.

179
00:10:23,310 --> 00:10:26,930
Alle unsere
Eingaben waren offensichtlich nützlich,

180
00:10:26,930 --> 00:10:30,870
da der Verlust
ohne diese Funktionen angestiegen ist.

181
00:10:30,870 --> 00:10:34,130
Sie haben gesehen,
welche Funktionen nützlich sind.

182
00:10:34,130 --> 00:10:36,180
Sie können manuell vorgehen

183
00:10:36,180 --> 00:10:40,760
und die Ausführung nur mit
dem durchschnittlichen Einkommen testen.

184
00:10:40,760 --> 00:10:44,030
Das durchschnittliche
Einkommen ist ein sehr guter Indikator.

185
00:10:44,030 --> 00:10:47,720
Wenn wir diese Funktion
verwenden und den Vorgang wiederholen,

186
00:10:47,720 --> 00:10:50,495
nimmt der Verlust erheblich ab.

187
00:10:50,495 --> 00:10:52,905
Die anderen
Funktionen sind nicht sehr nützlich.

188
00:10:52,905 --> 00:10:56,450
Sie sind nützlich, aber nicht so sehr
wie das durchschnittliche Einkommen,

189
00:10:56,450 --> 00:10:59,770
das wiedergibt,
was sich die Menschen leisten können,

190
00:10:59,770 --> 00:11:05,235
was sich wiederum auf den Markt auswirkt.

191
00:11:05,235 --> 00:11:09,810
Der durchschnittliche
Verlust beträgt jetzt 0,69.

192
00:11:09,810 --> 00:11:16,410
Wir konnten den Wert also durch eine
einzige Funktion von 1,38 auf 0,69 senken.

193
00:11:16,410 --> 00:11:19,230
Das durchschnittliche
Einkommen war extrem nützlich.

194
00:11:19,230 --> 00:11:21,345
Wie relevant ist es jedoch für die Praxis?

195
00:11:21,345 --> 00:11:24,160
In der Praxis kommt es darauf an,

196
00:11:24,160 --> 00:11:26,625
welche Daten Sie erfassen.

197
00:11:26,625 --> 00:11:29,320
Wenn Sie dieses Dataset haben,

198
00:11:29,320 --> 00:11:33,240
aber nicht das durchschnittliche
Einkommen der Menschen erfassen,

199
00:11:33,240 --> 00:11:36,180
ist Ihr Modell relativ nutzlos.

200
00:11:36,180 --> 00:11:39,060
Deshalb betonen wir immer,

201
00:11:39,060 --> 00:11:44,220
dass es nicht so sehr
auf das Modell selbst ankommt,

202
00:11:44,220 --> 00:11:47,325
sondern auf die verwendeten Daten.

203
00:11:47,325 --> 00:11:51,045
Bei diesem
spezifischen Modell ist es sehr wichtig,

204
00:11:51,045 --> 00:11:55,200
das durchschnittliche Einkommen
der Menschen in dem Stadtteil zu erfassen,

205
00:11:55,200 --> 00:11:58,350
um die Immobilienpreise
für den Stadtteil vorhersagen zu können.

206
00:11:58,350 --> 00:12:00,370
Das ist eine wichtige Funktion.

207
00:12:00,370 --> 00:12:03,010
Damit sie genutzt werden kann,

208
00:12:03,010 --> 00:12:06,880
müssen die Daten über eine entsprechende
Data Engineering-Pipeline erfasst werden.