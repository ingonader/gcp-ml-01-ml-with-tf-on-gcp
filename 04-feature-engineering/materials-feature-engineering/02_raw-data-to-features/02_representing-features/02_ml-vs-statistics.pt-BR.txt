Mas se você pegar qualquer estatística,
poderá ver se havia valores ausentes, normalmente você atribuiria um valor,
como a média dessa coluna. É aí que, filosoficamente, o ML e as
estatísticas começam a divergir. No ML, a ideia é que você crie
o modelo separado para essa situação em que você
tem os dados e para quando não tem. Podemos nos dar ao luxo
de fazer isso no ML, quando, de fato, temos os dados
e quando não temos, porque no ML temos dados
suficientes se queremos criar algo o mais refinado possível. As estatísticas, porém, tratam sobre
manter os dados que você tem para conseguir os melhores
resultados deles. A diferença na filosofia afeta
como você trata os valores atípicos. No ML, você encontra
valores atípicos suficientes para poder treinar. Lembre-se aquela regra
das cinco amostras? Com as estatísticas, você diz: "eu tenho todos os
dados que poderei coletar". Então você descarta valores atípicos. É uma diferença filosófica por causa dos cenários onde o ML
e as estatísticas são usados. Estatísticas são usadas em um regime de
dados limitados, e o ML opera com muitos. Ter uma coluna extra
para sinalizar se você está perdendo dados, é o que
você faria no ML, quando há dados suficientes e você
atribuiu para substituí-lo por uma média. Este exemplo prevê o valor de um imóvel. O conjunto de dados inclui latitude
e dois picos que você vê aqui, um para SFO e outro para LAS, isso é São Francisco e Los Angeles. Não faz sentido mostrar a latitude como
um atributo de ponto flutuante no modelo. Porque não existe uma relação linear entre a latitude e os valores do imóvel. Por exemplo,
casas na latitude 35 e não 35, 34 vezes mais caras do que
casas na latitude 34. No entanto, latitudes individuais são
um bom indicador dos valores da moradia. O que fazemos com a magnitude? E se fizermos isso: em vez de ter um atributo
de ponto flutuante, vamos ter 11 atributos
booleanos distintos. Sim/não latitudeBin1, latitudeBin2 até latitudeBin11
com valores booleanos sim/não E aqui, nós apenas
usamos limites binários fixos. E outras opções usadas entre cientistas
de dados é ter limites por quantil para que os valores
em cada agrupamento sejam constantes. Você verá muito isso em
problemas de regressão. Poucos ciclos de treino serão gastos
tentando corrigir as instâncias incomuns. Então, você está reduzindo
a extensão do ML em relação a removê-los do conjunto
na estatística normal. Se a casa é de 50 quartos, ordenamos que
tenha 4, que é o topo da nossa extensão. A ideia é que o preço de uma casa fique na casa dos milhares, enquanto coisas como
o número de quartos são números pequenos. Os otimizadores geralmente
têm dificuldade em lidar com isso. O preço acaba dominando seu gradiente. As arquiteturas modernas para ML
acabam levando em conta uma magnitude variável, devido ao que
é chamado de normalização em lote. Ainda que você possa ter problemas
se um lote de exemplos tiver todos os valores incomuns. Então, isso não é tão importante
quanto costumava ser.