1
00:00:00,000 --> 00:00:04,290
So now let's compare good versus bad features.

2
00:00:04,290 --> 00:00:06,480
So, what makes a good feature?

3
00:00:06,480 --> 00:00:08,760
Well, you want to take your raw data and

4
00:00:08,760 --> 00:00:11,400
represent it in a form that's amenable to machine learning.

5
00:00:11,400 --> 00:00:14,850
So ultimately, a good feature has to be related to the objective,

6
00:00:14,850 --> 00:00:17,385
you can't just throw a random data in there,

7
00:00:17,385 --> 00:00:19,550
that would just make the ML problem harder.

8
00:00:19,550 --> 00:00:22,040
And the idea is to make the problem easier, right?

9
00:00:22,040 --> 00:00:24,150
Easier for something to you to find a solution for.

10
00:00:24,150 --> 00:00:27,180
So, it's not something related to all thing what you're trying to do,

11
00:00:27,180 --> 00:00:29,035
throw that data field away.

12
00:00:29,035 --> 00:00:32,010
You have to make sure that it's known at production time,

13
00:00:32,010 --> 00:00:33,630
this can be surprisingly tricky,

14
00:00:33,630 --> 00:00:35,580
we'll talk about some instances of this.

15
00:00:35,580 --> 00:00:37,470
Third, it's got to be numeric.

16
00:00:37,470 --> 00:00:40,725
Fourth, you've got to have enough examples for it in your data set,

17
00:00:40,725 --> 00:00:44,640
and lastly, you need to bring in your own human insights into the problem.

18
00:00:44,640 --> 00:00:47,145
So let's start with the first one.

19
00:00:47,145 --> 00:00:51,270
First off, a good feature needs to be related to what you're actually predicting,

20
00:00:51,270 --> 00:00:54,360
since it has some kind of reasonable hypothesis of why

21
00:00:54,360 --> 00:00:57,450
a particular feature might matter for this particular problem.

22
00:00:57,450 --> 00:00:59,520
Don't just throw arbitrary data in there and

23
00:00:59,520 --> 00:01:01,845
just hope that you can get some kind of relationship out of it.

24
00:01:01,845 --> 00:01:04,040
You don't want to do what's called data dredging,

25
00:01:04,040 --> 00:01:06,240
you don't want to dredge your large data set and

26
00:01:06,240 --> 00:01:08,940
find whatever spurious correlations might exist,

27
00:01:08,940 --> 00:01:10,740
because the larger the data set is,

28
00:01:10,740 --> 00:01:13,880
the more likely it is that there is a lot of these spurious correlations,

29
00:01:13,880 --> 00:01:18,195
and your ML model would just get confused with this mass of data you're throwing out.

30
00:01:18,195 --> 00:01:19,710
For a housing example,

31
00:01:19,710 --> 00:01:23,235
just because we have a data point on whether chairs exist on the porch,

32
00:01:23,235 --> 00:01:24,360
and a house photo,

33
00:01:24,360 --> 00:01:27,180
or how many concrete blocks make up the driveway,

34
00:01:27,180 --> 00:01:29,440
doesn't mean that we should include them in our housing model,

35
00:01:29,440 --> 00:01:31,530
just because we have those data points.

36
00:01:31,530 --> 00:01:34,320
Show some reasonable idea of why these things,

37
00:01:34,320 --> 00:01:39,075
why these data points and these features could actually affect the outcome.

38
00:01:39,075 --> 00:01:42,690
The outcome is basically what's represented by this label that we're putting them,

39
00:01:42,690 --> 00:01:46,620
and you have to have some reasonable idea of why they could be related to the output.

40
00:01:46,620 --> 00:01:48,600
So, why would concrete blocks in

41
00:01:48,600 --> 00:01:52,305
the driveway affect the ultimate price of a house? Does that make sense?

42
00:01:52,305 --> 00:01:55,020
No. Now, you might be thinking that if you

43
00:01:55,020 --> 00:01:57,750
can tell if a driveway had cracks in it from the photo,

44
00:01:57,750 --> 00:02:00,600
that could be a good feature for a housing problem,

45
00:02:00,600 --> 00:02:02,490
keep that in mind we're going to come back to that later.

46
00:02:02,490 --> 00:02:09,060
So, what are the good features shown here for this horse problem?

47
00:02:09,700 --> 00:02:13,070
If you said it depends on what you're predicting,

48
00:02:13,070 --> 00:02:16,370
you're exactly right, and you paid attention to me for the last five minutes.

49
00:02:16,370 --> 00:02:20,270
If the objective is to find what features make a good race horse,

50
00:02:20,270 --> 00:02:23,835
you might want to go with the data points on breed and age.

51
00:02:23,835 --> 00:02:26,270
However, if your objective was to determine if

52
00:02:26,270 --> 00:02:29,150
the horses are more predisposed to eye disease,

53
00:02:29,150 --> 00:02:32,570
eye color may also be a completely valid feature.

54
00:02:32,570 --> 00:02:36,190
The key learning here is that different problems in the same domain,

55
00:02:36,190 --> 00:02:38,139
may need different features,

56
00:02:38,139 --> 00:02:39,375
and it depends on you,

57
00:02:39,375 --> 00:02:41,450
and your subject matter expertise to determine which

58
00:02:41,450 --> 00:02:44,980
fields you want to start with for your hypothesis.