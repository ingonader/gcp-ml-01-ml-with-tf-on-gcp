1
00:00:00,000 --> 00:00:01,455
All right. Here's a nice quiz.

2
00:00:01,455 --> 00:00:05,700
Which of these features will be difficult to get enough examples for?

3
00:00:05,700 --> 00:00:08,640
So again, let's consider that we're trying to predict the number

4
00:00:08,640 --> 00:00:11,040
of customers that use a discount coupon,

5
00:00:11,040 --> 00:00:13,290
and we have as a feature for example,

6
00:00:13,290 --> 00:00:15,495
the percent discount of the coupon.

7
00:00:15,495 --> 00:00:19,020
So, let's have a coupon that's a 10 percent discount.

8
00:00:19,020 --> 00:00:22,080
Well, I'm sure we'll have at least five samples hopefully of

9
00:00:22,080 --> 00:00:26,145
a 10 percent discount coupon code being used as mean your promotions success,

10
00:00:26,145 --> 00:00:29,510
and if you had say a five percent off coupon or 15 percent off,

11
00:00:29,510 --> 00:00:33,075
nationally we have at least five samples of these at least five.

12
00:00:33,075 --> 00:00:37,385
Perhaps if you gave one special customer an 85 percent discount,

13
00:00:37,385 --> 00:00:40,325
because you use that your dataset? Well no.

14
00:00:40,325 --> 00:00:45,135
You're not going have enough samples or examples that 85 percent is now way too specific.

15
00:00:45,135 --> 00:00:48,750
You don't have enough examples that 85 percent discount, so, you've got to throw it out,

16
00:00:48,750 --> 00:00:50,610
or we have to find five,

17
00:00:50,610 --> 00:00:51,885
at least five samples,

18
00:00:51,885 --> 00:00:54,450
where you did give somebody an 85 percent discount.

19
00:00:54,450 --> 00:00:57,600
So, it's great if we have discrete values.

20
00:00:57,600 --> 00:01:00,000
But what happens if you have continuous numbers?

21
00:01:00,000 --> 00:01:02,055
Well, it's a continuous.

22
00:01:02,055 --> 00:01:04,290
You may have to group them up,

23
00:01:04,290 --> 00:01:07,260
and then see if we have discrete bands,

24
00:01:07,260 --> 00:01:11,770
you'll have at least five examples in each band that grouping,

25
00:01:11,770 --> 00:01:13,820
Okay, onto number two,

26
00:01:13,820 --> 00:01:18,370
the date that a promotional offer starts, can we use that?

27
00:01:18,370 --> 00:01:21,704
Assuming again you have to group things,

28
00:01:21,704 --> 00:01:24,420
all promotional offers that start in January.

29
00:01:24,420 --> 00:01:27,510
You have at least five promotional offers that started in January,

30
00:01:27,510 --> 00:01:30,780
or you have at least five promotional offers that started in February.

31
00:01:30,780 --> 00:01:33,915
If you don't, you may have to group things yet again,

32
00:01:33,915 --> 00:01:35,775
you may not even be able to use date,

33
00:01:35,775 --> 00:01:37,770
you may not even be able to use month,

34
00:01:37,770 --> 00:01:39,505
you might have to use something like quarter.

35
00:01:39,505 --> 00:01:42,390
Do you have at least five examples of things that started in Q1

36
00:01:42,390 --> 00:01:45,600
and Q2 and Q3 and Q4 for example.

37
00:01:45,600 --> 00:01:48,100
You may have to group up your values,

38
00:01:48,100 --> 00:01:51,210
see you have enough examples of each value.

39
00:01:51,210 --> 00:01:55,890
Next up, the number of customers who open an advertising email.

40
00:01:55,890 --> 00:01:58,095
Yes. Well, whatever number you pick,

41
00:01:58,095 --> 00:02:00,570
hopefully you have enough examples of that.

42
00:02:00,570 --> 00:02:02,960
You have different types of advertising emails,

43
00:02:02,960 --> 00:02:05,550
you may have some that have been opened by a thousand people,

44
00:02:05,550 --> 00:02:06,920
and some that have opened by 1,200,

45
00:02:06,920 --> 00:02:09,500
and some that have been open by 8000.

46
00:02:09,500 --> 00:02:12,985
Maybe you'll have enough to get to the very tail end of your distribution,

47
00:02:12,985 --> 00:02:17,270
and then you only have one email that was actually opened by 15 million customers,

48
00:02:17,270 --> 00:02:18,380
and you know that's an outlier.

49
00:02:18,380 --> 00:02:22,060
So, then you can't use that 15 million in your dataset.