Imaginen que están creando
un modelo de aprendizaje automático que determina si un automóvil es un taxi. Sabemos que los autos blancos en Roma
y los amarillos en Nueva York suelen ser taxis. Pero queremos
que nuestro modelo de AA lo aprenda a partir de un conjunto de datos
que consiste en registros de automóviles. Supongamos que sus datos
de entrada se ven así rojo, Roma; blanco, Roma, etc. y que las etiquetas indican
si es o no un taxi. Básicamente, el color del auto y la ciudad son sus dos atributos de entrada
y deben usarlos en su modelo lineal para predecir si el auto es un taxi. ¿Cómo lo harían? Toman la primera entrada el color del auto,
y realizan una codificación de un solo 1. Toman la segunda entrada el nombre de la ciudad,
y realizan una codificación de un solo 1. Luego, las envían directo
al modelo lineal. Ahora, supongamos
que asignan un peso de 0.8 a los amarillos
porque el 80% de ellos en su conjunto de datos
de entrenamiento son taxis. Ahora, w3 es 0.8. Por supuesto,
no le asignarán un peso de 0.8. Este peso se aprenderá
mediante el descenso del gradiente es lo que el descenso hará. Desafortunadamente, el peso de 0.8
es verdad para los autos amarillos en todas las ciudades,
no solo en Nueva York. ¿Cómo lo arreglarían? ¿Le asignarían un peso alto a Nueva York? Eso no funciona. Todos los autos en Nueva York
tendrían ese peso alto. ¿Ven el problema? ¿Qué ocurre si agregan
una combinación de atributos? Ahora, tenemos un nodo
de entrada que corresponde a los autos rojos en Nueva York
y otro a los amarillos un tercero a los blancos un cuarto a los verdes y lo mismo para los automóviles en Roma. Y ahora, el modelo puede aprender
bastante rápido que los autos amarillos en Nueva York
y los blancos en Roma suelen ser taxis y les asignamos un peso alto
a esos dos nodos. Todo lo demás, será cero. Problema resuelto. Es por esto por lo que las combinaciones
de atributos son tan poderosas. Las combinaciones de atributos les dan
mucho poder a los modelos lineales. Su uso, además de los datos masivos,
es una estrategia muy eficiente para aprender
espacios muy complejos. Las redes neuronales ofrecen
otra manera de aprender espacios muy complejos. Las combinaciones de atributos permiten que se sigan usando
los modelos lineales. Sin ellas, la expresividad
de los modelos lineales sería limitada. Con ellas, una vez que se tiene
un conjunto de datos masivo un modelo lineal puede aprender
de todos los recovecos del espacio de entrada. Las combinaciones de atributos
permiten a un modelo lineal memorizar grandes conjuntos de datos. La idea es que pueden asignar un peso
a cada combinación y de este modo el modelo aprende
acerca de las combinaciones de atributos. Aunque sea un modelo lineal la relación subyacente
entre entradas y salidas es no lineal. ¿Por qué nos preocupa tanto
que los modelos lineales funcionen bien? Acuérdense del curso anterior. Hablamos sobre los problemas
convexos y no convexos. Las redes neuronales
con muchas capas son no convexas. Pero optimizar modelos lineales
es un problema convexo y los problemas convexos
son mucho más fáciles que los problemas no convexos. Durante mucho tiempo los modelos lineales dispersos
eran los únicos algoritmos que podíamos usar
y escalar a miles de millones de ejemplos de entrenamiento
y de atributos de entrada. Los predecesores de TensorFlow
en Google: seti, smartass, sybil eran clasificadores a escala masiva. Esto cambió en los últimos años y las redes neuronales ahora pueden
manejar datos a escalas masivas por lo general,
con la ayuda de GPU y TPU pero los modelos lineales dispersos
siguen siendo una opción rápida y barata. Usar modelos lineales dispersos
como procesadores previos de sus atributos, a menudo significará que sus redes neuronales
convergerán mucho más rápido.