1
00:00:00,000 --> 00:00:04,950
Remember these diagrams that they used to explain what neural networks were?

2
00:00:04,950 --> 00:00:10,760
You could think of the blue dots as maybe customers who buy a particular phone,

3
00:00:10,760 --> 00:00:15,285
and the yellow dots as customers who don't buy the phone.

4
00:00:15,285 --> 00:00:21,240
Perhaps the x-axis is the time since this customer last bought a phone,

5
00:00:21,240 --> 00:00:26,625
and perhaps the y-axis is the income level of the customer.

6
00:00:26,625 --> 00:00:29,695
Essentially, people who buy the product

7
00:00:29,695 --> 00:00:34,320
if it has been a long time since they bought the phone and they're relatively wealthy.

8
00:00:34,320 --> 00:00:36,870
So, look at this data.

9
00:00:36,870 --> 00:00:42,960
Can you come up with a line that more or less separates these two classes?

10
00:00:42,960 --> 00:00:44,930
Sure we can.

11
00:00:44,930 --> 00:00:46,980
It may have a little bit of error,

12
00:00:46,980 --> 00:00:49,020
it's not perfectly separable,

13
00:00:49,020 --> 00:00:52,530
but a linear model is probably pretty good here.

14
00:00:52,530 --> 00:00:54,780
So this is a linear problem.

15
00:00:54,780 --> 00:01:00,510
The blue dots and the yellow dots are linearly separable by the green line.

16
00:01:00,510 --> 00:01:06,495
Great. But what if our data looks like this?

17
00:01:06,495 --> 00:01:08,720
Can we still use a linear model?

18
00:01:08,720 --> 00:01:13,000
Well, it seems that I cannot draw

19
00:01:13,000 --> 00:01:18,490
a line that manages to separate the blue dots from the yellow dots.

20
00:01:18,490 --> 00:01:22,460
No, wherever I draw my line,

21
00:01:22,460 --> 00:01:25,330
there are blue points on either side of the line.

22
00:01:25,330 --> 00:01:29,755
That data are not linearly separable.

23
00:01:29,755 --> 00:01:32,270
So I cannot use a linear model.

24
00:01:32,270 --> 00:01:37,720
Can we be a bit more specific about what we mean by linear model?

25
00:01:37,720 --> 00:01:40,510
So lets Aksum axis here,

26
00:01:40,510 --> 00:01:43,500
x1 is one of our input variables,

27
00:01:43,500 --> 00:01:46,515
x2 is the other input variable.

28
00:01:46,515 --> 00:01:52,420
And what we mean when we say we cannot use a linear model is that there is no way to

29
00:01:52,420 --> 00:01:55,750
linearly combine x1 and x2 to get

30
00:01:55,750 --> 00:02:00,065
a single decision boundary that would fit the data well.

31
00:02:00,065 --> 00:02:02,505
In machine learning terminology,

32
00:02:02,505 --> 00:02:04,485
y is the target.

33
00:02:04,485 --> 00:02:07,905
Maybe blue equals one and yellow equals zero,

34
00:02:07,905 --> 00:02:09,285
those are the labels,

35
00:02:09,285 --> 00:02:11,440
and the w's and b,

36
00:02:11,440 --> 00:02:14,375
are the weights and bias that we are trying to learn.

37
00:02:14,375 --> 00:02:22,975
There is no way that we can modify the w's and or the b to fit this decision boundary.

38
00:02:22,975 --> 00:02:27,950
But is there some other way that we can continue to use a linear model?

39
00:02:29,030 --> 00:02:33,720
For simplicity, lets put that two axes in the center of

40
00:02:33,720 --> 00:02:39,465
the diagram so that the origin (0,0) is at the center of the diagram.

41
00:02:39,465 --> 00:02:44,805
You can obviously get the current x1 and x2 from the previous x1 and x2,

42
00:02:44,805 --> 00:02:47,010
by simply subtracting a constant.

43
00:02:47,010 --> 00:02:49,200
So, a linear model now,

44
00:02:49,200 --> 00:02:52,935
will still be a linear model in the old coordinate system,

45
00:02:52,935 --> 00:02:55,215
but now to this space,

46
00:02:55,215 --> 00:02:58,725
let's define a new feature, x3.

47
00:02:58,725 --> 00:03:04,045
X3 is going to be a feature cross, ready?

48
00:03:04,045 --> 00:03:10,050
So, define a new feature x3 as a product of x1 and x2.

49
00:03:10,050 --> 00:03:11,865
So, how does this help?

50
00:03:11,865 --> 00:03:15,315
So, take x3, the product of x1 and x2,

51
00:03:15,315 --> 00:03:17,580
where is it positive?

52
00:03:17,580 --> 00:03:22,800
Exactly, when x1 and x2 are both positive,

53
00:03:22,800 --> 00:03:26,880
or when x1 and x2 are both negative.

54
00:03:26,880 --> 00:03:28,575
And where is it negative,

55
00:03:28,575 --> 00:03:30,480
where is x3 negative?

56
00:03:30,480 --> 00:03:36,235
Exactly, when x1 or x2 is negative and the other one is positive.

57
00:03:36,235 --> 00:03:38,605
So, now we have x3.

58
00:03:38,605 --> 00:03:45,910
Can you see how the addition of x3 makes this solvable via a linear model?

59
00:03:46,010 --> 00:03:53,855
So, now we can find a rule such that the sine of x3 essentially gives us y.

60
00:03:53,855 --> 00:03:56,440
Of course that's just what we did.

61
00:03:56,440 --> 00:03:59,070
W1 and zero, w2 and zero,

62
00:03:59,070 --> 00:04:00,990
and w3 is one.

63
00:04:00,990 --> 00:04:05,055
Essentially, y is a sine of x3.

64
00:04:05,055 --> 00:04:10,375
The feature cross made this a linear problem.

65
00:04:10,375 --> 00:04:12,680
Pretty neat, don't you think?

66
00:04:12,680 --> 00:04:14,865
So, in traditional machine learning,

67
00:04:14,865 --> 00:04:17,205
feature crosses don't play much of a role,

68
00:04:17,205 --> 00:04:22,470
but that's because traditional ML methods were developed for relatively small datasets,

69
00:04:22,470 --> 00:04:24,345
and once you have datasets with

70
00:04:24,345 --> 00:04:28,875
hundreds of thousands to millions and billions of examples,

71
00:04:28,875 --> 00:04:33,570
feature crosses become an extremely useful technique to have in your tool chest.

72
00:04:33,570 --> 00:04:37,980
So, recall that we said that the layers in a neural network,

73
00:04:37,980 --> 00:04:41,190
allow you to combine the inputs and that

74
00:04:41,190 --> 00:04:44,895
is part of what makes neural networks so powerful.

75
00:04:44,895 --> 00:04:48,295
Deep neural networks let you have many layers,

76
00:04:48,295 --> 00:04:52,050
and since each layer combines the layers before it,

77
00:04:52,050 --> 00:04:57,200
DNNs can model complex multidimensional spaces.

78
00:04:57,250 --> 00:05:02,280
Well, feature crosses also let you combine features.

79
00:05:02,280 --> 00:05:03,940
And the good thing is,

80
00:05:03,940 --> 00:05:06,320
you can get a way with the simpler model,

81
00:05:06,320 --> 00:05:08,870
a linear model, and this is a good thing,

82
00:05:08,870 --> 00:05:10,400
simpler models are a good thing.

83
00:05:10,400 --> 00:05:14,270
So, feature crosses are a way to bring

84
00:05:14,270 --> 00:05:19,585
non-linear inputs to a linear learner, a linear model.

85
00:05:19,585 --> 00:05:23,380
But there is a bit of a caveat.

86
00:05:23,380 --> 00:05:26,650
Let me explain it in an intuitive way.

87
00:05:26,650 --> 00:05:32,210
Remember that I started this section by moving the axis into the middle of the diagram.

88
00:05:32,210 --> 00:05:35,670
Why did I do that?