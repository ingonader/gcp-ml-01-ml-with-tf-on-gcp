1
00:00:00,000 --> 00:00:03,250
TensorFlow Playground
を開いています

2
00:00:03,250 --> 00:00:06,030
このようなデータセットがあります

3
00:00:06,030 --> 00:00:08,830
青い点が右上にあり

4
00:00:08,830 --> 00:00:12,290
オレンジの点が左下にあり

5
00:00:12,290 --> 00:00:17,930
ここに分割線を引いて
これらの2つを区切ります

6
00:00:17,930 --> 00:00:20,820
そのために入力として

7
00:00:20,820 --> 00:00:24,225
X1、X2、X1の2乗、X2の2乗、

8
00:00:24,225 --> 00:00:26,970
それに「X1掛けるX2」があります

9
00:00:26,970 --> 00:00:31,925
まずこの中で
生の入力はどれですか？

10
00:00:31,925 --> 00:00:35,465
また作成された特徴はどれですか？

11
00:00:35,465 --> 00:00:38,555
X1とX2が生の入力です

12
00:00:38,555 --> 00:00:41,840
X1の2乗、X2の2乗、 X1X2は

13
00:00:41,840 --> 00:00:47,875
生の入力X1とX2から作成した特徴です

14
00:00:47,875 --> 00:00:51,190
どれが特徴クロスでしょうか？

15
00:00:51,190 --> 00:00:55,005
X1X2は明らかに特徴クロスです

16
00:00:55,005 --> 00:00:58,290
でももう少し考えると

17
00:00:58,290 --> 00:01:01,195
X1の2乗も特徴クロス つまり

18
00:01:01,195 --> 00:01:02,795
自己クロスです

19
00:01:02,795 --> 00:01:05,495
自己結合と言ってもいいでしょう

20
00:01:05,495 --> 00:01:09,915
X1とX1を掛けると
X1の2乗になります

21
00:01:09,915 --> 00:01:11,560
1つの考え方として

22
00:01:11,560 --> 00:01:14,500
生の2つの入力X1とX2があり

23
00:01:14,500 --> 00:01:16,050
3つの特徴クロス

24
00:01:16,050 --> 00:01:18,680
X1の2乗、X2の2乗、X1X2があります

25
00:01:18,680 --> 00:01:21,545
でもこれは単なる用語の問題です

26
00:01:21,545 --> 00:01:23,820
X1の2乗やX2の2乗を

27
00:01:23,820 --> 00:01:28,635
特徴クロスではなく
入力の変換と呼んでも問題ありません

28
00:01:28,635 --> 00:01:31,155
5つの入力を持つモデルを

29
00:01:31,155 --> 00:01:33,810
これからトレーニングしましょう

30
00:01:33,810 --> 00:01:37,600
再生ボタンを押してトレーニングを開始すると

31
00:01:37,600 --> 00:01:40,590
おかしなことが起きていますね

32
00:01:40,590 --> 00:01:45,300
ここの左下隅が

33
00:01:45,300 --> 00:01:47,910
青くなっています

34
00:01:47,910 --> 00:01:50,865
しばらくすると消えますが

35
00:01:50,865 --> 00:01:53,975
このオプションがなかったら
どうでしょう

36
00:01:53,975 --> 00:01:55,570
もう一度します

37
00:01:55,570 --> 00:01:58,365
どれくらいトレーニングするのか
わかりませんが

38
00:01:58,365 --> 00:01:59,900
ここまできたら

39
00:01:59,900 --> 00:02:03,415
230エポックのトレーニングです
長いですね

40
00:02:03,415 --> 00:02:09,330
230エポックのトレーニングで
何かがおかしくなりました

41
00:02:09,330 --> 00:02:12,420
何でしょう？
ここです

42
00:02:12,420 --> 00:02:17,240
この三角形は過剰適合を示唆しています

43
00:02:17,240 --> 00:02:21,500
ここには実際に
データがありませんから

44
00:02:21,500 --> 00:02:24,225
そう考えるのが妥当です

45
00:02:24,225 --> 00:02:28,530
モデルを十分にシンプルにしていないから

46
00:02:28,530 --> 00:02:31,605
ここに何か現れたのです

47
00:02:31,605 --> 00:02:37,930
これが起こった理由の1つは
モデルの過剰適合を許容しているからです

48
00:02:37,940 --> 00:02:40,390
モデルが過剰適合する1つの原因は

49
00:02:40,390 --> 00:02:43,265
同じデータを複数の方法で与えることです

50
00:02:43,265 --> 00:02:46,705
X1X2をオフにしたらどうでしょう

51
00:02:46,705 --> 00:02:48,695
この時点で X1、X2、

52
00:02:48,695 --> 00:02:52,295
X1の2乗、X2の2乗だけになりました

53
00:02:52,295 --> 00:02:54,320
やり直してみると

54
00:02:54,320 --> 00:03:00,895
この時点でまたこの変な境界が

55
00:03:00,895 --> 00:03:04,935
トレーニングの初期段階で発生します

56
00:03:04,935 --> 00:03:08,185
もう一度やってみると

57
00:03:08,185 --> 00:03:11,090
ここで止めたら
約200エポックです

58
00:03:11,090 --> 00:03:16,580
200エポックの時点で このとおり
境界がうまくいきません

59
00:03:16,580 --> 00:03:20,930
白い部分がここにあって変ですね

60
00:03:20,930 --> 00:03:24,500
余分な特徴X1とX2があるからです

61
00:03:24,500 --> 00:03:27,590
X1とX2を取り除くとどうですか？

62
00:03:27,590 --> 00:03:31,570
これで生データ
X1とX2だけになりました

63
00:03:31,570 --> 00:03:38,110
これで開始して
200エポックあたりで停止します

64
00:03:38,110 --> 00:03:42,015
今度は完璧ですね

65
00:03:42,015 --> 00:03:46,840
この線だけです
これでわかるとおり

66
00:03:46,840 --> 00:03:50,140
良いものが多すぎてもだめです

67
00:03:50,140 --> 00:03:56,050
特徴クロスではモデルの
過剰適合が起こりやすいです

68
00:03:56,050 --> 00:03:59,120
他にも気づかなかった
ことがあります

69
00:03:59,120 --> 00:04:02,590
とても長い時間トレーニングすると

70
00:04:02,590 --> 00:04:05,260
これはオフにして最初に戻し

71
00:04:05,260 --> 00:04:11,435
長い時間トレーニングすると
基本的に良くなりますが

72
00:04:11,435 --> 00:04:16,130
それでも過剰適合が起こるので

73
00:04:16,130 --> 00:04:18,860
この境界が曲線になります

74
00:04:18,860 --> 00:04:21,900
これも過剰適合の症状です

75
00:04:21,900 --> 00:04:25,850
長い間トレーニングすると

76
00:04:25,850 --> 00:04:27,590
こちらは消えます

77
00:04:27,590 --> 00:04:30,860
この左下の生成物は
なくなりましたが

78
00:04:30,860 --> 00:04:33,580
まだ曲線の境界があります

79
00:04:33,580 --> 00:04:35,640
曲線の境界の代わりに

80
00:04:35,640 --> 00:04:40,560
直線という最も単純で
効率的なモデルにならない理由は

81
00:04:40,560 --> 00:04:44,010
モデルにかなりの自由を与えたからです

82
00:04:44,010 --> 00:04:45,955
これをご覧ください

83
00:04:45,955 --> 00:04:51,710
X1とX2の重みが
他の3つよりも高くなっています

84
00:04:51,710 --> 00:04:57,675
しかし特徴クロスX1 x X2には重みがあり

85
00:04:57,675 --> 00:05:02,860
この重みのせいで混沌とする
可能性があります

86
00:05:02,875 --> 00:05:08,370
驚くことに モデルの決定境界が
おかしくなっています

87
00:05:08,380 --> 00:05:12,640
特に左下のこの領域に
青がわずかに見えますが

88
00:05:12,640 --> 00:05:18,705
データでは点がまったく見えません

89
00:05:18,705 --> 00:05:23,540
Playgroundはランダムな開始点を使用するので
結果は異なるかもしれません

90
00:05:23,540 --> 00:05:26,670
これは私の結果を表示したものです

91
00:05:26,670 --> 00:05:30,690
皆様は少し違う結果に
なったかもしれません

92
00:05:30,690 --> 00:05:36,940
入力から出力に走る5本の線の
相対的な太さに注目してください

93
00:05:36,940 --> 00:05:41,655
これらの線は5つの特徴の
相対的な重みを示します

94
00:05:41,655 --> 00:05:44,865
X1とX2から出る線は

95
00:05:44,865 --> 00:05:49,270
他の特徴クロスから出る線より
太くなっています

96
00:05:49,270 --> 00:05:51,315
この特徴クロスは

97
00:05:51,315 --> 00:05:57,920
普通のクロスしていない
特徴よりも影響度が低いですが

98
00:05:57,920 --> 00:06:02,500
一般化をおかしくする程度の
影響を与えます

99
00:06:02,500 --> 00:06:06,560
特徴クロスを完全に削除したら
どうでしょう？

100
00:06:06,560 --> 00:06:09,685
つまり生データだけを使うのです

101
00:06:09,685 --> 00:06:12,740
特徴クロスをすべて削除すると

102
00:06:12,740 --> 00:06:15,280
より合理的なモデルになり

103
00:06:15,280 --> 00:06:19,630
過剰適合を示唆する
曲線の境界もなくなりました

104
00:06:19,630 --> 00:06:24,160
1,000回反復した後の
テスト損失の値は

105
00:06:24,160 --> 00:06:28,510
特徴クロスを使ったときより
少し低くなるはずです

106
00:06:28,510 --> 00:06:33,680
結果はデータセットによって
少し異なるかもしれません

107
00:06:33,680 --> 00:06:38,410
この演習のデータは
線形データとノイズです

108
00:06:38,410 --> 00:06:43,810
複雑すぎるモデルを このように
シンプルなデータに使った場合や

109
00:06:43,810 --> 00:06:47,020
モデルの特徴クロスが多すぎる場合には

110
00:06:47,020 --> 00:06:51,655
トレーニングデータの
ノイズに適合する可能性があります

111
00:06:51,655 --> 00:06:54,170
これを診断するには 通常

112
00:06:54,170 --> 00:06:59,755
独立したテストデータでモデルが
どのように動作するか確認できます

113
00:06:59,755 --> 00:07:02,020
ちなみに正則化について

114
00:07:02,020 --> 00:07:06,250
Art and Science of Machine Learning
で説明します

115
00:07:06,250 --> 00:07:12,720
L1正則化がなぜ優れているかを説明します

116
00:07:12,720 --> 00:07:14,880
L1正則化は

117
00:07:14,880 --> 00:07:18,965
必要に応じて特徴の重みをゼロにします

118
00:07:18,965 --> 00:07:25,680
つまりL1正則化には
特徴を取り除く効果があります