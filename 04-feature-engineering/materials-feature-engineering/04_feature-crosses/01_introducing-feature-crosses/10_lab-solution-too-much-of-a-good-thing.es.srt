1
00:00:00,540 --> 00:00:03,250
Estamos en TensorFlow Playground.

2
00:00:03,250 --> 00:00:05,800
Tenemos un conjunto de datos
que se ve así.

3
00:00:05,800 --> 00:00:08,830
Tenemos los puntos azules
en la esquina superior derecha

4
00:00:08,830 --> 00:00:11,820
los naranjas
en la esquina inferior izquierda

5
00:00:11,820 --> 00:00:17,370
y estamos tratando de trazar
una línea de separación entre ambos.

6
00:00:17,930 --> 00:00:20,880
Y para hacerlo,
tenemos estas entradas

7
00:00:20,880 --> 00:00:22,200
x1, x2

8
00:00:22,200 --> 00:00:24,435
x1² y x2²

9
00:00:24,435 --> 00:00:26,080
y x1 por x2.

10
00:00:27,010 --> 00:00:32,225
Primero,
¿cuáles de estas son entradas sin procesar

11
00:00:32,225 --> 00:00:35,755
y cuáles son atributos creados?

12
00:00:35,755 --> 00:00:38,845
Bueno, x1 y x2
son las entradas sin procesar.

13
00:00:38,845 --> 00:00:42,600
x1², x2² y x1x2

14
00:00:42,600 --> 00:00:47,875
son atributos que creamos a partir
de las entradas sin procesar x1 y x2.

15
00:00:48,765 --> 00:00:51,460
¿Cuáles son combinaciones de atributos?

16
00:00:52,150 --> 00:00:54,910
x1 por x2 es obviamente
una combinación de atributos

17
00:00:55,500 --> 00:00:59,925
pero si observan con cuidado
podrán ver que x1²

18
00:00:59,925 --> 00:01:01,950
también es una combinación de atributos.

19
00:01:01,950 --> 00:01:05,425
Es una autocombinación.
Es un "self JOIN", por así decirlo.

20
00:01:05,425 --> 00:01:10,345
Se toman x1 y x1
y se combinan para obtener x1².

21
00:01:10,345 --> 00:01:14,115
Una forma de verlo es que tenemos
dos entradas sin procesar, x1 y x2

22
00:01:14,115 --> 00:01:16,220
y tenemos tres combinaciones de atributos

23
00:01:16,220 --> 00:01:18,600
x1², x2² y x1x2.

24
00:01:20,300 --> 00:01:21,710
Pero solo es terminología.

25
00:01:21,710 --> 00:01:24,375
Pueden llamar a x1² y x2²

26
00:01:24,375 --> 00:01:27,910
una transformación de la entrada
en lugar de una combinación de atributos.

27
00:01:27,910 --> 00:01:29,130
No hay problema.

28
00:01:29,130 --> 00:01:31,635
Tenemos cinco entradas en nuestro modelo

29
00:01:31,635 --> 00:01:33,385
y deseamos entrenarlo.

30
00:01:33,385 --> 00:01:34,570
Hagámoslo.

31
00:01:34,570 --> 00:01:37,720
Apretaré el botón de reproducir
y comenzará el entrenamiento.

32
00:01:37,720 --> 00:01:40,610
Observen que algo extraño ocurre.

33
00:01:41,460 --> 00:01:45,390
Aquí abajo,
en la esquina inferior izquierda

34
00:01:45,390 --> 00:01:47,660
¿vieron eso azul?

35
00:01:48,510 --> 00:01:53,940
Desapareció después de un rato,
pero imaginen que eso no hubiera sucedido.

36
00:01:53,940 --> 00:01:56,855
Intentemos de nuevo.

37
00:01:56,855 --> 00:01:58,990
No sabemos
por cuánto tiempo entrenaremos.

38
00:01:58,990 --> 00:02:00,935
Supongamos que entrenamos hasta aquí

39
00:02:00,935 --> 00:02:03,550
por 230 repeticiones. Eso es mucho tiempo.

40
00:02:03,550 --> 00:02:09,245
Entrenamos por 230 repeticiones
y vemos algo extraño.

41
00:02:11,475 --> 00:02:12,840
Esto.

42
00:02:12,840 --> 00:02:16,820
Ese triángulo
es un indicador de sobreajuste.

43
00:02:17,850 --> 00:02:20,240
En realidad, no hay datos ahí

44
00:02:20,240 --> 00:02:23,710
por lo que es una explicación factible

45
00:02:23,710 --> 00:02:28,455
y no estamos tratando
de simplificar el modelo.

46
00:02:29,225 --> 00:02:32,250
Entonces, el modelo coloca datos ahí.

47
00:02:33,740 --> 00:02:35,365
Una de las razones

48
00:02:35,365 --> 00:02:38,220
es porque estamos permitiendo
que el modelo se sobreajuste.

49
00:02:38,220 --> 00:02:40,940
Una manera de hacer que esto pase

50
00:02:40,940 --> 00:02:43,580
es alimentar los mismos datos
de varias formas.

51
00:02:43,580 --> 00:02:46,205
¿Qué ocurre si desactivo x1x2?

52
00:02:47,245 --> 00:02:51,475
Ahora, solo tenemos
x1, x2, x1² y x2².

53
00:02:52,085 --> 00:02:53,965
Reinicio

54
00:02:53,965 --> 00:02:57,450
y ahora, de nuevo observen

55
00:02:57,450 --> 00:03:04,915
que hay un límite extraño que ocurre
en la etapa temprana del entrenamiento.

56
00:03:04,915 --> 00:03:08,285
Hagámoslo de nuevo.
Lo reiniciamos

57
00:03:08,285 --> 00:03:11,965
y lo interrumpiremos
alrededor de las 200 repeticiones.

58
00:03:11,965 --> 00:03:16,880
Alrededor de las 200 repeticiones,
observamos que el límite no es tan bueno

59
00:03:16,880 --> 00:03:20,860
todavía hay esto blanco y extraño aquí

60
00:03:20,860 --> 00:03:24,460
debido a los atributos adicionales,
x1² y x2².

61
00:03:24,860 --> 00:03:27,210
¿Qué ocurre si quito x1² y x2²?

62
00:03:27,210 --> 00:03:31,680
Ahora solo tenemos los datos
sin procesar, x1 y x2.

63
00:03:31,680 --> 00:03:37,200
Reinicio el entrenamiento y lo interrumpo
alrededor de las 200 repeticiones.

64
00:03:38,210 --> 00:03:42,200
Notarán que ahora es casi perfecto.

65
00:03:42,200 --> 00:03:46,945
Solo tengo esta línea
y eso es algo que deben tomar en cuenta

66
00:03:46,945 --> 00:03:50,230
que pueden tener mucho de algo bueno

67
00:03:50,230 --> 00:03:56,010
y que las combinaciones de atributos
son una tentación para el sobreajuste.

68
00:03:56,600 --> 00:03:59,150
Pero también observamos algo más

69
00:03:59,150 --> 00:04:02,740
que si entrenan durante mucho tiempo…

70
00:04:02,740 --> 00:04:05,640
quitemos esto,
es con lo que comenzamos.

71
00:04:05,640 --> 00:04:09,540
Si entrenamos durante mucho tiempo

72
00:04:09,540 --> 00:04:16,385
tiende a mejorar,
pero sabemos que hay sobreajuste

73
00:04:16,385 --> 00:04:19,020
porque tenemos este límite curvo

74
00:04:19,020 --> 00:04:22,470
ese es otro síntoma del sobreajuste.

75
00:04:23,010 --> 00:04:26,170
Si entrenamos durante mucho tiempo

76
00:04:26,170 --> 00:04:28,410
esto desaparece

77
00:04:28,410 --> 00:04:31,030
este artefacto
en la esquina inferior izquierda

78
00:04:31,030 --> 00:04:36,380
pero aún tenemos este límite curvo
y la razón por la que tenemos esto

79
00:04:36,380 --> 00:04:38,480
en lugar de una línea recta

80
00:04:38,480 --> 00:04:40,900
que sabemos que es el modelo
más simple y eficaz

81
00:04:40,900 --> 00:04:44,020
es porque permitimos al modelo
varios grados de libertad.

82
00:04:44,020 --> 00:04:46,310
Para ser franco, si observan esto

83
00:04:46,310 --> 00:04:52,045
los pesos de x1 y x2 son mucho más altos
que cualquiera de estos tres.

84
00:04:52,045 --> 00:04:55,700
Pero x1 por x2, esa combinación

85
00:04:55,700 --> 00:04:57,305
obtiene un peso

86
00:04:57,735 --> 00:05:02,290
y por esa razón,
puede tener un efecto negativo.

87
00:05:03,880 --> 00:05:08,715
Sorprendentemente, el límite de decisión
del modelo se ve un poco extraño.

88
00:05:08,715 --> 00:05:12,610
En particular, esta región
en la parte inferior izquierda

89
00:05:12,610 --> 00:05:18,360
que tiende al azul, aunque no hay
un respaldo visible en los datos.

90
00:05:19,180 --> 00:05:22,335
TensorFlow Playground
usa puntos de inicio aleatorios

91
00:05:22,335 --> 00:05:24,700
por lo que sus resultados
podrían ser diferentes.

92
00:05:24,700 --> 00:05:26,920
Por eso muestro una imagen
de lo que obtuve.

93
00:05:26,920 --> 00:05:29,940
Es posible que tengan algo diferente.

94
00:05:31,070 --> 00:05:34,630
Observen el grosor relativo
de las cinco líneas

95
00:05:34,630 --> 00:05:37,220
que van de la entrada a la salida.

96
00:05:37,220 --> 00:05:42,180
Estas líneas muestran los pesos relativos
de los cinco atributos.

97
00:05:42,180 --> 00:05:45,695
Estas líneas que salen de x1 y x2

98
00:05:45,695 --> 00:05:50,005
son mucho más gruesas
que las que salen de las combinaciones.

99
00:05:50,005 --> 00:05:52,010
Entonces, las combinaciones de atributos

100
00:05:52,010 --> 00:05:57,325
contribuyen mucho menos al modelo
que los atributos normales

101
00:05:58,315 --> 00:06:02,850
pero contribuyen lo suficiente
como para afectar la generalización.

102
00:06:03,780 --> 00:06:06,630
¿Y si quitamos las combinaciones
de atributos por completo?

103
00:06:06,630 --> 00:06:09,490
En otras palabras, ¿si solo usamos
datos sin procesar?

104
00:06:10,310 --> 00:06:12,905
Quitar todas las combinaciones

105
00:06:12,905 --> 00:06:15,740
produce un modelo más sensato.

106
00:06:15,740 --> 00:06:20,000
Ya no hay el límite curvo
que sugiere un sobreajuste.

107
00:06:20,000 --> 00:06:23,960
Luego de 1,000 iteraciones,
la pérdida de prueba debería ser un valor

108
00:06:23,960 --> 00:06:28,800
ligeramente menor
que cuando se usan las combinaciones

109
00:06:28,800 --> 00:06:32,830
aunque sus resultados pueden variar
un poco según el conjunto de datos.

110
00:06:33,620 --> 00:06:38,660
Los datos de este ejercicio
son lineales más ruido.

111
00:06:38,660 --> 00:06:44,100
Si usamos un modelo que es
muy complicado para datos tan simples

112
00:06:44,100 --> 00:06:47,240
si usamos un modelo
con demasiadas combinaciones

113
00:06:47,240 --> 00:06:50,000
le damos la oportunidad
de ajustarse al ruido

114
00:06:50,000 --> 00:06:52,260
en los datos de entrenamiento.

115
00:06:52,260 --> 00:06:55,445
A menudo, pueden diagnosticar esto
si observan el rendimiento

116
00:06:55,445 --> 00:06:59,120
del modelo en datos
de prueba independientes.

117
00:07:00,240 --> 00:07:03,485
A propósito… y hablaremos
sobre la regularización

118
00:07:03,485 --> 00:07:06,480
más tarde en este curso
sobre el arte y la ciencia del AA.

119
00:07:06,480 --> 00:07:11,110
A propósito, esto explica
por qué la regularización L1

120
00:07:11,110 --> 00:07:13,010
puede ser algo tan bueno.

121
00:07:13,010 --> 00:07:15,720
Lo que la regularización L1 hace

122
00:07:15,720 --> 00:07:19,260
es convertir el peso de un atributo
en 0, si es necesario.

123
00:07:19,260 --> 00:07:22,994
En otras palabras, el objetivo
de la regularización L1

124
00:07:22,994 --> 00:07:25,389
es quitar atributos.