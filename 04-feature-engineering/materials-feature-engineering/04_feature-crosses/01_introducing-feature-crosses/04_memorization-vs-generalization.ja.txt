x1軸を離散化するために 1本の白い直線ではなく 黒い直線をたくさん引いたら
どうでしょう？ 同様にx2にも
黒い線をたくさん引きます こうしてx1軸とx2軸を離散化しました 2つの白い線を引いたときは 4象限で終わりました 今はどうですか？ m本の垂直線とn本の水平線を引くと (m+1) x (n+1) 個の
グリッドセルができますね？ ここでx1とx2を離散化して
その積を求めると どうなるでしょうか これは先ほどの入力空間を象限に
分割したときの図です 基本的には象限ごとに
違う予測をする必要があります この緑色の四角はどうでしょう？ この四角の中は
どう予測されますか 黄色ですね？ これはどうですか？ 青ですが黄色も少しあります 青い点と黄色の点を数えて
たとえば85パーセントが青だとすると ここで確率が登場します 次にこれはどうなりますか？ ともかくこれが線形モデルとして
機能する理由を見てみましょう 最初の値セットでは
1つホット、残りはコールド 2つ目の値セットでも
1つホット、残りはコールド それらに特徴クロスを適用すると そのバケットに入る点を扱う
1つのノードが残ります つまりx3=1になるのは
x1=1およびx2=1のときだけです 入力空間にある
それぞれの点に対して 1つのバケットだけが該当します さて特徴クロスを適用した値を
線形回帰に入力する場合 重みw3はどうすべきでしょう？ グリッドセル内の青い点と黄色の点の
比率はx1とx2に相当します こうして特徴クロスは
とても強力になります 入力空間を離散化して
トレーニングデータセットを記憶します でもこれには問題が潜んでいます 十分なデータがなかったら
どうなりますか？ この場合モデルは何を学習しますか？ 「青」の予測を学習しますが
本当ですか？ これには回避策があります 入力空間を均一に離散化する
必要はありません 代わりにさまざまなサイズの
四角を使えます 四角の中のエントロピーや情報内容に
関連するサイズにできます 四角をグループ化、クラスタ化できます 回避策があるのです しかし特徴クロスでは
記憶を使うことに注意してください 記憶は
機械学習の目標である一般化とは逆です では記憶を使うべきですか？ 実際の機械学習システムでは 両方を使えます 記憶が役立つのは 入力空間の各グリッドセルに
大量のデータが存在し データの分散（分布）が
統計的に重要な場合です この場合には記憶を使えます 基本的に各グリッドセルで
平均値だけを学習します もちろんディープラーニングでも 空間に大量のデータが必要です 特徴クロスを使うときも
多数の層を使うときも 大量のデータが必要です 従来の機械学習に慣れている方は 特徴クロスに馴染みがないかもしれません 記憶を使うこと
大きなデータセットでのみ機能することが その理由かもしれません でも特徴クロスは現実のデータセットでは
極めて役立ちます データ量が大きいほど 小さな四角を作ることができ より細かく記憶できます 特徴クロスは大きなデータセットを
事前処理する強力なテクニックです