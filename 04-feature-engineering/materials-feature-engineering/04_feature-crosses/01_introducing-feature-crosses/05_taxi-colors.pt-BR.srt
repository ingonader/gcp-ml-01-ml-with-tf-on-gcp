1
00:00:00,000 --> 00:00:03,090
Imagine que você está gravando um modelo
de aprendizado de máquina

2
00:00:03,090 --> 00:00:07,710
que olha para um carro e diz se
é ou não um táxi.

3
00:00:07,710 --> 00:00:13,710
Sabemos que carros brancos em Roma e
amarelos em Nova York tendem a ser táxis.

4
00:00:13,710 --> 00:00:17,040
Mas queremos que nosso modelo de
aprendizado de máquina aprenda isso

5
00:00:17,040 --> 00:00:20,180
de um conjunto de dados que
consiste em registros de carro.

6
00:00:21,070 --> 00:00:25,235
Suponhamos que os dados de entrada
sejam assim: vermelho,

7
00:00:25,235 --> 00:00:30,700
Roma, branco, Roma etc, os rótulos são
independentes de ser um táxi ou não.

8
00:00:30,700 --> 00:00:35,250
Basicamente, a cor do carro e a cidade são
os dois recursos de entrada,

9
00:00:35,250 --> 00:00:37,620
e você precisa usá-los no

10
00:00:37,620 --> 00:00:42,490
modelo linear para prever se o
carro é ou não um táxi.

11
00:00:42,490 --> 00:00:44,220
Como você faria?

12
00:00:45,010 --> 00:00:46,820
Você pega a primeira entrada,

13
00:00:46,820 --> 00:00:50,020
a cor do carro, e aplica uma
codificação one-hot.

14
00:00:50,020 --> 00:00:52,110
Você pega a segunda entrada,

15
00:00:52,110 --> 00:00:54,920
o nome da cidade, e aplica uma
codificação one-hot.

16
00:00:54,920 --> 00:01:00,150
Pegue isso e envie diretamente
para o modelo linear.

17
00:01:00,150 --> 00:01:05,429
Agora, digamos que você dê um peso de 0,8
para carros amarelos,

18
00:01:05,429 --> 00:01:09,615
porque 80% dos carros amarelos no conjunto
de dados de treinamento são táxis.

19
00:01:09,615 --> 00:01:12,840
Portanto, w3 agora é 0,8.

20
00:01:12,840 --> 00:01:15,945
Claro, você não vai dar um peso de 0,8.

21
00:01:15,945 --> 00:01:18,910
Esse peso será aprendido pelo
gradiente descendente,

22
00:01:18,910 --> 00:01:21,515
mas é isso que ele fará.

23
00:01:21,515 --> 00:01:27,315
Infelizmente, o peso 0,8 é verdadeiro
para carros amarelos em todas as cidades,

24
00:01:27,315 --> 00:01:28,950
não apenas em Nova York.

25
00:01:30,100 --> 00:01:31,890
Como você consertaria isso?

26
00:01:32,560 --> 00:01:35,145
Você daria um alto peso a Nova York?

27
00:01:35,145 --> 00:01:37,275
Isso não funciona.

28
00:01:37,275 --> 00:01:40,860
Agora, todos os carros de
Nova York têm esse peso alto.

29
00:01:41,320 --> 00:01:43,710
Você vê o problema?

30
00:01:47,130 --> 00:01:50,225
Adicione um cruzamento de
atributo e o que acontece?

31
00:01:50,225 --> 00:01:55,550
Temos um node de entrada correspondente
a carros vermelhos em Nova York

32
00:01:55,550 --> 00:01:58,119
e outro a carros amarelos em Nova York,

33
00:01:58,119 --> 00:02:00,590
e um terceiro a carros
brancos em Nova York,

34
00:02:00,590 --> 00:02:02,880
e um quarto a carros
verdes em Nova York

35
00:02:02,880 --> 00:02:05,525
e, da mesma forma, para carros em Roma.

36
00:02:05,525 --> 00:02:11,170
Agora, o modelo pode aprender rapidamente
que carros amarelos em Nova York

37
00:02:11,170 --> 00:02:17,255
e carros brancos em Roma tendem a ser
táxis, e dão aos dois nodes um peso alto.

38
00:02:17,255 --> 00:02:20,310
Todo o resto, peso zero.

39
00:02:20,310 --> 00:02:21,975
Problema resolvido.

40
00:02:21,975 --> 00:02:26,560
É por isso que os cruzamentos de
atributos são tão poderosos.

41
00:02:29,290 --> 00:02:33,440
Os cruzamentos de atributos trazem
muita energia para modelos lineares.

42
00:02:33,440 --> 00:02:38,280
Usar cruzamentos de atributos
e dados massivos é

43
00:02:38,280 --> 00:02:43,835
uma estratégia muito eficiente para
aprender espaços altamente complexos.

44
00:02:44,555 --> 00:02:49,620
As redes neurais fornecem outra maneira
de aprender espaços altamente complexos.

45
00:02:49,620 --> 00:02:54,285
Mas os cruzamentos de atributos permitem
que modelos lineares permaneçam no jogo.

46
00:02:54,285 --> 00:03:00,875
Sem os cruzamentos, a expressividade dos
modelos lineares seria bastante limitada.

47
00:03:00,875 --> 00:03:04,879
Com os cruzamentos de atributos, depois de
ter um conjunto de dados grande,

48
00:03:04,879 --> 00:03:08,910
um modelo linear pode aprender todos os
detalhes do espaço de entrada.

49
00:03:08,910 --> 00:03:14,785
Os cruzamentos de atributos permitem que
um modelo linear memorize grandes dados.

50
00:03:14,785 --> 00:03:19,269
A ideia é: você pode atribuir um peso para
cada cruzamento de atributos,

51
00:03:19,269 --> 00:03:23,580
e, desta maneira, o modelo aprende
sobre combinações de atributos.

52
00:03:23,580 --> 00:03:26,035
Portanto, mesmo que seja um modelo linear,

53
00:03:26,035 --> 00:03:32,630
o relacionamento subjacente real
entre entradas e saídas não é linear.

54
00:03:34,280 --> 00:03:39,615
Por que estamos tão preocupados em
fazer modelos lineares funcionarem bem?

55
00:03:39,615 --> 00:03:42,065
Pense no curso anterior.

56
00:03:42,065 --> 00:03:47,005
Conversamos sobre problemas
convexos e problemas não convexos.

57
00:03:47,005 --> 00:03:52,445
Redes neurais com muitas
camadas não são convexas.

58
00:03:52,445 --> 00:03:57,665
Mas otimizar modelos lineares
é um problema convexo,

59
00:03:57,665 --> 00:04:00,895
e problemas convexos são muito,

60
00:04:00,895 --> 00:04:04,645
muito mais fáceis do que
problemas não convexos.

61
00:04:04,645 --> 00:04:06,905
Assim, por muito tempo,

62
00:04:06,905 --> 00:04:12,555
modelos lineares esparsos eram o único
algoritmo que tínhamos capaz de

63
00:04:12,555 --> 00:04:18,010
escalonar para bilhões de exemplos de
treino e de recursos de entrada.

64
00:04:18,010 --> 00:04:23,360
Os predecessores do TensorFlow no
Google, SETI, SmartAss, Siebel,

65
00:04:23,360 --> 00:04:26,410
eram todos aprendizes
de grande escala.

66
00:04:26,410 --> 00:04:29,490
Isso mudou nos últimos anos

67
00:04:29,490 --> 00:04:34,840
e as redes neurais agora também podem
lidar com dados em grande escala,

68
00:04:34,840 --> 00:04:38,580
geralmente com a ajuda de GPUs e TPUs,

69
00:04:38,580 --> 00:04:43,775
mas modelos lineares esparsos ainda são
uma opção rápida e de baixo custo.

70
00:04:43,775 --> 00:04:48,460
Usar modelos lineares esparsos como
um pré-processador para

71
00:04:48,460 --> 00:04:54,080
atributos geralmente significa que a rede
neural converge muito mais rápido.