1
00:00:00,000 --> 00:00:03,040
x1軸を離散化するために

2
00:00:03,040 --> 00:00:05,485
1本の白い直線ではなく

3
00:00:05,485 --> 00:00:09,465
黒い直線をたくさん引いたら
どうでしょう？

4
00:00:09,465 --> 00:00:13,965
同様にx2にも
黒い線をたくさん引きます

5
00:00:13,965 --> 00:00:18,810
こうしてx1軸とx2軸を離散化しました

6
00:00:18,810 --> 00:00:21,180
2つの白い線を引いたときは

7
00:00:21,180 --> 00:00:23,265
4象限で終わりました

8
00:00:23,265 --> 00:00:24,885
今はどうですか？

9
00:00:24,885 --> 00:00:28,845
m本の垂直線とn本の水平線を引くと

10
00:00:28,845 --> 00:00:34,185
(m+1) x (n+1) 個の
グリッドセルができますね？

11
00:00:34,185 --> 00:00:42,165
ここでx1とx2を離散化して
その積を求めると どうなるでしょうか

12
00:00:42,165 --> 00:00:48,390
これは先ほどの入力空間を象限に
分割したときの図です

13
00:00:48,390 --> 00:00:53,895
基本的には象限ごとに
違う予測をする必要があります

14
00:00:53,895 --> 00:00:56,960
この緑色の四角はどうでしょう？

15
00:00:56,960 --> 00:01:00,190
この四角の中は
どう予測されますか

16
00:01:00,190 --> 00:01:02,200
黄色ですね？

17
00:01:02,200 --> 00:01:04,280
これはどうですか？

18
00:01:04,280 --> 00:01:07,425
青ですが黄色も少しあります

19
00:01:07,425 --> 00:01:14,670
青い点と黄色の点を数えて
たとえば85パーセントが青だとすると

20
00:01:14,685 --> 00:01:18,795
ここで確率が登場します

21
00:01:18,795 --> 00:01:21,760
次にこれはどうなりますか？

22
00:01:21,760 --> 00:01:26,925
ともかくこれが線形モデルとして
機能する理由を見てみましょう

23
00:01:26,925 --> 00:01:31,299
最初の値セットでは
1つホット、残りはコールド

24
00:01:31,299 --> 00:01:36,834
2つ目の値セットでも
1つホット、残りはコールド

25
00:01:36,834 --> 00:01:40,310
それらに特徴クロスを適用すると

26
00:01:40,310 --> 00:01:46,050
そのバケットに入る点を扱う
1つのノードが残ります

27
00:01:46,050 --> 00:01:54,755
つまりx3=1になるのは
x1=1およびx2=1のときだけです

28
00:01:54,755 --> 00:01:58,750
入力空間にある
それぞれの点に対して

29
00:01:58,750 --> 00:02:02,180
1つのバケットだけが該当します

30
00:02:02,180 --> 00:02:08,940
さて特徴クロスを適用した値を
線形回帰に入力する場合

31
00:02:08,940 --> 00:02:12,315
重みw3はどうすべきでしょう？

32
00:02:12,315 --> 00:02:19,805
グリッドセル内の青い点と黄色の点の
比率はx1とx2に相当します

33
00:02:19,805 --> 00:02:23,970
こうして特徴クロスは
とても強力になります

34
00:02:23,970 --> 00:02:30,830
入力空間を離散化して
トレーニングデータセットを記憶します

35
00:02:30,830 --> 00:02:33,560
でもこれには問題が潜んでいます

36
00:02:33,560 --> 00:02:37,205
十分なデータがなかったら
どうなりますか？

37
00:02:37,205 --> 00:02:39,580
この場合モデルは何を学習しますか？

38
00:02:39,580 --> 00:02:44,460
「青」の予測を学習しますが
本当ですか？

39
00:02:44,460 --> 00:02:47,655
これには回避策があります

40
00:02:47,655 --> 00:02:51,970
入力空間を均一に離散化する
必要はありません

41
00:02:51,970 --> 00:02:56,103
代わりにさまざまなサイズの
四角を使えます

42
00:02:56,103 --> 00:03:01,470
四角の中のエントロピーや情報内容に
関連するサイズにできます

43
00:03:01,470 --> 00:03:04,860
四角をグループ化、クラスタ化できます

44
00:03:04,860 --> 00:03:06,910
回避策があるのです

45
00:03:06,910 --> 00:03:13,445
しかし特徴クロスでは
記憶を使うことに注意してください

46
00:03:13,445 --> 00:03:19,415
記憶は
機械学習の目標である一般化とは逆です

47
00:03:19,415 --> 00:03:22,440
では記憶を使うべきですか？

48
00:03:22,440 --> 00:03:25,340
実際の機械学習システムでは

49
00:03:25,340 --> 00:03:26,985
両方を使えます

50
00:03:26,985 --> 00:03:28,920
記憶が役立つのは

51
00:03:28,920 --> 00:03:33,960
入力空間の各グリッドセルに
大量のデータが存在し

52
00:03:33,960 --> 00:03:38,600
データの分散（分布）が
統計的に重要な場合です

53
00:03:38,600 --> 00:03:41,460
この場合には記憶を使えます

54
00:03:41,460 --> 00:03:46,700
基本的に各グリッドセルで
平均値だけを学習します

55
00:03:46,700 --> 00:03:49,750
もちろんディープラーニングでも

56
00:03:49,750 --> 00:03:52,560
空間に大量のデータが必要です

57
00:03:52,560 --> 00:03:56,820
特徴クロスを使うときも
多数の層を使うときも

58
00:03:56,820 --> 00:03:59,055
大量のデータが必要です

59
00:03:59,055 --> 00:04:02,530
従来の機械学習に慣れている方は

60
00:04:02,530 --> 00:04:05,655
特徴クロスに馴染みがないかもしれません

61
00:04:05,655 --> 00:04:12,080
記憶を使うこと
大きなデータセットでのみ機能することが

62
00:04:12,080 --> 00:04:14,520
その理由かもしれません

63
00:04:14,520 --> 00:04:20,650
でも特徴クロスは現実のデータセットでは
極めて役立ちます

64
00:04:20,650 --> 00:04:22,950
データ量が大きいほど

65
00:04:22,950 --> 00:04:25,585
小さな四角を作ることができ

66
00:04:25,585 --> 00:04:28,115
より細かく記憶できます

67
00:04:28,115 --> 00:04:35,160
特徴クロスは大きなデータセットを
事前処理する強力なテクニックです