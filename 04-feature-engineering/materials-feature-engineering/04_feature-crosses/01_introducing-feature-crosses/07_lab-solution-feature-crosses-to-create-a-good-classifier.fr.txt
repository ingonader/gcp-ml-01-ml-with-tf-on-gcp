Et voilà. Nous somme dans TensorFlow Playground
et l'ensemble de données que nous avons semble contenir essentiellement
des points bleus en bas à gauche et en haut à droite, et des points orange en haut à gauche
et en bas à droite. Nous avons
les valeurs d'entrée brutes x1 et x2, et nous voulons les utiliser
pour entraîner le modèle. Entraînons le modèle qui prend x1 et x2 en entrée
dans cet ensemble de données. Comme vous pouvez le voir, le modèle peut poursuivre l'entraînement, mais l'image de fond
ne change pas beaucoup. Les couleurs sont pâles,
car x1 et x2, et le modèle linéaire, ne fonctionnent pas très bien
en termes de capacités d'apprentissage. Le modèle n'apprend donc pas beaucoup. Arrêtons ceci et reprenons. En réalité, c'est une combinaison de x1 et x2
qui est importante. Si x1 et x2 sont négatifs,
nous avons du bleu. Si x1 et x2 sont positifs,
nous avons du bleu. Et si x1 et x2 ont des signes différents, nous avons apparemment de l'orange. Qu'est-ce que ça vous rappelle ? C'est un croisement
de caractéristiques entre x1 et x2. Ajoutons donc le croisement
des caractéristiques x1 et x2 en entrée. Procédons maintenant à l'entraînement.
Nous voyons presque immédiatement que nous avons un bon modèle
qui sépare le bleu et l'orange. Le fond des points bleus
a tendance à être bleu et celui des points orange à être orange. Il y a bien sûr du bruit au niveau
des erreurs de classification, ce qui est normal, car l'ensemble
de données est complexe. L'idée principale est donc que, grâce à l'utilisation
d'un renseignement humain, selon lequel une combinaison de x1 et x2 nous permettra
de mieux classifier l'ensemble de données, nous avons pu ajouter x1x2. Ce n'est pas vraiment une nouvelle
valeur d'entrée. C'est une extraction de caractéristiques que nous avons effectuée sur les données
d'entrée d'origine x1 et x2. Elle nous permet de séparer
plutôt bien les points bleus et orange. Prenons un nouvel exemple. Ici, les points bleus sont au centre et les points orange sur les bords. À nouveau, si je n'utilise que x1 et x2,
et que je procède à l'entraînement, l'image de fond est pâle, car ce modèle n'apprend pas bien. Nous devrions donc voir quel type d'extraction
de caractéristiques utiliser. J'arrête donc ceci. Quel type d'extraction de caractéristiques
peut servir à effectuer cette séparation ? Notre intuition nous dit à nouveau que, si x1 et x2 sont tous les deux
petits, nous avons du bleu et que, si x1 et x2 sont grands, nous avons de l'orange. En réalité, ce n'est pas ça. Si vous regardez ce point ici, x1 est très petit, mais x2 est grand. Une autre manière d'envisager le problème est de considérer ceci
comme le centre de l'image. Les points qui en sont proches
ont tendance à être bleus et les points qui en sont éloignés
à être orange. Qu'est-ce que ça vous rappelle ? Des points proches et éloignés :
il s'agit d'une distance. Quelle est l'équation de la distance ? √(x² + y²). Nous n'avons pas besoin de racine carrée,
car nous nous contentons ici d'utiliser des caractéristiques d'entrée
dans un réseau de neurones. Nous avons besoin de x² et y². Ajoutons-les comme valeurs d'entrée. Si je procède à l'entraînement, vous pouvez voir que j'obtiens
presque immédiatement une bonne séparation
entre les points bleus et orange. Arrêtons ceci. Examinons ces deux exemples. Dans ces deux cas, la frontière
de séparation est-elle linéaire ? Dans ce cas, c'est assez évident. Il ne s'agit pas d'une frontière linéaire. Même si nous utilisons un modèle linéaire, il n'y a ni couche cachée, ni réseau de neurones. C'est une simple combinaison linéaire
des valeurs d'entrée. Nous avons pu obtenir
une frontière non linéaire. C'est un point important. Si vous avez des croisements
de caractéristiques, même si vous utilisez un modèle linéaire, comme le croisement
de caractéristiques est non linéaire, le modèle est en réalité non linéaire. Une des raisons pour lesquelles
les croisements de caractéristiques fonctionnent est parce qu'ils apportent la puissance
de la non-linéarité au problème. Ils permettent de résoudre le problème sans avoir à payer le prix
de la non-linéarité. Nous n'avons pas à nous inquiéter
de la profondeur des modèles ou des problèmes d'entraînement, etc. C'est un modèle linéaire. L'avantage d'un modèle linéaire est que sa surface est convexe, ce qui signifie que nous avons
un minimum global unique. Il est relativement facile à trouver. Vous avez donc les avantages
d'un modèle linéaire avec la puissance de la non-linéarité. C'est l'intérêt des croisements
de caractéristiques. Même dans l'autre exemple, il s'agit d'une frontière non linéaire,
car nous avons deux lignes, pas une ligne unique. Ce n'est pas aussi évident
que dans cet exemple où nous avons une ellipse,
qui n'est clairement pas une ligne. Souvenez-vous donc que, même si vous disposez
de la puissance d'un réseau de neurones et que vous voulez l'utiliser, vous devrez peut-être ajouter à votre kit
les croisements de caractéristiques. Ainsi, vous obtiendrez un modèle simple
tout en bénéficiant de la non-linéarité.