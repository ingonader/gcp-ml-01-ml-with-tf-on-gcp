1
00:00:00,000 --> 00:00:03,250
So, here we are in the Tenserflow playground.

2
00:00:03,250 --> 00:00:05,800
We have some dataset that looks like this.

3
00:00:05,800 --> 00:00:08,830
We have the blue dots on the upper right corner,

4
00:00:08,830 --> 00:00:12,890
the orange dots on the lower left corner and we are trying to

5
00:00:12,890 --> 00:00:17,930
basically draw a separation line that separates these two things.

6
00:00:17,930 --> 00:00:19,490
And in order to do that,

7
00:00:19,490 --> 00:00:21,570
as inputs we have X1,

8
00:00:21,570 --> 00:00:24,435
X2, X1 squared, X2 squared,

9
00:00:24,435 --> 00:00:26,550
and X1 times X2.

10
00:00:26,550 --> 00:00:31,925
First of all, which of these are raw inputs,

11
00:00:31,925 --> 00:00:35,465
and which of these are created features?

12
00:00:35,465 --> 00:00:38,555
Well, X1 and X2 are the raw inputs.

13
00:00:38,555 --> 00:00:41,840
X1 squared, X2 squared, and X1,

14
00:00:41,840 --> 00:00:47,875
X2 are features that we created from the raw inputs X1 and X2.

15
00:00:47,875 --> 00:00:51,190
Which of these are feature crosses?

16
00:00:51,190 --> 00:00:55,005
X1X2 is obviously a feature cross,

17
00:00:55,005 --> 00:00:58,290
but if you squint it at a little bit you can realize

18
00:00:58,290 --> 00:01:01,535
that X1 squared is also a feature cross.

19
00:01:01,535 --> 00:01:03,405
It's a self cross.

20
00:01:03,405 --> 00:01:05,265
It's a self join, if you will.

21
00:01:05,265 --> 00:01:09,915
You're taking X1 and X1 and crossing them together to get X1 squared.

22
00:01:09,915 --> 00:01:13,980
So, one way to think about it is that we have two raw inputs X1 and X2,

23
00:01:13,980 --> 00:01:17,000
and we have three feature crosses X1 squared,

24
00:01:17,000 --> 00:01:18,680
X2 squared, and X1X2.

25
00:01:18,680 --> 00:01:21,545
But now, it's just terminology.

26
00:01:21,545 --> 00:01:23,820
You can call X1 squared and X2

27
00:01:23,820 --> 00:01:28,635
some transformation of the input rather than a feature cross. No problem.

28
00:01:28,635 --> 00:01:31,155
So, we have five inputs to our model,

29
00:01:31,155 --> 00:01:32,490
and we want to train it.

30
00:01:32,490 --> 00:01:33,900
So, let's go ahead and do that.

31
00:01:33,900 --> 00:01:36,600
I'll go ahead and pick the play button and we start

32
00:01:36,600 --> 00:01:40,590
training it and notice something strange that's happening.

33
00:01:40,590 --> 00:01:45,300
Right down here, at the lower left corner,

34
00:01:45,300 --> 00:01:47,910
you see that blue that happened?

35
00:01:47,910 --> 00:01:53,855
It went away after a while but imagine that we didn't have that option.

36
00:01:53,855 --> 00:01:55,590
So, let's try this again.

37
00:01:55,590 --> 00:01:58,365
We don't know how long we are going to be training.

38
00:01:58,365 --> 00:02:00,120
Let's say we trained up to this point,

39
00:02:00,120 --> 00:02:03,075
we train for 230 epochs. That's a long time.

40
00:02:03,075 --> 00:02:09,330
We trained on 230 epochs and we come up with something strange.

41
00:02:09,330 --> 00:02:12,420
What? This thing here.

42
00:02:12,420 --> 00:02:17,240
That triangle is an indicator of overfitting.

43
00:02:17,240 --> 00:02:19,990
There is really no data there.

44
00:02:19,990 --> 00:02:25,045
So, it is a plausible explanation and the model,

45
00:02:25,045 --> 00:02:28,530
we're not trying to make it any simpler than it needs to be.

46
00:02:28,530 --> 00:02:31,605
So, it goes ahead and puts stuff in there.

47
00:02:31,605 --> 00:02:34,820
Now, one of the reasons that this

48
00:02:34,820 --> 00:02:37,940
happens is because we are allowing the model to overfit.

49
00:02:37,940 --> 00:02:40,580
And one way that we can allow our model overfit,

50
00:02:40,580 --> 00:02:43,205
is to give it the same data in multiple ways.

51
00:02:43,205 --> 00:02:46,705
What happens if I turn off X1X2.

52
00:02:46,705 --> 00:02:49,085
So, at this point you only have X1,

53
00:02:49,085 --> 00:02:51,275
X2, X1 squared, and X2 squared.

54
00:02:51,275 --> 00:02:55,280
I'll restart this and at this point,

55
00:02:55,280 --> 00:02:58,115
again notice that there is

56
00:02:58,115 --> 00:03:04,935
this crazy boundary that happens in the early stage of training.

57
00:03:04,935 --> 00:03:08,185
Lets do this again. We will stop this and

58
00:03:08,185 --> 00:03:11,660
will stop at around 200 epochs. So, there we go.

59
00:03:11,660 --> 00:03:16,520
At 200 epochs, and again you see that the boundary isn't great,

60
00:03:16,520 --> 00:03:20,930
there is this white stuff in here with craziness.

61
00:03:20,930 --> 00:03:24,650
Again because we have those extra features, X1 and X2.

62
00:03:24,650 --> 00:03:26,950
What happens if we take out X1 and X2?

63
00:03:26,950 --> 00:03:31,340
So, we now only have the raw data X1 and X2 alone.

64
00:03:31,340 --> 00:03:36,890
So, I will basically do this and I'll start it and again I'll stop at around 200 epochs.

65
00:03:36,890 --> 00:03:42,015
And you notice that now it is pretty perfect.

66
00:03:42,015 --> 00:03:46,840
I just have this line and that is something that you want to be aware of,

67
00:03:46,840 --> 00:03:50,140
that you can have too much of a good thing that

68
00:03:50,140 --> 00:03:56,150
feature crosses are a temptation for the model to overfit.

69
00:03:56,150 --> 00:03:58,900
But we also didn't notice something,

70
00:03:58,900 --> 00:04:02,590
that if you train for a very long time,

71
00:04:02,590 --> 00:04:05,260
let's just take these off this is what he started with,

72
00:04:05,260 --> 00:04:08,685
if we train for a very long time,

73
00:04:08,685 --> 00:04:13,670
this tends to get better but still the fact that it's

74
00:04:13,670 --> 00:04:18,860
because it's an overfitting happens is why you get this curved boundary,

75
00:04:18,860 --> 00:04:21,740
that's another symptom of things being overfit.

76
00:04:21,740 --> 00:04:25,850
So, if we train for a very long time,

77
00:04:25,850 --> 00:04:27,590
this thing goes away,

78
00:04:27,590 --> 00:04:30,860
this artifact in the lower left corner goes away,

79
00:04:30,860 --> 00:04:34,910
but we still have this curved boundary and the reason you can have

80
00:04:34,910 --> 00:04:36,830
a curved boundary rather than

81
00:04:36,830 --> 00:04:40,490
a straight line that we know is the simplest effective model,

82
00:04:40,490 --> 00:04:43,640
is because we gave the model lots of degrees of freedom.

83
00:04:43,640 --> 00:04:45,955
Now to be frank, if you look at this,

84
00:04:45,955 --> 00:04:51,710
the weights of X1 and X2 are much higher than the weights of any of these three things.

85
00:04:51,710 --> 00:04:55,395
But, X1 times X2 that feature cross,

86
00:04:55,395 --> 00:05:00,290
does get the weight and because it does get a weight,

87
00:05:00,290 --> 00:05:02,875
it can mess things up.

88
00:05:02,875 --> 00:05:08,380
Surprisingly, the models decision boundary looks kind of crazy.

89
00:05:08,380 --> 00:05:13,420
In particular, there is this region in the bottom left that's hinting

90
00:05:13,420 --> 00:05:18,685
towards blue even though there is no visible support for that in the data.

91
00:05:18,685 --> 00:05:21,990
Tensorflow playground uses random starting point,

92
00:05:21,990 --> 00:05:23,920
so your result might be different.

93
00:05:23,920 --> 00:05:26,870
This is why I put up what I got as a picture.

94
00:05:26,870 --> 00:05:29,120
You might have gotten something slightly different.

95
00:05:29,120 --> 00:05:36,940
Notice a relative thickness of the five lines running from input to output.

96
00:05:36,940 --> 00:05:41,605
These lines show the relative weights of the five features.

97
00:05:41,605 --> 00:05:45,385
The lines emanating from X1 and X2

98
00:05:45,385 --> 00:05:49,270
are much thicker than those coming from the feature crosses.

99
00:05:49,270 --> 00:05:52,765
So, the feature crosses are contributing

100
00:05:52,765 --> 00:05:57,920
far less to the model than the normal uncrossed features,

101
00:05:57,920 --> 00:06:03,070
but they're contributing enough to mess with a generalisation.

102
00:06:03,070 --> 00:06:06,490
What if we remove the feature crosses completely?

103
00:06:06,490 --> 00:06:09,685
In other words, use only the raw data.

104
00:06:09,685 --> 00:06:12,740
Removing all the feature crosses,

105
00:06:12,740 --> 00:06:15,280
gives you a more sensible model.

106
00:06:15,280 --> 00:06:19,630
There is no longer a curved boundary suggestive of overfitting.

107
00:06:19,630 --> 00:06:24,160
After 1,000 iterations, test loss should be

108
00:06:24,160 --> 00:06:28,510
a slightly lower value than when the feature crosses are used.

109
00:06:28,510 --> 00:06:32,650
Although your results may vary a bit depending on the dataset.

110
00:06:32,650 --> 00:06:38,590
The data in this exercise is basically linear data plus noise.

111
00:06:38,590 --> 00:06:43,810
If we use a model that is too complicated for such simple data,

112
00:06:43,810 --> 00:06:47,020
if we use a model with too many feature crosses,

113
00:06:47,020 --> 00:06:51,655
we give it the opportunity to fit to the noise in the training data.

114
00:06:51,655 --> 00:06:55,210
You can often diagnose this by looking at how

115
00:06:55,210 --> 00:06:59,605
the model performs on independent tests data.

116
00:06:59,605 --> 00:07:02,330
Incidentally, and we'll talk about

117
00:07:02,330 --> 00:07:06,250
regularisation later in the course on art and science of ML,

118
00:07:06,250 --> 00:07:12,720
incidentally this explains why L1 regularization can be such a good thing.

119
00:07:12,720 --> 00:07:15,740
What L1 regularization does,

120
00:07:15,740 --> 00:07:18,965
is that it zeroes out the weight of a feature if necessary.

121
00:07:18,965 --> 00:07:25,680
In other words, the impact of L1 regularisation is to remove features.