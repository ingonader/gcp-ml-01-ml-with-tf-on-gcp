1
00:00:00,000 --> 00:00:03,250
Então, aqui estamos no
TensorFlow Playground.

2
00:00:03,250 --> 00:00:05,800
Temos conjuntos de dados
que se parecem com isso.

3
00:00:05,800 --> 00:00:08,830
Temos os pontos azuis
no canto superior direito,

4
00:00:08,830 --> 00:00:13,350
os pontos laranja no canto inferior
esquerdo e estamos tentando

5
00:00:13,350 --> 00:00:17,930
desenhar uma linha de separação
entre essas duas coisas.

6
00:00:17,930 --> 00:00:19,490
E para fazer isso,

7
00:00:19,490 --> 00:00:20,810
como entradas,

8
00:00:20,810 --> 00:00:24,435
temos x1, x2, x1 ao quadrado,
x2 ao quadrado

9
00:00:24,435 --> 00:00:26,550
e x1 vezes x2.

10
00:00:26,550 --> 00:00:31,925
Em primeiro lugar, quais são
entradas brutas

11
00:00:31,925 --> 00:00:35,465
e quais desses são atributos criados?

12
00:00:35,465 --> 00:00:38,555
x1 e x2 são as entradas brutas.

13
00:00:38,555 --> 00:00:41,840
X1 ao quadrado, x2 ao quadrado e x1x2

14
00:00:41,840 --> 00:00:47,875
são atributos que criamos
das entradas brutas x1 e x2.

15
00:00:47,875 --> 00:00:51,190
Quais destes são cruzamentos de atributos?

16
00:00:51,190 --> 00:00:55,005
x1x2 é obviamente um
cruzamento de atributo,

17
00:00:55,005 --> 00:00:58,290
mas se você olhar atentamente perceberá

18
00:00:58,290 --> 00:01:01,535
que x1 ao quadrado também é
um cruzamento de atributo.

19
00:01:01,535 --> 00:01:03,405
É um autocruzamento.

20
00:01:03,405 --> 00:01:05,265
É uma autojunção, se preferir.

21
00:01:05,265 --> 00:01:09,915
Você está tomando x1 e x1 e os cruzando
juntos para ter x1 ao quadrado.

22
00:01:09,915 --> 00:01:13,980
Então, uma maneira de pensar sobre isso
é que temos duas entradas brutas, x1 e x2,

23
00:01:13,980 --> 00:01:17,000
e temos três cruzamentos de
atributos, x1 ao quadrado,

24
00:01:17,000 --> 00:01:18,680
x2 ao quadrado e x1x2.

25
00:01:18,680 --> 00:01:21,545
Mas agora, é apenas terminologia.

26
00:01:21,545 --> 00:01:23,940
Você pode chamar x1 ao quadrado
e x2 ao quadrado,

27
00:01:23,940 --> 00:01:28,635
de uma transformação da entrada em vez de
um cruzamento de atributos, sem problemas.

28
00:01:28,635 --> 00:01:31,155
Então, temos cinco entradas
para o nosso modelo

29
00:01:31,155 --> 00:01:32,490
e queremos treiná-lo.

30
00:01:32,490 --> 00:01:33,900
Vamos em frente e fazer isso.

31
00:01:33,900 --> 00:01:36,600
Vou em frente, escolho o
botão de reprodução e começamos

32
00:01:36,600 --> 00:01:40,590
a treiná-lo, e note algo estranho
que está acontecendo.

33
00:01:40,590 --> 00:01:45,300
Bem aqui embaixo,
no canto inferior esquerdo,

34
00:01:45,300 --> 00:01:47,910
você vê aquele azul que apareceu?

35
00:01:47,910 --> 00:01:53,855
Ele desapareceu depois de um tempo,
mas imagine que não tínhamos essa opção.

36
00:01:53,855 --> 00:01:55,590
Então, vamos tentar isso de novo.

37
00:01:55,590 --> 00:01:58,365
Não sabemos quanto tempo
vamos treinar.

38
00:01:58,365 --> 00:02:00,380
Vamos dizer que
treinamos até este ponto.

39
00:02:00,380 --> 00:02:03,075
Treinamos por 230 períodos.
Isso é muito tempo.

40
00:02:03,075 --> 00:02:09,330
Treinamos por 230 períodos e 
chegamos a algo estranho.

41
00:02:09,330 --> 00:02:10,230
O quê?

42
00:02:11,295 --> 00:02:12,165
Isto aqui.

43
00:02:12,950 --> 00:02:17,240
Esse triângulo é um
indicador de sobreajuste.

44
00:02:17,240 --> 00:02:19,990
Realmente não há dados ali.

45
00:02:19,990 --> 00:02:25,045
Portanto, é uma explicação plausível,
e o modelo,

46
00:02:25,045 --> 00:02:28,530
não estamos tentando torná-lo
mais simples do que precisa ser.

47
00:02:28,530 --> 00:02:31,605
Então, isso segue
e coloca itens ali.

48
00:02:33,675 --> 00:02:34,820
Uma das razões disso

49
00:02:34,820 --> 00:02:37,940
é porque estamos permitindo
o sobreajuste ao modelo.

50
00:02:37,940 --> 00:02:40,580
E uma forma de permitir isso

51
00:02:40,580 --> 00:02:43,205
é dar a ele os mesmos dados
de várias maneiras

52
00:02:43,205 --> 00:02:46,705
O que acontece se eu desligar o x1x2.

53
00:02:46,705 --> 00:02:49,085
Então, neste ponto, você só tem x1,

54
00:02:49,085 --> 00:02:51,275
x2, x1 ao quadrado e x2 ao quadrado.

55
00:02:51,275 --> 00:02:55,280
Vou reiniciar isso e, neste ponto,

56
00:02:55,280 --> 00:02:58,115
notarei novamente que há

57
00:02:58,115 --> 00:03:04,935
esse limite estranho que aparece
no estágio inicial do treinamento.

58
00:03:04,935 --> 00:03:08,185
Vamos fazer isso de novo.
Vamos interromper isso

59
00:03:08,185 --> 00:03:11,660
e interrompemos por volta de 200 períodos.
Então, lá vamos nós.

60
00:03:11,660 --> 00:03:16,520
Em 200 períodos, e novamente você
vê que o limite não é grande,

61
00:03:16,520 --> 00:03:19,140
há algo branco e estranho aqui.

62
00:03:20,930 --> 00:03:24,650
Novamente, porque temos esses
atributos extras, x1 e x2.

63
00:03:24,650 --> 00:03:27,170
O que acontece se tirarmos x1 e x2?

64
00:03:27,170 --> 00:03:31,340
Agora só temos os dados brutos,
x1 e x2 sozinhos.

65
00:03:31,340 --> 00:03:36,890
Vou fazer isso, começar e parar,
novamente, por volta de 200 períodos.

66
00:03:38,060 --> 00:03:42,015
E você percebe que agora é perfeito.

67
00:03:42,015 --> 00:03:46,840
Eu só tenho essa linha. E isto é
algo para você estar ciente:

68
00:03:46,840 --> 00:03:50,140
você pode ter muito de algo bom,

69
00:03:50,140 --> 00:03:56,150
que cruzamentos de atributos são
uma tentação para o modelo sobreajustar.

70
00:03:56,150 --> 00:03:58,900
Mas nós também notamos

71
00:03:58,900 --> 00:04:02,590
que se você treinar por
um tempo muito longo,

72
00:04:02,590 --> 00:04:05,260
só vamos tirar isso,
isso é com o que ele começou,

73
00:04:05,260 --> 00:04:08,685
se treinarmos por muito tempo,

74
00:04:08,685 --> 00:04:11,215
isso tende a melhorar.

75
00:04:11,215 --> 00:04:12,870
Mas, ainda assim,

76
00:04:12,870 --> 00:04:18,860
por ser um sobreajuste, é o motivo
de haver esse limite curvo.

77
00:04:18,860 --> 00:04:22,210
Este é outro sintoma de que as
coisas estão sobreajustadas.

78
00:04:22,210 --> 00:04:25,850
Então, se treinarmos por muito tempo,

79
00:04:25,850 --> 00:04:27,590
isso vai embora,

80
00:04:27,590 --> 00:04:30,860
esse artefato no canto inferior
esquerdo desaparece,

81
00:04:30,860 --> 00:04:34,910
mas ainda temos esse limite curvo e a
razão pela qual você tem

82
00:04:34,910 --> 00:04:36,830
um limite curvo em vez de

83
00:04:36,830 --> 00:04:40,490
uma linha reta, que sabemos que é o
modelo efetivo mais simples,

84
00:04:40,490 --> 00:04:43,640
é porque demos ao modelo muitos
graus de liberdade.

85
00:04:43,640 --> 00:04:45,955
Para ser franco, se você olhar para isso,

86
00:04:45,955 --> 00:04:51,710
os pesos de x1 e x2 são mais altos do que
os de qualquer um dos outros três.

87
00:04:51,710 --> 00:04:55,395
Mas, x1x2, o
cruzamento de atributo,

88
00:04:55,395 --> 00:05:00,290
recebe um peso e,
por receber um peso,

89
00:05:00,290 --> 00:05:02,865
ele pode atrapalhar as coisas.

90
00:05:03,705 --> 00:05:08,380
Surpreendentemente, o limite de decisão
do modelo parece meio doido.

91
00:05:08,380 --> 00:05:13,420
Em particular, há essa região no canto
inferior esquerdo que está apontando

92
00:05:13,420 --> 00:05:18,685
para o azul, mesmo que não haja suporte
visível para isso nos dados.

93
00:05:18,685 --> 00:05:21,990
O TensorFlow Playground usa
um ponto de partida aleatório,

94
00:05:21,990 --> 00:05:23,920
então o resultado pode ser diferente.

95
00:05:23,920 --> 00:05:26,720
É por isso que eu coloquei o que
eu tenho como um cenário.

96
00:05:26,720 --> 00:05:29,600
Você pode ter conseguido
algo ligeiramente diferente.

97
00:05:30,790 --> 00:05:36,940
Observe uma espessura relativa das
cinco linhas que vão da entrada à saída.

98
00:05:36,940 --> 00:05:41,605
Essas linhas mostram os
pesos relativos dos cinco atributos.

99
00:05:41,605 --> 00:05:45,385
As linhas que emanam de x1 e x2

100
00:05:45,385 --> 00:05:49,270
são muito mais grossas do que as que
vêm dos cruzamentos de atributos.

101
00:05:49,270 --> 00:05:52,765
Portanto, os cruzamentos de
atributos estão contribuindo

102
00:05:52,765 --> 00:05:57,920
menos para o modelo do que os
atributos normais não cruzados.

103
00:05:57,920 --> 00:06:03,070
Mas contribuem o suficiente para
bagunçar uma generalização

104
00:06:03,070 --> 00:06:06,490
E se removermos completamente
o cruzamento de atributos?

105
00:06:06,490 --> 00:06:09,685
Em outras palavras,
usar apenas os dados brutos.

106
00:06:09,685 --> 00:06:12,740
Remover todos os cruzamentos de atributos

107
00:06:12,740 --> 00:06:15,280
oferece a você um modelo mais sensato.

108
00:06:15,280 --> 00:06:19,630
Não há mais um limite curvo
sugerindo um sobreajuste.

109
00:06:19,630 --> 00:06:24,160
Após mil iterações, a perda
de teste será

110
00:06:24,160 --> 00:06:28,510
um valor ligeiramente menor do que quando
os cruzamentos de atributos foram usados.

111
00:06:28,510 --> 00:06:32,650
Mesmo que os resultados possam variar
um pouco, dependendo do conjunto de dados.

112
00:06:32,650 --> 00:06:38,590
Os dados neste exercício são
basicamente dados lineares mais ruído.

113
00:06:38,590 --> 00:06:43,810
Se usarmos um modelo muito
complicado para dados tão simples,

114
00:06:43,810 --> 00:06:47,020
um modelo com muitos
cruzamentos de atributos,

115
00:06:47,020 --> 00:06:51,655
daremos a oportunidade de ajustar o
ruído nos dados de treinamento.

116
00:06:51,655 --> 00:06:55,210
Você pode diagnosticar
isso observando como

117
00:06:55,210 --> 00:06:59,595
o modelo se comporta em dados
de testes independentes.

118
00:07:00,195 --> 00:07:02,910
Aliás, e vamos falar sobre regularização

119
00:07:02,910 --> 00:07:06,250
mais adiante no curso
em arte e ciência do ML,

120
00:07:06,250 --> 00:07:12,720
alías, isso explica por que a
regularização de L1 pode ser algo tão bom.

121
00:07:12,720 --> 00:07:15,740
O que a regularização L1 faz é

122
00:07:15,740 --> 00:07:18,965
zerar o peso de um atributo,
se necessário.

123
00:07:18,965 --> 00:07:25,220
Em outras palavras, o impacto da
regularização L1 é remover atributos.