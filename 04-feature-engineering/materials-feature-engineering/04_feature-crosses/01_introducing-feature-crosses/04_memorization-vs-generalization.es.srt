1
00:00:00,490 --> 00:00:03,690
¿Y si discretizo el eje x1
mediante el trazado

2
00:00:03,690 --> 00:00:07,845
no solo de una línea blanca
sino de varias líneas negras?

3
00:00:07,845 --> 00:00:13,475
Y si hacemos lo mismo con el eje x2
con el trazado de varias líneas blancas.

4
00:00:13,475 --> 00:00:18,810
Hemos discretizado el eje x1 y el eje x2.

5
00:00:19,370 --> 00:00:21,180
Cuando trazamos dos líneas blancas

6
00:00:21,180 --> 00:00:23,645
obtuvimos cuatro cuadrantes.

7
00:00:23,645 --> 00:00:24,885
¿Y ahora?

8
00:00:25,285 --> 00:00:28,845
Si tenemos m líneas verticales
y n líneas horizontales

9
00:00:28,845 --> 00:00:34,185
terminaremos con m más 1
por n más 1 celdas de cuadrícula.

10
00:00:34,785 --> 00:00:40,565
Ahora, consideremos cómo se vería esto
si discretizamos x1 y x2

11
00:00:40,565 --> 00:00:42,165
y luego multiplicamos.

12
00:00:42,995 --> 00:00:46,870
¿Recuerdan este diagrama
en el que dividimos el espacio

13
00:00:46,870 --> 00:00:48,720
de entrada en cuadrantes?

14
00:00:48,720 --> 00:00:53,895
Básicamente, podemos hacer una predicción
diferente para cada uno de los cuadrantes.

15
00:00:54,355 --> 00:00:56,840
¿Y qué sobre este cuadro verde?

16
00:00:56,840 --> 00:01:00,020
¿Cuál será nuestra predicción
para ese cuadro?

17
00:01:00,430 --> 00:01:01,730
Amarillo, ¿verdad?

18
00:01:02,810 --> 00:01:04,280
¿Y ahora?

19
00:01:04,280 --> 00:01:07,695
Azul, pero hay un poco
de amarillo también.

20
00:01:07,695 --> 00:01:11,360
Contemos la cantidad
de puntos azules y la de amarillos

21
00:01:11,360 --> 00:01:14,685
y lo llamaremos 85% azul.

22
00:01:15,205 --> 00:01:18,795
¿Pueden ver ahora
cómo aparecen las probabilidades?

23
00:01:19,465 --> 00:01:20,860
¿Y ahora?

24
00:01:23,540 --> 00:01:26,925
Veamos por qué esto funciona bien
como modelo lineal.

25
00:01:26,925 --> 00:01:30,869
Cuando codifican de un solo 1
el primer conjunto de valores

26
00:01:30,869 --> 00:01:34,924
y, luego, codifican de un solo 1
el segundo conjunto

27
00:01:34,924 --> 00:01:37,720
y, luego, realizan una combinación
de atributos

28
00:01:37,720 --> 00:01:45,520
se quedan con un nodo que activa
puntos que caen en ese grupo.

29
00:01:46,670 --> 00:01:55,045
Piénsenlo, x3 será 1
solo si x1 = 1 y x2 = 1.

30
00:01:55,655 --> 00:01:59,110
Entonces, para cualquier punto
en el espacio de entrada

31
00:01:59,110 --> 00:02:02,180
solo un grupo se activa.

32
00:02:02,890 --> 00:02:06,100
Ahora, si toman estos valores
de atributos combinados

33
00:02:06,100 --> 00:02:08,940
y los alimentan a la regresión lineal

34
00:02:09,620 --> 00:02:12,315
¿cuál será el peso de w3?

35
00:02:13,855 --> 00:02:17,865
La relación de puntos azules
a amarillos en la cuadrícula de celdas

36
00:02:17,865 --> 00:02:20,025
que corresponden a x1 y x2.

37
00:02:20,025 --> 00:02:24,230
Por eso, la combinación
de atributos es tan poderosa.

38
00:02:24,230 --> 00:02:27,260
Básicamente,
se discretiza el espacio de entrada

39
00:02:27,260 --> 00:02:30,830
y se memoriza el conjunto
de datos de entrenamiento.

40
00:02:31,350 --> 00:02:34,620
¿Pero se dan cuenta de
por qué esto podría ser problemático?

41
00:02:35,240 --> 00:02:37,245
¿Y si no tienen suficientes datos?

42
00:02:37,785 --> 00:02:39,700
¿Qué aprenderá el modelo aquí?

43
00:02:40,380 --> 00:02:44,900
Aprenderá que la predicción
debe ser azul, ¿correcto?

44
00:02:45,970 --> 00:02:48,045
Hay formas de solucionar esto.

45
00:02:48,045 --> 00:02:51,330
No necesitan discretizar el espacio
de entrada en partes iguales.

46
00:02:51,330 --> 00:02:54,720
En vez, pueden usar cuadros
de diferentes tamaños

47
00:02:54,720 --> 00:02:58,920
y usar tamaños de cuadros
que estén vinculados a la entropía

48
00:02:58,920 --> 00:03:01,430
o el contenido
de la información en el cuadro.

49
00:03:01,430 --> 00:03:04,860
También pueden agrupar cuadros.

50
00:03:04,860 --> 00:03:06,540
Hay formas de superar el problema.

51
00:03:06,540 --> 00:03:10,445
Aun así, deben comprender
que la combinación de atributos

52
00:03:10,445 --> 00:03:12,835
se trata de la memorización

53
00:03:12,835 --> 00:03:17,140
que es lo opuesto a la generalización

54
00:03:17,140 --> 00:03:20,025
que es el objetivo 
del aprendizaje automático.

55
00:03:20,025 --> 00:03:22,920
Entonces, ¿deberían hacerlo?

56
00:03:22,920 --> 00:03:25,530
En un sistema de aprendizaje automático
del mundo real

57
00:03:25,530 --> 00:03:27,135
hay lugar para ambos.

58
00:03:27,135 --> 00:03:31,360
La memorización funciona bien
cuando tienen tantos datos

59
00:03:31,360 --> 00:03:35,090
que para cualquier celda única
de cuadrícula en su espacio de entrada

60
00:03:35,090 --> 00:03:38,490
la distribución de los datos
es estadísticamente significativa.

61
00:03:38,490 --> 00:03:41,980
Cuando ese es el caso, se puede memorizar.

62
00:03:41,980 --> 00:03:47,130
Básicamente, se está aprendiendo
la media para cada celda de la cuadrícula.

63
00:03:47,680 --> 00:03:52,770
Por supuesto, el aprendizaje profundo
requiere de muchos datos para este espacio

64
00:03:52,770 --> 00:03:57,040
ya sea que deseen combinar los atributos
o usar muchas capas

65
00:03:57,040 --> 00:03:58,755
necesitan muchos datos.

66
00:03:59,435 --> 00:04:02,740
A propósito, si conocen el aprendizaje
automático tradicional

67
00:04:02,740 --> 00:04:06,055
es posible que no hayan oído
mucho sobre la combinación de atributos.

68
00:04:06,055 --> 00:04:10,560
Una de las razones puede ser
porque la combinación de atributos

69
00:04:10,560 --> 00:04:14,520
memoriza y solo funciona
en grandes conjuntos de datos.

70
00:04:15,300 --> 00:04:19,500
Pero verán que las combinaciones
de atributos son muy útiles

71
00:04:19,500 --> 00:04:23,320
en los conjuntos de datos reales.
Mientras más grandes los datos

72
00:04:23,320 --> 00:04:25,745
más pequeños se pueden hacer los cuadros

73
00:04:25,745 --> 00:04:28,465
y se puede memorizar con más detalle.

74
00:04:28,465 --> 00:04:32,000
La combinación de atributos
es una poderosa técnica

75
00:04:32,000 --> 00:04:34,840
de procesamiento previo
en grandes conjuntos de datos.