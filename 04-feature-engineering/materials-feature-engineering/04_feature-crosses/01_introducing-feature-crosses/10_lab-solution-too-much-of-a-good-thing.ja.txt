TensorFlow Playground
を開いています このようなデータセットがあります 青い点が右上にあり オレンジの点が左下にあり ここに分割線を引いて
これらの2つを区切ります そのために入力として X1、X2、X1の2乗、X2の2乗、 それに「X1掛けるX2」があります まずこの中で
生の入力はどれですか？ また作成された特徴はどれですか？ X1とX2が生の入力です X1の2乗、X2の2乗、 X1X2は 生の入力X1とX2から作成した特徴です どれが特徴クロスでしょうか？ X1X2は明らかに特徴クロスです でももう少し考えると X1の2乗も特徴クロス つまり 自己クロスです 自己結合と言ってもいいでしょう X1とX1を掛けると
X1の2乗になります 1つの考え方として 生の2つの入力X1とX2があり 3つの特徴クロス X1の2乗、X2の2乗、X1X2があります でもこれは単なる用語の問題です X1の2乗やX2の2乗を 特徴クロスではなく
入力の変換と呼んでも問題ありません 5つの入力を持つモデルを これからトレーニングしましょう 再生ボタンを押してトレーニングを開始すると おかしなことが起きていますね ここの左下隅が 青くなっています しばらくすると消えますが このオプションがなかったら
どうでしょう もう一度します どれくらいトレーニングするのか
わかりませんが ここまできたら 230エポックのトレーニングです
長いですね 230エポックのトレーニングで
何かがおかしくなりました 何でしょう？
ここです この三角形は過剰適合を示唆しています ここには実際に
データがありませんから そう考えるのが妥当です モデルを十分にシンプルにしていないから ここに何か現れたのです これが起こった理由の1つは
モデルの過剰適合を許容しているからです モデルが過剰適合する1つの原因は 同じデータを複数の方法で与えることです X1X2をオフにしたらどうでしょう この時点で X1、X2、 X1の2乗、X2の2乗だけになりました やり直してみると この時点でまたこの変な境界が トレーニングの初期段階で発生します もう一度やってみると ここで止めたら
約200エポックです 200エポックの時点で このとおり
境界がうまくいきません 白い部分がここにあって変ですね 余分な特徴X1とX2があるからです X1とX2を取り除くとどうですか？ これで生データ
X1とX2だけになりました これで開始して
200エポックあたりで停止します 今度は完璧ですね この線だけです
これでわかるとおり 良いものが多すぎてもだめです 特徴クロスではモデルの
過剰適合が起こりやすいです 他にも気づかなかった
ことがあります とても長い時間トレーニングすると これはオフにして最初に戻し 長い時間トレーニングすると
基本的に良くなりますが それでも過剰適合が起こるので この境界が曲線になります これも過剰適合の症状です 長い間トレーニングすると こちらは消えます この左下の生成物は
なくなりましたが まだ曲線の境界があります 曲線の境界の代わりに 直線という最も単純で
効率的なモデルにならない理由は モデルにかなりの自由を与えたからです これをご覧ください X1とX2の重みが
他の3つよりも高くなっています しかし特徴クロスX1 x X2には重みがあり この重みのせいで混沌とする
可能性があります 驚くことに モデルの決定境界が
おかしくなっています 特に左下のこの領域に
青がわずかに見えますが データでは点がまったく見えません Playgroundはランダムな開始点を使用するので
結果は異なるかもしれません これは私の結果を表示したものです 皆様は少し違う結果に
なったかもしれません 入力から出力に走る5本の線の
相対的な太さに注目してください これらの線は5つの特徴の
相対的な重みを示します X1とX2から出る線は 他の特徴クロスから出る線より
太くなっています この特徴クロスは 普通のクロスしていない
特徴よりも影響度が低いですが 一般化をおかしくする程度の
影響を与えます 特徴クロスを完全に削除したら
どうでしょう？ つまり生データだけを使うのです 特徴クロスをすべて削除すると より合理的なモデルになり 過剰適合を示唆する
曲線の境界もなくなりました 1,000回反復した後の
テスト損失の値は 特徴クロスを使ったときより
少し低くなるはずです 結果はデータセットによって
少し異なるかもしれません この演習のデータは
線形データとノイズです 複雑すぎるモデルを このように
シンプルなデータに使った場合や モデルの特徴クロスが多すぎる場合には トレーニングデータの
ノイズに適合する可能性があります これを診断するには 通常 独立したテストデータでモデルが
どのように動作するか確認できます ちなみに正則化について Art and Science of Machine Learning
で説明します L1正則化がなぜ優れているかを説明します L1正則化は 必要に応じて特徴の重みをゼロにします つまりL1正則化には
特徴を取り除く効果があります