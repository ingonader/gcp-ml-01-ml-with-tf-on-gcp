1
00:00:00,000 --> 00:00:03,690
What if I discretize the x1 axis by drawing

2
00:00:03,690 --> 00:00:07,845
not just one white line but lots of these black lines?

3
00:00:07,845 --> 00:00:13,245
And we do the same thing for the x2 axis by drawing a whole bunch of black lines.

4
00:00:13,245 --> 00:00:18,810
Now, we have discretized the x1 axis and the x2 axis.

5
00:00:18,810 --> 00:00:21,180
When we drew two white lines,

6
00:00:21,180 --> 00:00:23,265
we ended up with four quadrants.

7
00:00:23,265 --> 00:00:24,885
So what about now?

8
00:00:24,885 --> 00:00:28,845
If I have m vertical lines and n horizontal lines,

9
00:00:28,845 --> 00:00:34,185
we will end up with m plus one times n plus one grid cells, right?

10
00:00:34,185 --> 00:00:42,165
Now, let's consider what this looks like when we discretize x1 and x2 and then multiply.

11
00:00:42,165 --> 00:00:48,390
Now, remember this diagram that we had when we divided the input space into quadrants.

12
00:00:48,390 --> 00:00:53,895
Essentially, we get to make a different prediction for each of the quadrants.

13
00:00:53,895 --> 00:00:56,400
So what about this green box?

14
00:00:56,400 --> 00:00:59,430
What is going to be your prediction for that box?

15
00:00:59,430 --> 00:01:01,510
Yellow, right?

16
00:01:01,510 --> 00:01:04,280
How about now?

17
00:01:04,280 --> 00:01:07,425
Blue, but there's a hint of yellow, too.

18
00:01:07,425 --> 00:01:10,500
Let's count the number of blue points and the number of yellow

19
00:01:10,500 --> 00:01:14,685
points and call it 85 percent blue.

20
00:01:14,685 --> 00:01:18,795
You see now how the probabilities are coming in.

21
00:01:18,795 --> 00:01:22,470
What about now?

22
00:01:22,470 --> 00:01:26,925
Anyway, let's see why this works well as a linear model.

23
00:01:26,925 --> 00:01:30,869
When you one hot and cold the first set of values,

24
00:01:30,869 --> 00:01:34,924
and then you one hot and cold the second set of values,

25
00:01:34,924 --> 00:01:37,200
and then you feature cross them,

26
00:01:37,200 --> 00:01:46,050
you're essentially left with one node that fires for points that fall into that bucket.

27
00:01:46,050 --> 00:01:55,335
So think about it, the x3 will be one only if x1 equals one and x2 equals one.

28
00:01:55,335 --> 00:01:58,750
So for any point in the input space,

29
00:01:58,750 --> 00:02:02,180
only one bucket fires.

30
00:02:02,180 --> 00:02:08,940
Now, if you take these feature crossed values and feed them into a linear regression,

31
00:02:08,940 --> 00:02:12,315
what does the wait w3 have to be?

32
00:02:12,315 --> 00:02:19,805
Yup, the ratio of blue dots to yellow dots in the grid cell corresponding to x1 and x2.

33
00:02:19,805 --> 00:02:23,970
So that's why a feature cross is so powerful.

34
00:02:23,970 --> 00:02:30,830
You essentially discretize the input space and memorize the training data set.

35
00:02:30,830 --> 00:02:33,560
But can you see how this could be problematic?

36
00:02:33,560 --> 00:02:37,245
What if you don't have enough data?

37
00:02:37,245 --> 00:02:39,300
What's a model going to learn here?

38
00:02:39,300 --> 00:02:44,460
It's going to learn that the prediction has to be blue, is that true?

39
00:02:44,460 --> 00:02:47,655
Well, there are ways around this.

40
00:02:47,655 --> 00:02:51,330
You don't have to discretize the input space equally.

41
00:02:51,330 --> 00:02:54,720
Instead, you can use different sized boxes,

42
00:02:54,720 --> 00:03:01,110
and use box sizes that are tied to the entropy or the information content in the box.

43
00:03:01,110 --> 00:03:04,860
You can also group or cluster boxes together.

44
00:03:04,860 --> 00:03:06,630
So there are ways around this.

45
00:03:06,630 --> 00:03:12,765
Still, you should realize that feature crosses are about memorization,

46
00:03:12,765 --> 00:03:15,660
and memorization is the opposite of

47
00:03:15,660 --> 00:03:19,575
generalization which is what machine learning aims to do.

48
00:03:19,575 --> 00:03:22,440
So, should you do this?

49
00:03:22,440 --> 00:03:25,530
In a real world machine learning system,

50
00:03:25,530 --> 00:03:26,985
there is place for both.

51
00:03:26,985 --> 00:03:29,550
Memorization works when you have

52
00:03:29,550 --> 00:03:35,090
so much data that for any single grid cell in your input space,

53
00:03:35,090 --> 00:03:38,240
the distribution of data is statistically significant.

54
00:03:38,240 --> 00:03:41,460
When that's the case, you can memorize.

55
00:03:41,460 --> 00:03:47,130
You're essentially just learning the mean for every grid cell.

56
00:03:47,130 --> 00:03:52,770
Of course, deep learning also needs lots of data to play in this space.

57
00:03:52,770 --> 00:03:56,670
Whether you want to feature cross or you want to use many layers,

58
00:03:56,670 --> 00:03:58,755
you need lots of data.

59
00:03:58,755 --> 00:04:02,880
Incidentally, if you're familiar with traditional machine learning,

60
00:04:02,880 --> 00:04:05,415
you may not have heard much about feature crosses.

61
00:04:05,415 --> 00:04:09,540
The fact that feature crosses memorize and only work on

62
00:04:09,540 --> 00:04:14,520
larger data sets is one reason that you may not have heard much about it.

63
00:04:14,520 --> 00:04:21,000
But you will find feature crosses extremely useful in real-world data sets.

64
00:04:21,000 --> 00:04:22,950
The larger your data,

65
00:04:22,950 --> 00:04:25,345
the smaller you can make your boxes,

66
00:04:25,345 --> 00:04:28,115
and the more finely you can memorize.

67
00:04:28,115 --> 00:04:35,160
So, feature crosses or a powerful pre-processing technique on large data sets.