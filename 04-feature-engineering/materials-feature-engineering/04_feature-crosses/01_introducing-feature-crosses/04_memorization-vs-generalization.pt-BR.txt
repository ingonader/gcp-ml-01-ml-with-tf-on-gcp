E se eu discretizar o eixo x1 desenhando não apenas uma linha branca, mas
muitas dessas linhas pretas? E fazemos o mesmo para o eixo x2
desenhando um monte de linhas pretas. Agora, nós discretizamos os eixos
x1 e x2. Quando desenhamos duas linhas brancas, temos quatro quadrantes. E agora? Se eu tiver m linhas verticais e
n linhas horizontais, acabaremos com m + 1 x n + 1
célula da grade, certo? Vamos considerar como isso fica quando
discretizamos x1 e x2 e multiplicamos. Lembre-se do diagrama que tínhamos ao
dividir o espaço de entrada em quadrantes. Basicamente, nós podemos fazer uma
previsão diferente para cada quadrante. Então, e esta caixa verde? Qual será a previsão para essa caixa? Amarelo, certo? Que tal agora? Azul, mas também há um toque amarelo. Vamos contar o número de pontos azuis e o
número de pontos amarelos e chamá-lo de 85% azul. Você vê agora como as
probabilidades estão chegando. E agora? De qualquer forma, vamos ver porque isso
funciona bem como um modelo linear. Quando você aplica uma codificação
one-hot no primeiro conjunto de valores e, em seguida, aplica uma codificação
one-hot no segundo conjunto de valores, e, então, você aplica o
cruzamento de atributos basicamente deixa com um node que é
acionado para pontos nesse intervalo. Então pense nisso, o x3 será 1 só se x1
for igual a 1 e x2 for igual a 1. Portanto, para qualquer ponto
no espaço de entrada, apenas um intervalo é disparado. Se você pegar esses valores do cruzamento
e alimentá-los na regressão linear, o que o w3 de espera precisa ser? Sim, a proporção de azuis para amarelos na
célula da grade correspondente a x1 e x2. É por isso que um cruzamento de
atributo é tão poderoso. Você discretiza o espaço de entrada e
memoriza o conjunto de dados de treino. Mas você enxerga como isso
pode ser problemático? E se você não tiver dados suficientes? O que um modelo vai aprender aqui? Vai aprender que a previsão precisa
ser azul, verdade? Bem, existem maneiras de contornar isso. Você não tem que discretizar o
espaço de entrada igualmente. Em vez disso, você pode usar caixas
de tamanhos diferentes e usar tamanhos vinculados à entropia
ou ao conteúdo da informação na caixa. Você também pode
agrupar as caixas. Então, há maneiras de contornar isso. Ainda assim, perceba que os cruzamentos
de atributos tratam sobre memorização, e memorização é o oposto da generalização, que é o que o aprendizado
de máquina pretende fazer. Então, você deveria fazer isso? Em um sistema de aprendizado
de máquina do mundo real, há lugar para ambos. A memorização funciona quando você tem tantos dados que, para qualquer célula de
grade única no espaço de entrada, a distribuição de dados é
estatisticamente significativa. Quando esse é o caso, você pode memorizar. Você está apenas aprendendo a média
para cada célula da grade. A aprendizagem profunda também precisa de
muitos dados para atuar nesse espaço. Se você quer usar cruzamento de
atributos ou usar várias camadas, precisa de muitos dados. A propósito, se você estiver familiarizado
com o aprendizado de máquina tradicional, talvez não tenha ouvido falar
sobre cruzamentos de atributos. O fato de que os cruzamentos de atributo
memorizam e só funcionam em conjuntos de dados maiores é uma das razões pelas
quais você pode não ter ouvido falar. Mas eles serão extremamente úteis em
conjuntos de dados do mundo real. Quanto maiores forem os dados, menores serão as caixas e mais você poderá memorizá-las. O cruzamento é uma ótima técnica de
pré-processamento para grandes dados.