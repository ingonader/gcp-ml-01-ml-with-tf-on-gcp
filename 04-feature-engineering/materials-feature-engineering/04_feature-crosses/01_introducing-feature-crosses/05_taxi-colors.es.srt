1
00:00:00,200 --> 00:00:03,130
Imaginen que están creando
un modelo de aprendizaje automático

2
00:00:03,130 --> 00:00:07,710
que determina si un automóvil es un taxi.

3
00:00:08,200 --> 00:00:12,090
Sabemos que los autos blancos en Roma
y los amarillos en Nueva York

4
00:00:12,090 --> 00:00:13,710
suelen ser taxis.

5
00:00:13,710 --> 00:00:16,660
Pero queremos
que nuestro modelo de AA lo aprenda

6
00:00:16,660 --> 00:00:21,020
a partir de un conjunto de datos
que consiste en registros de automóviles.

7
00:00:21,020 --> 00:00:24,055
Supongamos que sus datos
de entrada se ven así

8
00:00:24,055 --> 00:00:28,125
rojo, Roma; blanco, Roma, etc.

9
00:00:28,125 --> 00:00:30,860
y que las etiquetas indican
si es o no un taxi.

10
00:00:30,860 --> 00:00:33,470
Básicamente, el color del auto y la ciudad

11
00:00:33,470 --> 00:00:39,060
son sus dos atributos de entrada
y deben usarlos en su modelo lineal

12
00:00:39,060 --> 00:00:42,810
para predecir si el auto es un taxi.

13
00:00:42,810 --> 00:00:44,430
¿Cómo lo harían?

14
00:00:45,260 --> 00:00:46,820
Toman la primera entrada

15
00:00:46,820 --> 00:00:50,520
el color del auto,
y realizan una codificación de un solo 1.

16
00:00:50,520 --> 00:00:52,110
Toman la segunda entrada

17
00:00:52,110 --> 00:00:55,380
el nombre de la ciudad,
y realizan una codificación de un solo 1.

18
00:00:55,380 --> 00:01:00,150
Luego, las envían directo
al modelo lineal.

19
00:01:00,850 --> 00:01:04,519
Ahora, supongamos
que asignan un peso de 0.8

20
00:01:04,519 --> 00:01:07,525
a los amarillos
porque el 80% de ellos

21
00:01:07,525 --> 00:01:09,905
en su conjunto de datos
de entrenamiento son taxis.

22
00:01:09,905 --> 00:01:12,950
Ahora, w3 es 0.8.

23
00:01:12,950 --> 00:01:15,945
Por supuesto,
no le asignarán un peso de 0.8.

24
00:01:15,945 --> 00:01:18,910
Este peso se aprenderá
mediante el descenso del gradiente

25
00:01:18,910 --> 00:01:22,115
es lo que el descenso hará.

26
00:01:22,115 --> 00:01:26,165
Desafortunadamente, el peso de 0.8
es verdad para los autos amarillos

27
00:01:26,165 --> 00:01:29,590
en todas las ciudades,
no solo en Nueva York.

28
00:01:30,170 --> 00:01:31,890
¿Cómo lo arreglarían?

29
00:01:32,810 --> 00:01:35,145
¿Le asignarían un peso alto a Nueva York?

30
00:01:35,815 --> 00:01:37,275
Eso no funciona.

31
00:01:37,275 --> 00:01:41,220
Todos los autos en Nueva York
tendrían ese peso alto.

32
00:01:41,960 --> 00:01:44,040
¿Ven el problema?

33
00:01:47,130 --> 00:01:50,565
¿Qué ocurre si agregan
una combinación de atributos?

34
00:01:50,565 --> 00:01:53,770
Ahora, tenemos un nodo
de entrada que corresponde

35
00:01:53,770 --> 00:01:58,119
a los autos rojos en Nueva York
y otro a los amarillos

36
00:01:58,119 --> 00:02:00,590
un tercero a los blancos

37
00:02:00,590 --> 00:02:03,010
un cuarto a los verdes

38
00:02:03,010 --> 00:02:05,525
y lo mismo para los automóviles en Roma.

39
00:02:06,065 --> 00:02:10,180
Y ahora, el modelo puede aprender
bastante rápido que los autos amarillos

40
00:02:10,180 --> 00:02:13,895
en Nueva York
y los blancos en Roma suelen ser taxis

41
00:02:13,895 --> 00:02:16,945
y les asignamos un peso alto
a esos dos nodos.

42
00:02:17,855 --> 00:02:20,310
Todo lo demás, será cero.

43
00:02:20,310 --> 00:02:21,975
Problema resuelto.

44
00:02:21,975 --> 00:02:26,770
Es por esto por lo que las combinaciones
de atributos son tan poderosas.

45
00:02:29,320 --> 00:02:33,740
Las combinaciones de atributos les dan
mucho poder a los modelos lineales.

46
00:02:33,740 --> 00:02:39,030
Su uso, además de los datos masivos,
es una estrategia

47
00:02:39,030 --> 00:02:43,835
muy eficiente para aprender
espacios muy complejos.

48
00:02:44,585 --> 00:02:47,970
Las redes neuronales ofrecen
otra manera de aprender

49
00:02:47,970 --> 00:02:49,930
espacios muy complejos.

50
00:02:49,930 --> 00:02:51,405
Las combinaciones de atributos

51
00:02:51,405 --> 00:02:54,595
permiten que se sigan usando
los modelos lineales.

52
00:02:54,595 --> 00:03:01,005
Sin ellas, la expresividad
de los modelos lineales sería limitada.

53
00:03:01,005 --> 00:03:04,879
Con ellas, una vez que se tiene
un conjunto de datos masivo

54
00:03:04,879 --> 00:03:07,910
un modelo lineal puede aprender
de todos los recovecos

55
00:03:07,910 --> 00:03:09,370
del espacio de entrada.

56
00:03:09,370 --> 00:03:13,425
Las combinaciones de atributos
permiten a un modelo lineal memorizar

57
00:03:13,425 --> 00:03:15,085
grandes conjuntos de datos.

58
00:03:15,085 --> 00:03:19,269
La idea es que pueden asignar un peso
a cada combinación

59
00:03:19,269 --> 00:03:23,580
y de este modo el modelo aprende
acerca de las combinaciones de atributos.

60
00:03:23,580 --> 00:03:26,035
Aunque sea un modelo lineal

61
00:03:26,035 --> 00:03:32,870
la relación subyacente
entre entradas y salidas es no lineal.

62
00:03:34,500 --> 00:03:39,615
¿Por qué nos preocupa tanto
que los modelos lineales funcionen bien?

63
00:03:40,185 --> 00:03:42,065
Acuérdense del curso anterior.

64
00:03:42,065 --> 00:03:47,005
Hablamos sobre los problemas
convexos y no convexos.

65
00:03:47,465 --> 00:03:53,035
Las redes neuronales
con muchas capas son no convexas.

66
00:03:53,035 --> 00:03:57,855
Pero optimizar modelos lineales
es un problema convexo

67
00:03:57,855 --> 00:04:02,345
y los problemas convexos
son mucho más fáciles

68
00:04:02,345 --> 00:04:04,635
que los problemas no convexos.

69
00:04:05,375 --> 00:04:07,175
Durante mucho tiempo

70
00:04:07,175 --> 00:04:10,375
los modelos lineales dispersos
eran los únicos algoritmos

71
00:04:10,375 --> 00:04:14,615
que podíamos usar
y escalar a miles de millones

72
00:04:14,615 --> 00:04:18,485
de ejemplos de entrenamiento
y de atributos de entrada.

73
00:04:18,485 --> 00:04:23,270
Los predecesores de TensorFlow
en Google: seti, smartass, sybil

74
00:04:23,270 --> 00:04:27,090
eran clasificadores a escala masiva.

75
00:04:27,090 --> 00:04:29,820
Esto cambió en los últimos años

76
00:04:29,820 --> 00:04:34,820
y las redes neuronales ahora pueden
manejar datos a escalas masivas

77
00:04:34,820 --> 00:04:38,400
por lo general,
con la ayuda de GPU y TPU

78
00:04:38,400 --> 00:04:43,940
pero los modelos lineales dispersos
siguen siendo una opción rápida y barata.

79
00:04:44,390 --> 00:04:48,425
Usar modelos lineales dispersos
como procesadores previos

80
00:04:48,425 --> 00:04:51,010
de sus atributos, a menudo significará

81
00:04:51,010 --> 00:04:54,220
que sus redes neuronales
convergerán mucho más rápido.