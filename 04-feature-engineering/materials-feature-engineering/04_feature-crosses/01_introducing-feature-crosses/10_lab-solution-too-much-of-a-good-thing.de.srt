1
00:00:00,000 --> 00:00:03,250
Wir befinden uns
hier im TensorFlow Playground.

2
00:00:03,250 --> 00:00:05,800
Wir haben 
einige Datasets, die so aussehen.

3
00:00:05,800 --> 00:00:08,830
Wir haben blaue Punkte oben rechts,

4
00:00:08,830 --> 00:00:11,640
orange Punkte unten links.

5
00:00:11,640 --> 00:00:17,930
Wir möchten eine Trennungslinie zeichnen,
die diese beiden Farben separiert.

6
00:00:17,930 --> 00:00:20,820
Dazu haben wir als Eingaben

7
00:00:20,820 --> 00:00:24,470
x1, x2, x1 zum Quadrat, x2 zum Quadrat

8
00:00:24,470 --> 00:00:26,550
und x1 mal x2.

9
00:00:26,550 --> 00:00:31,925
Zuerst müssen wir klären,
welche davon Roheingaben sind

10
00:00:31,925 --> 00:00:35,465
und welche erstellte Merkmale sind.

11
00:00:35,465 --> 00:00:38,555
x1 und x2 sind die Roheingaben.

12
00:00:38,555 --> 00:00:43,440
x1 zum Quadrat,
x2 zum Quadrat und x1x2 sind Merkmale,

13
00:00:43,440 --> 00:00:47,875
die wir aus den Roheingaben x1
und x2 erstellt haben.

14
00:00:47,875 --> 00:00:51,190
Welche sind Merkmalkreuzungen?

15
00:00:51,190 --> 00:00:55,005
Offensichtlich
ist x1x2 eine Merkmalkreuzung.

16
00:00:55,005 --> 00:00:58,290
Wenn Sie genauer hinsehen, erkennen Sie,

17
00:00:58,290 --> 00:01:01,535
dass x1 zum Quadrat
auch eine Merkmalkreuzung ist,

18
00:01:01,535 --> 00:01:03,405
eine Selbstkreuzung,

19
00:01:03,405 --> 00:01:05,265
eine Zusammenführung mit sich selbst.

20
00:01:05,265 --> 00:01:09,915
Sie kreuzen x1 mit x1,
um x1 zum Quadrat zu erhalten.

21
00:01:09,915 --> 00:01:13,980
Eine Betrachtungsweise wäre daher,
dass wir zwei Roheingaben x1 und x2 haben

22
00:01:13,980 --> 00:01:16,100
und die drei Merkmalkreuzungen

23
00:01:16,100 --> 00:01:18,680
x1 zum Quadrat, x2 zum Quadrat und x1x2.

24
00:01:18,680 --> 00:01:21,545
Das ist aber nur Terminologie.

25
00:01:21,545 --> 00:01:23,820
Sie können x1 Quadrat und x2 Quadrat

26
00:01:23,820 --> 00:01:27,397
stattdessen auch
als Transformation der Eingabe betrachten.

27
00:01:27,397 --> 00:01:28,635
Das ist kein Problem.

28
00:01:28,635 --> 00:01:31,155
Unser Modell hat also fünf Eingaben.

29
00:01:31,155 --> 00:01:32,490
Wir möchten es trainieren.

30
00:01:32,490 --> 00:01:33,900
Also machen wir das jetzt.

31
00:01:33,900 --> 00:01:37,350
Ich starte hier
und wir beginnen mit dem Training.

32
00:01:37,350 --> 00:01:40,590
Sehen Sie das Seltsame?

33
00:01:40,590 --> 00:01:45,300
Hier unten in der linken Ecke.

34
00:01:45,300 --> 00:01:47,910
Sehen Sie den blauen Bereich?

35
00:01:47,910 --> 00:01:50,282
Er ist nach einiger Zeit verschwunden,

36
00:01:50,282 --> 00:01:53,855
aber stellen Sie sich vor,
wir hätten diese Option nicht.

37
00:01:53,855 --> 00:01:55,590
Versuchen wir es noch einmal.

38
00:01:55,590 --> 00:01:58,365
Wir wissen nicht,
wie lange wir trainieren werden.

39
00:01:58,365 --> 00:02:02,067
Sagen wir,
wir haben bis 230 Epochen trainiert.

40
00:02:02,067 --> 00:02:03,075
Eine lange Zeit.

41
00:02:03,075 --> 00:02:09,330
Wir haben 230 Epochen trainiert
und erhalten etwas Seltsames.

42
00:02:09,330 --> 00:02:12,420
Was? Den Bereich hier.

43
00:02:12,420 --> 00:02:17,240
Das Dreieck
ist ein Anzeichen für Überanpassung.

44
00:02:17,240 --> 00:02:19,990
Es gibt dafür wirklich keine Daten.

45
00:02:19,990 --> 00:02:23,335
Es ist also eine plausible Erklärung.

46
00:02:23,335 --> 00:02:28,530
Wir versuchen nicht, das Modell
einfacher zu gestalten, als es sein muss.

47
00:02:28,530 --> 00:02:31,605
Es läuft und erstellt hier Werte.

48
00:02:31,605 --> 00:02:35,520
Einer der Gründe,
warum dies passiert, besteht darin,

49
00:02:35,520 --> 00:02:37,940
dass wir
dem Modell Überanpassung gestatten.

50
00:02:37,940 --> 00:02:39,760
Eine Möglichkeit, dies zu tun, ist,

51
00:02:39,760 --> 00:02:43,205
dem Modell dieselben Daten
auf unterschiedliche Weise zu übergeben,

52
00:02:43,205 --> 00:02:46,705
Was passiert, wenn ich x1x2 deaktiviere?

53
00:02:46,705 --> 00:02:49,085
Jetzt haben wir nur noch x1, x2,

54
00:02:49,085 --> 00:02:51,275
x1 zum Quadrat und x2 zum Quadrat.

55
00:02:51,275 --> 00:02:53,690
Ich starte noch einmal.

56
00:02:53,690 --> 00:03:05,015
Und wieder sehen Sie diese seltsame Grenze
in der frühen Trainingsphase.

57
00:03:05,015 --> 00:03:06,645
Noch einmal.

58
00:03:06,645 --> 00:03:10,582
Wir starten
und stoppen dann bei ca. 200 Epochen.

59
00:03:10,582 --> 00:03:11,660
Da sind wir.

60
00:03:11,660 --> 00:03:16,520
Wieder sehen Sie bei 200 Epochen,
dass die Grenze nicht gerade toll ist

61
00:03:16,520 --> 00:03:20,930
und das wir hier
diesen seltsamen weißen Bereich haben.

62
00:03:20,930 --> 00:03:24,650
Das liegt an den zusätzlichen Merkmalen
x1 zum Quadrat und x2 zum Quadrat.

63
00:03:24,650 --> 00:03:26,950
Was ist, wenn wir diese zwei deaktivieren?

64
00:03:26,950 --> 00:03:31,340
Nun haben wir nur die Rohdaten x1 und x2.

65
00:03:31,340 --> 00:03:36,890
Ich starte
und ich stoppe wieder bei ca. 200 Epochen.

66
00:03:36,890 --> 00:03:42,015
Sie sehen jetzt
ein ziemlich perfektes Modell.

67
00:03:42,015 --> 00:03:43,740
Wir haben nur diese Linie.

68
00:03:43,740 --> 00:03:50,140
Ihnen sollte also bewusst sein,
dass es auch zu viel des Guten gibt

69
00:03:50,140 --> 00:03:56,150
und dass Merkmalkreuzungen ein
Modell verleiten können, überanzupassen.

70
00:03:56,150 --> 00:03:58,900
Wir haben auch noch gesehen,

71
00:03:58,900 --> 00:04:02,590
was passiert,
wenn Sie sehr lange trainieren.

72
00:04:02,590 --> 00:04:05,260
Deaktivieren wir diese
und fangen wir wie eben an.

73
00:04:05,260 --> 00:04:08,685
Wenn wir sehr lange trainieren,

74
00:04:08,685 --> 00:04:13,670
scheint das Modell besser zu werden,

75
00:04:13,670 --> 00:04:18,860
doch bewirkt
die Überanpassung diese gebogene Grenze.

76
00:04:18,860 --> 00:04:21,740
Das ist
ein weiteres Symptom für Überanpassung.

77
00:04:21,740 --> 00:04:25,850
Wenn wir
für eine sehr lange Zeit trainieren,

78
00:04:25,850 --> 00:04:30,880
verschwindet
dieses Artefakt in der Ecke unten links,

79
00:04:30,880 --> 00:04:34,060
wir erhalten aber diese gebogene Grenze.

80
00:04:34,060 --> 00:04:39,180
Sie erhalten keine Gerade
für das einfachste effektive Modell,

81
00:04:39,180 --> 00:04:43,630
sondern eine Kurve, da wir
dem Modell viele Freiheiten gegeben haben.

82
00:04:43,640 --> 00:04:45,955
Wenn Sie sich dies ansehen,

83
00:04:45,955 --> 00:04:48,282
bemerken Sie,
dass die Gewichtungen von x1 und x2

84
00:04:48,282 --> 00:04:51,710
deutlich höher sind als die der anderen.

85
00:04:51,710 --> 00:04:55,395
Doch da die Merkmalkreuzung x1 mal x2

86
00:04:55,395 --> 00:05:00,290
eine gewisse Gewichtung erhält,

87
00:05:00,290 --> 00:05:02,875
kann sie für Chaos sorgen.

88
00:05:02,875 --> 00:05:08,380
Die Entscheidungsgrenze des Modells
sieht irgendwie seltsam aus.

89
00:05:08,380 --> 00:05:14,490
Insbesondere weist
diese Region unten links auf blau hin,

90
00:05:14,490 --> 00:05:18,685
auch wenn keine Daten
dies sichtbar unterstützen.

91
00:05:18,685 --> 00:05:21,990
TensorFlow Playground
verwendet einen zufälligen Startpunkt,

92
00:05:21,990 --> 00:05:23,920
Ihr Ergebnis könnte anders ausfallen.

93
00:05:23,920 --> 00:05:26,870
Daher habe ich
mein Ergebnis im Bild festgehalten.

94
00:05:26,870 --> 00:05:29,600
Sie haben vielleicht
etwas anderes erhalten.

95
00:05:30,940 --> 00:05:36,940
Beachten Sie die relative Dicke
der fünf Linien von Eingabe zu Ausgabe.

96
00:05:36,940 --> 00:05:41,605
Diese Linien zeigen
die relative Gewichtung der fünf Merkmale.

97
00:05:41,605 --> 00:05:47,105
Die Linien von x1
und x2 sind wesentlich dicker

98
00:05:47,105 --> 00:05:49,270
als die von den Merkmalkreuzungen.

99
00:05:49,270 --> 00:05:54,955
Die Merkmalkreuzungen tragen also
weitaus weniger zum Modell bei

100
00:05:54,955 --> 00:05:57,920
als die normalen 
nicht gekreuzten Merkmale,

101
00:05:57,920 --> 00:06:03,070
doch tragen sie genug bei,
um die Generalisierung zu stören.

102
00:06:03,070 --> 00:06:06,490
Was wäre, wenn wir
die Merkmalkreuzungen komplett entfernen?

103
00:06:06,490 --> 00:06:09,685
Wenn wir nur die Rohdaten verwenden?

104
00:06:09,685 --> 00:06:12,740
Wenn Sie alle Merkmalkreuzungen entfernen,

105
00:06:12,740 --> 00:06:15,280
erhalten Sie ein vernünftigeres Modell.

106
00:06:15,280 --> 00:06:19,630
Die gebogene Grenze, die auf Überanpassung
hinweist, ist nicht mehr vorhanden.

107
00:06:19,630 --> 00:06:25,770
Nach 1.000 Durchläufen
sollte der Testverlust etwas geringer sein

108
00:06:25,770 --> 00:06:28,510
als bei Verwendung der Merkmalkreuzungen.

109
00:06:28,510 --> 00:06:32,650
Ihre Ergebnisse können jedoch
abhängig vom Dataset ein wenig variieren.

110
00:06:32,650 --> 00:06:38,590
Die Daten in dieser Übung sind
im Grunde lineare Daten plus Rauschen.

111
00:06:38,590 --> 00:06:43,810
Wenn wir für solch einfache Daten
ein zu komplexes Modell verwenden,

112
00:06:43,810 --> 00:06:47,020
wenn wir ein Modell
mit zu vielen Merkmalkreuzungen verwenden,

113
00:06:47,020 --> 00:06:51,655
lassen wir zu, dass es sich
an das Rauschen in den Daten anpasst.

114
00:06:51,655 --> 00:06:54,230
Sie können dies häufig erkennen,

115
00:06:54,230 --> 00:06:59,605
indem Sie sich die Modellleistung
bei unabhängigen Testdaten ansehen.

116
00:06:59,605 --> 00:07:06,260
Wir behandeln Regularisierung später
im Kurs zu Kunst und Wissenschaft des ML,

117
00:07:06,260 --> 00:07:08,980
aber dies erklärt auch zufällig,

118
00:07:08,980 --> 00:07:12,720
warum L1-Regularisierung
so eine tolle Sache sein kann.

119
00:07:12,720 --> 00:07:18,970
L1-Regularisierung setzt die Gewichtung
eines Merkmals gegebenenfalls auf null.

120
00:07:18,970 --> 00:07:25,630
Anders gesagt kann
die L1-Regularisierung Merkmale entfernen.