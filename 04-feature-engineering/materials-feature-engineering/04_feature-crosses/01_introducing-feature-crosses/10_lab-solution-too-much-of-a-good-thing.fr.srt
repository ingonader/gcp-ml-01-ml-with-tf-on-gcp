1
00:00:00,000 --> 00:00:03,250
Nous sommes donc dans
TensorFlow Playground.

2
00:00:03,250 --> 00:00:05,820
Nous avons un ensemble de données
qui ressemble à ceci.

3
00:00:05,820 --> 00:00:08,830
Nous avons
les points bleus en haut à droite

4
00:00:08,830 --> 00:00:12,890
et les points orange en bas à gauche,
et nous essayons

5
00:00:12,890 --> 00:00:17,930
de tracer une ligne pour les séparer.

6
00:00:17,930 --> 00:00:19,490
Pour ce faire,

7
00:00:19,490 --> 00:00:21,570
nous avons les valeurs d'entrée

8
00:00:21,570 --> 00:00:24,435
x1, x2, x1², x2²

9
00:00:24,435 --> 00:00:26,550
et x1 fois x2.

10
00:00:26,550 --> 00:00:31,925
Pour commencer, lesquelles sont
des valeurs d'entrée brutes

11
00:00:31,925 --> 00:00:35,465
et lesquelles sont
des caractéristiques créées ?

12
00:00:35,465 --> 00:00:38,555
x1 et x2 sont les valeurs d'entrée brutes.

13
00:00:38,555 --> 00:00:42,270
x1², x2² et x1x2

14
00:00:42,270 --> 00:00:47,875
sont des caractéristiques créées à partir
des valeurs d'entrée brutes x1 et x2.

15
00:00:47,875 --> 00:00:51,190
Lesquelles sont des croisements
de caractéristiques ?

16
00:00:51,190 --> 00:00:55,005
x1x2 est évidemment
un croisement de caractéristiques,

17
00:00:55,005 --> 00:00:58,290
mais, si vous poussez un petit peu
le raisonnement, vous pouvez voir

18
00:00:58,290 --> 00:01:01,535
que x1² en est un aussi.

19
00:01:01,535 --> 00:01:03,405
La valeur est croisée avec elle-même.

20
00:01:03,405 --> 00:01:05,265
C'est un peu une jointure réflexive.

21
00:01:05,265 --> 00:01:09,915
Vous prenez x1 et x1,
et vous les croisez pour obtenir x1².

22
00:01:09,915 --> 00:01:13,980
Nous avons donc les deux valeurs
d'entrée brutes x1 et x2,

23
00:01:13,980 --> 00:01:16,510
et les trois croisements
de caractéristiques

24
00:01:16,510 --> 00:01:18,680
x1², x2² et x1x2.

25
00:01:18,680 --> 00:01:21,395
Cependant, ce ne sont que des termes.

26
00:01:21,395 --> 00:01:25,390
Vous pouvez appeler x1² et x2²
une transformation de la valeur d'entrée

27
00:01:25,390 --> 00:01:27,635
plutôt qu'un croisement
de caractéristiques.

28
00:01:27,635 --> 00:01:28,635
Aucun problème.

29
00:01:28,635 --> 00:01:31,205
Nous avons donc cinq valeurs d'entrée
pour notre modèle

30
00:01:31,205 --> 00:01:32,540
et nous voulons l'entraîner.

31
00:01:32,540 --> 00:01:33,900
Commençons.

32
00:01:33,900 --> 00:01:36,330
J'appuie sur le bouton de lecture

33
00:01:36,330 --> 00:01:40,600
pour démarrer l'entraînement. Je remarque
qu'il se produit quelque chose d'étrange.

34
00:01:40,600 --> 00:01:44,150
Vous voyez cette couleur bleue

35
00:01:44,150 --> 00:01:47,910
qui apparaît en bas à gauche ?

36
00:01:47,910 --> 00:01:53,855
Elle a fini par disparaître,
mais imaginez que ce ne soit pas le cas.

37
00:01:53,855 --> 00:01:55,590
Réessayons.

38
00:01:55,590 --> 00:01:58,405
Nous ne savons pas combien de temps
l'entraînement va durer.

39
00:01:58,405 --> 00:02:01,110
Disons que nous avons effectué
l'entraînement jusqu'ici,

40
00:02:01,110 --> 00:02:03,075
pendant 230 itérations.
C'est plutôt long.

41
00:02:03,075 --> 00:02:09,330
Suite à ces 230 itérations,
nous avons obtenu quelque chose d'étrange.

42
00:02:09,330 --> 00:02:12,420
Ce triangle ici.

43
00:02:12,420 --> 00:02:17,240
C'est un signe de surapprentissage.

44
00:02:17,240 --> 00:02:19,990
Il n'y a pas de données à cet endroit.

45
00:02:19,990 --> 00:02:25,045
C'est donc une explication plausible.

46
00:02:25,045 --> 00:02:28,530
Nous ne cherchons pas à simplifier
les choses plus que nécessaire.

47
00:02:28,530 --> 00:02:31,605
Le modèle a donc choisi
d'y mettre quelque chose.

48
00:02:31,605 --> 00:02:34,820
L'une des raisons de ce phénomène

49
00:02:34,820 --> 00:02:37,940
est que nous avons laissé
le modèle surapprendre.

50
00:02:37,940 --> 00:02:40,580
Le surapprentissage peut se produire

51
00:02:40,580 --> 00:02:43,725
quand on fournit au modèle
les mêmes données de plusieurs façons.

52
00:02:43,725 --> 00:02:46,705
Que se passe-t-il si je désactive x1x2 ?

53
00:02:46,705 --> 00:02:48,815
Il ne me reste plus que

54
00:02:48,815 --> 00:02:51,275
x1, x2, x1² et x2².

55
00:02:51,275 --> 00:02:55,280
Je redémarre

56
00:02:55,280 --> 00:02:58,115
et je remarque cette fois

57
00:02:58,115 --> 00:03:04,935
cette étrange frontière qui apparaît
au début de l'entraînement.

58
00:03:04,935 --> 00:03:07,055
Recommençons.

59
00:03:07,055 --> 00:03:08,185
Je démarre ceci.

60
00:03:08,185 --> 00:03:10,660
Je vais l'arrêter
à 200 itérations environ.

61
00:03:10,660 --> 00:03:11,660
Voilà.

62
00:03:11,660 --> 00:03:16,520
À 200 itérations, vous pouvez voir
à nouveau que la frontière est bancale,

63
00:03:16,520 --> 00:03:20,930
avec du blanc dans cette zone bizarre.

64
00:03:20,930 --> 00:03:24,650
Nous avons toujours des caractéristiques
supplémentaires, x1 et x2.

65
00:03:24,650 --> 00:03:26,950
Que se passe-t-il si je les enlève ?

66
00:03:26,950 --> 00:03:31,340
Il ne nous reste plus
que les données brutes x1 et x2.

67
00:03:31,340 --> 00:03:36,890
Je redémarre et j'arrête à nouveau
à 200 itérations environ.

68
00:03:36,890 --> 00:03:42,015
Vous voyez que c'est maintenant
presque parfait.

69
00:03:42,015 --> 00:03:44,600
J'ai seulement cette ligne.

70
00:03:44,600 --> 00:03:46,840
Il faut donc être conscient
de ce problème.

71
00:03:46,840 --> 00:03:50,140
Le mieux est l'ennemi du bien.

72
00:03:50,140 --> 00:03:56,150
Un croisement de caractéristiques peut
causer un surapprentissage du modèle.

73
00:03:56,150 --> 00:03:58,900
Un autre point important

74
00:03:58,900 --> 00:04:02,590
est la durée d'entraînement.

75
00:04:02,590 --> 00:04:05,260
Supprimons ces valeurs.
Revenons à notre point de départ.

76
00:04:05,260 --> 00:04:08,685
Si vous entraînez le modèle
très longtemps,

77
00:04:08,685 --> 00:04:12,990
le problème a tendance à s'améliorer.

78
00:04:12,990 --> 00:04:18,860
Cependant, en raison du surapprentissage,
vous avez toujours cette frontière courbe

79
00:04:18,860 --> 00:04:21,740
C'est un autre signe de surapprentissage.

80
00:04:21,740 --> 00:04:25,850
Si vous entraînez très longtemps,

81
00:04:25,850 --> 00:04:27,590
le triangle,

82
00:04:27,590 --> 00:04:30,860
l'artefact en bas à gauche, disparaît,

83
00:04:30,860 --> 00:04:34,170
mais il y a toujours la frontière courbe.

84
00:04:34,170 --> 00:04:36,830
Si nous obtenons cette ligne courbe

85
00:04:36,830 --> 00:04:40,490
au lieu d'une ligne droite,
qui est le modèle efficace le plus simple,

86
00:04:40,490 --> 00:04:43,640
c'est parce que nous avons donné
beaucoup de liberté au modèle.

87
00:04:43,640 --> 00:04:45,955
À vrai dire,

88
00:04:45,955 --> 00:04:51,710
les pondérations de x1 et x2 sont bien
supérieures à celles de ces trois valeurs.

89
00:04:51,710 --> 00:04:57,515
Mais le croisement de caractéristiques
x1x2 possède une pondération.

90
00:04:57,515 --> 00:05:00,290
C'est pour cette raison

91
00:05:00,290 --> 00:05:02,875
qu'il peut semer la confusion.

92
00:05:02,875 --> 00:05:08,380
Étonnamment, la frontière de décision
du modèle semble bizarre.

93
00:05:08,380 --> 00:05:13,420
Par exemple, cette zone en bas à gauche

94
00:05:13,420 --> 00:05:18,685
contient du bleu, même si rien
dans les données n'appuie ce phénomène.

95
00:05:18,685 --> 00:05:21,990
TensorFlow Playground utilise
un point de départ aléatoire.

96
00:05:21,990 --> 00:05:23,920
Votre résultat peut donc être différent.

97
00:05:23,920 --> 00:05:26,870
C'est pourquoi je vous montre le mien.

98
00:05:26,870 --> 00:05:29,920
Vous avez peut-être obtenu
quelque chose de légèrement différent.

99
00:05:29,920 --> 00:05:36,940
Notez l'épaisseur relative des cinq lignes
entre les valeurs d'entrée et le résultat.

100
00:05:36,940 --> 00:05:41,605
Ces lignes indiquent la pondération
relative des cinq caractéristiques.

101
00:05:41,605 --> 00:05:45,385
Les lignes partant de x1 et x2

102
00:05:45,385 --> 00:05:49,270
sont bien plus épaisses que celles partant
des croisements de caractéristiques.

103
00:05:49,270 --> 00:05:52,765
Ces derniers contribuent donc

104
00:05:52,765 --> 00:05:57,920
beaucoup moins au modèle
que les caractéristiques normales,

105
00:05:57,920 --> 00:06:03,070
mais suffisamment pour compromettre
la généralisation.

106
00:06:03,070 --> 00:06:06,490
Que se passe-t-il
si on les supprime complètement,

107
00:06:06,490 --> 00:06:09,685
c'est-à-dire si on utilise uniquement
les données brutes ?

108
00:06:09,685 --> 00:06:12,740
Supprimer tous les croisements
de caractéristiques

109
00:06:12,740 --> 00:06:15,280
donne un modèle plus sensé.

110
00:06:15,280 --> 00:06:19,630
Il n'y a plus de frontière courbe
indiquant un surapprentissage.

111
00:06:19,630 --> 00:06:24,160
Après 1 000 itérations, la perte de test

112
00:06:24,160 --> 00:06:28,510
devrait être légèrement inférieure qu'avec
les croisements de caractéristiques.

113
00:06:28,510 --> 00:06:32,650
Vos résultats peuvent varier quelque peu
en fonction de l'ensemble de données.

114
00:06:32,650 --> 00:06:38,590
Les données de cet exercice sont
des données linéaires avec du bruit.

115
00:06:38,590 --> 00:06:43,810
Si nous utilisons un modèle trop compliqué
pour des données aussi simples,

116
00:06:43,810 --> 00:06:47,020
par exemple avec trop de croisements
de caractéristiques,

117
00:06:47,020 --> 00:06:51,655
nous lui donnons l'opportunité d'intégrer
le bruit aux données d'entraînement.

118
00:06:51,655 --> 00:06:54,790
Ce problème se diagnostique souvent
en regardant

119
00:06:54,790 --> 00:06:59,605
les performances du modèle
sur des données de test indépendantes.

120
00:06:59,605 --> 00:07:03,020
Par la suite, dans le cours
"Art et science du ML",

121
00:07:03,020 --> 00:07:06,250
nous parlerons de la régularisation.

122
00:07:06,250 --> 00:07:12,720
Ceci explique les avantages
de la régularisation L1.

123
00:07:12,720 --> 00:07:16,580
Cette dernière remet à zéro

124
00:07:16,580 --> 00:07:18,965
la pondération
d'une caractéristique si nécessaire.

125
00:07:18,965 --> 00:07:25,680
En d'autres termes,
elle supprime des caractéristiques.