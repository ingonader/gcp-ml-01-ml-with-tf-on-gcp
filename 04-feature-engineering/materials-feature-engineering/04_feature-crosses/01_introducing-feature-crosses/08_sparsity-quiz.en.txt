Feature crosses combine two or more categorical features. If we have continuous features we can discretize them and then do a feature cross. Let's see what this means for the size of the input space by looking at a couple of examples. Suppose we want to predict a traffic level in a city, we have two raw inputs, the hour of the day and the day of the week. How many inputs would we have if we simply one hot encoded the hour of day and the day of the week and provided it to the model? Did you say 24 plus 7, so 31? When you one hot encode the hour of the day you get 24 input nodes, well, you can get away with 23 by treating all zeros as a valid input, but normally we reserve all zeros for missing data, so let's say 24 input nodes. And similarly, when you one hot encode the day of the week you get seven input nodes. So, in total we have 31 input nodes. But we know that the traffic is not the same at 5:00 PM every day. 5:00 PM on Wednesday is very different from 5:00 PM on the weekend. Is there an easy way to get the model to learn this? Sure. You know to do this? Now, Feature cross, the two raw inputs. We are now concatenating the hour of the day with the day of the week. And this letter model learned the combination of hour and day quickly. Great. But how many inputs do we now have? Not 24 plus 7, we now have 24 times 7, all the possible combinations. So, we went from 24 plus 7 equals 31 inputs to 24 times 7 equals 168 inputs. When you do feature crosses you get way more inputs. Data scientist often worry that one hot encoding categorical variables increases the size of their model. Even one hot encoding gives traditional machine learning frameworks a lot of trouble. How will they handle feature crosses? They'll have a heart attack. Internally, TensorFlow uses a sparse representation for both one hot encoding and for feature crosses, so it has no problem with this. For any particular raw of your input dataset, how many nodes in X3 are let up? Just one. Do you see why? For every label every observation in the table is taken at a specific time. That corresponds to a specific hour of a specific day of the week. So, you could have an observation at 3:00 PM, in the hour of the day in the input, and Wednesday in the day of the week input. So, feature cross this and what do you have? You have one input node. The input node that corresponds to 3:00 PM on Wednesday and that input node will be one. All the other input nodes for X3 will be zero. The input therefore will consist of 167 zeros and 1 one. And when you do a feature cross the input is very, very sparse. So, keep this in mind. TensorFlow will give us easy tools to deal with this. Let's look at the responses of the parse. a. Answer is no. Binning is good because it enables a model to learn linear relationships within a single feature. However, a city exists in more than one dimension, so learning cities specific relationships requires crossing latitude and longitude. So, how about the second one, b? Answer again is still no. Binning is a good idea, however, the city is a conjunction of latitude and longitude. So, separate feature crosses prevent the model from learning city-specific prices. How about c? That's yes. Crossing a binned latitude with a binned longitude enables the model to learn city-specific effects of rooms per person. Binning prevents a change in latitude producing the same result as a change in longitude. And depending on the granularity of the bins, this feature cross could learn city-specific or neighborhood-specific or even block-specific affects. Fourth one, no. In this example, crossing real valued features is not a good idea. Crossing the real value of say, latitude with rooms per person enables a 10 percent change in one feature, lets say a latitude, to be equivalent to a 10 percent change in the other feature say, rooms per person. This is so problematic that this is not even possible in TensorFlow. Crossing is only possible with categorical or discretized columns.