¿Y si discretizo el eje x1
mediante el trazado no solo de una línea blanca
sino de varias líneas negras? Y si hacemos lo mismo con el eje x2
con el trazado de varias líneas blancas. Hemos discretizado el eje x1 y el eje x2. Cuando trazamos dos líneas blancas obtuvimos cuatro cuadrantes. ¿Y ahora? Si tenemos m líneas verticales
y n líneas horizontales terminaremos con m más 1
por n más 1 celdas de cuadrícula. Ahora, consideremos cómo se vería esto
si discretizamos x1 y x2 y luego multiplicamos. ¿Recuerdan este diagrama
en el que dividimos el espacio de entrada en cuadrantes? Básicamente, podemos hacer una predicción
diferente para cada uno de los cuadrantes. ¿Y qué sobre este cuadro verde? ¿Cuál será nuestra predicción
para ese cuadro? Amarillo, ¿verdad? ¿Y ahora? Azul, pero hay un poco
de amarillo también. Contemos la cantidad
de puntos azules y la de amarillos y lo llamaremos 85% azul. ¿Pueden ver ahora
cómo aparecen las probabilidades? ¿Y ahora? Veamos por qué esto funciona bien
como modelo lineal. Cuando codifican de un solo 1
el primer conjunto de valores y, luego, codifican de un solo 1
el segundo conjunto y, luego, realizan una combinación
de atributos se quedan con un nodo que activa
puntos que caen en ese grupo. Piénsenlo, x3 será 1
solo si x1 = 1 y x2 = 1. Entonces, para cualquier punto
en el espacio de entrada solo un grupo se activa. Ahora, si toman estos valores
de atributos combinados y los alimentan a la regresión lineal ¿cuál será el peso de w3? La relación de puntos azules
a amarillos en la cuadrícula de celdas que corresponden a x1 y x2. Por eso, la combinación
de atributos es tan poderosa. Básicamente,
se discretiza el espacio de entrada y se memoriza el conjunto
de datos de entrenamiento. ¿Pero se dan cuenta de
por qué esto podría ser problemático? ¿Y si no tienen suficientes datos? ¿Qué aprenderá el modelo aquí? Aprenderá que la predicción
debe ser azul, ¿correcto? Hay formas de solucionar esto. No necesitan discretizar el espacio
de entrada en partes iguales. En vez, pueden usar cuadros
de diferentes tamaños y usar tamaños de cuadros
que estén vinculados a la entropía o el contenido
de la información en el cuadro. También pueden agrupar cuadros. Hay formas de superar el problema. Aun así, deben comprender
que la combinación de atributos se trata de la memorización que es lo opuesto a la generalización que es el objetivo 
del aprendizaje automático. Entonces, ¿deberían hacerlo? En un sistema de aprendizaje automático
del mundo real hay lugar para ambos. La memorización funciona bien
cuando tienen tantos datos que para cualquier celda única
de cuadrícula en su espacio de entrada la distribución de los datos
es estadísticamente significativa. Cuando ese es el caso, se puede memorizar. Básicamente, se está aprendiendo
la media para cada celda de la cuadrícula. Por supuesto, el aprendizaje profundo
requiere de muchos datos para este espacio ya sea que deseen combinar los atributos
o usar muchas capas necesitan muchos datos. A propósito, si conocen el aprendizaje
automático tradicional es posible que no hayan oído
mucho sobre la combinación de atributos. Una de las razones puede ser
porque la combinación de atributos memoriza y solo funciona
en grandes conjuntos de datos. Pero verán que las combinaciones
de atributos son muy útiles en los conjuntos de datos reales.
Mientras más grandes los datos más pequeños se pueden hacer los cuadros y se puede memorizar con más detalle. La combinación de atributos
es una poderosa técnica de procesamiento previo
en grandes conjuntos de datos.