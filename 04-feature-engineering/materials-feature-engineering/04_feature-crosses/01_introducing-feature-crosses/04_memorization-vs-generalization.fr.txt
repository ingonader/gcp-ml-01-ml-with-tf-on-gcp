Et si je discrétisais l'axe x1 en traçant non pas une ligne blanche,
mais de nombreuses lignes noires ? Et si nous faisions de même pour l'axe x2
en traçant de multiples lignes noires ? Nous avons maintenant discrétisé
l'axe x1 et l'axe x2. Lorsque nous avons tracé
deux lignes blanches, nous avons obtenu quatre quadrants. Et maintenant ? Si j'ai m lignes verticales
et n lignes horizontales, Je vais obtenir (m+1)×(n+1) cellules,
n'est-ce pas ? Voyons ce qui se passe si nous discrétisons
x1 et x2, puis multiplions. Souvenez-vous de ce schéma dans lequel
nous avons divisé l'espace en quadrants. Nous pouvons faire une prédiction
différente pour chaque quadrant. Prenons cette case verte. Quelle sera la prédiction correspondante ? Orange, n'est-ce pas ? Et maintenant ? Bleu, mais il y a un peu d'orange aussi. Comptons le nombre de points bleus
et de points orange. Disons que nous avons 85 % de bleu. Vous voyez maintenant comment
les probabilités vont entrer en jeu. Et maintenant ? Quoi qu'il en soit… Voyons comment
un modèle linéaire peut fonctionner. Si vous encodez en mode one-hot
le premier ensemble de valeurs, puis le deuxième, et que vous effectuez
un croisement de caractéristiques, vous obtenez un nœud qui s'active
pour les points correspondant à ce bucket. x3 ne sera donc un que si x1=1 et x2=1. Pour chaque point de l'espace d'entrée, un seul bucket est activé. Si vous transmettez maintenant
ces valeurs croisées à la régression linéaire, à quoi doit correspondre
la pondération w3 ? Au rapport entre les points bleus
et les points orange dans la cellule correspondant à x1 et x2. C'est pourquoi le croisement
de caractéristiques est si puissant. En bref, vous discrétisez
l'espace d'entrée et mémorisez l'ensemble
de données d'entraînement. Mais voyez-vous
ce qui pourrait poser problème ? Que ce passe-t-il
si vous n'avez pas assez de données ? Que va apprendre votre modèle ? Il va apprendre que la prédiction
doit être bleue, n'est-ce pas ? Il est possible de contourner ce problème. Vous n'êtes pas obligé
de discrétiser l'espace de manière égale. Vous pouvez utiliser
des cases de taille différente, et choisir
des tailles de cases liées à l'entropie ou aux informations qu'elles contiennent. Vous pouvez aussi regrouper des cases. Il y a donc des solutions à ce problème. Vous devez tout de même comprendre que les croisements de caractéristiques
reposent sur la mémorisation. La mémorisation est le contraire de la généralisation
que le ML cherche à atteindre. Alors, faut-il y avoir recours ? Dans un système de machine learning
en conditions réelles, elles sont toutes deux utiles. La mémorisation fonctionne lorsque vous avez tellement de données pour
chaque cellule de votre espace d'entrée que la distribution des données est
statistiquement significative. Dans ce cas, vous pouvez mémoriser. Vous apprenez la moyenne
pour chaque cellule. Le deep learning implique aussi
beaucoup de données pour cet espace. Que vous vouliez utiliser
le croisement de caractéristiques ou de nombreuses couches, il vous faut beaucoup de données. Si vous connaissez
le machine learning conventionnel, vous n'avez peut-être pas entendu parler
de croisements de caractéristiques. Le fait qu'ils fonctionnent uniquement
sur de grands ensembles de données et les mémorisent
peut l'expliquer en partie. Ils sont cependant extrêmement utiles
sur des ensembles de données réels. Plus vos données sont importantes, plus vous pouvez
réduire la taille des cases et mémoriser précisément. Les croisements
de caractéristiques sont donc une technique de prétraitement puissante sur les grands ensembles de données.