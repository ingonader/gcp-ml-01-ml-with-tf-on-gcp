1
00:00:00,000 --> 00:00:02,065
Datalabを起動しました

2
00:00:02,065 --> 00:00:06,685
そして特徴エンジニアリング用の
iPythonノートブックを開いています

3
00:00:06,685 --> 00:00:08,945
では手順を見てみましょう

4
00:00:08,945 --> 00:00:12,355
このノートブックでは 特徴の列を操作し

5
00:00:12,355 --> 00:00:14,585
TensorFlowに特徴クロスを追加し

6
00:00:14,585 --> 00:00:16,555
BigQueryからデータを読み取り

7
00:00:16,555 --> 00:00:18,630
Dataflowでデータセットを作成し

8
00:00:18,630 --> 00:00:20,490
ワイド＆ディープモデルを使います

9
00:00:20,490 --> 00:00:23,445
これまでに説明した手順を
ここに結集させます

10
00:00:23,445 --> 00:00:27,020
ワイド＆ディープモデルについては
後で説明します

11
00:00:27,020 --> 00:00:29,600
では問題に取り組んでいきましょう

12
00:00:29,600 --> 00:00:32,505
これまでタクシーのモデルを
構築してきましたが

13
00:00:32,505 --> 00:00:37,245
人間による分析はまだ取り入れていないので
これからそれを行います

14
00:00:37,245 --> 00:00:41,170
私たちが持つ知識 たとえば

15
00:00:41,170 --> 00:00:45,780
タクシーの仕組みやニューヨークの区画
などに関する知識を活用し

16
00:00:45,780 --> 00:00:50,130
モデルにヒントを与えて
効果的に学習できるようにします

17
00:00:50,130 --> 00:00:54,700
具体的な分析情報をどこから得るか
についても説明します

18
00:00:54,700 --> 00:00:58,350
まずは…

19
00:01:02,660 --> 00:01:04,700
インポートまで移動します

20
00:01:04,700 --> 00:01:07,100
プロジェクトは必ず変更してください

21
00:01:07,100 --> 00:01:10,630
Qwiklabsのプロジェクトを
割り当てました

22
00:01:10,630 --> 00:01:14,715
バケットも Qwiklabsのバケットに
変更してあります

23
00:01:14,715 --> 00:01:19,430
リージョンの割り当ては
コードの実行場所に設定しました

24
00:01:19,430 --> 00:01:22,410
次にクエリを実行しましょう

25
00:01:22,410 --> 00:01:25,730
クエリを行うと
データがクリーンアップされます

26
00:01:25,730 --> 00:01:29,195
ここでは取り込むデータを定義します

27
00:01:29,195 --> 00:01:32,715
距離が正の値であること

28
00:01:32,715 --> 00:01:35,315
乗車料金が2.5より大きいこと

29
00:01:35,315 --> 00:01:38,970
乗車地の経度と緯度が
範囲内に収まっていること

30
00:01:38,970 --> 00:01:42,125
乗客を乗せていることなどが条件です

31
00:01:42,125 --> 00:01:46,880
特定のタクシー利用について
収集したデータが正しいことを確認して

32
00:01:46,880 --> 00:01:49,605
トレーニングに使用します

33
00:01:49,605 --> 00:01:53,040
次に データを分割します

34
00:01:53,040 --> 00:01:57,760
乗車日時のハッシュを基準に
データセットを作成します

35
00:01:57,760 --> 00:02:00,810
続いて クエリを作成しました

36
00:02:00,810 --> 00:02:06,100
通行料と運賃を合わせたものを
乗車料金と呼んでいます

37
00:02:06,100 --> 00:02:09,750
これが 乗客が支払う合計料金です

38
00:02:09,750 --> 00:02:13,825
また 曜日を
dayofweek として取得します

39
00:02:13,825 --> 00:02:15,935
なぜ曜日が必要なのでしょう？

40
00:02:15,935 --> 00:02:20,190
それは 曜日によって交通状況が
異なるためです

41
00:02:20,190 --> 00:02:25,100
金曜の交通量は日曜の交通量より
多くなります

42
00:02:25,100 --> 00:02:30,215
交通量は 時間帯によっても変わります

43
00:02:30,215 --> 00:02:36,890
同じ金曜日でも 午前2時の交通量は
午後4時に比べると少ないはずです

44
00:02:36,890 --> 00:02:42,535
なぜ交通量が重要か？
ここに人間による分析が入ります

45
00:02:42,535 --> 00:02:48,325
ニューヨークでは 乗車距離だけでなく
乗車時間も料金に影響するためです

46
00:02:48,325 --> 00:02:53,605
タクシーが渋滞にはまったら
その時間分 料金が増えます

47
00:02:53,605 --> 00:02:57,040
その間 運転手は他の乗客を
乗せられないからです

48
00:02:57,040 --> 00:03:01,370
したがって 移動にかかった時間は重要です

49
00:03:01,370 --> 00:03:05,650
そして 移動を開始する前は
目的地までの時間はわかりません

50
00:03:05,650 --> 00:03:09,560
機械学習モデルに
それを学習させる必要があります

51
00:03:09,560 --> 00:03:13,950
乗車時間がどれくらいかかるかを
決定する重要な要素は

52
00:03:13,950 --> 00:03:16,370
乗客を拾った時刻です

53
00:03:16,370 --> 00:03:20,180
降車時刻ではありません
乗客がいつ降車するかはわかりません

54
00:03:20,180 --> 00:03:22,720
でも 乗客がいつ乗車したかはわかります

55
00:03:22,720 --> 00:03:27,250
したがって
乗車の日時、曜日、時間帯を

56
00:03:27,250 --> 00:03:29,320
モデルへの入力として使用します

57
00:03:29,320 --> 00:03:31,730
乗客を拾う場所もわかります

58
00:03:31,730 --> 00:03:35,000
降ろす場所もわかります

59
00:03:35,000 --> 00:03:39,140
乗客を降ろす時刻はわかりませんが
目的地はわかります

60
00:03:39,140 --> 00:03:42,130
つまり降車地の経度と緯度が
わかるので

61
00:03:42,130 --> 00:03:44,145
この情報も入力として使います

62
00:03:44,145 --> 00:03:46,100
さらに乗客数を取得します

63
00:03:46,100 --> 00:03:49,040
そして 通常はキーを作成します

64
00:03:49,040 --> 00:03:50,630
今回は使いませんが

65
00:03:50,630 --> 00:03:56,415
大量のデータを取り込んで
バッチ予測などを行う場合には

66
00:03:56,415 --> 00:04:01,205
データセットの各行に
固有のIDがあると役立ちます

67
00:04:01,205 --> 00:04:05,110
つまりこれは すべての入力列の
一意のIDです

68
00:04:05,110 --> 00:04:10,110
以上の情報を
有効なデータから取得します

69
00:04:10,110 --> 00:04:14,490
この段階まで行うと
データセットを作成する準備が整います

70
00:04:14,490 --> 00:04:16,374
データセットを作成するには

71
00:04:16,374 --> 00:04:20,559
まず 既存のデータセットがあれば
すべて削除します

72
00:04:20,559 --> 00:04:23,715
それが終わったら

73
00:04:23,715 --> 00:04:31,190
これらすべての列から
CSVファイルを作成します

74
00:04:31,190 --> 00:04:34,085
まず 列が揃っていることを確認します

75
00:04:34,085 --> 00:04:37,690
乗車料金の列、曜日の列
時間帯の列などです

76
00:04:37,690 --> 00:04:40,010
これらの列を取り込みます

77
00:04:40,010 --> 00:04:44,420
ただしデータセット内の曜日を
BigQueryで取得すると

78
00:04:44,420 --> 00:04:48,230
「2」などの数値になりますが
それでは困ります

79
00:04:48,230 --> 00:04:50,730
2がどの曜日なのか
わからないからです

80
00:04:50,730 --> 00:04:53,455
週は日曜から始まるのか？
それとも月曜なのか？

81
00:04:53,455 --> 00:04:57,410
そういったことに クライアントコードなどで
対処したくありません

82
00:04:57,410 --> 00:04:59,495
そこでどうするかというと

83
00:04:59,495 --> 00:05:04,450
これらの数値を
実際の曜日の名前で置き換えます

84
00:05:04,450 --> 00:05:07,760
曜日が1であれば
日曜日で置き換え

85
00:05:07,760 --> 00:05:10,520
曜日が2であれば
月曜日で置き換えます

86
00:05:10,520 --> 00:05:12,650
それを行っているのがここです

87
00:05:12,650 --> 00:05:14,755
BigQueryの結果を取得し

88
00:05:14,755 --> 00:05:18,730
数値となっている曜日を
文字列で置き換えます

89
00:05:18,730 --> 00:05:23,565
そして各データの間にカンマを追加します

90
00:05:23,565 --> 00:05:27,985
これがCSVファイルの出力になります

91
00:05:27,985 --> 00:05:30,280
出力を書き出すには

92
00:05:30,280 --> 00:05:36,180
作成したクエリを使用して
BigQueryからデータを読み取ります

93
00:05:36,180 --> 00:05:39,920
さきほど説明した関数を使って
データをCSVに変換します

94
00:05:39,920 --> 00:05:45,525
唯一の変更点は 曜日を数値ではなく
文字列にすることです

95
00:05:45,525 --> 00:05:49,010
最後にテキストファイルとして
CSVファイルに書き出します

96
00:05:49,010 --> 00:05:51,540
そして実行すると

97
00:05:51,540 --> 00:05:56,660
コードの前処理が行われます

98
00:05:56,660 --> 00:05:59,140
そして次のセルでは

99
00:05:59,140 --> 00:06:04,010
DataflowRunnerに対して
preprocessを呼び出します

100
00:06:04,010 --> 00:06:08,620
またはDirectRunnerで
より小さなデータセットを作成できます

101
00:06:08,620 --> 00:06:12,225
今回はDataflowRunnerで実行します

102
00:06:12,225 --> 00:06:16,595
実行が完了するまでには
しばらく時間がかかるので

103
00:06:16,595 --> 00:06:20,240
コンソールに移動します

104
00:06:20,240 --> 00:06:26,015
DataflowRunner内でジョブが
開始されたことを確認します

105
00:06:26,015 --> 00:06:29,145
Dataflowに移動します

106
00:06:43,485 --> 00:06:44,705
ありました

107
00:06:44,705 --> 00:06:48,420
Dataflow API が有効でないとあります

108
00:06:48,420 --> 00:06:53,680
このような場合
つまりエラーが発生している場合には

109
00:06:53,680 --> 00:06:58,410
こちらで [APIとサービス]にアクセスして

110
00:07:00,750 --> 00:07:04,895
有効にするサービスを検索します

111
00:07:04,895 --> 00:07:09,685
ここで有効にしたいサービスは
Dataflowです

112
00:07:09,685 --> 00:07:13,165
検索するとDataflow APIが表示されます

113
00:07:13,165 --> 00:07:16,445
このAPIを有効にします

114
00:07:16,445 --> 00:07:20,400
APIが有効化されると...

115
00:07:22,260 --> 00:07:24,655
有効化されるまで待ちましょう

116
00:07:24,655 --> 00:07:27,100
セルを再実行できるはずです

117
00:07:27,100 --> 00:07:29,560
有効化されました

118
00:07:29,560 --> 00:07:36,900
Datalabノートブックに戻り
セルを再実行します

119
00:07:38,730 --> 00:07:44,240
今回は起動できるはずです

120
00:07:44,240 --> 00:07:46,865
できました

121
00:07:46,865 --> 00:07:51,067
メニューからDataflowに戻ると…

122
00:07:55,950 --> 00:07:59,155
コードが実行中になっていることがわかります

123
00:07:59,155 --> 00:08:02,995
実行には時間がかかりますが
完了すると

124
00:08:02,995 --> 00:08:05,355
クラウド上のバケット内に

125
00:08:05,355 --> 00:08:10,850
トレーニングに使用できる
ファイルが生成されます

126
00:08:13,660 --> 00:08:15,900
ここを見てみましょう

127
00:08:18,220 --> 00:08:22,525
以上のことを行いました

128
00:08:22,525 --> 00:08:26,435
完了するまで待ち それから再開しましょう

129
00:08:26,435 --> 00:08:29,695
それまで録画を一旦停止します

130
00:08:29,695 --> 00:08:35,289
Dataflowジョブが完了したら
再開します

131
00:08:35,289 --> 00:08:39,565
さて ここに示されているように
このジョブには約8分かかりました

132
00:08:39,565 --> 00:08:41,520
最後のステップは成功しています

133
00:08:41,520 --> 00:08:45,415
この時点で ワーカーの数が再び減っています

134
00:08:45,415 --> 00:08:48,640
所要時間は 利用できるワーカーの数と

135
00:08:48,640 --> 00:08:52,625
ジョブ内で実行されている
ワーカーの数に左右されます

136
00:08:52,625 --> 00:08:55,610
ジョブが完了すれば

137
00:08:55,610 --> 00:09:01,910
ノートブックに戻って
出力ファイルの存在を確認できます

138
00:09:01,910 --> 00:09:05,490
バケットに対して gsutil lsを実行すると

139
00:09:05,490 --> 00:09:09,375
train.csvがあることがわかります

140
00:09:09,375 --> 00:09:11,395
ここにはvalid.csvがあります

141
00:09:11,395 --> 00:09:15,620
つまり トレーニングファイルと
検証ファイルがあります

142
00:09:15,620 --> 00:09:19,050
catを実行することもできます

143
00:09:19,050 --> 00:09:24,900
catはファイルの全行をリストする
Unixコマンドです

144
00:09:24,900 --> 00:09:29,840
ここではパイプでheadに渡して
最初の数行を表示しています

145
00:09:29,840 --> 00:09:32,905
表示を見ると 意図したとおり

146
00:09:32,905 --> 00:09:37,135
曜日が「Fri」「Wed」などの
文字列になっています

147
00:09:37,135 --> 00:09:41,610
その後に 乗車地点と降車地点の
経度と緯度が続きます

148
00:09:41,610 --> 00:09:46,070
そして最後の列はキーです

149
00:09:46,070 --> 00:09:48,890
このモデルでは無視しますが

150
00:09:48,890 --> 00:09:53,820
データセット内の各行に固有のIDを
設定する場合に利用できます

151
00:09:53,820 --> 00:09:55,575
このファイルを使用して

152
00:09:55,575 --> 00:09:58,930
モデルを開発できます

153
00:09:58,930 --> 00:10:02,590
開発を進める中で

154
00:10:02,590 --> 00:10:05,455
毎回クラウドに戻るのは面倒です

155
00:10:05,455 --> 00:10:08,420
そこでsampleというディレクトリを作成して

156
00:10:08,420 --> 00:10:12,360
そこに これらのファイルのうち
1つだけをコピーします

157
00:10:12,360 --> 00:10:14,005
ファイルを短くしたので

158
00:10:14,005 --> 00:10:21,280
短くしたファイルの先頭部分だけを
sampleローカルディレクトリにコピーします

159
00:10:21,280 --> 00:10:29,215
これが済んだら
次はコード自体を見ていきます

160
00:10:29,215 --> 00:10:32,170
コードはノートブック内でも確認できますが

161
00:10:32,170 --> 00:10:34,245
外部で確認してみます

162
00:10:34,245 --> 00:10:36,800
タクシー料金のフォルダがあります

163
00:10:36,800 --> 00:10:41,235
フォルダ内には 前と同じように
trainerがあります

164
00:10:41,235 --> 00:10:44,830
同じく
model.piとtasks.piがあります

165
00:10:44,830 --> 00:10:47,200
ただし 今回のmodel.piは

166
00:10:47,200 --> 00:10:49,570
単なる生データではありません

167
00:10:49,570 --> 00:10:52,940
特徴エンジニアリングが
適用されているからです

168
00:10:52,940 --> 00:10:55,910
これが それらの列です

169
00:10:55,910 --> 00:10:59,460
以前より列が増えている点に
注目してください

170
00:10:59,460 --> 00:11:02,700
曜日や時間帯などの列があります

171
00:11:02,700 --> 00:11:07,725
これらの列は入力列です

172
00:11:07,725 --> 00:11:09,420
曜日の列には

173
00:11:09,420 --> 00:11:14,040
日曜、月曜、火曜、木曜などの
文字列の曜日があります

174
00:11:14,040 --> 00:11:17,805
時間帯もカテゴリ列ですが

175
00:11:17,805 --> 00:11:20,260
この列にはIDがあります

176
00:11:20,260 --> 00:11:22,610
つまり整数の値であるということです

177
00:11:22,610 --> 00:11:24,640
１、２、3、４といった具合です

178
00:11:24,640 --> 00:11:26,760
そして数値列があります

179
00:11:26,760 --> 00:11:31,280
これには乗車地の経度、緯度や
降車地の緯度、経度などがあります

180
00:11:31,280 --> 00:11:34,015
この他に作成するのは

181
00:11:34,015 --> 00:11:39,110
エンジニアリング対象の列です
コードは後で確認します

182
00:11:39,110 --> 00:11:42,529
エンジニアリング列には
緯度の差を格納します

183
00:11:42,529 --> 00:11:44,260
なぜ緯度の差かというと

184
00:11:44,260 --> 00:11:48,835
マンハッタンでの南北方向の
移動状況がわかるからです

185
00:11:48,835 --> 00:11:54,470
緯度の変化を把握するのは
かなり良い考えです

186
00:11:54,470 --> 00:11:56,890
経度の差も役に立ちます

187
00:11:56,890 --> 00:11:59,320
ニューヨーク市は南北に長くなく

188
00:11:59,320 --> 00:12:06,165
通行料を支払って橋を渡ると
経度に大きな差が出てきます

189
00:12:06,165 --> 00:12:08,880
したがって経度の差についての
情報も役立ちます

190
00:12:08,880 --> 00:12:13,085
そしてユークリッド距離を追加します

191
00:12:13,085 --> 00:12:16,510
これは乗車地と降車地の間の直線距離です

192
00:12:16,510 --> 00:12:18,695
これも有用な特徴です

193
00:12:18,695 --> 00:12:21,460
距離をあらかじめモデルに入力できるため

194
00:12:21,460 --> 00:12:24,100
モデルが距離を学習する必要がないためです

195
00:12:24,100 --> 00:12:26,900
以上の特徴エンジニアリングを適用します

196
00:12:26,900 --> 00:12:29,615
Estimatorを作成する準備は整いました

197
00:12:29,615 --> 00:12:33,315
基本的に Estimatorには
入力列をすべて投入します

198
00:12:33,315 --> 00:12:35,660
これらの入力列を使用します

199
00:12:35,660 --> 00:12:40,845
住宅データセットの
特徴エンジニアリングの演習と同様に

200
00:12:40,845 --> 00:12:44,270
緯度と経度をバケット化します

201
00:12:44,270 --> 00:12:50,265
乗車地の緯度を38～42の経度に
バケット化します

202
00:12:50,265 --> 00:12:54,360
経度を-76～-72の経度に
バケット化します

203
00:12:54,360 --> 00:12:57,130
これらはニューヨーク市の境界です

204
00:12:57,130 --> 00:13:00,655
続いてバケット化した
乗車地の緯度を取得し

205
00:13:00,655 --> 00:13:02,935
バケット化した降車地の
緯度を取得します

206
00:13:02,935 --> 00:13:05,040
経度についても同じです

207
00:13:05,040 --> 00:13:07,815
乗車地の経度と降車地の経度も

208
00:13:07,815 --> 00:13:09,690
すべてバケット化します

209
00:13:09,690 --> 00:13:13,560
バケット化すると
どうなるのでしょうか？

210
00:13:13,560 --> 00:13:17,265
バケット化は基本的に
数値を離散化します

211
00:13:17,265 --> 00:13:21,360
数値をいずれかのバケットに入れて
カテゴリ化するということです

212
00:13:21,360 --> 00:13:26,160
これらのカテゴリ値を取得し
特徴クロスを作成します

213
00:13:26,160 --> 00:13:32,810
乗車地の緯度と経度の特徴クロスを
作成するとどうなるのでしょうか？

214
00:13:32,810 --> 00:13:37,000
緯度があって経度があって
これらの特徴クロスを作成すると

215
00:13:37,000 --> 00:13:40,470
それが乗車地になります

216
00:13:40,470 --> 00:13:43,579
つまり乗車地に対応する
グリッドセルになります

217
00:13:43,579 --> 00:13:45,585
それがplocです

218
00:13:45,585 --> 00:13:47,875
plocはグリッドセルのようなものです

219
00:13:47,875 --> 00:13:52,280
同様にdlocは降車地に対応する
グリッドセルです

220
00:13:52,280 --> 00:13:55,105
どちらもグリッドセルのポイントです

221
00:13:55,105 --> 00:14:01,470
次に乗車地と降車地の
特徴クロスを作成します

222
00:14:01,470 --> 00:14:06,105
つまり ある場所からある場所まで
タクシーで移動すると

223
00:14:06,105 --> 00:14:11,305
料金はいくらになるのかを学習します

224
00:14:11,305 --> 00:14:15,770
その唯一の方法は何度も繰り返すことです

225
00:14:15,770 --> 00:14:21,805
特徴クロスは非常に強力ですが
十分なデータを必要とします

226
00:14:21,805 --> 00:14:25,170
なぜなら特徴クロスは
記憶であるためです

227
00:14:25,170 --> 00:14:30,285
記憶であるため 各バケットに十分な
データがなければ役立ちません

228
00:14:30,285 --> 00:14:34,105
この例では何百万もの
タクシー乗車データがあります

229
00:14:34,105 --> 00:14:37,050
これだけのデータがあれば十分です

230
00:14:37,050 --> 00:14:41,920
乗車地の経度と
降車地の経度をバケット化し

231
00:14:41,920 --> 00:14:46,085
それを使って乗車地plocと
降車地dlocを作成し

232
00:14:46,085 --> 00:14:51,520
それらの特徴クロスを作成して
乗車地と降車地のペアを取得します

233
00:14:51,530 --> 00:14:54,260
さらに曜日と時間帯も処理します

234
00:14:54,260 --> 00:14:57,995
交通状況は曜日と時間帯によって
変わるためです

235
00:14:57,995 --> 00:15:02,865
同じ午後3時でも金曜と水曜と日曜では
交通状況が違います

236
00:15:02,865 --> 00:15:10,020
この特徴クロスを作成するには
バケットの使用数を決める必要があります

237
00:15:10,020 --> 00:15:15,400
バケットの数は
可能性のある合計値数の2倍

238
00:15:15,400 --> 00:15:19,850
可能性のある値数の4乗根など
任意の数を選べます

239
00:15:19,850 --> 00:15:23,760
この例では 値の合計数を 
そのまま使います

240
00:15:23,760 --> 00:15:26,070
つまり24×7個のバケットです

241
00:15:26,070 --> 00:15:28,980
これはいろいろ試して決めることです

242
00:15:28,980 --> 00:15:32,085
それにはハイパーパラメータ調整を
有効にします

243
00:15:32,085 --> 00:15:38,990
使用すべきハッシュバケット数に
正解はありません

244
00:15:38,990 --> 00:15:41,985
バケットの数を決めたら
データ全体を調べて

245
00:15:41,985 --> 00:15:45,260
データがまばらな列、カテゴリ列

246
00:15:45,260 --> 00:15:49,500
データ密度が濃い列、数値列などを
確認します

247
00:15:49,500 --> 00:15:54,380
データがまばらな列とカテゴリ列は
ネットワークのワイドな部分に入れます

248
00:15:54,380 --> 00:15:57,545
こうした列には線形モデルが有効だからです

249
00:15:57,545 --> 00:16:02,505
そしてデータ密度が濃い列と
数値列についてですが

250
00:16:02,505 --> 00:16:05,740
データ密度が濃い列の例として
埋め込み列が挙げられます

251
00:16:05,740 --> 00:16:10,520
なぜなら まばらなデータを取得して
まとめているからです

252
00:16:10,520 --> 00:16:14,675
このようなデータ密度が濃い列も役立ちます

253
00:16:14,675 --> 00:16:18,955
さて まばらな列はすべて
ワイド列の中に入れ

254
00:16:18,955 --> 00:16:23,490
密度の濃いデータはすべて
ディープ列に入れます

255
00:16:23,490 --> 00:16:27,625
そして DNN線形結合リグレッサーを
作成します

256
00:16:27,625 --> 00:16:31,480
これはモデルをさらに効果的にします

257
00:16:31,480 --> 00:16:34,550
あるいは 単にDNNリグレッサーを適用して

258
00:16:34,550 --> 00:16:37,980
すべてディープ列として渡すこともできます

259
00:16:37,980 --> 00:16:40,810
ただし DNN線形結合では

260
00:16:40,810 --> 00:16:45,000
疎データを密度の濃いデータとは
異なる方法で処理できます

261
00:16:45,000 --> 00:16:48,645
別々の最適化ツールを使えるのです

262
00:16:48,645 --> 00:16:56,550
実際のデータセットでは
密度が濃かったりまばらだったりします

263
00:16:56,550 --> 00:17:00,900
そのようなデータでは この種の
リグレッサーが大いに役立ちます

264
00:17:00,900 --> 00:17:05,425
どの特徴を線形モデルに渡し

265
00:17:05,425 --> 00:17:08,795
どの特徴を深層ニューラルネットモデルに
渡すかを決めたら

266
00:17:08,795 --> 00:17:13,155
DNNモデルに使用する
ユニット数を指定します

267
00:17:13,155 --> 00:17:17,400
これがモデルですが
特徴エンジニアリングの説明を思い出しましょう

268
00:17:17,400 --> 00:17:20,760
データを元のままではなく
強化してから使います

269
00:17:20,760 --> 00:17:25,020
特徴エンジニアリング列として
latdiffとlondiffを用意してあります

270
00:17:25,020 --> 00:17:26,569
計算方法を説明すると

271
00:17:26,569 --> 00:17:29,695
latdiffは2つの緯度の差です

272
00:17:29,695 --> 00:17:32,745
londiffは2つの経度の差です

273
00:17:32,745 --> 00:17:38,150
これらの入力に対応する
関数を指定します

274
00:17:38,150 --> 00:17:41,930
緯度と経度は
エンドユーザーが入力します

275
00:17:41,930 --> 00:17:45,740
エンドユーザーは londiffとlatdiffを
入力する必要はありません

276
00:17:45,740 --> 00:17:49,140
計算は不要で
元データだけ入力してもらいます

277
00:17:49,140 --> 00:17:52,120
すべての入力列を処理しますが

278
00:17:52,120 --> 00:17:54,990
最初の2つは例外です

279
00:17:54,990 --> 00:18:00,115
1つ目はラベルで
入力値ではありません

280
00:18:00,115 --> 00:18:02,815
2つ目は何だったでしょうか

281
00:18:02,815 --> 00:18:05,105
入力列を調べてみましょう

282
00:18:05,105 --> 00:18:08,380
無視するべきなのは

283
00:18:08,380 --> 00:18:10,595
この2つの列です

284
00:18:10,595 --> 00:18:14,280
曜日と時間帯を無視します

285
00:18:14,280 --> 00:18:17,980
その他の列はすべて取得します

286
00:18:17,980 --> 00:18:22,000
いずれも浮動小数点数となります

287
00:18:22,000 --> 00:18:23,995
曜日は文字列です

288
00:18:23,995 --> 00:18:26,640
時間帯はint 32です

289
00:18:26,640 --> 00:18:30,925
これを使用して入力の
レシーバーを作成します

290
00:18:30,925 --> 00:18:36,160
エンドユーザーが入力する特徴だけでなく

291
00:18:36,160 --> 00:18:40,860
エンジニアリングした特徴も追加して
モデルに全特徴を考慮させます

292
00:18:40,860 --> 00:18:46,010
この時点で 以前と同様に
データを読み取ります

293
00:18:46,010 --> 00:18:49,200
トレーニングと評価は
以前とよく似ています

294
00:18:49,200 --> 00:18:52,405
これで実行できる状態になりました

295
00:18:52,405 --> 00:18:54,010
ではこちらに戻って

296
00:18:54,010 --> 00:18:57,995
小さいデータセットで
モデルを試しましょう

297
00:18:57,995 --> 00:19:00,490
その後 クラウドでトレーニングします

298
00:19:00,490 --> 00:19:05,510
Google Cloud ML Engineを実行します

299
00:19:05,510 --> 00:19:10,450
RMSEはわずかに
改善されているはずです

300
00:19:10,450 --> 00:19:13,460
これだけでも
より良いモデルが得られますが

301
00:19:13,460 --> 00:19:16,790
さらに ハイパーパラメータ調整を行います

302
00:19:16,790 --> 00:19:20,495
これによって 
モデルに適したパラメータを見つけます

303
00:19:20,495 --> 00:19:23,305
調整の詳細については

304
00:19:23,305 --> 00:19:26,492
次のステップで説明します

305
00:19:26,492 --> 00:19:30,780
この例の場合の最適なパラメータは
こうなりました

306
00:19:30,780 --> 00:19:32,405
完了したら

307
00:19:32,405 --> 00:19:35,820
遥かに大きなデータセットに対して
モデルを実行します

308
00:19:35,820 --> 00:19:39,150
大規模なデータセットで
トレーニングすることで

309
00:19:39,150 --> 00:19:41,060
高い精度が得られます

310
00:19:41,060 --> 00:19:45,300
先ほど実行したDataflowジョブは

311
00:19:45,300 --> 00:19:47,965
完了するまでに
10分程度かかりました

312
00:19:47,965 --> 00:19:50,390
次に実行するDataflowジョブでは

313
00:19:50,390 --> 00:19:54,200
約1時間かけて100万行の
データセットを作成します

314
00:19:54,200 --> 00:19:57,580
そして そのデータセットで
トレーニングを行います

315
00:19:57,580 --> 00:20:00,910
完了すると RMSEが
大きく改善されるはずです

316
00:20:00,910 --> 00:20:03,450
しかし ここで重要な概念は

317
00:20:03,450 --> 00:20:07,184
元データに特徴エンジニアリングを適用して

318
00:20:07,184 --> 00:20:11,245
人間による分析を取り込み
データを強化することです

319
00:20:11,245 --> 00:20:13,490
交通状況、移動の距離

320
00:20:13,490 --> 00:20:17,880
境界を越えるかどうか

321
00:20:17,880 --> 00:20:22,370
東西南北のどちらの方向に進むのか などです

322
00:20:22,370 --> 00:20:26,595
londiff、latdiff、ユークリッド距離
特徴クロスなど

323
00:20:26,595 --> 00:20:29,610
すべてがモデルの改善に役立ちます