特徴エンジニアリングを行える場所は
3つありましたね 特徴を事前処理する方法と TensorFlowでオンザフライで
特徴を作成する方法を見ました もう1つのオプションは
Cloud Dataflowで 特徴の作成を事前処理することです Dataflowを使って 事前処理済み
または拡張されたデータセットを作り それを使ってモデルをトレーニングします 予測中になんらかの方法で
同じ事前処理ステップを行う必要があります この方法はDataflowが予測ランタイムにも 含まれる場合に最適に機能します すでに見たとおり
GCPの参照アーキテクチャがこれを行います Dataflowはストリーミングデータと
バッチデータを処理できるので トレーニングと予測の
両方のパイプラインに含まれます これを行う場合 Dataflowは事前処理に適した場所です Dataflowは時間枠ごとの集約を
扱う特徴に最適です たとえば特徴として 過去1時間にプロダクトを見た
平均人数を表すことができます トレーニングではDataflowを使って
ログファイルからそれを計算できます しかし このような特徴の性質上 実際のトラフィックに基づきリアルタイムで
Dataflowで計算する必要があるでしょう Dataflowで任意のPTransformに
フィールドを追加できます この例のadd_fieldsでは pardueが
input_fieldsを受け入れ 利用者数を導き出してそれを累積し 訪問者数を
過去1時間の数値として追加します バッチとストリームの両方で
同じコードとDataflowが機能するので トレーニングと予測の両方のパイプラインで
add_fieldsメソッドを使えます 3つ目のオプションは
混合的な手法です Googleの研究者がこの方法を
最近公開しました 次のモジュールで
これを詳しく見ていきますが 要約すると トレーニング中にDataflowで
事前処理データを作成しますが 変換自体はTensorFlowで実装されます こうすると予測中に 特徴エンジニアリングが
TensorFlowグラフに含まれます これはかなりメリットがあります
なぜなら Dataflowはデータ集計に優れていて 他方TensorFlowは inputフィールドをオンザフライで
操作するのに優れているからです