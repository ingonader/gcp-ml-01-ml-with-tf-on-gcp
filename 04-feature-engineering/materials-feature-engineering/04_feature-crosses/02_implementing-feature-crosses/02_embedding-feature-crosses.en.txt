Remember I said that using a large value for hash buckets leads to a very sparse representation. But what if we do one more thing, what if, instead of one-hot encoding the feature cross and then using it as is, we pass it through a dense layer. We can then train the model to predict traffic as before. This dense layer shown by the yellow and green nodes, this dense layer here creates what is called an embedding. The grey and blue boxes denote zeroes and ones, for any row in the input data set, for any training example, only one of the boxes is lit, and that box shown in blue is one, the grey boxes for that example they are zero. Atdifferent training example will correspond to a different hour of day, and so it will light up a different box, and that box will be one and the other boxes will be zero. However, the yellow and green boxes they're different, they're not one hat encoded, they will be real valued numbers, floating point values. Why? Because they are a weighted some of the feature crossed values. So what's happening at the yellow and green nodes? The thing to realize is that the weights that go into the embedding layer, the weights that go into the yellow and green nodes those weights are learned from the data. Imagine that we have lots and lots of traffic observations, maybe every time a car or a bicycle or a truck passes a particular signal, we have a traffic observation, so we have the data for an entire city all the signals, so millions of training examples. Wait a second, did I just say what I just said? That my data set consists of traffic observations, one trainee example for every vehicle at a signal? If you're new to machine learning I can almost bet that you thought that our training dataset consisted of aggregated traffic counts, maybe the total number of vehicles on the road at every hour of every day. But that's a small data set and it's just a toy problem. If you do that you will only learn averages, and that's fundamentally uninteresting and fit only for writing newspaper articles such as, models predict the traffic levels next year will be 10 percent more. But remember what we said, that machine learning is a way to learn the long tail to make fine grained predictions and derive insights beyond just a gross averages. Well this is what that means in practice. Instead of dealing with a few hundred rows of an aggregated data set, we have minute fine grained observations of cars at every signal and that is a traffic data set we are going to use. Our predictions are going to be, number of cars, number of trucks, number of bicycles, at any given time, at any given point in the city. Fine grained predictions, that's what machine learning is about. Anyway, let's go back to our lesson. So, we have vehicle observations, the data set might include the vehicle type, car, bicycle, bus, truck, the direction of travel, location, et cetera. That data set includes a timestamp from which we extract the day and the hour, and then we feature cross them to get x_3 in the diagram. And as we discussed x_3 is essentially one hat encoded into a number of hash buckets. We now take this and pass it through a dense layer whose weights are trained to predict a number of things about the traffic, maybe we're going to predict the time for the next vehicle to arrive at the intersection, so that we can control the length of the traffic signal. The point is, that by training these weights on this data set something neat happens. The feature cross of day hour has 168 unique values, but we are forcing it to be represented with just two real valued numbers. And so the model learns how to embed the feature cross in lower-dimensional space. Maybe the green box tends to capture the traffic in pedestrians and bicycles, while the yellow tends to capture automobiles. So, 8:00 AM on Tuesday and 9:00 AM on Wednesday may correspond to completely different boxes in the feature cross. However, if the traffic patterns in most intersections in the city are similar at those two times, the real valued representation of this two day hour combinations will end up being quite similar. Maybe there are lots of people bicycling and walking at those times and also lots of cars, the weights for 8 AM and 9 AM get adjusted such that the real valued numbers with the green and the yellow are quite similar at that time. But at 11 AM on Tuesday, and 2 PM on Wednesday there are not that many pedestrians but still you have a moderate number of cars. So you see that the numbers are close. Similarly 2:00 Am on Tuesday and 3:00 Am on Wednesday might end up with very similar numbers reflecting no traffic at all. The key thing is that similar day hour combinations in terms of traffic tend to be similar, and day hour combinations that have very different traffic conditions tend to be far apart in the two dimensional space. This is what we mean when we say that the model learns to embed the feature cross in a lower-dimensional space. So how do you implement this in Tenserflow? To create an embedding, use the embedding column method in TFF feature column. Pass in the categorical column you want to embed, here we are passing in the feature cross, and then specify the number of embedding dimensions, and that's it, for such a powerful idea, it's a super easy. Why do I say it's a powerful idea? One of the cool things about embedding is that the embedding that you have learned on one problem can often apply to other similar machine learning models. Maybe you have learned how to represent day hour combinations based on a fine grained traffic data set in London, but now you're putting in new traffic signals in Frankfurt but you haven't collected this data for Frankfurt. As a quick shortcut, you could use a learned embedding from London in Frankfurt. After all, you just want to present day hour combinations in a suitable way and the embedding trained on London data is going to be better than building the data using heuristics, like early morning or rush hour. So how do you do it? You simply load it from the saved model for London, and tell the model not to train this layer. You could also choose to load the embedding from London, and simply use it as a starting point for Frankfurt. And if you want to do that, you would set trainable equals true in the layer. Embeddings are an extremely powerful concept, and transfer learning of embeddings makes them even more so. They are particularly useful when dealing with very sparse columns. For day hour where we had 168 unique combinations, it's not that big deal, but we'll see embeddings a lot than we get the language models. There you might have 100,000 unique words and you want to embed them, represent them in the lower-dimensions space of maybe 30 or 50 dimensions. Feature crosses and embeddings are very useful in real world machine learning models. So if necessary, go back and review these two lessons before you proceed.