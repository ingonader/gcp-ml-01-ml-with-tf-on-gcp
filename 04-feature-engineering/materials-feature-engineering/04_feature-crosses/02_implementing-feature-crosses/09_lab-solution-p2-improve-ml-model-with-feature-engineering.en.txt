So I've started Datalab, and I've opened up the featureengineering.ipythonnotebook, and so let's now walk through this. So, in this notebook, we're going to learn how to work with feature columns, we'll add feature crosses in TensorFlow. We're going to be reading our data from BigQuery, creating data sets using Dataflow, and we'll be using a wide and deep model. So, we're going to put together a lot of these things that we talked about, the wide and deep we haven't quite talked about but we will talk about it now. So, here's the issue here. So far, we've built a taxi cab model, but we have not brought in human insight into it at all. So, that's what we're going to do now, we're going to basically take some advantage of some of the things that we know about how taxis work, about how New York is laid out, and start giving the model hints, so that it can learn those things better. So, as we walk through, I'll talk about where a particular insight comes from. So, first thing is let's go ahead and get the import. Make sure to change your project. So, I've changed my project to map to my Qwiklabs project, I've changed my bucket to map to my Qwiklabs bucket and I've set my region to map to where I want the code to run. So, then, let's go ahead and run the query. So, the query now actually does some cleanup. So, here, I'm basically making sure that we're pulling in only data that has positive distances, that the fare amount is greater than 2.5, that the pickup longitude, pickup latitude, et cetera, are within reasonable counts, and there were people in the taxi. So, we want to make sure that the data that was collected for a particular taxi cab trip was correct before we actually use it for training. I'm going to divide up my data as we talked about earlier, when we talked about creating data sets based on the hash of the pickup date time. Having done that, I've created my query, and my query is basically going to take the toll amount, and the fare amount, and calling that the fare amount, so that we're learning the total costs that will cost somebody, and taking the day of the week as a day of the week. Why do we take that? Well, we know that traffic conditions are going to be different depending on the day of the week. We know that Fridays are going to have more traffic than Sundays. We also know that the hour of the day matters, even on Friday, 2:00 AM on a Friday is probably not going to have as much traffic as 4:00 PM on a Friday, and why does this matter? It matters because in New York, and this is something that human insight brings in, people pay for the amount of time they spent in a taxi in addition to the distance. So, if the tax is stuck in traffic, you're going to have to pay for it because you're occupying the taxi, and the taxi driver is not able to pick up other fares. So, the time matters, your time that's spent in the trip matters. Before the trip starts, we don't know how long it's going to take. We want the machine learning model to have to learn this, and we know that a key determinant of how long the trip takes is when the pickup happened. Not the drop-off, because we don't know where they're going to get dropped off, but we do know when they're going to get picked up. So, we're basically using the pickup date time, the day of the week, and the hour of the day as inputs to our model. We also know where they're going to be picked up. We also know where the customer wants to get dropped off. We don't know the time they're going to get dropped off, but we do know where they're going to go. So, we know that drop-off longitude and the drop of latitude. So those are going to be our inputs as well. We're going to take a passenger count, and we're going to basically create a key. I'm going to use this key, but if we wanted to do some kind of thing like batch prediction, for example, we're going to be sending a lot of data in, it's helpful if each of the rows in your data set has a unique ID, and so this is a unique ID form from all of the input columns. I'm basically doing this where all of the data are valid. So, at this point, we're now ready to create our data set. To create a data set, we'll basically go ahead and remove any data set that may exist. Having done that, we will go ahead and create a CSV file from all of these columns. First thing is that we want to make sure that the columns are fare amount, day of the week, hour of the day, et cetera. So, those are the columns that we want to do, but the day of the week in the data set when we do BigQuery, the day of the week is going to be a number like 2. We don't want a number like 2 because we don't know if 2, what day of the week is it? Does a week start with Sunday, or Monday, or Tuesday? We don't want to have our client code, et cetera, have to worry about that. So, what we will do, is that we'll replace those magic numbers by the actual names of the days of the week. So, if the day of the week is one, it is Sunday. If the day of the week is two, it's Monday, et cetera. So, that's exactly what I'm doing here. I'm taking the BigQuery result, day of the week which is a number, and replacing it with a string, and now I'm basically appending them all with a comma in between, and that is now my CSV file output. Now, to write this out, what I'm going to do is I'm going to read the data from BigQuery using this query that we just created, converting it to CSV using that function that I just talked about. The only change that we are doing is that we are changing the days of the week from magic numbers to strings. Then writing it out to a text file, a CSV file. Now, when I run this, at this point, we basically have the code pre-process. Right. In the next cell, I am calling the pre-process on the dataflow runner if I wanted, or I could create a smaller dataset on the direct runner to run it locally. So, in this case, I will run it on the dataflow runner, and this is going to run, and it's going to take a while. So, we'll go to the console, and we will see in dataflow runner that the job has started. So, we'll go into dataflow and what does it happen? What is it saying? Dataflow. I see. It's a dataflow API has not been used or enabled, so what we will have to do is that we will have to go in here. If you see that error, you will have to go into the APIs and services and search for the date of enabled services. So, the one that we want to enable is called data flow. So, when we do that, we get the dataflow API, and let's go ahead and enable the API. Once the API has been enabled, let's wait for it to get enabled. We should be able to rerun this cell. Okay. It has been enabled. So, now we go back to our Datalab Notebook and rerun this cell, and this time, hopefully, it launches. Okay. There it is. It has launched, and so I can now go back to the dataflow part of the menu, and you will see that this code is running. This will take a while to run, and when it's finished running, on the cloud in your bucket, you will have training files that you can use for training. So, let's go down here. So, we could do that, but let's see. So, let's go ahead and wait for it to be done, and then once it's done, we would be able to come back. So, I'll pause the video here. We'll come back and we'll start once the dataflow job is complete. We can see that this job took about eight minutes for me, the last step succeeded, and at this point, the number of workers is coming back down. Of course, your mileage will vary depending on how many workers you have available and how many workers you actually have running in your job. But once it's done, you can go back to the notebook and make sure that the output files exist, and that's what I'm doing here, I'm doing GS, on gsutil ls, on the bucket, and we see that there is a train.csv, and there is a valid.csv. So, it basically have a training file under validation file, and we can also just go ahead and cut. Cut is a Unix command that basically lists the first few. Actually, it lists all of the lines and basically piping it through ahead, so that I get the first few lines, and we see that as we expect, the day of the week is a string: Friday, Wednesday, et cetera. Then we basically have latitudes, longitudes, pickup, and dropoff points. We also have the last thing. The last column is a key that we will just ignore in our model, but it's there if we want a unique ID for every row in our data set. So, we have this file, and now we can basically use it to develop our model. So, in order to do our development, it's good not to have to go back to the Cloud each time. So, what I am doing is that I'm making a directory called sample and copying just one of those files in it. Because we have shorted files, I'm just copying the first part of the shorted file into my local directory sample. Having done this, we can now basically go ahead and look at our code itself. So, let's go ahead and look at our code. We could do this in the notebook, but let's go and look at it outside. So, we have our taxi fare. So, in our taxi fare as before, we will have a trainer, and as before, we will have a model.pi and tasks.pi. But model.pi, in this case, is not going to be just a raw input. It's going to have some feature engineering in it. So, these are the columns that for present and notice that we now have some extra columns from the ones that we had before. We have the day of the week, we have the hour of the day, et cetera. So, what we're doing is that we're basically saying that these are my input columns, I have the day of the week, it has a vocabulary which is a Sunday, Monday, Tuesday, et cetera, the days of the week. The hour of the day is also a categorical column, but it has an identity. In other words, it already is an integerized number. So, one, two, three, four, et cetera. Then, we have numeric columns for pickup longitude, pickup latitude, drop-off latitude, drop-off longitude, et cetera, and then I'm also going to create some engineered columns and we look at that later in the code, but the engineer columns are going to be the difference in latitude. Now, why does that matter? The difference in latitude basically tells you if you're going North-South in Manhattan. So, that's a pretty good idea of to how much change in latitude that's happened. The longitude difference is actually very useful because New York City is not south in extent, and all of the bridges on which you pay tolls tend to be dramatic changes in longitude. So, knowing the longitude difference is also useful, and I add in a Euclidean distance which is known as the bird flies, between the pickup point and the dropoff point. That's a pretty good feature to use as well because that way the model doesn't have to learn distances, the distance is already given to it on a platter. So, we basically do these feature engineering, and now we're ready to build our estimator. In our estimator, we basically take all of our input columns. So, those are the input columns that we have, and then just as we did in our feature engineering exercise on the housing dataset, we bucketize the latitude buckets and the longitude buckets. So, we take the pick up latitude and we bucketize them to between 38 and 42, and the longitude from -76- -72 because this is New York, and those are the bounds of New York City. So, we go ahead and we get a bucketized pickup latitude, a bucketized dropoff latitude, and the same thing for the longitudes. Pickup longitude and dropoff longitudes, all of them are bucketized. Once you have them bucketized, what does bucketization do? It discretizes things, it basically takes a numeric value, and makes it categorical because it's in one of those buckets. We take those categorical values, and we feature cross them. So, what happens when we feature cross the pickup latitude and the pickup longitude? So, we have the latitude and we have the longitude, and we feature cross it, we essentially put the pickup location, the grid cell that corresponds the pickup location, that's what ploc is. Ploc is now like a grid cell. Similarly, dloc is a grid cell that corresponds to the dropoff, these are both just grid-cells points. Now, I basically feature cross the pickup location and the dropoff location. So, now we're basically saying that let's learn from all the taxi trips from this location to this location, what do they cost? The only way that we can do this, and this is something that we have to repeat over and over again is that feature crossing is extremely powerful, but this only works if you have enough data because feature crossing is memorization. It's memorization, and it works if you have enough data in each of those buckets. In this case, we have millions of taxicab rides, so we have enough data and we can afford to do this. So, we basically bucketize the pickup longitude, bucketize the dropoff longitude, use it to create the pickup loc, dropoff loc, do the feature cross of those, and now we have the a pick-up dropoff pair, that's a feature cross as well, and then we do the day and hour, again, because traffic depends on the day and the hour, Friday 03:00 PM is different from Wednesday 03:00 PM is different from Sunday 3:00 pm. So, we do that cross and we have to decide on the number of buckets that we want to use. You can choose a number anywhere from twice total number of possible values to the fourth root of the possible number of values. In this case, I'm basically using the total number of values themselves. 24 by 7 for the number of buckets, but that is something that you will have to try out and you will have to do hyper-parameter tuning on. There is no right answer here for how many hash buckets you should be using. Then we'll go back and look at all of our data, and say which of these are sparse and categorical and which of these are dense and numeric? The sparse and categorical columns go into the wide part of a network because linear models tend to work well for those, and the dense and numeric columns and embedding columns are an example of dense columns because we've taken the sparse data, and put them into the shoe handed in, those are also useful things that are dense. So, we take all of our sparse columns and we throw them into the white columns, we take all of our dense data, and we throw them into our deep columns, and we create what is called a DNN linear combined regressor. So, this is an extra umph that we're giving to the model, if you wanted, you could have just done a DNN regressor, parsing in all of these things as deep columns, and that would have been fine, but DNN linear combined lets us treat the sparse data differently from the dense data, uses a different optimizer for the sparse versus the dense, it is tuned to this idea that if you have a real-world dataset, some of your features will be dense and some of your features will be sparse, so this is a kind regressor that works very well for that kind of data. So we're doing this, we're parsing in which of our features need a linear model, and which of our features need a deep neural net model, and we specify the number of units that we want for our DNN model. So that's our model, but remember that we talked about feature engineering. We don't want to just take our raw data, we want to add things to it, and we had our feature engineering columns already, the latdiff, londiff, well, this is how you compute them. The latdiff is essentially the difference of the two latitudes, the londiff is the difference of the two longitudes, and then we specify the serving input function, this is what things does the end user have to give us. The end-user does not have to give us a londiff and a latdiff, they don't know how to compute it, they only have to give us a raw data. So, we basically go through all of the input columns, except for the first two which happened to be I think the free and fair amount which is a label, which is obviously not an input, and what is the second one that we are ignoring. We go into our input columns, the second one that we are ignoring, so we are ignoring these two. We're ignoring the day of the week and we're ignoring the hour of the day. Everything else, we're basically taking it, and basically saying that there's all floating point numbers, the day of the week is a string, the hour of the day is an int 32, and we basically use it to create a serving input receiver, but make sure that in addition to the features that the end users give us, we add all of our engineered feature so that our model sees everything. So, at this point then no reading the data is very similar to what we have seen before, the train and evaluate is very similar to what we have seen before, and we can basically go ahead and run this. So let's go back here, and we can try out our model on a smaller dataset, and then we can train it on the cloud. So, we can basically go ahead and do GCloud ML Engine and when you run it, you should get a slightly better RMSE, but this by itself we've gotten a better model. The next thing to do is basically do hyper-parameter tuning to find good parameters of the model. In order to do that, you would basically go ahead and we'll talk about hyper-parameter tuning, where you basically getting the parameters for those models, in this case these turned out to be the best parameters. So, having done that, we can now run this on a much larger dataset. One of the key things about machine learning is that you get the best performance by training on large datasets. So, where earlier I did a dataflow job that would finish in about 10 minutes or so, so that we could get going. We're now going to do a dataflow job that runs for about an hour that creates a much larger, a million row dataset, and then we can train on it, and having done that, you will basically see a much better RMSE. But the key idea here is to basically take your raw data and do feature engineering to bring a human insight into the kinds of things that matter, traffic, the distance of the trips, whether they cross the boundaries, whether they go East-West or North-South etc, the londiff, the latdiff, the Euclidean distance, the feature crosses, all of these are going to help improve our model.