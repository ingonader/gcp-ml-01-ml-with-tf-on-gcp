Datalabを起動しました そして特徴エンジニアリング用の
iPythonノートブックを開いています では手順を見てみましょう このノートブックでは 特徴の列を操作し TensorFlowに特徴クロスを追加し BigQueryからデータを読み取り Dataflowでデータセットを作成し ワイド＆ディープモデルを使います これまでに説明した手順を
ここに結集させます ワイド＆ディープモデルについては
後で説明します では問題に取り組んでいきましょう これまでタクシーのモデルを
構築してきましたが 人間による分析はまだ取り入れていないので
これからそれを行います 私たちが持つ知識 たとえば タクシーの仕組みやニューヨークの区画
などに関する知識を活用し モデルにヒントを与えて
効果的に学習できるようにします 具体的な分析情報をどこから得るか
についても説明します まずは… インポートまで移動します プロジェクトは必ず変更してください Qwiklabsのプロジェクトを
割り当てました バケットも Qwiklabsのバケットに
変更してあります リージョンの割り当ては
コードの実行場所に設定しました 次にクエリを実行しましょう クエリを行うと
データがクリーンアップされます ここでは取り込むデータを定義します 距離が正の値であること 乗車料金が2.5より大きいこと 乗車地の経度と緯度が
範囲内に収まっていること 乗客を乗せていることなどが条件です 特定のタクシー利用について
収集したデータが正しいことを確認して トレーニングに使用します 次に データを分割します 乗車日時のハッシュを基準に
データセットを作成します 続いて クエリを作成しました 通行料と運賃を合わせたものを
乗車料金と呼んでいます これが 乗客が支払う合計料金です また 曜日を
dayofweek として取得します なぜ曜日が必要なのでしょう？ それは 曜日によって交通状況が
異なるためです 金曜の交通量は日曜の交通量より
多くなります 交通量は 時間帯によっても変わります 同じ金曜日でも 午前2時の交通量は
午後4時に比べると少ないはずです なぜ交通量が重要か？
ここに人間による分析が入ります ニューヨークでは 乗車距離だけでなく
乗車時間も料金に影響するためです タクシーが渋滞にはまったら
その時間分 料金が増えます その間 運転手は他の乗客を
乗せられないからです したがって 移動にかかった時間は重要です そして 移動を開始する前は
目的地までの時間はわかりません 機械学習モデルに
それを学習させる必要があります 乗車時間がどれくらいかかるかを
決定する重要な要素は 乗客を拾った時刻です 降車時刻ではありません
乗客がいつ降車するかはわかりません でも 乗客がいつ乗車したかはわかります したがって
乗車の日時、曜日、時間帯を モデルへの入力として使用します 乗客を拾う場所もわかります 降ろす場所もわかります 乗客を降ろす時刻はわかりませんが
目的地はわかります つまり降車地の経度と緯度が
わかるので この情報も入力として使います さらに乗客数を取得します そして 通常はキーを作成します 今回は使いませんが 大量のデータを取り込んで
バッチ予測などを行う場合には データセットの各行に
固有のIDがあると役立ちます つまりこれは すべての入力列の
一意のIDです 以上の情報を
有効なデータから取得します この段階まで行うと
データセットを作成する準備が整います データセットを作成するには まず 既存のデータセットがあれば
すべて削除します それが終わったら これらすべての列から
CSVファイルを作成します まず 列が揃っていることを確認します 乗車料金の列、曜日の列
時間帯の列などです これらの列を取り込みます ただしデータセット内の曜日を
BigQueryで取得すると 「2」などの数値になりますが
それでは困ります 2がどの曜日なのか
わからないからです 週は日曜から始まるのか？
それとも月曜なのか？ そういったことに クライアントコードなどで
対処したくありません そこでどうするかというと これらの数値を
実際の曜日の名前で置き換えます 曜日が1であれば
日曜日で置き換え 曜日が2であれば
月曜日で置き換えます それを行っているのがここです BigQueryの結果を取得し 数値となっている曜日を
文字列で置き換えます そして各データの間にカンマを追加します これがCSVファイルの出力になります 出力を書き出すには 作成したクエリを使用して
BigQueryからデータを読み取ります さきほど説明した関数を使って
データをCSVに変換します 唯一の変更点は 曜日を数値ではなく
文字列にすることです 最後にテキストファイルとして
CSVファイルに書き出します そして実行すると コードの前処理が行われます そして次のセルでは DataflowRunnerに対して
preprocessを呼び出します またはDirectRunnerで
より小さなデータセットを作成できます 今回はDataflowRunnerで実行します 実行が完了するまでには
しばらく時間がかかるので コンソールに移動します DataflowRunner内でジョブが
開始されたことを確認します Dataflowに移動します ありました Dataflow API が有効でないとあります このような場合
つまりエラーが発生している場合には こちらで [APIとサービス]にアクセスして 有効にするサービスを検索します ここで有効にしたいサービスは
Dataflowです 検索するとDataflow APIが表示されます このAPIを有効にします APIが有効化されると... 有効化されるまで待ちましょう セルを再実行できるはずです 有効化されました Datalabノートブックに戻り
セルを再実行します 今回は起動できるはずです できました メニューからDataflowに戻ると… コードが実行中になっていることがわかります 実行には時間がかかりますが
完了すると クラウド上のバケット内に トレーニングに使用できる
ファイルが生成されます ここを見てみましょう 以上のことを行いました 完了するまで待ち それから再開しましょう それまで録画を一旦停止します Dataflowジョブが完了したら
再開します さて ここに示されているように
このジョブには約8分かかりました 最後のステップは成功しています この時点で ワーカーの数が再び減っています 所要時間は 利用できるワーカーの数と ジョブ内で実行されている
ワーカーの数に左右されます ジョブが完了すれば ノートブックに戻って
出力ファイルの存在を確認できます バケットに対して gsutil lsを実行すると train.csvがあることがわかります ここにはvalid.csvがあります つまり トレーニングファイルと
検証ファイルがあります catを実行することもできます catはファイルの全行をリストする
Unixコマンドです ここではパイプでheadに渡して
最初の数行を表示しています 表示を見ると 意図したとおり 曜日が「Fri」「Wed」などの
文字列になっています その後に 乗車地点と降車地点の
経度と緯度が続きます そして最後の列はキーです このモデルでは無視しますが データセット内の各行に固有のIDを
設定する場合に利用できます このファイルを使用して モデルを開発できます 開発を進める中で 毎回クラウドに戻るのは面倒です そこでsampleというディレクトリを作成して そこに これらのファイルのうち
1つだけをコピーします ファイルを短くしたので 短くしたファイルの先頭部分だけを
sampleローカルディレクトリにコピーします これが済んだら
次はコード自体を見ていきます コードはノートブック内でも確認できますが 外部で確認してみます タクシー料金のフォルダがあります フォルダ内には 前と同じように
trainerがあります 同じく
model.piとtasks.piがあります ただし 今回のmodel.piは 単なる生データではありません 特徴エンジニアリングが
適用されているからです これが それらの列です 以前より列が増えている点に
注目してください 曜日や時間帯などの列があります これらの列は入力列です 曜日の列には 日曜、月曜、火曜、木曜などの
文字列の曜日があります 時間帯もカテゴリ列ですが この列にはIDがあります つまり整数の値であるということです １、２、3、４といった具合です そして数値列があります これには乗車地の経度、緯度や
降車地の緯度、経度などがあります この他に作成するのは エンジニアリング対象の列です
コードは後で確認します エンジニアリング列には
緯度の差を格納します なぜ緯度の差かというと マンハッタンでの南北方向の
移動状況がわかるからです 緯度の変化を把握するのは
かなり良い考えです 経度の差も役に立ちます ニューヨーク市は南北に長くなく 通行料を支払って橋を渡ると
経度に大きな差が出てきます したがって経度の差についての
情報も役立ちます そしてユークリッド距離を追加します これは乗車地と降車地の間の直線距離です これも有用な特徴です 距離をあらかじめモデルに入力できるため モデルが距離を学習する必要がないためです 以上の特徴エンジニアリングを適用します Estimatorを作成する準備は整いました 基本的に Estimatorには
入力列をすべて投入します これらの入力列を使用します 住宅データセットの
特徴エンジニアリングの演習と同様に 緯度と経度をバケット化します 乗車地の緯度を38～42の経度に
バケット化します 経度を-76～-72の経度に
バケット化します これらはニューヨーク市の境界です 続いてバケット化した
乗車地の緯度を取得し バケット化した降車地の
緯度を取得します 経度についても同じです 乗車地の経度と降車地の経度も すべてバケット化します バケット化すると
どうなるのでしょうか？ バケット化は基本的に
数値を離散化します 数値をいずれかのバケットに入れて
カテゴリ化するということです これらのカテゴリ値を取得し
特徴クロスを作成します 乗車地の緯度と経度の特徴クロスを
作成するとどうなるのでしょうか？ 緯度があって経度があって
これらの特徴クロスを作成すると それが乗車地になります つまり乗車地に対応する
グリッドセルになります それがplocです plocはグリッドセルのようなものです 同様にdlocは降車地に対応する
グリッドセルです どちらもグリッドセルのポイントです 次に乗車地と降車地の
特徴クロスを作成します つまり ある場所からある場所まで
タクシーで移動すると 料金はいくらになるのかを学習します その唯一の方法は何度も繰り返すことです 特徴クロスは非常に強力ですが
十分なデータを必要とします なぜなら特徴クロスは
記憶であるためです 記憶であるため 各バケットに十分な
データがなければ役立ちません この例では何百万もの
タクシー乗車データがあります これだけのデータがあれば十分です 乗車地の経度と
降車地の経度をバケット化し それを使って乗車地plocと
降車地dlocを作成し それらの特徴クロスを作成して
乗車地と降車地のペアを取得します さらに曜日と時間帯も処理します 交通状況は曜日と時間帯によって
変わるためです 同じ午後3時でも金曜と水曜と日曜では
交通状況が違います この特徴クロスを作成するには
バケットの使用数を決める必要があります バケットの数は
可能性のある合計値数の2倍 可能性のある値数の4乗根など
任意の数を選べます この例では 値の合計数を 
そのまま使います つまり24×7個のバケットです これはいろいろ試して決めることです それにはハイパーパラメータ調整を
有効にします 使用すべきハッシュバケット数に
正解はありません バケットの数を決めたら
データ全体を調べて データがまばらな列、カテゴリ列 データ密度が濃い列、数値列などを
確認します データがまばらな列とカテゴリ列は
ネットワークのワイドな部分に入れます こうした列には線形モデルが有効だからです そしてデータ密度が濃い列と
数値列についてですが データ密度が濃い列の例として
埋め込み列が挙げられます なぜなら まばらなデータを取得して
まとめているからです このようなデータ密度が濃い列も役立ちます さて まばらな列はすべて
ワイド列の中に入れ 密度の濃いデータはすべて
ディープ列に入れます そして DNN線形結合リグレッサーを
作成します これはモデルをさらに効果的にします あるいは 単にDNNリグレッサーを適用して すべてディープ列として渡すこともできます ただし DNN線形結合では 疎データを密度の濃いデータとは
異なる方法で処理できます 別々の最適化ツールを使えるのです 実際のデータセットでは
密度が濃かったりまばらだったりします そのようなデータでは この種の
リグレッサーが大いに役立ちます どの特徴を線形モデルに渡し どの特徴を深層ニューラルネットモデルに
渡すかを決めたら DNNモデルに使用する
ユニット数を指定します これがモデルですが
特徴エンジニアリングの説明を思い出しましょう データを元のままではなく
強化してから使います 特徴エンジニアリング列として
latdiffとlondiffを用意してあります 計算方法を説明すると latdiffは2つの緯度の差です londiffは2つの経度の差です これらの入力に対応する
関数を指定します 緯度と経度は
エンドユーザーが入力します エンドユーザーは londiffとlatdiffを
入力する必要はありません 計算は不要で
元データだけ入力してもらいます すべての入力列を処理しますが 最初の2つは例外です 1つ目はラベルで
入力値ではありません 2つ目は何だったでしょうか 入力列を調べてみましょう 無視するべきなのは この2つの列です 曜日と時間帯を無視します その他の列はすべて取得します いずれも浮動小数点数となります 曜日は文字列です 時間帯はint 32です これを使用して入力の
レシーバーを作成します エンドユーザーが入力する特徴だけでなく エンジニアリングした特徴も追加して
モデルに全特徴を考慮させます この時点で 以前と同様に
データを読み取ります トレーニングと評価は
以前とよく似ています これで実行できる状態になりました ではこちらに戻って 小さいデータセットで
モデルを試しましょう その後 クラウドでトレーニングします Google Cloud ML Engineを実行します RMSEはわずかに
改善されているはずです これだけでも
より良いモデルが得られますが さらに ハイパーパラメータ調整を行います これによって 
モデルに適したパラメータを見つけます 調整の詳細については 次のステップで説明します この例の場合の最適なパラメータは
こうなりました 完了したら 遥かに大きなデータセットに対して
モデルを実行します 大規模なデータセットで
トレーニングすることで 高い精度が得られます 先ほど実行したDataflowジョブは 完了するまでに
10分程度かかりました 次に実行するDataflowジョブでは 約1時間かけて100万行の
データセットを作成します そして そのデータセットで
トレーニングを行います 完了すると RMSEが
大きく改善されるはずです しかし ここで重要な概念は 元データに特徴エンジニアリングを適用して 人間による分析を取り込み
データを強化することです 交通状況、移動の距離 境界を越えるかどうか 東西南北のどちらの方向に進むのか などです londiff、latdiff、ユークリッド距離
特徴クロスなど すべてがモデルの改善に役立ちます