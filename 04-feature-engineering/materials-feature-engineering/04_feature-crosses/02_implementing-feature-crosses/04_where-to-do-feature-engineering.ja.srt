1
00:00:00,000 --> 00:00:03,660
すでにご説明したとおり
多くの方法で特徴を表現して

2
00:00:03,660 --> 00:00:06,390
特徴エンジニアリングを
行うことができます

3
00:00:06,390 --> 00:00:08,405
スケーリング、特徴クロス

4
00:00:08,405 --> 00:00:10,420
作成、埋め込みなどです

5
00:00:10,420 --> 00:00:14,400
これは機械学習モデル内の
どこに位置するのでしょうか？

6
00:00:14,400 --> 00:00:17,595
モデルの構成を思い出してください

7
00:00:17,595 --> 00:00:20,805
データを読み書きする入力関数や

8
00:00:20,805 --> 00:00:25,200
読み込みのプレースホルダとして
機能する特徴列や

9
00:00:25,200 --> 00:00:28,975
特徴クロスの受け渡しを
作成するEstimatorがあり

10
00:00:28,975 --> 00:00:32,900
さらにtrain_spec、eval_spec
exporterなどをセットアップし

11
00:00:32,900 --> 00:00:35,760
最後にtrainと
evaluateを呼び出します

12
00:00:35,760 --> 00:00:39,480
特徴エンジニアリングはこの中の
どこに当てはまりますか？

13
00:00:39,480 --> 00:00:42,570
特徴エンジニアリングには
3つの場所が考えられます

14
00:00:42,570 --> 00:00:45,510
読み込むときに即座に
データ内で実行する つまり

15
00:00:45,510 --> 00:00:47,350
入力関数で行うことも

16
00:00:47,350 --> 00:00:49,575
特徴列を作成することもできます

17
00:00:49,575 --> 00:00:54,600
別の方法は 独立したステップとして
トレーニング前に実施することです

18
00:00:54,600 --> 00:00:58,110
その後 入力関数は
事前処理された特徴を読み込みます

19
00:00:58,110 --> 00:01:01,740
別個の事前処理ステップとして行う場合は

20
00:01:01,740 --> 00:01:04,560
データフロー内でそれを処理すると

21
00:01:04,560 --> 00:01:07,920
スケーラブルかつ分散的に実施できます

22
00:01:07,920 --> 00:01:10,725
Pythonのデータフローでも
これを実施できますが

23
00:01:10,725 --> 00:01:16,740
データフローがパイプラインに含まれる
場合にのみ行ってください

24
00:01:16,740 --> 00:01:20,520
言い換えるとストリーム予測ジョブの
バッチを実行し

25
00:01:20,520 --> 00:01:25,185
こうして同じ事前処理ステップを
入力に適用できます

26
00:01:25,185 --> 00:01:29,940
3つ目の選択肢は
事前処理をデータフロー内で実施し

27
00:01:29,940 --> 00:01:32,775
事前処理済みの特徴を作成することです

28
00:01:32,775 --> 00:01:38,470
ただしTensorFlowで
同じ変換を行う必要があることを

29
00:01:38,470 --> 00:01:40,875
予測グラフに指示します

30
00:01:40,875 --> 00:01:44,655
そのためには
TensorFlow変換を使います

31
00:01:44,655 --> 00:01:47,280
前のセクションで見たように

32
00:01:47,280 --> 00:01:52,860
いくつかの事前処理ではTensorFlowで
新しい特徴列を作成できます

33
00:01:52,860 --> 00:01:56,410
次に列をバケット化して新しい列を作成し

34
00:01:56,410 --> 00:01:58,200
事前処理を行い

35
00:01:58,200 --> 00:02:02,460
この特徴列をEstimatorに送ります

36
00:02:02,460 --> 00:02:07,050
ここでは床面積の特徴列を

37
00:02:07,050 --> 00:02:09,990
4つの間隔に分けます

38
00:02:09,990 --> 00:02:14,760
最初の間隔は
500平方フィート未満の住宅です

39
00:02:14,760 --> 00:02:18,555
2つ目は500～1,000平方フィート

40
00:02:18,555 --> 00:02:22,410
3つ目は1,000～
2,500平方フィートの住宅

41
00:02:22,410 --> 00:02:27,060
最後は2,500平方フィートを超える住宅です

42
00:02:27,060 --> 00:02:32,910
バケット化した列を
元の特徴列のリストに追加します

43
00:02:32,910 --> 00:02:40,895
そして両方が2つの形式の面積が
線形回帰で認識されます

44
00:02:40,895 --> 00:02:43,125
つまり実数値の列と

45
00:02:43,125 --> 00:02:46,725
バケット化したカテゴリ列です

46
00:02:46,725 --> 00:02:49,050
もちろん必要に応じて

47
00:02:49,050 --> 00:02:52,605
数値列をバケット化列に置き換えると

48
00:02:52,605 --> 00:02:57,635
カテゴリ形式の面積だけが
線形回帰で認識されます

49
00:02:57,635 --> 00:02:59,450
ここではそれを行い

50
00:02:59,450 --> 00:03:05,500
featcols [0]を
バケット化バージョンに置き換えます

51
00:03:05,520 --> 00:03:09,745
これはもう1つの特徴クロスの例ですが

52
00:03:09,745 --> 00:03:13,450
今度は埋め込みの内部でも行います

53
00:03:13,450 --> 00:03:17,560
住宅の緯度と経度に基づき

54
00:03:17,560 --> 00:03:21,220
離散化の間隔を定義できます

55
00:03:21,220 --> 00:03:27,380
ここでは等間隔のN個のバケットを使用します

56
00:03:27,380 --> 00:03:33,565
境界の算出に使った1つの方法は
approx quantizeでBigQueryのSQL関数です

57
00:03:33,565 --> 00:03:38,570
これで容器ごとに同数の
トレーニングサンプルを入れることができます

58
00:03:38,570 --> 00:03:41,270
どんな方法で境界が得られた場合も

59
00:03:41,270 --> 00:03:42,794
たとえばこの例の

60
00:03:42,794 --> 00:03:45,665
latbucketsと
lonbucketsが得られたら

61
00:03:45,665 --> 00:03:51,470
その後 住宅の緯度と経度を
b_latとb_lonの中に格納できます

62
00:03:51,470 --> 00:03:53,860
そして すでにご説明したとおり

63
00:03:53,860 --> 00:03:57,292
2つのカテゴリ列b_latとb_lonに
特徴クロスを適用できます

64
00:03:57,292 --> 00:03:59,755
ここでは特徴クロスを適用して

65
00:03:59,755 --> 00:04:04,990
nbucketsの2乗の
ハッシュバケットにします

66
00:04:04,990 --> 00:04:11,005
すると各ハッシュバケットに入る
特徴クロス数は平均して1つだけになります

67
00:04:11,005 --> 00:04:13,350
これは私の経験則ですが

68
00:04:13,350 --> 00:04:16,050
前のレッスンでご説明したとおり

69
00:04:16,050 --> 00:04:19,389
1/2の平方根から2倍までの間です

70
00:04:19,389 --> 00:04:25,495
最後にデータをnbucketsに
4次元で埋め込みました

71
00:04:25,495 --> 00:04:31,929
事前処理をTensorFlowに
直接埋め込むメリットは

72
00:04:31,929 --> 00:04:35,350
これらの操作が
モデルグラフの一部になり

73
00:04:35,350 --> 00:04:43,105
トレーニングと本番で同じ方法で
これらが実施されることです

74
00:04:43,105 --> 00:04:47,982
これは現実世界でどんな意味がありますか？

75
00:04:47,982 --> 00:04:50,605
まず緯度を離散化します

76
00:04:50,605 --> 00:04:54,935
さまざまな実数値を単に
区分けしているだけなので

77
00:04:54,935 --> 00:05:00,515
ほぼ同じ緯度にある複数の住宅は
すべて同じ値になります

78
00:05:00,515 --> 00:05:03,580
これは過剰適合に
少し役立つもしれませんが

79
00:05:03,580 --> 00:05:07,895
緯度を離散化するだけなので
それ程でもありません

80
00:05:07,895 --> 00:05:11,325
次に経度を離散化します

81
00:05:11,325 --> 00:05:17,265
経度の値を区分化すると過剰適合に
少し役立つかもしれませんが

82
00:05:17,265 --> 00:05:22,280
経度を離散化しても
やはり大したことはありません

83
00:05:22,280 --> 00:05:28,610
でも2つの離散化した値に特徴クロスを
適用するとどうなるでしょう？

84
00:05:28,610 --> 00:05:34,075
実質的に地図を
複数のグリッドセルに分割し

85
00:05:34,075 --> 00:05:41,015
それぞれの家が1つのグリッドセルにだけ
属するようになります

86
00:05:41,015 --> 00:05:44,830
こうするとトレーニング中に

87
00:05:44,830 --> 00:05:50,455
各グリッドセル内の住宅の平均価格を
記憶させることができます

88
00:05:50,455 --> 00:05:57,850
明らかにグリッドの間隔を細かくすれば
予測がより詳細になりますが

89
00:05:57,850 --> 00:06:00,965
一般化も難しくなります
なぜなら

90
00:06:00,965 --> 00:06:04,290
適切に見積もるのに十分な数の住宅が

91
00:06:04,290 --> 00:06:08,190
グリッドセル内で売られていない
可能性があるからです

92
00:06:08,190 --> 00:06:11,050
予測中に1つの住宅を指定すると

93
00:06:11,050 --> 00:06:14,660
どのグリッドセルに属するかわかるので

94
00:06:14,660 --> 00:06:18,460
そのグリッドセルの
記憶済み値を引き出せます

95
00:06:18,460 --> 00:06:24,505
埋め込みでは
互いに類似するグリッドセル

96
00:06:24,505 --> 00:06:30,640
たとえばオーシャンフロントにある
すべてのグリッドセルに

97
00:06:30,640 --> 00:06:33,960
類似する値を持たせることができます