1
00:00:00,000 --> 00:00:02,065
Eu iniciei o Datalab,

2
00:00:02,065 --> 00:00:06,685
abri o
featureengineering.ipythonnotebook

3
00:00:06,685 --> 00:00:08,945
e vou mostrar como funciona.

4
00:00:08,945 --> 00:00:10,105
Neste bloco de notas,

5
00:00:10,105 --> 00:00:12,350
veremos como trabalhar
com colunas de atributos

6
00:00:12,350 --> 00:00:14,795
e adicionar cruzamentos
de atributos no TensorFlow.

7
00:00:14,795 --> 00:00:16,555
Vamos ler nossos dados no BigQuery,

8
00:00:16,555 --> 00:00:18,630
criar conjuntos de dados com o Dataflow

9
00:00:18,630 --> 00:00:20,490
e usar um modelo amplo e profundo.

10
00:00:20,490 --> 00:00:23,055
Vamos usar muito do que falamos até aqui.

11
00:00:23,055 --> 00:00:27,020
Só não falamos ainda sobre
o modelo amplo e profundo.

12
00:00:27,020 --> 00:00:29,600
Este é o problema.

13
00:00:29,600 --> 00:00:32,505
Até agora, criamos um modelo de táxi,

14
00:00:32,505 --> 00:00:35,545
mas não incluímos insights humanos.

15
00:00:35,545 --> 00:00:37,230
É isso que faremos agora.

16
00:00:37,230 --> 00:00:40,130
Aproveitaremos algumas das coisas

17
00:00:40,130 --> 00:00:43,310
que sabemos sobre o
funcionamento de um táxi,

18
00:00:43,310 --> 00:00:45,295
sobre o mapa de Nova York,

19
00:00:45,295 --> 00:00:48,050
e começaremos a dar dicas para o modelo

20
00:00:48,050 --> 00:00:50,135
para que ele possa aprender isso melhor.

21
00:00:50,135 --> 00:00:51,600
Conforme trabalhamos,

22
00:00:51,600 --> 00:00:54,700
falarei de onde vem um insight específico.

23
00:00:54,700 --> 00:00:58,290
Primeiro, vamos…

24
00:01:02,520 --> 00:01:04,700
começar a importação.

25
00:01:04,700 --> 00:01:06,790
Altere seu projeto.

26
00:01:06,790 --> 00:01:10,630
Eu mudei meu projeto para
mapear o projeto do Qwiklabs,

27
00:01:10,630 --> 00:01:14,485
mudei meu intervalo para
mapear o intervalo do Qwiklabs

28
00:01:14,485 --> 00:01:19,430
e mudei minha região para mapear
onde o código será executado.

29
00:01:19,430 --> 00:01:22,410
Vamos executar a consulta.

30
00:01:22,410 --> 00:01:25,730
A consulta faz uma limpeza.

31
00:01:25,730 --> 00:01:28,985
Basicamente, preciso garantir

32
00:01:28,985 --> 00:01:32,720
que só dados com distâncias
positivas sejam extraídos,

33
00:01:32,720 --> 00:01:35,330
para que "fare_amount" seja maior que 2,5,

34
00:01:35,330 --> 00:01:36,830
para que "pickup_longitude",

35
00:01:36,830 --> 00:01:38,210
"pickup_latitude" etc.,

36
00:01:38,210 --> 00:01:40,430
fiquem dentro de um limite razoável

37
00:01:40,430 --> 00:01:41,985
e que haja alguém no táxi.

38
00:01:41,985 --> 00:01:44,690
Precisamos garantir que os dados coletados

39
00:01:44,690 --> 00:01:49,605
em uma corrida específica estejam
corretos antes de usá-los no treinamento.

40
00:01:49,605 --> 00:01:53,040
Dividirei os dados, conforme mencionamos

41
00:01:53,040 --> 00:01:57,760
quando falamos sobre como criar conjuntos
de dados com base no hash do horário.

42
00:01:57,760 --> 00:02:00,810
Depois, criei minha consulta,

43
00:02:00,810 --> 00:02:04,670
que vai coletar o valor e a tarifa,

44
00:02:04,670 --> 00:02:06,080
e chamarei de "fare_amount",

45
00:02:06,080 --> 00:02:09,750
para sabermos o custo
total para uma pessoa,

46
00:02:09,750 --> 00:02:13,825
e usando o dia da semana
como "dayoftheweek".

47
00:02:13,825 --> 00:02:16,655
Por que precisamos disso?
Sabemos que

48
00:02:16,655 --> 00:02:20,190
as condições de trânsito são
diferentes dependendo do dia.

49
00:02:20,190 --> 00:02:25,100
Sabemos que a sexta tem
mais trânsito que o domingo.

50
00:02:25,100 --> 00:02:30,215
Também sabemos que a hora
é importante, mesmo na sexta.

51
00:02:30,215 --> 00:02:36,890
2h de sexta provavelmente
não terá tanto trânsito quanto às 16h.

52
00:02:36,890 --> 00:02:38,355
Por que isso é importante?

53
00:02:38,355 --> 00:02:40,545
Porque, em Nova York,

54
00:02:40,545 --> 00:02:43,355
e isso é algo que o
insight humano oferece,

55
00:02:43,355 --> 00:02:48,310
as pessoas pagam pelo
tempo gasto, além da distância.

56
00:02:48,310 --> 00:02:50,630
Se o táxi ficar preso no trânsito,

57
00:02:50,630 --> 00:02:54,135
você paga por isso,
porque está ocupando o táxi

58
00:02:54,135 --> 00:02:56,850
e o taxista não consegue
pegar outras corridas.

59
00:02:56,850 --> 00:02:58,689
O tempo é importante,

60
00:02:58,689 --> 00:03:01,570
o tempo gasto na corrida é importante.

61
00:03:01,570 --> 00:03:03,850
Antes da corrida começar,

62
00:03:03,850 --> 00:03:05,650
não sabemos quanto tempo ela levará.

63
00:03:05,650 --> 00:03:09,560
Queremos que o modelo de
aprendizado de máquina aprenda isso,

64
00:03:09,560 --> 00:03:12,620
e sabemos que um fator
determinante da duração

65
00:03:12,620 --> 00:03:16,370
da corrida é quando ela começou.

66
00:03:16,370 --> 00:03:20,180
Não o fim da corrida, porque não
sabemos quando a pessoa sairá do táxi,

67
00:03:20,180 --> 00:03:22,630
mas sabemos quando ela começará a corrida.

68
00:03:22,630 --> 00:03:25,190
Basicamente, usamos o horário de início,

69
00:03:25,190 --> 00:03:29,320
o dia da semana e a hora do dia
como entradas para o modelo.

70
00:03:29,320 --> 00:03:31,730
Também sabemos onde a corrida começará.

71
00:03:31,730 --> 00:03:35,000
E onde o cliente quer ser deixado.

72
00:03:35,000 --> 00:03:37,430
Não sabemos o horário
de término da corrida,

73
00:03:37,430 --> 00:03:39,125
mas sabemos
para onde o cliente vai.

74
00:03:39,125 --> 00:03:42,130
Conhecemos a longitude
e a latitude do local.

75
00:03:42,130 --> 00:03:44,145
Elas também farão parte da entrada.

76
00:03:44,145 --> 00:03:46,100
Criaremos uma contagem de passageiros

77
00:03:46,100 --> 00:03:49,040
e criaremos uma chave.

78
00:03:49,040 --> 00:03:50,320
Eu usarei essa chave,

79
00:03:50,320 --> 00:03:53,300
mas, se você quiser fazer
um tipo de previsão em lote,

80
00:03:53,300 --> 00:03:56,505
com a inserção de muitos dados,

81
00:03:56,505 --> 00:04:01,205
é útil se cada linha do conjunto
de dados tiver um código exclusivo.

82
00:04:01,205 --> 00:04:05,110
Esse é um tipo de formulário de
código das colunas de entrada.

83
00:04:05,970 --> 00:04:10,110
Estou fazendo isso,
onde todos os dados são válidos.

84
00:04:10,110 --> 00:04:11,730
Neste momento,

85
00:04:11,730 --> 00:04:14,115
estamos prontos para
criar o conjunto de dados.

86
00:04:14,115 --> 00:04:15,890
Para criar o conjunto de dados,

87
00:04:15,890 --> 00:04:20,279
vamos excluir qualquer
conjunto de dados existente.

88
00:04:20,279 --> 00:04:23,715
Depois disso, podemos avançar

89
00:04:23,715 --> 00:04:31,190
e criar um arquivo CSV com essas colunas.

90
00:04:31,190 --> 00:04:35,395
Primeiro, precisamos garantir que
as colunas sejam "fare_amount",

91
00:04:35,395 --> 00:04:37,690
"dayoftheweek", "houroftheday" etc.

92
00:04:37,690 --> 00:04:40,010
Essas são as colunas que queremos criar,

93
00:04:40,010 --> 00:04:44,420
mas o dia da semana,
no conjunto de dados do BigQuery,

94
00:04:44,420 --> 00:04:46,760
será um número como 2.

95
00:04:46,760 --> 00:04:49,700
Não queremos
um número 2 porque não sabemos

96
00:04:49,700 --> 00:04:50,890
qual dia da semana ele é.

97
00:04:50,890 --> 00:04:52,350
A semana começa no domingo,

98
00:04:52,350 --> 00:04:53,535
na segunda ou na terça?

99
00:04:53,535 --> 00:04:55,250
Não queremos que o código do cliente

100
00:04:55,250 --> 00:04:57,365
precise lidar com isso.

101
00:04:57,365 --> 00:04:59,495
O que fazemos é substituir

102
00:04:59,495 --> 00:05:04,450
esses números pelo
nome dos dias da semana.

103
00:05:04,450 --> 00:05:07,950
Se o primeiro dia é o domingo,

104
00:05:07,950 --> 00:05:09,120
e o número mostrado é 2,

105
00:05:09,120 --> 00:05:10,570
significa segunda-feira.

106
00:05:10,570 --> 00:05:12,320
É isso que estou fazendo.

107
00:05:12,320 --> 00:05:14,335
Estou usando o resultado do BigQuery,

108
00:05:14,335 --> 00:05:16,150
um dia da semana expresso como número,

109
00:05:16,150 --> 00:05:18,679
e substituindo por uma string.

110
00:05:18,679 --> 00:05:23,565
Agora, eu reúno todos
separados por vírgulas,

111
00:05:23,565 --> 00:05:27,985
gerando um arquivo CSV.

112
00:05:27,985 --> 00:05:29,790
Agora, para gravar isso,

113
00:05:29,790 --> 00:05:32,540
eu preciso ler os dados

114
00:05:32,540 --> 00:05:36,180
no BigQuery usando a consulta criada

115
00:05:36,180 --> 00:05:39,920
e convertê-la em CSV
com a função que comentamos.

116
00:05:39,920 --> 00:05:42,500
A única alteração é que estamos mudando

117
00:05:42,500 --> 00:05:45,525
os dias da semana de números para strings

118
00:05:45,525 --> 00:05:49,010
e criando um arquivo de texto CSV.

119
00:05:49,010 --> 00:05:51,540
Agora, quando executo o código,

120
00:05:51,540 --> 00:05:56,110
temos o pré-processamento do texto.

121
00:05:56,110 --> 00:05:59,140
Na próxima célula,

122
00:05:59,140 --> 00:06:04,010
posso chamar o pré-processamento
no executor do Dataflow, se quiser,

123
00:06:04,010 --> 00:06:08,620
ou posso criar um conjunto de dados
menor para executá-lo localmente.

124
00:06:08,620 --> 00:06:12,225
Nesse caso, eu executo no Dataflow,

125
00:06:12,225 --> 00:06:14,295
ele será executado

126
00:06:14,295 --> 00:06:16,585
e levará um tempo.

127
00:06:16,585 --> 00:06:20,240
Vamos para o console,

128
00:06:20,240 --> 00:06:26,015
e podemos ver que o job
foi iniciado no executor.

129
00:06:26,015 --> 00:06:31,080
Quando vamos para o Dataflow…

130
00:06:33,165 --> 00:06:35,845
O que ele diz?

131
00:06:37,790 --> 00:06:40,185
Dataflow…

132
00:06:43,845 --> 00:06:44,645
Achei.

133
00:06:44,645 --> 00:06:48,420
Uma mensagem de que
a API Dataflow não foi usada ou ativada.

134
00:06:48,420 --> 00:06:52,250
Temos que vir aqui.

135
00:06:52,250 --> 00:06:53,730
Se você vir esse erro,

136
00:06:53,730 --> 00:06:58,430
precisará acessar "APIs and Services"

137
00:07:00,140 --> 00:07:04,895
e procurar os dados de serviços ativos.

138
00:07:04,895 --> 00:07:09,685
Queremos ativar o Dataflow.

139
00:07:09,685 --> 00:07:12,935
Quando procuramos,
acessamos a API Dataflow.

140
00:07:12,935 --> 00:07:16,445
Vamos ativar a API.

141
00:07:16,445 --> 00:07:20,110
Quando ela for ativada…

142
00:07:22,620 --> 00:07:24,655
Vamos esperar a ativação.

143
00:07:24,655 --> 00:07:27,100
Poderemos executar a célula novamente.

144
00:07:27,100 --> 00:07:29,560
Certo, ela foi ativada.

145
00:07:29,560 --> 00:07:37,260
Agora, voltamos ao bloco de notas
do Datalab e executamos a célula.

146
00:07:38,350 --> 00:07:44,240
Dessa vez, ela será executada.

147
00:07:44,240 --> 00:07:46,865
Pronto, deu certo.

148
00:07:46,865 --> 00:07:56,260
Agora, posso voltar
para o Dataflow no menu

149
00:07:56,260 --> 00:07:59,155
e você verá o código em execução.

150
00:07:59,155 --> 00:08:01,235
Isso demorará um pouco.

151
00:08:01,235 --> 00:08:02,955
Quando terminar,

152
00:08:02,955 --> 00:08:05,355
na nuvem, no seu intervalo,

153
00:08:05,355 --> 00:08:11,220
você terá os arquivos que
pode usar para o treinamento.

154
00:08:13,170 --> 00:08:15,450
Vamos ver aqui.

155
00:08:18,420 --> 00:08:22,525
Podemos fazer isso, mas vamos ver.

156
00:08:22,525 --> 00:08:24,820
Vamos esperar ele terminar.

157
00:08:24,820 --> 00:08:26,665
Quando terminar,

158
00:08:26,665 --> 00:08:28,105
poderemos voltar.

159
00:08:28,105 --> 00:08:29,695
Vou pausar o vídeo aqui,

160
00:08:29,695 --> 00:08:35,289
voltamos depois e continuamos
quando o job for concluído.

161
00:08:35,289 --> 00:08:39,565
O job levou cerca de oito minutos,

162
00:08:39,565 --> 00:08:41,520
a última etapa foi bem-sucedida.

163
00:08:41,520 --> 00:08:42,735
Nesse momento,

164
00:08:42,735 --> 00:08:45,380
o número de workers
está diminuindo novamente.

165
00:08:45,380 --> 00:08:48,860
A milhagem varia dependendo
de quantos workers você tem

166
00:08:48,860 --> 00:08:52,625
e quantos estão em execução no job.

167
00:08:52,625 --> 00:08:55,200
Quando terminar,

168
00:08:55,200 --> 00:09:01,110
você pode voltar ao bloco de notas e
ver se os arquivos de saída estão lá.

169
00:09:01,110 --> 00:09:02,640
É isso que estou fazendo.

170
00:09:02,640 --> 00:09:05,490
Estou usando GS no gsutil ls

171
00:09:05,490 --> 00:09:09,375
no intervalo, e consigo ver
um arquivo train.csv

172
00:09:09,375 --> 00:09:11,395
e um arquivo valid.csv.

173
00:09:11,395 --> 00:09:15,620
Temos um arquivo de treinamento
no arquivo de validação,

174
00:09:15,620 --> 00:09:19,050
e também podemos usar "cut".

175
00:09:19,050 --> 00:09:23,380
"Cut" é um comando do Unix
que lista as primeiras linhas.

176
00:09:23,430 --> 00:09:27,720
Na verdade, ele lista
todas e gera um canal

177
00:09:27,720 --> 00:09:29,830
para mostrar as primeiras linhas.

178
00:09:29,830 --> 00:09:32,905
Assim, podemos ver o que esperamos,

179
00:09:32,905 --> 00:09:37,035
o dia da semana é uma string:
sexta, quarta etc.

180
00:09:37,035 --> 00:09:39,230
Nós temos latitudes,

181
00:09:39,230 --> 00:09:41,610
longitudes, pontos de
início e término da corrida.

182
00:09:41,610 --> 00:09:45,070
Também temos uma última coisa.

183
00:09:45,070 --> 00:09:48,890
A última coluna é uma chave
que vamos ignorar no modelo,

184
00:09:48,890 --> 00:09:53,820
mas que existe se precisarmos de
um código exclusivo para cada linha.

185
00:09:53,820 --> 00:09:55,575
Este arquivo está pronto,

186
00:09:55,575 --> 00:09:58,930
e podemos usá-lo para
desenvolver nosso modelo.

187
00:09:58,930 --> 00:10:02,590
Para esse desenvolvimento,

188
00:10:02,590 --> 00:10:05,455
é melhor não precisar
voltar sempre para o Cloud.

189
00:10:05,455 --> 00:10:08,860
Por isso, farei
um diretório chamado "sample"

190
00:10:08,860 --> 00:10:12,360
e copiarei um dos arquivos para ele.

191
00:10:12,360 --> 00:10:14,005
Como temos arquivos fragmentados,

192
00:10:14,005 --> 00:10:21,280
estou copiando apenas a primeira parte
para a amostra do meu diretório local.

193
00:10:21,280 --> 00:10:29,215
Depois, podemos ver o código.

194
00:10:29,215 --> 00:10:30,930
Vamos ver nosso código.

195
00:10:30,930 --> 00:10:32,690
Podemos fazer isso no bloco de notas,

196
00:10:32,690 --> 00:10:34,245
mas vamos ver fora dele.

197
00:10:34,245 --> 00:10:36,700
Temos nossa tarifa de táxi.

198
00:10:36,700 --> 00:10:39,735
Nessa tarifa, assim como antes,

199
00:10:39,735 --> 00:10:41,205
temos um trainer e,

200
00:10:41,205 --> 00:10:44,830
como antes, temos model.pi e tasks.pi.

201
00:10:44,830 --> 00:10:47,200
Mas model.pi, nesse caso,

202
00:10:47,200 --> 00:10:49,570
não será apenas uma entrada bruta.

203
00:10:49,570 --> 00:10:52,210
Ele terá alguma engenharia de atributos.

204
00:10:52,210 --> 00:10:55,910
Estas são as colunas presentes, e observe

205
00:10:55,910 --> 00:10:59,460
que agora há algumas colunas a mais.

206
00:10:59,460 --> 00:11:00,820
Temos o dia da semana,

207
00:11:00,820 --> 00:11:02,645
o horário do dia etc.

208
00:11:02,645 --> 00:11:07,725
Estas são as colunas de entrada,

209
00:11:07,725 --> 00:11:09,420
eu tenho o dia da semana,

210
00:11:09,420 --> 00:11:11,490
um vocabulário, constituído de domingo,

211
00:11:11,490 --> 00:11:14,030
segunda, terça etc., os dias da semana.

212
00:11:14,030 --> 00:11:17,805
O horário também é uma coluna categórica,

213
00:11:17,805 --> 00:11:20,260
mas tem uma identidade.

214
00:11:20,260 --> 00:11:22,610
Ou seja, já é um número inteiro.

215
00:11:22,610 --> 00:11:24,640
1, 2, 3, 4 etc.

216
00:11:24,640 --> 00:11:27,370
Depois, temos as colunas
numéricas de longitude

217
00:11:27,370 --> 00:11:29,330
e latitude de início, latitude

218
00:11:29,330 --> 00:11:31,250
e longitude de término etc.

219
00:11:31,250 --> 00:11:33,845
Eu também criarei

220
00:11:33,845 --> 00:11:39,110
algumas colunas com engenharia
para usar posteriormente no código,

221
00:11:39,110 --> 00:11:42,529
mas as colunas serão
a diferença de latitude.

222
00:11:42,529 --> 00:11:44,260
Por que isso é importante?

223
00:11:44,260 --> 00:11:48,835
A diferença de latitude diz se você
vai do norte para o sul de Manhattan.

224
00:11:48,835 --> 00:11:54,470
Isso dá uma ideia de
quanto a latitude mudou.

225
00:11:54,470 --> 00:11:56,890
A diferença de longitude é útil,

226
00:11:56,890 --> 00:11:59,320
porque Nova York
não tem tanta extensão ao sul,

227
00:11:59,320 --> 00:12:06,165
e todas as pontes com pedágio
geram mudanças drásticas na longitude.

228
00:12:06,165 --> 00:12:08,880
Por isso, é útil saber
a diferença na longitude.

229
00:12:08,880 --> 00:12:13,485
Também adicionei uma distância euclidiana

230
00:12:13,485 --> 00:12:16,510
entre os pontos de início e de término.

231
00:12:16,510 --> 00:12:18,695
Esse também é um bom recurso,

232
00:12:18,695 --> 00:12:21,680
porque o modelo não
precisa aprender distâncias,

233
00:12:21,680 --> 00:12:24,100
ela já é dada de início.

234
00:12:24,100 --> 00:12:26,900
Nós fazemos essa engenharia de atributos

235
00:12:26,900 --> 00:12:29,615
e estamos prontos para criar um Estimator.

236
00:12:29,615 --> 00:12:33,315
No Estimator, nós usamos
todas as colunas de entrada.

237
00:12:33,315 --> 00:12:35,660
Essas são as colunas que nós temos.

238
00:12:35,660 --> 00:12:40,845
Assim como no exercício de engenharia
no conjunto de dados de armazenamento,

239
00:12:40,845 --> 00:12:44,270
nós intervalamos a latitude e a longitude.

240
00:12:44,270 --> 00:12:50,265
Nós intervalamos
a latitude de início entre 38 e 42,

241
00:12:50,265 --> 00:12:55,150
e a longitude de -76 a -72,
porque é Nova York

242
00:12:55,150 --> 00:12:57,130
e esses são os limites da cidade.

243
00:12:57,130 --> 00:13:00,655
Vamos intervalar a latitude de início,

244
00:13:00,655 --> 00:13:02,935
a latitude de término

245
00:13:02,935 --> 00:13:05,040
e as longitudes,

246
00:13:05,040 --> 00:13:07,815
tanto de início quanto de término.

247
00:13:07,815 --> 00:13:09,690
Todas são intervaladas.

248
00:13:09,690 --> 00:13:12,050
Depois de intervalar,

249
00:13:12,050 --> 00:13:13,475
o que isso faz?

250
00:13:13,475 --> 00:13:17,595
Isso distingue as coisas,
pega um valor numérico

251
00:13:17,595 --> 00:13:21,360
e o torna categórico,
porque faz parte de um intervalo.

252
00:13:21,360 --> 00:13:23,950
Nós usamos esses valores categóricos

253
00:13:23,950 --> 00:13:26,160
e fazemos o cruzamento de atributos.

254
00:13:26,160 --> 00:13:32,810
O que acontece quando cruzamos
a latitude e a longitude de início?

255
00:13:32,810 --> 00:13:34,880
Nós temos a latitude e a longitude

256
00:13:34,880 --> 00:13:36,655
e fazemos o cruzamento de atributos.

257
00:13:36,655 --> 00:13:38,280
Nós essencialmente colocamos

258
00:13:38,280 --> 00:13:43,579
o local de início,
a célula de grade correspondente.

259
00:13:43,579 --> 00:13:45,585
Isso é o ploc.

260
00:13:45,585 --> 00:13:47,875
Ploc agora é como uma grade.

261
00:13:47,875 --> 00:13:52,280
Do mesmo modo, dloc é uma
grade que corresponde ao término.

262
00:13:52,280 --> 00:13:55,105
Ambos são pontos em uma grade.

263
00:13:55,105 --> 00:14:01,470
Eu faço o cruzamento de atributos
dos locais de início e término.

264
00:14:01,470 --> 00:14:06,105
Estamos falando para o modelo aprender

265
00:14:06,105 --> 00:14:11,305
com todas as corridas de táxi
daqui até aqui. Quanto elas custam?

266
00:14:11,305 --> 00:14:13,510
A única maneira de fazer isso,

267
00:14:13,510 --> 00:14:15,790
e isso é algo que
precisamos repetir sempre,

268
00:14:15,790 --> 00:14:19,345
é que o cruzamento de
atributos é muito poderoso,

269
00:14:19,345 --> 00:14:25,170
mas só funciona se você tiver dados
o bastante, porque ele usa memorização.

270
00:14:25,170 --> 00:14:30,285
Ele funciona com memorização se você
tiver dados suficientes nos intervalos.

271
00:14:30,285 --> 00:14:34,105
Neste caso, temos
milhões de corridas de táxi,

272
00:14:34,105 --> 00:14:36,640
então temos
dados suficientes para fazer isso.

273
00:14:37,620 --> 00:14:40,120
Nós intervalamos a longitude de início,

274
00:14:40,120 --> 00:14:41,915
a longitude de término,

275
00:14:41,915 --> 00:14:43,815
usamos para criar o ploc,

276
00:14:43,815 --> 00:14:46,680
o dloc, fazemos o cruzamento de atributos,

277
00:14:46,680 --> 00:14:49,820
e agora temos um par de início e término

278
00:14:49,820 --> 00:14:51,530
que também é um cruzamento.

279
00:14:51,530 --> 00:14:54,260
Depois, usamos o dia e a hora,

280
00:14:54,260 --> 00:14:57,865
porque o tráfego
depende dessas informações.

281
00:14:57,865 --> 00:15:02,865
Sexta às 15h é diferente
de quarta às 15h e de domingo às 15h.

282
00:15:02,865 --> 00:15:08,960
Fazemos o cruzamento e decidimos
o número de intervalos a ser usado.

283
00:15:11,100 --> 00:15:15,400
Você pode escolher um valor
qualquer, do dobro do total

284
00:15:15,400 --> 00:15:19,850
de valores possíveis até a
quarta raiz do número possível.

285
00:15:19,850 --> 00:15:23,760
Neste caso, estou usando
o número total de valores.

286
00:15:23,760 --> 00:15:26,070
24/7 para o número de intervalos.

287
00:15:26,070 --> 00:15:28,790
Mas isso é algo que você precisa testar,

288
00:15:28,790 --> 00:15:31,465
além de ajustar os hiperparâmetros.

289
00:15:31,465 --> 00:15:38,990
Não há uma resposta certa para
quantos intervalos de hash usar.

290
00:15:38,990 --> 00:15:41,985
Voltamos a analisar nossos dados

291
00:15:41,985 --> 00:15:45,260
para dizer quais são esparsos

292
00:15:45,260 --> 00:15:49,500
e categóricos e quais
são densos e numéricos.

293
00:15:49,500 --> 00:15:52,310
As colunas esparsas e categóricas ficam

294
00:15:52,310 --> 00:15:57,545
na parte maior de uma rede, porque
modelos lineares são melhores para elas.

295
00:15:57,545 --> 00:16:02,505
E as colunas densas e numéricas -

296
00:16:02,505 --> 00:16:04,330
colunas de incorporação
são um exemplo

297
00:16:04,330 --> 00:16:07,020
de colunas densas
porque colocamos os dados esparsos

298
00:16:07,020 --> 00:16:10,460
em um espaço pequeno -

299
00:16:10,460 --> 00:16:14,675
são úteis também.

300
00:16:14,675 --> 00:16:18,955
Precisamos colocas nossas
colunas esparsas nas colunas brancas.

301
00:16:18,955 --> 00:16:21,360
Nossos dados densos

302
00:16:21,360 --> 00:16:23,485
vão para as colunas profundas,

303
00:16:23,485 --> 00:16:27,625
e criamos um
DNNLinearCombinedRegressor.

304
00:16:27,625 --> 00:16:32,350
Isso é um recurso extra
para o modelo. Se você quiser,

305
00:16:32,350 --> 00:16:34,550
pode fazer apenas um regressor DNN,

306
00:16:34,550 --> 00:16:37,710
analisando tudo isso
como colunas profundas.

307
00:16:37,710 --> 00:16:40,810
Isso seria ótimo, mas o
DNNLinearCombined permite

308
00:16:40,810 --> 00:16:44,360
tratar os dados esparsos e os
densos de maneiras diferentes.

309
00:16:44,360 --> 00:16:48,255
Ele usa um otimizador diferente
para os dados esparsos,

310
00:16:48,255 --> 00:16:52,755
é ajustado para a ideia de que,
em um conjunto de dados real,

311
00:16:52,755 --> 00:16:56,550
alguns dos atributos serão
densos e outros serão esparsos.

312
00:16:56,550 --> 00:17:00,900
Então, esse tipo de regressor
funciona bem com esse tipo de dados.

313
00:17:00,900 --> 00:17:05,685
Com isso, estamos analisando quais
atributos precisam de um modelo linear

314
00:17:05,685 --> 00:17:08,795
e quais precisam
de um modelo de rede neural profundo.

315
00:17:08,795 --> 00:17:13,165
Além disso, especificamos o número
de unidades que queremos no modelo.

316
00:17:13,165 --> 00:17:14,405
Este é o modelo.

317
00:17:14,405 --> 00:17:17,400
Lembra que falamos
sobre a engenharia de atributos?

318
00:17:17,400 --> 00:17:19,200
Não queremos apenas os dados brutos,

319
00:17:19,200 --> 00:17:20,750
queremos adicionar itens a eles,

320
00:17:20,750 --> 00:17:23,190
e já temos colunas
de engenharia de atributos:

321
00:17:23,190 --> 00:17:25,160
latdiff, londiff…

322
00:17:25,160 --> 00:17:26,569
é assim que você as processa.

323
00:17:26,569 --> 00:17:29,695
A latdiff é a diferença
entre as duas latitudes,

324
00:17:29,695 --> 00:17:32,745
a londiff é a diferença
entre as duas longitudes,

325
00:17:32,745 --> 00:17:38,150
e depois especificamos a função
de entrada de disponibilização,

326
00:17:38,150 --> 00:17:41,930
ela diz o que o usuário
final precisa fornecer.

327
00:17:41,930 --> 00:17:45,740
O usuário não precisa
fornecer latdiff e londiff,

328
00:17:45,740 --> 00:17:47,100
ele não sabe como computar,

329
00:17:47,100 --> 00:17:49,125
só precisa fornecer os dados brutos.

330
00:17:49,125 --> 00:17:52,120
Nós passamos por todas
as colunas de entrada,

331
00:17:52,120 --> 00:17:54,990
exceto as duas primeiras,

332
00:17:54,990 --> 00:17:58,085
que são o valor da tarifa,
que é um marcador,

333
00:17:58,085 --> 00:18:00,190
obviamente não é uma entrada,

334
00:18:00,190 --> 00:18:02,815
e a segunda estamos ignorando.

335
00:18:02,815 --> 00:18:05,105
Vamos ver as colunas de entrada.

336
00:18:05,105 --> 00:18:07,810
A segunda que estamos ignorando…

337
00:18:08,960 --> 00:18:10,595
Estamos ignorando estas duas.

338
00:18:10,595 --> 00:18:14,280
Dia da semana e horário.

339
00:18:14,280 --> 00:18:17,980
Usamos basicamente todo o resto

340
00:18:17,980 --> 00:18:22,000
para dizer que são números pontuais.

341
00:18:22,000 --> 00:18:23,995
O dia da semana é uma string,

342
00:18:23,995 --> 00:18:26,640
e o horário é um int32.

343
00:18:26,640 --> 00:18:30,925
Ele basicamente é usado para
criar um receptor de entrada,

344
00:18:30,925 --> 00:18:36,160
mas, além dos atributos
fornecidos pelos usuários,

345
00:18:36,160 --> 00:18:40,860
adicionamos os atributos com engenharia
para que o modelo veja tudo.

346
00:18:40,860 --> 00:18:46,010
Agora, a leitura dos dados
é semelhante ao que já vimos,

347
00:18:46,010 --> 00:18:49,200
o treinamento e a avaliação também,

348
00:18:49,200 --> 00:18:52,405
então, podemos executar.

349
00:18:52,405 --> 00:18:54,010
Vamos voltar aqui

350
00:18:54,010 --> 00:18:57,995
e testar nosso modelo em
um conjunto de dados menor.

351
00:18:57,995 --> 00:19:00,490
Podemos treiná-lo na nuvem também.

352
00:19:00,490 --> 00:19:05,510
Vamos para o GCloud
ML Engine e, ao executar,

353
00:19:05,510 --> 00:19:10,450
você gera um RMSE um pouco melhor,

354
00:19:10,450 --> 00:19:13,460
mas aqui temos um modelo melhor.

355
00:19:13,460 --> 00:19:15,160
O próximo passo é ajustar

356
00:19:15,160 --> 00:19:18,645
os hiperparâmetros
para encontrar parâmetros bons do modelo.

357
00:19:18,645 --> 00:19:20,040
Para isso,

358
00:19:20,040 --> 00:19:24,570
falaremos sobre o
ajuste de hiperparâmetros,

359
00:19:24,570 --> 00:19:27,840
em que você recebe os
parâmetros para esses modelos.

360
00:19:27,840 --> 00:19:30,780
Neste caso, estes foram os melhores.

361
00:19:30,780 --> 00:19:32,405
Depois de fazer isso,

362
00:19:32,405 --> 00:19:35,820
podemos executar em um
conjunto de dados muito maior.

363
00:19:35,820 --> 00:19:38,320
Um aspecto importante
do aprendizado de máquina

364
00:19:38,320 --> 00:19:41,230
é que você consegue o melhor
desempenho com conjuntos grandes.

365
00:19:41,230 --> 00:19:46,320
Antes, eu executei um job do
Dataflow que levaria 10 minutos

366
00:19:46,320 --> 00:19:47,965
para continuarmos.

367
00:19:47,965 --> 00:19:50,390
Agora, vamos executar um job do Dataflow

368
00:19:50,390 --> 00:19:53,150
que dura cerca de uma hora
para criar um conjunto

369
00:19:53,150 --> 00:19:55,070
muito maior, com milhões de linhas.

370
00:19:55,070 --> 00:19:57,020
Podemos treinar nele.

371
00:19:57,020 --> 00:19:58,410
Depois disso,

372
00:19:58,410 --> 00:20:00,910
você verá um RMSE muito melhor.

373
00:20:00,910 --> 00:20:05,180
Mas a ideia principal
é usar seus dados brutos

374
00:20:05,180 --> 00:20:08,294
para fazer a engenharia de atributos
e incluir insight humano

375
00:20:08,294 --> 00:20:11,635
nos elementos importantes, como tráfego,

376
00:20:11,635 --> 00:20:13,490
distância das corridas,

377
00:20:13,490 --> 00:20:17,510
se elas cruzam limites,

378
00:20:17,830 --> 00:20:23,000
o sentido delas nas distâncias londiff,

379
00:20:23,000 --> 00:20:25,415
latdiff e distância euclidiana,

380
00:20:25,415 --> 00:20:29,610
o cruzamento de atributos,
tudo isso melhora seu modelo.