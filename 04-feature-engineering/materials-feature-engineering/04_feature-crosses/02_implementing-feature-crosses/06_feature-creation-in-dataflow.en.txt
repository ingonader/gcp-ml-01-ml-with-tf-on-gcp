Recall that there are three possible places to do feature engineering. We have looked at how to do feature preprocessing and feature creation on the fly intenser flow. The other option is to do the preprocessing of feature creation in Cloud Dataflow. Here, dataflow is used to create a preprocessed or augmented data set and this new data set is used to train the model. During prediction, we need to figure out a way to carry out the same preprocessing steps. So this method works best if dataflow is part of a prediction runtime as well. Recall that the reference architecture for GCP does this. Dataflow, because it can handle both streaming data and batch data, is part of the pipeline in both training and prediction. If you do this, then dataflow is a fine place to do preprocessing. Dataflow is ideal for features that involve time-windowed aggregation. For example, you might want to use as a feature the average number of people who look at a product in the past one hour. In training, you can use dataflow to compute this from log files, but the nature of such a feature implies that you have to use dataflow in real time to compute that based on your real time traffic. You could add extra fields in any Ptransform in dataflow. The add fields in this example, it's a pardue that takes the input fields, pulls out the passenger count, accumulates them and adds a visitor count as the past hour count. The same code and dataflow works in both batch and stream so you simply have the add fields method in both the training pipeline and in the predictions pipeline. The third option is to use a hybrid approach. Google researchers publish how to do this recently, and we look at it in detail in the next module. But the gist of this is this, during training you will create a preprocessed data set using dataflow. However, your transformations themselves will be implemented in TenserFlow. So that during predictions, the feature engineering is part of the TenserFlow graph. This is very advantageous because dataflow is great at computing aggregates over all of the data. While TensorFlow is advantageous when it comes to manipulating the input fields on the fly.