1
00:00:00,500 --> 00:00:03,140
In the first course in
this specialization,

2
00:00:03,140 --> 00:00:06,010
we talked about ML Fairness.

3
00:00:06,010 --> 00:00:10,480
Now that we have a real world
model to predict taxi fares,

4
00:00:10,480 --> 00:00:14,550
let's take a look at whether
questions of ML fairness arise.

5
00:00:15,770 --> 00:00:18,453
Our model uses features crosses.

6
00:00:18,453 --> 00:00:20,130
Is it fair to do so?

7
00:00:21,180 --> 00:00:22,560
Is there a potential problem?

8
00:00:24,940 --> 00:00:29,680
Can the resolution of the feature cross,
the feature cross of latitude and

9
00:00:29,680 --> 00:00:34,980
longitude, can the resolution of that
feature cross amplify injustice?

10
00:00:37,130 --> 00:00:40,879
It all depends on how the ML
model is going to be used.

11
00:00:42,030 --> 00:00:46,070
A pure taxi fare model appears innocent.

12
00:00:46,070 --> 00:00:50,090
But you have to realize that machine
learning models will be used to make

13
00:00:50,090 --> 00:00:50,920
decisions.

14
00:00:52,050 --> 00:00:56,918
And if the model's estimated
fare is used to advertize

15
00:00:56,918 --> 00:01:02,170
a potential passenger to
a bunch of taxi drivers,

16
00:01:02,170 --> 00:01:06,720
then the idiosyncracies of this
machine learning model will start to

17
00:01:06,720 --> 00:01:11,260
determine whether or
not a passenger gets picked up.

18
00:01:12,600 --> 00:01:16,150
Imagine a neighborhood that's right
below the Queen's Borough Bridge.

19
00:01:17,240 --> 00:01:22,180
To get there, a taxi needs to drive
a long way on the bridge, and

20
00:01:22,180 --> 00:01:25,425
then below the bridge, and
then repeat this on the way back.

21
00:01:25,425 --> 00:01:28,790
The accuracy of fair estimates for

22
00:01:28,790 --> 00:01:34,857
such a neighbourhood relies heavily on
the resolution of the feature cross.

23
00:01:34,857 --> 00:01:39,790
Yet the more fine grain the feature cross,
the more

24
00:01:39,790 --> 00:01:45,196
likely it is that one of behaviours
start to play a larger impact.

25
00:01:45,196 --> 00:01:51,470
For example, asingle passenger
who lives in that neighborhood

26
00:01:51,470 --> 00:01:56,610
who always makes a taxi wait with
meter running, or asks the taxi

27
00:01:56,610 --> 00:02:01,930
to take a roundabout route just because
he wants to go through Central Park.

28
00:02:01,930 --> 00:02:06,740
A single passenger can completely throw
off the system because a feature cross

29
00:02:06,740 --> 00:02:07,579
is so fine grained.

30
00:02:08,670 --> 00:02:14,660
So it appears that using the feature
cross can make the system more adjust.

31
00:02:14,660 --> 00:02:15,770
Maybe we should use it.

32
00:02:16,810 --> 00:02:18,770
But what's the alternative?

33
00:02:18,770 --> 00:02:21,360
Remember that if we don't
use the feature cross,

34
00:02:21,360 --> 00:02:23,659
then we are at the mercy
of our raw features.

35
00:02:24,750 --> 00:02:28,790
And although we didn't look at
feature importance, it turns out that

36
00:02:28,790 --> 00:02:35,200
the Euclidean distance is a most important
feature once you remove feature crosses.

37
00:02:35,200 --> 00:02:37,450
So if we don't use feature crosses,

38
00:02:37,450 --> 00:02:40,480
we'll be extremely reliant
on the Euclidean distance.

39
00:02:41,630 --> 00:02:42,965
This seems pretty straightforward.

40
00:02:42,965 --> 00:02:46,800
There shouldn't be any fairness
problem with Euclidean distance right?

41
00:02:47,950 --> 00:02:51,980
However, what if I told you
that richer neighborhoods

42
00:02:51,980 --> 00:02:55,650
tend to have better access to highways so

43
00:02:55,650 --> 00:03:01,220
that the straight line distance tends to
be quite accurate for such neighborhoods.

44
00:03:02,540 --> 00:03:07,110
So a low res feature cross will
tend to have bad fare estimates for

45
00:03:07,110 --> 00:03:08,930
poorer neighborhoods.

46
00:03:08,930 --> 00:03:14,100
And so it's poorer neighborhoods that
start to pop up with weirdly high fair

47
00:03:14,100 --> 00:03:19,170
estimates if you have high resolution,
are always incorrect fair estimates,

48
00:03:19,170 --> 00:03:21,620
if we have very low
resolution feature process.

49
00:03:22,880 --> 00:03:25,190
There are no easy answers,

50
00:03:25,190 --> 00:03:30,640
there is no shortcut to actually
knowing the data and the domain.

51
00:03:30,640 --> 00:03:34,910
So the way to check would be
to look at the final impact,

52
00:03:34,910 --> 00:03:37,390
the final decision being made.

53
00:03:37,390 --> 00:03:41,000
And you would have to
model this decision and

54
00:03:41,000 --> 00:03:45,700
ensure that all stakeholders
understand what the model predicts,

55
00:03:46,730 --> 00:03:49,770
and what the impact in
the real world would be.