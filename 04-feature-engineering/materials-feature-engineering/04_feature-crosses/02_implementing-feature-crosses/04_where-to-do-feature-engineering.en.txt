So, we have talked about quite a few ways to represent features, and to do feature engineering, with scaling feature crosses, creating, embedding et cetera. But where does this go into your machine learning model? Recall that your model consists of these parts, an input function to read and the data, feature columns that act as placeholders for the things that you read, an estimator that you create passing in the feature columns, and then you set up your train spec, eval spec, exporter et cetera, and finally you call train and evaluate. Where does the feature engineering fit into all of this? There are three possible places to do feature engineering. You could do it on the fly as you read in the data, in the input function itself, or by creating feature columns. Alternately, you could do it as a separate step before you do the training. Then your input function reads the preprocessed features. And if you do it as a separate preprocessing step, you will do the preprocessing in data flow, so, that you can do at scale in a distributed way. You could do it in plain Python data flow, but you should do that only if data flow is also part of your serving pipeline. In other words, you are doing a batch of stream prediction job. And so, you can apply the same preprocessing steps on the serving inputs. The third option is to do the preprocessing in data flow, and create a preprocessor features, but tell prediction graph that you want the same transformations carried out in tensorflow during serving. To do that you will use tensorflow transform. As we saw in the previous section, some preprocessing can be done in tensorflow by creating a new feature column. Then your bucketize the column to create a new column, you are doing preprocessing. And that's a feature column that you will send to the estimator. So here, I'm taking the square footage feature column, and bending it into four intervals. The first interval is houses less than 500 square feet. The second is 500 to 1,000 square feet. Third, are houses between 1,000 and 2,500 square feet, and the last are houses of more than 2,500 square feet. I append the bucketized column into the original feature column list. And now both linear regressor sees the square footage in two forms. As a real valued numeric column, and as a bucketized categorical column. Of course if I wanted to, I could replace a numeric column by the bucketized one, so that the linear regressor only sees the square footage in categorical form. So, that's what I'm doing here, replacing feet calls square brackets zero with the bucketized version. Here is another example of doing feature crosses, but this time also within embedding. We could take the latitude and longitude of the houses, and define the intervals into which we want to discretize them. Here, I'm using N buckets equally spaced intervals. One method I've used to figure out the boundaries is to use approx quantize, a big query sequel function. This allows each of the bins to contain the same number of training examples. Regardless of how you get the boundaries though, once we have the boundaries, lat buckets and lon buckets in my case, we could build the house latitudes and longitudes into b_lat and b_lon. And then as we discussed, we could feature cross the two categorical columns b_lat and b_lon. Here, I'm choosing to feature cross them into nbucket squared hash buckets. On average then, each hash bucket will contain only one feature cross. This is in-between my rule of thumb of 1/2 square root and twice and that I talked about in the previous lesson. And finally, I embedded the data into nbuckets by four dimensions. The advantage of doing this, of putting the preprocessing directly in tensorflow is that these operations are part of your model graph, and so they are carried out in an identical fashion in both training and in surfing. Now, what does this mean in real life? First, we discretize the latitudes. This just brings the real value numbers, so that all the houses in approximately the same latitude get the same value. It might help a bit with overfitting, but just discretizing the latitude doesn't accomplish much. Then we discretize the longitudes. This bends the longitude values it might help us before a bit with overfitting, but discretizing the longitudes doesn't accomplish much either. But what happens when the feature across the two discretized values. We have essentially taken the map and broken it into grid cells such that any house belongs to only one of those grid cells. So, during training this will allow us to memorize the average price of houses in each grid cell. Obviously, the finer the resolution of the grid the more specific the prediction will be. But it will also be less generalizable, because there might not be enough houses sold in a grid cell for us to form a good estimate. During prediction given a house, we know which grid cell it belongs to, and so, we can pull out the memorized value for that grid cell. What embedding does is it allows the grid cells that are similar to each other, maybe all the grid cells that are along the ocean front take all this grid cells, and make them have similar values.