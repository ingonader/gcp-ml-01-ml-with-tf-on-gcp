Now that you understand what features are and why we use them, let's move on to showing you how to implement feature crosses. To create a feature cross using TensorFlow, use the method crossed column in the model tf.feature_column. This is the same model that you've got numeric column and categorical column with vocabulary list. This is the same model those come from. So, the first argument in my example is the list of categorical columns. You're passing in a list so you can cross two columns or three columns or any number of columns. But remember that these columns have to be categorical columns. If you have numeric data, bucketize them first and then you can do a feature cross. When your bucketize a numeric column, you're essentially drawing those black lines we talked about. You're discretizing the column. So, what is the second argument? 24 times 7 is the total number of hash buckets. What TensorFlow does is that it does a feature cross, then computes a hash of the feature cross, and puts the hash into one of several buckets. Even though I specified 24 times 7 here, there is no guarantee that there will be no collision. It is quite possible that the hash of 3:00 PM on Wednesday, model 168 happens to be the same as 4:00 PM on Sunday, model 168. In which case, these two day-hour combinations will be considered together. Let's delve into this a little bit. Now, TensorFlow will skip these steps and go straight to the hash feature cross representation, but it's good to think about what's happening. For simplicity, let's say that instead of 24 times 7, I had specified six here. What happens? We do the feature cross passing in two categorical columns. Day of week has seven unique values. Hour of day has 24 unique values. So the feature cross has 24 times 7 or 168 unique values. Now consider 3:00 PM on Wednesday. 3:00 PM, let's say, is our number 15, and Wednesday, let's say, is day number three. This makes a feature crossed value be, let's say, 87 out of 168. But then, I compute the hash of 87 and do a model of six. Let's assume that this gives me box number three for this hashed feature cross. This is what the day-hour feature column is going to contain for 3:00 PM on Wednesday. A one hot encoded value corresponding to the number three. Again, TensorFlow doesn't actually go through this. It doesn't have to one hot encode before doing the feature cross. If it did that, things would not be very efficient memory wise. But this helps to show you what's happening conceptually. The number of hash buckets controls sparsity and collisions. If, as we did on the previous slide, we set the hash buckets to be much smaller than the number of unique feature crossed values, there will be lots of collisions. Maybe 3:00 PM Wednesday, 7:00 PM Wednesday, 2:00 AM Thursday, et cetera, all fall into the same bucket and will be treated the same. On average, one-sixth of all the feature cross values will be in a bucket. Since we have 168 unique values, on average, each bucket will contain 28 different day-hour combinations. Because of this, the amount to which the feature cross can memorize the data is limited. But the memory used will also be quite low, it's just six buckets. In some way, we are aggregating several day-hour combinations into a bucket. But what if we go to the other extreme and set the number of hash buckets to be so high that there is very little chance of collision? Let's say we set the number of hash buckets to be 300. Now, on average, a bucket will contain one day-hour combination or zero day-hour combinations. It might contain two, but the odds of that are very low. So, using a high value for hash buckets yields a sparse representation of the feature cross. In practice, I tend to choose a number between half square root n and twice n depending on how much I want to trade-off memorization versus sparsity, but this is simply my rule of thumb.