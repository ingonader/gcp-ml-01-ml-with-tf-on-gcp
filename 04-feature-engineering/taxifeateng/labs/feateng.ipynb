{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Engineering </h1>\n",
    "\n",
    "In this notebook, you will learn how to incorporate feature engineering into your pipeline.\n",
    "<ul>\n",
    "<li> Working with feature columns </li>\n",
    "<li> Adding feature crosses in TensorFlow </li>\n",
    "<li> Reading data from BigQuery </li>\n",
    "<li> Creating datasets using Dataflow </li>\n",
    "<li> Using a wide-and-deep model </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam only works in Python 2 at the moment, so we're going to switch to the Python 2 kernel. In the above menu, click the dropdown arrow and select `python2`. After that, run the following to ensure we've installed Beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local/envs/py2env\n",
      "\n",
      "  added / updated specs: \n",
      "    - pytz\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2019.1.23  |                0         126 KB  defaults\n",
      "    pytz-2018.9                |           py27_0         263 KB  defaults\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         389 KB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2018.03.07-0  defaults --> 2019.1.23-0   defaults\n",
      "    pytz:            2018.4-py27_0 defaults --> 2018.9-py27_0 defaults\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Uninstalling google-cloud-dataflow-2.0.0:\n",
      "  Successfully uninstalled google-cloud-dataflow-2.0.0\n",
      "Collecting apache-beam[gcp]\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/3d/90aa15779e884feebae4b0c26cad6f52cd4040397a94deb58dad9c8b7300/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl (2.4MB)\n",
      "Requirement already satisfied, skipping upgrade: dill<=0.2.8.2,>=0.2.6 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.2.6)\n",
      "Collecting pydot<1.3,>=1.2.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/f1/e61d6dfe6c1768ed2529761a68f70939e2569da043e9f15a8d84bf56cadf/pydot-1.2.4.tar.gz (132kB)\n",
      "Collecting httplib2<=0.11.3,>=0.8 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/ce/aa4a385e3e9fd351737fd2b07edaa56e7a730448465aceda6b35086a0d9b/httplib2-0.11.3.tar.gz (215kB)\n",
      "Requirement already satisfied, skipping upgrade: mock<3.0.0,>=1.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml<4.0.0,>=3.12 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (3.13)\n",
      "Collecting typing<3.7.0,>=3.6.0; python_version < \"3.5.0\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/3e/29f92b7aeda5b078c86d14f550bf85cff809042e3429ace7af6193c3bc9f/typing-3.6.6-py2-none-any.whl\n",
      "Collecting pyvcf<0.7.0,>=0.6.8 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/20/b6/36bfb1760f6983788d916096193fc14c83cce512c7787c93380e09458c09/PyVCF-0.6.8.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: oauth2client<4,>=2.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: future<1.0.0,>=0.16.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: avro<2.0.0,>=1.8.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.8.2)\n",
      "Requirement already satisfied, skipping upgrade: crcmod<2.0,>=1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.7)\n",
      "Collecting fastavro<0.22,>=0.21.4 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/da/be/a98f9a81f4f31813ba4714e0bf26fca4bcac7e25f76a515bd86aa45dcfe2/fastavro-0.21.17-cp27-cp27mu-manylinux1_x86_64.whl (1.1MB)\n",
      "Requirement already satisfied, skipping upgrade: futures<4.0.0,>=3.1.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2,>=1.8 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.17.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<4,>=3.5.0.post1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (3.6.1)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/96/4e/f82bd349c7893e1595429ecc95233369bc33c9a26e4859991439bfa01c1f/hdfs-2.2.2.tar.gz\n",
      "Collecting pytz<=2018.4,>=2018.3 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/83/15f7833b70d3e067ca91467ca245bae0f6fe56ddc7451aa0dc5606b120f2/pytz-2018.4-py2.py3-none-any.whl (510kB)\n",
      "Collecting google-cloud-pubsub==0.35.4; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/66/f9/bfa284399fb59a8896e0c4164b46185f61f35a90a18c67b366406ad472a6/google_cloud_pubsub-0.35.4-py2.py3-none-any.whl (93kB)\n",
      "Requirement already satisfied, skipping upgrade: proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.90.0)\n",
      "Collecting google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/26/abea123a1b5a2b0c1b49c0d8a2e030725f32ae0932d026f2c7a6ee32c8d3/google_apitools-0.5.24-py2-none-any.whl (129kB)\n",
      "Collecting google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/1b/2b95f2fefddbbece38110712c225bfb5649206f4056445653bd5ca4dc86d/google_cloud_bigquery-1.6.1-py2.py3-none-any.whl (83kB)\n",
      "Requirement already satisfied, skipping upgrade: googledatastore<7.1,>=7.0.1; python_version < \"3.0\" and extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (7.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from pydot<1.3,>=1.2.0->apache-beam[gcp]) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: funcsigs>=1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pbr>=0.11 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]) (5.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]) (1.10.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/envs/py2env/lib/python2.7/site-packages (from pyvcf<0.7.0,>=0.6.8->apache-beam[gcp]) (40.6.3)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]) (3.4.2)\n",
      "Requirement already satisfied, skipping upgrade: enum34>=1.0.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from grpcio<2,>=1.8->apache-beam[gcp]) (1.1.6)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.7.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (2.18.4)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=0.1.3 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]) (0.1.4)\n",
      "Collecting grpc-google-iam-v1<0.12dev,>=0.11.1 (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/28/f26f67381cb23e81271b8d66c00a846ad9d25a909ae1ae1df8222fad2744/grpc-google-iam-v1-0.11.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.5.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\"->apache-beam[gcp]) (1.5.5)\n",
      "Collecting fasteners>=0.14 (from google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/14/3a/096c7ad18e102d4f219f5dd15951f9728ca5092a3385d2e8f79a7c1e1017/fasteners-0.14.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media>=0.2.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<0.30dev,>=0.28.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]) (0.28.1)\n",
      "Requirement already satisfied, skipping upgrade: ordereddict in /usr/local/envs/py2env/lib/python2.7/site-packages (from funcsigs>=1->mock<3.0.0,>=1.0.1->apache-beam[gcp]) (1.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2.0.0dev,>=0.4.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]) (1.6.2)\n",
      "Collecting monotonic>=0.1 (from fasteners>=0.14->google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-auth<2.0.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]) (2.1.0)\n",
      "Building wheels for collected packages: pydot, httplib2, pyvcf, hdfs, docopt, grpc-google-iam-v1\n",
      "  Running setup.py bdist_wheel for pydot: started\n",
      "  Running setup.py bdist_wheel for pydot: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/6a/a5/14/25541ebcdeaf97a37b6d05c7ff15f5bd20f5e91b99d313e5b4\n",
      "  Running setup.py bdist_wheel for httplib2: started\n",
      "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/1b/9c/9e/1f6fdb21dbb1fe6a99101d697f12cb8c1fa96c1587df69adba\n",
      "  Running setup.py bdist_wheel for pyvcf: started\n",
      "  Running setup.py bdist_wheel for pyvcf: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/81/91/41/3272543c0b9c61da9c525f24ee35bae6fe8f60d4858c66805d\n",
      "  Running setup.py bdist_wheel for hdfs: started\n",
      "  Running setup.py bdist_wheel for hdfs: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/99/3f/b2/a09631bd4e2220031fa88949f4acc010cc48cc29011cb25922\n",
      "  Running setup.py bdist_wheel for docopt: started\n",
      "  Running setup.py bdist_wheel for docopt: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: started\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/b6/c6/31/c20321a5a3fde456fc375b7c2814135e6e98bc0d74c40239d9\n",
      "Successfully built pydot httplib2 pyvcf hdfs docopt grpc-google-iam-v1\n",
      "Installing collected packages: pydot, httplib2, typing, pyvcf, fastavro, docopt, hdfs, pytz, grpc-google-iam-v1, google-cloud-pubsub, monotonic, fasteners, google-apitools, google-cloud-bigquery, apache-beam\n",
      "  Found existing installation: httplib2 0.12.0\n",
      "    Uninstalling httplib2-0.12.0:\n",
      "      Successfully uninstalled httplib2-0.12.0\n",
      "  Found existing installation: pytz 2018.9\n",
      "    Uninstalling pytz-2018.9:\n",
      "      Successfully uninstalled pytz-2018.9\n",
      "  Found existing installation: google-apitools 0.5.10\n",
      "    Uninstalling google-apitools-0.5.10:\n",
      "      Successfully uninstalled google-apitools-0.5.10\n",
      "  Found existing installation: google-cloud-bigquery 0.23.0\n",
      "    Uninstalling google-cloud-bigquery-0.23.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-0.23.0\n",
      "Successfully installed apache-beam-2.9.0 docopt-0.6.2 fastavro-0.21.17 fasteners-0.14.1 google-apitools-0.5.24 google-cloud-bigquery-1.6.1 google-cloud-pubsub-0.35.4 grpc-google-iam-v1-0.11.4 hdfs-2.2.2 httplib2-0.11.3 monotonic-1.5 pydot-1.2.4 pytz-2018.4 pyvcf-0.6.8 typing-3.6.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.6.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\r",
      "ca-certificates-2019 | 126 KB    |            |   0% \r",
      "ca-certificates-2019 | 126 KB    | ########## | 100% \n",
      "\r",
      "pytz-2018.9          | 263 KB    |            |   0% \r",
      "pytz-2018.9          | 263 KB    | #########3 |  93% \r",
      "pytz-2018.9          | 263 KB    | ########## | 100% \n",
      "google-cloud-bigquery 1.6.1 has requirement google-api-core<2.0.0dev,>=1.0.0, but you'll have google-api-core 0.1.4 which is incompatible.\n",
      "googledatastore 7.0.1 has requirement httplib2<0.10,>=0.9.1, but you'll have httplib2 0.11.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py2env\n",
    "conda install -y pytz\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a pip install, you have to ```Reset Session``` so that the new packages are picked up.  Please click on the button in the above menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Environment variables for project and bucket </h2>\n",
    "\n",
    "<li> Your project id is the *unique* string that identifies your project (not the project name). You can find this from the GCP Console dashboard's Home page.  My dashboard reads:  <b>Project ID:</b> cloud-training-demos </li>\n",
    "<li> Cloud training often involves saving and restoring model files. Therefore, we should <b>create a single-region bucket</b>. If you don't have a bucket already, I suggest that you create one from the GCP console (because it will dynamically check whether the bucket name you want is available) </li>\n",
    "</ol>\n",
    "<b>Change the cell below</b> to reflect your Project ID and bucket name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "REGION = 'us-central1' # Choose an available region for Cloud MLE from https://cloud.google.com/ml-engine/docs/regions.\n",
    "BUCKET = 'cloud-training-demos-ml' # REPLACE WITH YOUR BUCKET NAME. Use a regional bucket in the region you selected.\n",
    "PROJECT = 'cloud-training-demos'    # CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwiklabs-gcp-ba899f4f2d93f6ad\n",
      "inna-gcp-ba899f4f2d93f6ad\n",
      "gsutil mb -l europe-west1 gs://inna-gcp-ba899f4f2d93f6ad\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output = os.popen(\"gcloud config get-value project\").readlines()\n",
    "project_name = output[0][:-1]\n",
    "\n",
    "# change these to try this notebook out\n",
    "PROJECT = project_name\n",
    "BUCKET = project_name\n",
    "BUCKET = BUCKET.replace(\"qwiklabs-\", \"inna-\")\n",
    "REGION = 'europe-west1'  ## note: Cloud ML Engine not availabe in europe-west3!\n",
    "\n",
    "print(PROJECT)\n",
    "print(BUCKET)\n",
    "print(\"gsutil mb -l {0} gs://{1}\".format(REGION, BUCKET))\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8' \n",
    "\n",
    "## ensure we're using python2 env\n",
    "os.environ['CLOUDSDK_PYTHON'] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "All components are up to date.\n",
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n",
      "Updated property [ml_engine/local_python].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## ensure gcloud is up to date\n",
    "gcloud components update\n",
    "\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "\n",
    "## ensure we predict locally with our current Python environment\n",
    "gcloud config set ml_engine/local_python `which python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Specifying query to pull the data </h2>\n",
    "\n",
    "Let's pull out a few extra columns from the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      "  (tolls_amount + fare_amount) AS fare_amount,\n",
      "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
      "  HOUR(pickup_datetime) AS hourofday,\n",
      "  pickup_longitude AS pickuplon,\n",
      "  pickup_latitude AS pickuplat,\n",
      "  dropoff_longitude AS dropofflon,\n",
      "  dropoff_latitude AS dropofflat,\n",
      "  passenger_count*1.0 AS passengers,\n",
      "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
      "FROM\n",
      "  [nyc-tlc:yellow.trips]\n",
      "WHERE\n",
      "  trip_distance > 0\n",
      "  AND fare_amount >= 2.5\n",
      "  AND pickup_longitude > -78\n",
      "  AND pickup_longitude < -70\n",
      "  AND dropoff_longitude > -78\n",
      "  AND dropoff_longitude < -70\n",
      "  AND pickup_latitude > 37\n",
      "  AND pickup_latitude < 45\n",
      "  AND dropoff_latitude > 37\n",
      "  AND dropoff_latitude < 45\n",
      "  AND passenger_count > 0\n",
      "   AND ABS(HASH(pickup_datetime)) % 100 == 2\n"
     ]
    }
   ],
   "source": [
    "def create_query(phase, EVERY_N):\n",
    "  if EVERY_N == None:\n",
    "    EVERY_N = 4 #use full dataset\n",
    "    \n",
    "  #select and pre-process fields\n",
    "  base_query = \"\"\"\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
    "  HOUR(pickup_datetime) AS hourofday,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  \"\"\"\n",
    "  \n",
    "  #add subsampling criteria by modding with hashkey\n",
    "  if phase == 'train': \n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} < 2\".format(base_query,EVERY_N)\n",
    "  elif phase == 'valid': \n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} == 2\".format(base_query,EVERY_N)\n",
    "  elif phase == 'test':\n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} == 3\".format(base_query,EVERY_N)\n",
    "  return query\n",
    "    \n",
    "print create_query('valid', 100) #example query using 1% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RequestException",
     "evalue": "HTTP request failed: Query text specifies use_legacy_sql:true, while API options specify:false",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRequestException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4f1a24e825e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \"\"\"\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/google/datalab/bigquery/_query.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, output_options, sampling, context, query_params)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \"\"\"\n\u001b[1;32m    338\u001b[0m     return self.execute_async(output_options, sampling=sampling, context=context,\n\u001b[0;32m--> 339\u001b[0;31m                               query_params=query_params).wait()\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/google/datalab/bigquery/_query.pyc\u001b[0m in \u001b[0;36mexecute_async\u001b[0;34m(self, output_options, sampling, context, query_params)\u001b[0m\n\u001b[1;32m    270\u001b[0m                                            query_params=query_params)\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'jobReference'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unexpected response from server'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRequestException\u001b[0m: HTTP request failed: Query text specifies use_legacy_sql:true, while API options specify:false"
     ]
    }
   ],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "\n",
    "query_test = \"\"\"\n",
    "#legacySQL\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
    "  HOUR(pickup_datetime) AS hourofday,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  AND ABS(HASH(pickup_datetime)) % 100 == 2\n",
    "  LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "res = bq.Query(query_test).execute().result().to_dataframe()\n",
    "res.head(n = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the query above in https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips if you want to see what it does (ADD LIMIT 10 to the query!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Row\tfare_amount\tdayofweek\thourofday\tpickuplon\tpickuplat\tdropofflon\tdropofflat\tpassengers\tkey\t \n",
    "1\t18.0\t5\t19\t-73.973092\t40.750065\t-74.009415\t40.7128\t1.0\t2013-05-02 19:25:00.000000-73.973140.750140.7128-74.0094\t \n",
    "2\t39.83\t6\t17\t-73.863462\t40.769847\t-73.983307\t40.735722\t3.0\t2013-12-06 17:50:00.000000-73.863540.769840.7357-73.9833\t \n",
    "3\t21.5\t2\t21\t-73.994028\t40.751273\t-73.977827\t40.680942\t1.0\t2012-11-26 21:45:00.000000-73.99440.751340.6809-73.9778\t \n",
    "4\t35.3\t5\t11\t-73.97348\t40.751112\t-73.861642\t40.768322\t1.0\t2012-11-15 11:32:00.000000-73.973540.751140.7683-73.8616\t \n",
    "5\t25.5\t6\t12\t-73.969537\t40.760892\t-73.885182\t40.77218\t1.0\t2012-11-16 12:20:00.000000-73.969540.760940.7722-73.8852\t \n",
    "6\t24.5\t6\t11\t-73.9526\t40.772473\t-73.861593\t40.76822\t5.0\t2011-04-29 11:30:00.000000-73.952640.772540.7682-73.8616\t \n",
    "7\t30.3\t6\t5\t-73.945982\t40.8038\t-73.872017\t40.774515\t6.0\t2013-02-15 05:54:00.000000-73.94640.803840.7745-73.872\t \n",
    "8\t39.33\t7\t18\t-73.871122\t40.773887\t-73.986117\t40.75057\t1.0\t2014-10-18 18:17:00.000000-73.871140.773940.7506-73.9861\t \n",
    "9\t30.5\t6\t8\t-73.994405\t40.690012\t-73.984307\t40.763057\t1.0\t2014-09-26 08:45:00.000000-73.994440.6940.7631-73.9843\t \n",
    "10\t18.0\t7\t1\t-74.00346\t40.725322\t-73.97648\t40.78495\t2.0\t2012-12-01 01:39:00.000000-74.003540.725340.785-73.9765\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Preprocessing Dataflow job from BigQuery </h2>\n",
    "\n",
    "This code reads from BigQuery and saves the data as-is on Google Cloud Storage.  We can do additional preprocessing and cleanup inside Dataflow, but then we'll have to remember to repeat that prepreprocessing during inference. It is better to use tf.transform which will do this book-keeping for you, or to do preprocessing within your TensorFlow model. We will look at this in future notebooks. For now, we are simply moving data from BigQuery to CSV using Dataflow.\n",
    "\n",
    "While we could read from BQ directly from TensorFlow (See: https://www.tensorflow.org/api_docs/python/tf/contrib/cloud/BigQueryReader), it is quite convenient to export to CSV and do the training off CSV.  Let's use Dataflow to do this at scale.\n",
    "\n",
    "Because we are running this on the Cloud, you should go to the GCP Console (https://console.cloud.google.com/dataflow) to look at the status of the job. It will take several minutes for the preprocessing job to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil -m rm -rf gs://$BUCKET/taxifare/ch4/taxi_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -rowdict: Dictionary. The beam bigquery reader returns a PCollection in\n",
    "#     which each row is represented as a python dictionary\n",
    "# Returns:\n",
    "#   -rowstring: a comma separated string representation of the record with dayofweek\n",
    "#     converted from int to string (e.g. 3 --> Tue)\n",
    "####\n",
    "def to_csv(rowdict):\n",
    "  days = ['null', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "  CSV_COLUMNS = 'fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key'.split(',')\n",
    "  rowdict['dayofweek'] = days[rowdict['dayofweek']]\n",
    "  rowstring = ','.join([str(rowdict[k]) for k in CSV_COLUMNS])\n",
    "  return rowstring\n",
    "\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -EVERY_N: Integer. Sample one out of every N rows from the full dataset.\n",
    "#     Larger values will yield smaller sample\n",
    "#   -RUNNER: 'DirectRunner' or 'DataflowRunner'. Specfy to run the pipeline\n",
    "#     locally or on Google Cloud respectively. \n",
    "# Side-effects:\n",
    "#   -Creates and executes dataflow pipeline. \n",
    "#     See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
    "####\n",
    "def preprocess(EVERY_N, RUNNER):\n",
    "  job_name = 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "  print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "  OUTPUT_DIR = 'gs://{0}/taxifare/ch4/taxi_preproc/'.format(BUCKET)\n",
    "\n",
    "  #dictionary of pipeline options\n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': PROJECT,\n",
    "    'runner': RUNNER\n",
    "  }\n",
    "  #instantiate PipelineOptions object using options dictionary\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  #instantantiate Pipeline object using PipelineOptions\n",
    "  with beam.Pipeline(options=opts) as p:\n",
    "      for phase in ['train', 'valid']:\n",
    "        query = create_query(phase, EVERY_N) \n",
    "        outfile = os.path.join(OUTPUT_DIR, '{}.csv'.format(phase))\n",
    "        (\n",
    "          p | 'read_{}'.format(phase) >> beam.io.Read(beam.io.BigQuerySource(query = query)) ##TODO: read from BigQuery\n",
    "            | 'tocsv_{}'.format(phase) >> beam.Map(to_csv) ##TODO: apply the to_csv function to every row\n",
    "            | 'write_{}'.format(phase) >> beam.io.Write(beam.io.WriteToText(outfile)) ##TODO: write to outfile\n",
    "        )\n",
    "  print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-190207-142539 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Dataset qwiklabs-gcp-ba899f4f2d93f6ad:temp_dataset_62de78bbf58d4f1ba93a4a9c6e4fe4ae does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset qwiklabs-gcp-ba899f4f2d93f6ad:temp_dataset_aef708d6126245389623792f218f8aba does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "preprocess(EVERY_N = 50 * 10000, RUNNER = 'DirectRunner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/\n"
     ]
    }
   ],
   "source": [
    "print 'gs://{0}/taxifare/ch4/taxi_preproc/'.format(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00000-of-00005\r\n",
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00001-of-00005\r\n",
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00002-of-00005\r\n",
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00003-of-00005\r\n",
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00004-of-00005\r\n",
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/valid.csv-00000-of-00002\r\n",
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/valid.csv-00001-of-00002\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/ch4/taxi_preproc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipleline on cloud on a larger sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-190207-142635 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:800: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  options = pbegin.pipeline.options.view_as(DebugOptions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# preprocess(EVERY_N = 50 * 100, RUNNER = 'DataflowRunner')  ## time: 9 min 33 secs in the training environment\n",
    "# #change first arg to None to preprocess full dataset\n",
    "\n",
    "## changed to the same as locally:\n",
    "preprocess(EVERY_N = 50 * 10000, RUNNER = 'DataflowRunner')  ## time: 7 min 8 secs\n",
    "\n",
    "## changed to 100 times more than locally:\n",
    "#preprocess(EVERY_N = 50 * 10000 * 100, RUNNER = 'DataflowRunner')  ## time: 6 min 59 sec\n",
    "\n",
    "## changed to 10000 times more than locally:\n",
    "#preprocess(EVERY_N = 50 * 10000 * 10000, RUNNER = 'DataflowRunner')  ## time: ?\n",
    "\n",
    "## but something went wrong: the number of elements processed ('added') in\n",
    "## dataflow seems way to low..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job completes, observe the files created in Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    566681  2019-02-07T14:31:11Z  gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00000-of-00001\n",
      "    114724  2019-02-07T14:26:20Z  gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00000-of-00005\n",
      "    114926  2019-02-07T14:26:20Z  gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00001-of-00005\n",
      "    114334  2019-02-07T14:26:20Z  gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00002-of-00005\n",
      "    113894  2019-02-07T14:26:20Z  gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00003-of-00005\n",
      "    108803  2019-02-07T14:26:20Z  gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00004-of-00005\n",
      "    220341  2019-02-07T14:31:20Z  gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/valid.csv-00000-of-00001\n",
      "    113602  2019-02-07T14:26:18Z  gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/valid.csv-00000-of-00002\n",
      "    106739  2019-02-07T14:26:18Z  gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/valid.csv-00001-of-00002\n",
      "                                 gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/tmp/\n",
      "TOTAL: 9 objects, 1574044 bytes (1.5 MiB)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://$BUCKET/taxifare/ch4/taxi_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.1,Sun,0,-73.964025,40.710115,-74.007547,40.741085,2.0,2012-04-01 00:55:49.000000-73.96440.710140.7411-74.0075\n",
      "19.0,Wed,0,-73.9899215698,40.7331581116,-73.9257507324,40.7587509155,1.0,2015-05-20 00:19:26.000000-73.989940.733240.7588-73.9258\n",
      "2.5,Sat,0,-73.986845,40.721068,-73.98649,40.721803,1.0,2009-01-17 00:04:32.000000-73.986840.721140.7218-73.9865\n",
      "17.3,Thu,0,-73.989998,40.761777,-73.985523,40.688012,1.0,2009-05-28 00:09:17.000000-73.9940.761840.688-73.9855\n",
      "27.07,Mon,0,-73.873086,40.774093,-73.977928,40.783805,1.0,2010-11-08 00:00:52.000000-73.873140.774140.7838-73.9779\n",
      "30.5,Fri,0,-73.9804077148,40.7457847595,-74.0038757324,40.6466941833,3.0,2015-03-20 00:37:15.000000-73.980440.745840.6467-74.0039\n",
      "15.3,Tue,0,-73.996457,40.742907,-73.96584,40.677997,1.0,2011-11-29 00:46:00.000000-73.996540.742940.678-73.9658\n",
      "16.1,Tue,0,-74.005553,40.726371,-73.957306,40.673426,2.0,2011-11-29 00:46:00.000000-74.005640.726440.6734-73.9573\n",
      "4.6,Sat,0,-73.95566,40.779521,-73.961876,40.768514,1.0,2009-01-17 00:04:32.000000-73.955740.779540.7685-73.9619\n",
      "14.1,Tue,0,-73.973183,40.760847,-73.957145,40.712024,2.0,2010-01-19 00:34:36.000000-73.973240.760840.712-73.9571\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "#print first 10 lines of first shard of train.csv\n",
    "gsutil cat \"gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Develop model with new inputs </h2>\n",
    "\n",
    "Download the first shard of the preprocessed data to enable local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘sample’: File exists\n",
      "Copying gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/train.csv-00000-of-00001...\n",
      "/ [0 files][    0.0 B/553.4 KiB]                                                \r",
      "/ [1 files][553.4 KiB/553.4 KiB]                                                \r\n",
      "Operation completed over 1 objects/553.4 KiB.                                    \n",
      "Copying gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_preproc/valid.csv-00000-of-00001...\n",
      "/ [0 files][    0.0 B/215.2 KiB]                                                \r",
      "/ [1 files][215.2 KiB/215.2 KiB]                                                \r\n",
      "Operation completed over 1 objects/215.2 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "mkdir sample\n",
    "#gsutil cp \"gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*\" sample/train.csv\n",
    "gsutil cp \"gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-00001\" sample/train.csv\n",
    "#gsutil cp \"gs://$BUCKET/taxifare/ch4/taxi_preproc/valid.csv-00000-of-*\" sample/valid.csv\n",
    "gsutil cp \"gs://$BUCKET/taxifare/ch4/taxi_preproc/valid.csv-00000-of-00001\" sample/valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the TODOs in taxifare/trainer/model.py so that the code below works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/trainer/model.py:    # TODO: Define feature columns for dayofweek, hourofday, pickuplon, pickuplat, dropofflat, dropofflon, passengers\r\n",
      "taxifare/trainer/model.py:    # TODO: Add any engineered columns here\r\n",
      "taxifare/trainer/model.py:     TODO: Build an estimator starting from INPUT COLUMNS.\r\n",
      "taxifare/trainer/model.py:    return None # TODO: Add estimator definition here\r\n",
      "taxifare/trainer/model.py:    # TODO: Add any engineered features to the dict\r\n",
      "taxifare/trainer/model.py:        # TODO: What features will user provide? What will their types be?\r\n",
      "taxifare/trainer/model.py:    # TODO: Add any extra placeholders for inputs you'll generate\r\n",
      "taxifare/trainer/model.py:      features, # TODO: Wrap this with a call to add_engineered\r\n",
      "taxifare/trainer/model.py:            return features, label # TODO: Wrap this with a call to add_engineered\r\n"
     ]
    }
   ],
   "source": [
    "!grep TODO taxifare/trainer/*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    # TODO: Define feature columns for dayofweek, hourofday, pickuplon, pickuplat, dropofflat, d\n",
    "ropofflon, passengers\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    # TODO: Add any engineered columns here\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:     TODO: Build an estimator starting from INPUT COLUMNS.\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    return None # TODO: Add estimator definition here\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    # TODO: Add any engineered features to the dict\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:        # TODO: What features will user provide? What will their types be?\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    # TODO: Add any extra placeholders for inputs you'll generate\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:      features, # TODO: Wrap this with a call to add_engineered\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:            return features, label # TODO: Wrap this with a call to add_engineered\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two new inputs in the INPUT_ColumNS, three engineered features, and the estimator involves bucketization and feature crosses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_COLUMNS = [\r\n",
      "    # TODO: Define feature columns for dayofweek, hourofday, pickuplon, pickuplat, dropofflat, dropofflon, passengers\r\n",
      "\r\n",
      "    ## categorical columns:\r\n",
      "    tf.feature_column.categorical_column_with_vocabulary_list(\r\n",
      "        'dayofweek', \r\n",
      "        vocabulary_list = ['Sun', 'Mon', 'Tues', 'Wed', 'Thu', 'Fri', 'Sat']),\r\n",
      "    tf.feature_column.categorical_column_with_identity('hourofday', num_buckets = 24),\r\n",
      "    \r\n",
      "    ## numeric columns:\r\n",
      "    tf.feature_column.numeric_column('pickuplat'),\r\n",
      "    tf.feature_column.numeric_column('pickuplon'),\r\n",
      "    tf.feature_column.numeric_column('dropofflat'),\r\n",
      "    tf.feature_column.numeric_column('dropofflon'),\r\n",
      "    tf.feature_column.numeric_column('passengers'),\r\n",
      "    \r\n",
      "    # TODO: Add any engineered columns here\r\n",
      "    tf.feature_column.numeric_column('latdiff'),\r\n",
      "    tf.feature_column.numeric_column('londiff'),\r\n",
      "    tf.feature_column.numeric_column('euclidean')\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!grep -A 20 \"INPUT_COLUMNS = \" taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def build_estimator(model_dir, nbuckets, hidden_units):\r\n",
      "    \"\"\"\r\n",
      "     TODO: Build an estimator starting from INPUT COLUMNS.\r\n",
      "     These include feature transformations and synthetic features.\r\n",
      "     The model is a wide-and-deep model, i.e. [wide_cols] & [deep_cols].\r\n",
      "    \"\"\"\r\n",
      "    \r\n",
      "    ## [[here]] -- change \"euclidean\" to \"pcount\" (12:49)\r\n",
      "\r\n",
      "    ## retrieve input features into separate variables:\r\n",
      "    #(dayofweek, hourofday, plat, plon, dlat, dlon, pcount, latdiff, londiff, euclidean) = INPUT_COLUMNS\r\n",
      "    (dayofweek, hourofday, plat, plon, dlat, dlon, pcount, latdiff, londiff, pcount) = INPUT_COLUMNS\r\n",
      "    \r\n",
      "    ## transform features: \r\n",
      "    ## bucketize lats & lons:\r\n",
      "    latbuckets = np.linspace(38.0, 42.0, nbuckets).tolist()\r\n",
      "    lonbuckets = np.linspace(-76.0, -72.0, nbuckets).tolist()\r\n",
      "    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\r\n",
      "    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\r\n",
      "    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\r\n",
      "    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\r\n",
      "--\r\n",
      "#    estimator = build_estimator(args['output_dir'], args['nbuckets'], args['hidden_units'])\r\n",
      "#    train_spec = tf.estimator.TrainSpec(\r\n",
      "#        input_fn = read_dataset(\r\n",
      "#            filename = args['train_data_paths'],\r\n",
      "#            mode = tf.estimator.ModeKeys.TRAIN,\r\n",
      "#            batch_size = args['train_batch_size']),\r\n",
      "#        max_steps = args['train_steps'])\r\n",
      "#    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\r\n",
      "#    eval_spec = tf.estimator.EvalSpec(\r\n",
      "#        input_fn = read_dataset(\r\n",
      "#            filename = args['eval_data_paths'],\r\n",
      "#            mode = tf.estimator.ModeKeys.EVAL,\r\n",
      "#            batch_size = args['eval_batch_size']),\r\n",
      "#        steps = None,\r\n",
      "#        exporters = exporter)\r\n",
      "#    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n",
      "\r\n",
      "## new implementation copied from coursera course video (18:45):\r\n",
      "def train_and_evaluate(args):\r\n",
      "    estimator = build_estimator(args['output_dir'], args['nbuckets'], args['hidden_units'].split(' '))\r\n",
      "    train_spec = tf.estimator.TrainSpec(\r\n",
      "        input_fn = read_dataset(\r\n",
      "            filename = args['train_data_paths'],\r\n",
      "            mode = tf.estimator.ModeKeys.TRAIN,\r\n",
      "            batch_size = args['train_batch_size']),\r\n",
      "        max_steps = args['train_steps'])\r\n",
      "    exporter = tf.estimator.LatestExporter('exporter, serving_input_fn')\r\n",
      "    eval_spec = tf.estimator.EvalSpec(\r\n",
      "        input_fn = read_dataset(\r\n",
      "            filename = args['eval_data_paths'],\r\n",
      "            mode = tf.estimator.ModeKeys.EVAL,\r\n",
      "            batch_size = args['eval_batch_size']),\r\n",
      "        steps = None,\r\n",
      "        expoerters = exporter)\r\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n",
      "\r\n",
      "def get_eval_metrics():\r\n",
      "    return {\r\n",
      "        'rmse': tflearn.MetricSpec(metric_fn = metrics.streaming_root_mean_squared_error),\r\n",
      "        'training/hptuning/metric': tflearn.MetricSpec(metric_fn = metrics.streaming_root_mean_squared_error),\r\n"
     ]
    }
   ],
   "source": [
    "!grep -A 20 \"build_estimator\" taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def add_engineered(features):\r\n",
      "    # TODO: Add any engineered features to the dict\r\n",
      "    lat1 = features['pickuplat']\r\n",
      "    lat2 = features['dropofflat']\r\n",
      "    lon1 = features['pickuplon']\r\n",
      "    lon2 = features['dropofflon']\r\n",
      "    latdiff = (lat1 - lat2)\r\n",
      "    londiff = (lon1 - lon2)\r\n",
      "    \r\n",
      "    ## add features to feature vector\r\n",
      "    ## for distance with sign that indicates direction:\r\n",
      "    features['latdiff'] = latdiff\r\n",
      "    features['londiff'] = londiff\r\n",
      "    dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\r\n",
      "    features['euclidean'] = dist\r\n",
      "    \r\n",
      "    return features   \r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# # Create serving input function to be able to serve predictions\r\n",
      "--\r\n",
      "#     features = add_engineered(feature_placeholders.copy())\r\n",
      "# \r\n",
      "#     # ## [[?]] this part is not part of the model solution... \r\n",
      "#     # what is going on? --> hence, commented out.\r\n",
      "#     # features = {\r\n",
      "#     #     key: tf.expand_dims(tensor, -1)\r\n",
      "#     #     for key, tensor in feature_placeholders.items()\r\n",
      "#     # }\r\n",
      "#     return tf.estimator.export.ServingInputReceiver(\r\n",
      "#       features, # TODO: Wrap this with a call to add_engineered\r\n",
      "#       feature_placeholders\r\n",
      "#     )\r\n",
      "\r\n",
      "## new implementation -- [[here]]:\r\n",
      "## similar to course video:\r\n",
      "\r\n",
      "# Create serving input function to be able to serve predictions\r\n",
      "def serving_input_fn():\r\n",
      "    feature_placeholders = {  \r\n",
      "        ## numeric features:\r\n",
      "        ## (ignoring the first two columns):\r\n",
      "--\r\n",
      "      add_engineered(features), \r\n",
      "      feature_placeholders\r\n",
      "    )\r\n",
      "\r\n",
      "# Create input function to load data into datasets\r\n",
      "def read_dataset(filename, mode, batch_size = 512):\r\n",
      "    def _input_fn():\r\n",
      "        def decode_csv(value_column):\r\n",
      "            columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\r\n",
      "            features = dict(zip(CSV_COLUMNS, columns))\r\n",
      "            label = features.pop(LABEL_COLUMN)\r\n",
      "            return add_engineered(features), label # TODO: Wrap this with a call to add_engineered\r\n",
      "        \r\n",
      "        # Create list of files that match pattern\r\n",
      "        file_list = tf.gfile.Glob(filename)\r\n",
      "\r\n",
      "        # Create dataset from file list\r\n",
      "        dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\r\n",
      "\r\n",
      "        if mode == tf.estimator.ModeKeys.TRAIN:\r\n",
      "            num_epochs = None # indefinitely\r\n",
      "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\r\n",
      "        else:\r\n",
      "            num_epochs = 1 # end-of-input after this\r\n",
      "\r\n",
      "        dataset = dataset.repeat(num_epochs).batch(batch_size)\r\n",
      "        batch_features, batch_labels = dataset.make_one_shot_iterator().get_next()\r\n",
      "        return batch_features, batch_labels\r\n",
      "    return _input_fn\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -A 20 \"add_engineered(\" taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 30, '_session_config': None, '_keep_checkpoint_max': 3, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6e79729c50>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxi_trained/', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 152706.53, step = 1\n",
      "INFO:tensorflow:global_step/sec: 23.4741\n",
      "INFO:tensorflow:loss = 56582.36, step = 101 (4.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.535\n",
      "INFO:tensorflow:loss = 51787.617, step = 201 (3.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.7533\n",
      "INFO:tensorflow:loss = 40157.062, step = 301 (3.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.7047\n",
      "INFO:tensorflow:loss = 49412.008, step = 401 (3.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.6121\n",
      "INFO:tensorflow:loss = 33964.832, step = 501 (3.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.744\n",
      "INFO:tensorflow:loss = 55918.016, step = 601 (3.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.9869\n",
      "INFO:tensorflow:loss = 39721.734, step = 701 (3.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.4618\n",
      "INFO:tensorflow:loss = 45181.58, step = 801 (3.394 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 834 into /content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3369\n",
      "INFO:tensorflow:loss = 39980.43, step = 901 (3.658 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 44014.734.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-07-14:43:42\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxi_trained/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-07-14:43:43\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 76.389915, global_step = 1000, loss = 36991.816\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'dayofweek': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=string>, 'passengers': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'euclidean': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=float32>, 'latdiff': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'londiff': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=float32>, 'hourofday': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=int32>, 'pickuplon': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'dayofweek': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=string>, 'passengers': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'euclidean': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=float32>, 'latdiff': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'londiff': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=float32>, 'hourofday': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=int32>, 'pickuplon': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxi_trained/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxi_trained/export/exporter/temp-1549550625/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "python -m trainer.task \\\n",
    "  --train_data_paths=${PWD}/sample/train.csv \\\n",
    "  --eval_data_paths=${PWD}/sample/valid.csv  \\\n",
    "  --output_dir=${PWD}/taxi_trained \\\n",
    "  --train_steps=1000 \\\n",
    "  --job-dir=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1549550625\r\n"
     ]
    }
   ],
   "source": [
    "!ls taxi_trained/export/exporter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/test.json\n"
     ]
    }
   ],
   "source": [
    "%writefile /tmp/test.json\n",
    "{\"dayofweek\": \"Sun\", \"hourofday\": 17, \"pickuplon\": -73.885262, \"pickuplat\": 40.773008, \"dropofflon\": -73.987232, \"dropofflat\": 40.732403, \"passengers\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS\n",
      "[11.333381652832031]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "model_dir=$(ls ${PWD}/taxi_trained/export/exporter)\n",
    "gcloud ml-engine local predict \\\n",
    "  --model-dir=${PWD}/taxi_trained/export/exporter/${model_dir} \\\n",
    "  --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if gcloud ml-engine local predict fails, might need to update glcoud\n",
    "#!gcloud --quiet components update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Train on cloud </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_trained europe-west1 lab4a_190207_144415\n",
      "jobId: lab4a_190207_144415\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [lab4a_190207_144415] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe lab4a_190207_144415\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs lab4a_190207_144415\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/taxifare/ch4/taxi_trained\n",
    "JOBNAME=lab4a_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=${PWD}/taxifare/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=BASIC \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --train_data_paths=\"gs://$BUCKET/taxifare/ch4/taxi_preproc/train*\" \\\n",
    "  --eval_data_paths=\"gs://${BUCKET}/taxifare/ch4/taxi_preproc/valid*\"  \\\n",
    "  --train_steps=5000 \\\n",
    "  --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 6. Inspect with TensorBoard </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_trained\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 7179. Click <a href=\"/_proxy/46409/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7179"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "OUTDIR='gs://{0}/taxifare/ch4/taxi_trained'.format(BUCKET)\n",
    "print OUTDIR\n",
    "TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://inna-gcp-ba899f4f2d93f6ad/taxifare/ch4/taxi_trained\n"
     ]
    }
   ],
   "source": [
    "print(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\r\n",
      "-rw-r--r-- 1 root root   677 Feb  7 14:14 __init__.py\r\n",
      "-rw-r--r-- 1 root root   200 Feb  7 14:34 __init__.pyc\r\n",
      "-rw-r--r-- 1 root root 11050 Feb  7 14:38 model.py\r\n",
      "-rw-r--r-- 1 root root  7106 Feb  7 14:38 model.pyc\r\n",
      "-rwxr-xr-x 1 root root   982 Feb  7 14:14 setup.py\r\n",
      "-rw-r--r-- 1 root root  3573 Feb  7 14:14 task.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./taxifare/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named taxifare.trainer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-268839ab43e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtaxifare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named taxifare.trainer"
     ]
    }
   ],
   "source": [
    "import taxifare.trainer.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your RMSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
