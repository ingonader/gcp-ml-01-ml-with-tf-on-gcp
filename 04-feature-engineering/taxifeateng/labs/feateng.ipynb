{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Engineering </h1>\n",
    "\n",
    "In this notebook, you will learn how to incorporate feature engineering into your pipeline.\n",
    "<ul>\n",
    "<li> Working with feature columns </li>\n",
    "<li> Adding feature crosses in TensorFlow </li>\n",
    "<li> Reading data from BigQuery </li>\n",
    "<li> Creating datasets using Dataflow </li>\n",
    "<li> Using a wide-and-deep model </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam only works in Python 2 at the moment, so we're going to switch to the Python 2 kernel. In the above menu, click the dropdown arrow and select `python2`. After that, run the following to ensure we've installed Beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /usr/local/envs/py2env\n",
      "\n",
      "  added / updated specs: \n",
      "    - pytz\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2019.1.23  |                0         126 KB  defaults\n",
      "    pytz-2018.9                |           py27_0         263 KB  defaults\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         389 KB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2018.03.07-0  defaults --> 2019.1.23-0   defaults\n",
      "    pytz:            2018.4-py27_0 defaults --> 2018.9-py27_0 defaults\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Uninstalling google-cloud-dataflow-2.0.0:\n",
      "  Successfully uninstalled google-cloud-dataflow-2.0.0\n",
      "Collecting apache-beam[gcp]\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/3d/90aa15779e884feebae4b0c26cad6f52cd4040397a94deb58dad9c8b7300/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl (2.4MB)\n",
      "Requirement already satisfied, skipping upgrade: dill<=0.2.8.2,>=0.2.6 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.2.6)\n",
      "Collecting pydot<1.3,>=1.2.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/f1/e61d6dfe6c1768ed2529761a68f70939e2569da043e9f15a8d84bf56cadf/pydot-1.2.4.tar.gz (132kB)\n",
      "Collecting httplib2<=0.11.3,>=0.8 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/ce/aa4a385e3e9fd351737fd2b07edaa56e7a730448465aceda6b35086a0d9b/httplib2-0.11.3.tar.gz (215kB)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml<4.0.0,>=3.12 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: mock<3.0.0,>=1.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (2.0.0)\n",
      "Collecting typing<3.7.0,>=3.6.0; python_version < \"3.5.0\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/3e/29f92b7aeda5b078c86d14f550bf85cff809042e3429ace7af6193c3bc9f/typing-3.6.6-py2-none-any.whl\n",
      "Collecting pyvcf<0.7.0,>=0.6.8 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/20/b6/36bfb1760f6983788d916096193fc14c83cce512c7787c93380e09458c09/PyVCF-0.6.8.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: oauth2client<4,>=2.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: future<1.0.0,>=0.16.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: avro<2.0.0,>=1.8.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.8.2)\n",
      "Requirement already satisfied, skipping upgrade: crcmod<2.0,>=1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.7)\n",
      "Collecting fastavro<0.22,>=0.21.4 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/da/be/a98f9a81f4f31813ba4714e0bf26fca4bcac7e25f76a515bd86aa45dcfe2/fastavro-0.21.17-cp27-cp27mu-manylinux1_x86_64.whl (1.1MB)\n",
      "Requirement already satisfied, skipping upgrade: futures<4.0.0,>=3.1.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2,>=1.8 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (1.17.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<4,>=3.5.0.post1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (3.6.1)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/96/4e/f82bd349c7893e1595429ecc95233369bc33c9a26e4859991439bfa01c1f/hdfs-2.2.2.tar.gz\n",
      "Collecting pytz<=2018.4,>=2018.3 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/83/15f7833b70d3e067ca91467ca245bae0f6fe56ddc7451aa0dc5606b120f2/pytz-2018.4-py2.py3-none-any.whl (510kB)\n",
      "Collecting google-cloud-pubsub==0.35.4; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/66/f9/bfa284399fb59a8896e0c4164b46185f61f35a90a18c67b366406ad472a6/google_cloud_pubsub-0.35.4-py2.py3-none-any.whl (93kB)\n",
      "Requirement already satisfied, skipping upgrade: proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (0.90.0)\n",
      "Collecting google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/26/abea123a1b5a2b0c1b49c0d8a2e030725f32ae0932d026f2c7a6ee32c8d3/google_apitools-0.5.24-py2-none-any.whl (129kB)\n",
      "Collecting google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/1b/2b95f2fefddbbece38110712c225bfb5649206f4056445653bd5ca4dc86d/google_cloud_bigquery-1.6.1-py2.py3-none-any.whl (83kB)\n",
      "Requirement already satisfied, skipping upgrade: googledatastore<7.1,>=7.0.1; python_version < \"3.0\" and extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]) (7.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from pydot<1.3,>=1.2.0->apache-beam[gcp]) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: funcsigs>=1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pbr>=0.11 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]) (5.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]) (1.10.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/envs/py2env/lib/python2.7/site-packages (from pyvcf<0.7.0,>=0.6.8->apache-beam[gcp]) (40.6.3)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]) (3.4.2)\n",
      "Requirement already satisfied, skipping upgrade: enum34>=1.0.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from grpcio<2,>=1.8->apache-beam[gcp]) (1.1.6)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.7.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (2.18.4)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=0.1.3 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]) (0.1.4)\n",
      "Collecting grpc-google-iam-v1<0.12dev,>=0.11.1 (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/28/f26f67381cb23e81271b8d66c00a846ad9d25a909ae1ae1df8222fad2744/grpc-google-iam-v1-0.11.4.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.5.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\"->apache-beam[gcp]) (1.5.5)\n",
      "Collecting fasteners>=0.14 (from google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/14/3a/096c7ad18e102d4f219f5dd15951f9728ca5092a3385d2e8f79a7c1e1017/fasteners-0.14.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media>=0.2.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<0.30dev,>=0.28.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]) (0.28.1)\n",
      "Requirement already satisfied, skipping upgrade: ordereddict in /usr/local/envs/py2env/lib/python2.7/site-packages (from funcsigs>=1->mock<3.0.0,>=1.0.1->apache-beam[gcp]) (1.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2.0.0dev,>=0.4.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]) (1.6.2)\n",
      "Collecting monotonic>=0.1 (from fasteners>=0.14->google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-auth<2.0.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]) (2.1.0)\n",
      "Building wheels for collected packages: pydot, httplib2, pyvcf, hdfs, docopt, grpc-google-iam-v1\n",
      "  Running setup.py bdist_wheel for pydot: started\n",
      "  Running setup.py bdist_wheel for pydot: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/6a/a5/14/25541ebcdeaf97a37b6d05c7ff15f5bd20f5e91b99d313e5b4\n",
      "  Running setup.py bdist_wheel for httplib2: started\n",
      "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/1b/9c/9e/1f6fdb21dbb1fe6a99101d697f12cb8c1fa96c1587df69adba\n",
      "  Running setup.py bdist_wheel for pyvcf: started\n",
      "  Running setup.py bdist_wheel for pyvcf: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/81/91/41/3272543c0b9c61da9c525f24ee35bae6fe8f60d4858c66805d\n",
      "  Running setup.py bdist_wheel for hdfs: started\n",
      "  Running setup.py bdist_wheel for hdfs: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/99/3f/b2/a09631bd4e2220031fa88949f4acc010cc48cc29011cb25922\n",
      "  Running setup.py bdist_wheel for docopt: started\n",
      "  Running setup.py bdist_wheel for docopt: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: started\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/b6/c6/31/c20321a5a3fde456fc375b7c2814135e6e98bc0d74c40239d9\n",
      "Successfully built pydot httplib2 pyvcf hdfs docopt grpc-google-iam-v1\n",
      "Installing collected packages: pydot, httplib2, typing, pyvcf, fastavro, docopt, hdfs, pytz, grpc-google-iam-v1, google-cloud-pubsub, monotonic, fasteners, google-apitools, google-cloud-bigquery, apache-beam\n",
      "  Found existing installation: httplib2 0.12.0\n",
      "    Uninstalling httplib2-0.12.0:\n",
      "      Successfully uninstalled httplib2-0.12.0\n",
      "  Found existing installation: pytz 2018.9\n",
      "    Uninstalling pytz-2018.9:\n",
      "      Successfully uninstalled pytz-2018.9\n",
      "  Found existing installation: google-apitools 0.5.10\n",
      "    Uninstalling google-apitools-0.5.10:\n",
      "      Successfully uninstalled google-apitools-0.5.10\n",
      "  Found existing installation: google-cloud-bigquery 0.23.0\n",
      "    Uninstalling google-cloud-bigquery-0.23.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-0.23.0\n",
      "Successfully installed apache-beam-2.9.0 docopt-0.6.2 fastavro-0.21.17 fasteners-0.14.1 google-apitools-0.5.24 google-cloud-bigquery-1.6.1 google-cloud-pubsub-0.35.4 grpc-google-iam-v1-0.11.4 hdfs-2.2.2 httplib2-0.11.3 monotonic-1.5 pydot-1.2.4 pytz-2018.4 pyvcf-0.6.8 typing-3.6.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.6.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\r",
      "ca-certificates-2019 | 126 KB    |            |   0% \r",
      "ca-certificates-2019 | 126 KB    | ########## | 100% \n",
      "\r",
      "pytz-2018.9          | 263 KB    |            |   0% \r",
      "pytz-2018.9          | 263 KB    | #########3 |  93% \r",
      "pytz-2018.9          | 263 KB    | ########## | 100% \n",
      "google-cloud-bigquery 1.6.1 has requirement google-api-core<2.0.0dev,>=1.0.0, but you'll have google-api-core 0.1.4 which is incompatible.\n",
      "googledatastore 7.0.1 has requirement httplib2<0.10,>=0.9.1, but you'll have httplib2 0.11.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py2env\n",
    "conda install -y pytz\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a pip install, you have to ```Reset Session``` so that the new packages are picked up.  Please click on the button in the above menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Environment variables for project and bucket </h2>\n",
    "\n",
    "<li> Your project id is the *unique* string that identifies your project (not the project name). You can find this from the GCP Console dashboard's Home page.  My dashboard reads:  <b>Project ID:</b> cloud-training-demos </li>\n",
    "<li> Cloud training often involves saving and restoring model files. Therefore, we should <b>create a single-region bucket</b>. If you don't have a bucket already, I suggest that you create one from the GCP console (because it will dynamically check whether the bucket name you want is available) </li>\n",
    "</ol>\n",
    "<b>Change the cell below</b> to reflect your Project ID and bucket name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "REGION = 'us-central1' # Choose an available region for Cloud MLE from https://cloud.google.com/ml-engine/docs/regions.\n",
    "BUCKET = 'cloud-training-demos-ml' # REPLACE WITH YOUR BUCKET NAME. Use a regional bucket in the region you selected.\n",
    "PROJECT = 'cloud-training-demos'    # CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwiklabs-gcp-0d074dd345826c6b\n",
      "qwiklabs-gcp-0d074dd345826c6b\n",
      "gsutil mb -l europe-west1 gs://qwiklabs-gcp-0d074dd345826c6b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output = os.popen(\"gcloud config get-value project\").readlines()\n",
    "project_name = output[0][:-1]\n",
    "\n",
    "# change these to try this notebook out\n",
    "PROJECT = project_name\n",
    "BUCKET = project_name\n",
    "#BUCKET = BUCKET.replace(\"qwiklabs-gcp-\", \"inna-bckt-\")\n",
    "REGION = 'europe-west1'  ## note: Cloud ML Engine not availabe in europe-west3!\n",
    "\n",
    "print(PROJECT)\n",
    "print(BUCKET)\n",
    "print(\"gsutil mb -l {0} gs://{1}\".format(REGION, BUCKET))\n",
    "\n",
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8' \n",
    "\n",
    "## ensure we're using python2 env\n",
    "os.environ['CLOUDSDK_PYTHON'] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Your current Cloud SDK version is: 229.0.0\n",
      "You will be upgraded to version: 233.0.0\n",
      "\n",
      "+--------------------------------------------------------+\n",
      "|           These components will be updated.            |\n",
      "+---------------------------------+------------+---------+\n",
      "|               Name              |  Version   |   Size  |\n",
      "+---------------------------------+------------+---------+\n",
      "| BigQuery Command Line Tool      |     2.0.40 | < 1 MiB |\n",
      "| Cloud SDK Core Libraries        | 2019.02.04 | 9.4 MiB |\n",
      "| Cloud Storage Command Line Tool |       4.36 | 3.6 MiB |\n",
      "| gcloud Alpha Commands           | 2019.01.19 | < 1 MiB |\n",
      "| gcloud Beta Commands            | 2019.01.19 | < 1 MiB |\n",
      "+---------------------------------+------------+---------+\n",
      "\n",
      "The following release notes are new in this upgrade.\n",
      "Please read carefully for information about new features, breaking changes,\n",
      "and bugs fixed.  The latest full release notes can be viewed at:\n",
      "  https://cloud.google.com/sdk/release_notes\n",
      "\n",
      "233.0.0 (2019-02-05)\n",
      "  Cloud Dataflow\n",
      "      o Added numWorkers, network, subnetwork and workerMachineType flags to\n",
      "    'gcloud beta dataflow jobs run' command\n",
      "\n",
      "  Cloud Datalab\n",
      "      o Updated the datalab component to the 20190116 release. Released\n",
      "        changes are documented in its tracking issue at\n",
      "        https://github.com/googledatalab/datalab/issues/2114\n",
      "        (https://github.com/googledatalab/datalab/issues/2114).\n",
      "\n",
      "  Cloud Filestore\n",
      "      o Promoted gcloud filestore command group to GA.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Release Cloud Firestore Emulator version 1.3.0\n",
      "        * Added a clearDatabase RPC to delete all data in a database\n",
      "        * Added logging to assist with FIRESTORE_EMULATOR_HOST environment\n",
      "          variable\n",
      "        * The getDocument RPC now supports a read_time consistency selector\n",
      "        * Fixed bug related to rule evaluation callbacks\n",
      "\n",
      "  Cloud Machine Learning Engine\n",
      "      o Added support for custom server configuration to ml-engine jobs\n",
      "        submit training in beta. Added the following flags:\n",
      "        * --master-machine-type\n",
      "        * --master-accelerator\n",
      "        * --master-image-uri\n",
      "        * --worker-machine-type\n",
      "        * --worker-count\n",
      "        * --worker-accelerator\n",
      "        * --worker-image-uri\n",
      "        * --parameter-server-machine-type\n",
      "        * --parameter-server-count\n",
      "        * --parameter-server-accelerator\n",
      "        * --parameter-server-image-uri\n",
      "\n",
      "  Cloud PubSub\n",
      "      o Promoted Snapshot & Seek features to GA. These features allow users\n",
      "        to create snapshots of subscription backlog state, and later restore\n",
      "        that state.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Fixed gcloud sql connect whitelisting issues that resulted from\n",
      "        invalid datetime formatting.\n",
      "\n",
      "  Cloud Storage\n",
      "      o Updated gsutil component to 4.36.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted the --force-attach flag of compute instances attach-disk to\n",
      "        GA\n",
      "      o Added <get|set>-iam-policy and <add|remove>-iam-policy-bindings to\n",
      "        gcloud beta compute networks subnets\n",
      "      o Promoted gcloud compute instances get-shielded-identity to beta.\n",
      "      o Promoted gcloud compute instance-groups managed update to GA together\n",
      "        with --health-check, --initial-delay and --clear-autohealing flags.\n",
      "      o Promoted --initial-delay and --health-check flags of gcloud compute\n",
      "        instance-groups managed create to GA.\n",
      "      o Enabled the use of multiple --network-interface flags with gcloud\n",
      "        compute <instances|instance-templates> create-with-container to support\n",
      "        using multiple network interfaces.\n",
      "      o Promoted gcloud compute instance-groups managed rolling-action\n",
      "        command group to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "232.0.0 (2019-01-29)\n",
      "  Breaking Changes\n",
      "      o **(Kubernetes Engine)** Added a warning on cluster and node-pool\n",
      "        creation to notify users that modifications on the boot disks of node\n",
      "        VMs do not persist across node recreations and must be done using a\n",
      "        DaemonSet.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Rolled back fix to gcloud sql connect that seems to be causing\n",
      "        additional issues connecting.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted <get|set>-iam-policy and <add|remove>-iam-policy-bindings to\n",
      "        GA in the following command groups:\n",
      "        * gcloud compute disks\n",
      "        * gcloud compute images\n",
      "        * gcloud compute instance-templates\n",
      "        * gcloud compute snapshots\n",
      "      o Added '--enable-display-device' to gcloud beta compute instances\n",
      "        <create|update>\n",
      "      o Deprecated gcloud compute instance-groups managed set-autohealing\n",
      "        command. Use gcloud compute instance-groups managed update instead.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted the --database-encryption-key flag of gcloud container\n",
      "        clusters\n",
      "    create to beta. The flag enables support for encryption of Kubernetes\n",
      "    Secrets.\n",
      "      o Modified the --enable-stackdriver-kubernetes flag to be a hard\n",
      "        requirement\n",
      "    for --addons=CloudRun. The CloudRun-on-GKE add-on depends on Stackdriver\n",
      "    Kubernetes Monitoring to enrich Kubernetes metadata for logs and metrics.\n",
      "      o Add --max-pods-per-node for gcloud beta container clusters create.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "231.0.0 (2019-01-23)\n",
      "  Breaking Changes\n",
      "      o **(Cloud SQL)** Updated the error messaging associated with failed\n",
      "        long-running operations.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.82. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "\n",
      "  BigQuery\n",
      "      o Added --ignore_unknown_values flag to bq mkdef command.\n",
      "      o Added support for BigQuery BI Engine reservations in bq cli.\n",
      "\n",
      "  Cloud Datastore Emulator\n",
      "      o Release Cloud Datastore Emulator version 2.1.0\n",
      "        * Implement export/import for emulator.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Fixed the display of error codes in gcloud sql operations list.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted gcloud compute instance-groups managed update to beta\n",
      "        together with --health-check, --initial-delay and --clear-autohealing\n",
      "        flags.\n",
      "      o Promoted --hostname flag of gcloud compute instances create to GA.\n",
      "      o Added --physical-block-size flag to gcloud beta compute disks create.\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Added --num-flaky-test-attempts flag to gcloud beta firebase test\n",
      "        android run and gcloud beta firebase test ios run to rerun failed\n",
      "        executions multiple times.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted the --security-group flag of gcloud container clusters\n",
      "        create to\n",
      "    beta. The flag enables support for Google Groups in Kubernetes RBAC rules.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "230.0.0 (2019-01-15)\n",
      "  Breaking Changes\n",
      "      o **(App Engine)** Fixed a bug where symlinked directories were skipped\n",
      "        on source upload. Second Generation runtimes and source directories\n",
      "        using .gcloudignore now upload the contents of symlinked directories,\n",
      "        matching the behavior of First Generation runtimes. To explicitly skip\n",
      "        a symlinked directory, add it to .gcloudignore.\n",
      "      o **(Cloud Functions)** Fixed a bug where symlinked directories were\n",
      "        skipped on source upload. To explicitly skip a symlinked directory, add\n",
      "        it to .gcloudignore.\n",
      "      o **(Cloud SQL)** Made the flags --region, --gce-zone, and --zone\n",
      "        mutually exclusive for the command gcloud sql instances create.\n",
      "      o **(Cloud SQL)** Deprecated the creation of First Generation Cloud SQL\n",
      "        instances, adding a warning and confirmation prompt to gcloud sql\n",
      "        instances create.\n",
      "\n",
      "  Cloud Build\n",
      "      o Released cloud-build-local v0.5.0; see release notes:\n",
      "        <https://github.com/GoogleCloudPlatform/cloud-build-local/releases/tag/v0.5.0>.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Added the flag --zone to gcloud sql instances create as an\n",
      "        alternative to --gce-zone, which is now deprecated.\n",
      "      o Deprecated First Generation Cloud SQL instances, adding warnings to\n",
      "        gcloud sql instances <describe|patch>.\n",
      "\n",
      "  Cloud Scheduler\n",
      "      o Added support for all of App Engine's regions to Cloud Scheduler.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted <get|set>-iam-policy and <add|remove>-iam-policy-bindings to\n",
      "        GA in the following command groups:\n",
      "        * gcloud compute instances\n",
      "        * gcloud compute sole-tenancy node-templates\n",
      "        * gcloud compute sole-tenancy node-groups\n",
      "      o Promoted --boot flag of gcloud compute instances attach-disk to GA.\n",
      "      o Deprecated --auto-create-routes flag of gcloud alpha compute networks\n",
      "        peerings create in Beta.\n",
      "      o Promoted gcloud compute networks peerings update command to Beta.\n",
      "      o Promoted import-custom-routes and export-custom-routes flags to Beta\n",
      "        in gcloud compute networks peerings create command.\n",
      "      o Deprecated and renamed the following --shielded-vm- flags:\n",
      "        * --shielded-vm-secure-boot as --shielded-secure-boot\n",
      "        * --shielded-vm-vtpm as --shielded-vtpm\n",
      "        * --shielded-vm-integrity-monitoring as\n",
      "          --shielded-integrity-monitoring\n",
      "        * --shielded-vm-learn-integrity-policy as\n",
      "          --shielded-learn-integrity-policy\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Deprecated the --app-package and --test-package flags in gcloud\n",
      "        firebase test android run commands; the application and test package\n",
      "        names will be parsed from the APK manifest by default.\n",
      "      o Removed three robo test args that were deprecated 6+ months ago:\n",
      "        --max-steps, --max-depth, and --app-initial-activity.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "Do you want to continue (Y/n)?  \n",
      "#============================================================#\n",
      "#= Creating update staging area                             =#\n",
      "#============================================================#\n",
      "#= Uninstalling: BigQuery Command Line Tool                 =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud SDK Core Libraries                   =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud Storage Command Line Tool            =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud Alpha Commands                      =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud Beta Commands                       =#\n",
      "#============================================================#\n",
      "#= Installing: BigQuery Command Line Tool                   =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud SDK Core Libraries                     =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud Storage Command Line Tool              =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud Alpha Commands                        =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud Beta Commands                         =#\n",
      "#============================================================#\n",
      "#= Creating backup and activating new installation          =#\n",
      "#============================================================#\n",
      "\n",
      "Performing post processing steps...\n",
      "..................................done.\n",
      "\n",
      "Update done!\n",
      "\n",
      "To revert your SDK to the previously installed version, you may run:\n",
      "  $ gcloud components update --version 229.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## ensure gcloud is up to date\n",
    "gcloud components update\n",
    "\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "\n",
    "## ensure we predict locally with our current Python environment\n",
    "gcloud config set ml_engine/local_python `which python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Specifying query to pull the data </h2>\n",
    "\n",
    "Let's pull out a few extra columns from the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      "  (tolls_amount + fare_amount) AS fare_amount,\n",
      "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
      "  HOUR(pickup_datetime) AS hourofday,\n",
      "  pickup_longitude AS pickuplon,\n",
      "  pickup_latitude AS pickuplat,\n",
      "  dropoff_longitude AS dropofflon,\n",
      "  dropoff_latitude AS dropofflat,\n",
      "  passenger_count*1.0 AS passengers,\n",
      "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
      "FROM\n",
      "  [nyc-tlc:yellow.trips]\n",
      "WHERE\n",
      "  trip_distance > 0\n",
      "  AND fare_amount >= 2.5\n",
      "  AND pickup_longitude > -78\n",
      "  AND pickup_longitude < -70\n",
      "  AND dropoff_longitude > -78\n",
      "  AND dropoff_longitude < -70\n",
      "  AND pickup_latitude > 37\n",
      "  AND pickup_latitude < 45\n",
      "  AND dropoff_latitude > 37\n",
      "  AND dropoff_latitude < 45\n",
      "  AND passenger_count > 0\n",
      "   AND ABS(HASH(pickup_datetime)) % 100 == 2\n"
     ]
    }
   ],
   "source": [
    "def create_query(phase, EVERY_N):\n",
    "  if EVERY_N == None:\n",
    "    EVERY_N = 4 #use full dataset\n",
    "    \n",
    "  #select and pre-process fields\n",
    "  base_query = \"\"\"\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
    "  HOUR(pickup_datetime) AS hourofday,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  \"\"\"\n",
    "  \n",
    "  #add subsampling criteria by modding with hashkey\n",
    "  if phase == 'train': \n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} < 2\".format(base_query,EVERY_N)\n",
    "  elif phase == 'valid': \n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} == 2\".format(base_query,EVERY_N)\n",
    "  elif phase == 'test':\n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} == 3\".format(base_query,EVERY_N)\n",
    "  return query\n",
    "    \n",
    "print create_query('valid', 100) #example query using 1% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RequestException",
     "evalue": "HTTP request failed: Query text specifies use_legacy_sql:true, while API options specify:false",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRequestException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4f1a24e825e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \"\"\"\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/google/datalab/bigquery/_query.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, output_options, sampling, context, query_params)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \"\"\"\n\u001b[1;32m    338\u001b[0m     return self.execute_async(output_options, sampling=sampling, context=context,\n\u001b[0;32m--> 339\u001b[0;31m                               query_params=query_params).wait()\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/google/datalab/bigquery/_query.pyc\u001b[0m in \u001b[0;36mexecute_async\u001b[0;34m(self, output_options, sampling, context, query_params)\u001b[0m\n\u001b[1;32m    270\u001b[0m                                            query_params=query_params)\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'jobReference'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unexpected response from server'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRequestException\u001b[0m: HTTP request failed: Query text specifies use_legacy_sql:true, while API options specify:false"
     ]
    }
   ],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "\n",
    "query_test = \"\"\"\n",
    "#legacySQL\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
    "  HOUR(pickup_datetime) AS hourofday,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  AND ABS(HASH(pickup_datetime)) % 100 == 2\n",
    "  LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "res = bq.Query(query_test).execute().result().to_dataframe()\n",
    "res.head(n = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the query above in https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips if you want to see what it does (ADD LIMIT 10 to the query!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Row\tfare_amount\tdayofweek\thourofday\tpickuplon\tpickuplat\tdropofflon\tdropofflat\tpassengers\tkey\t \n",
    "1\t18.0\t5\t19\t-73.973092\t40.750065\t-74.009415\t40.7128\t1.0\t2013-05-02 19:25:00.000000-73.973140.750140.7128-74.0094\t \n",
    "2\t39.83\t6\t17\t-73.863462\t40.769847\t-73.983307\t40.735722\t3.0\t2013-12-06 17:50:00.000000-73.863540.769840.7357-73.9833\t \n",
    "3\t21.5\t2\t21\t-73.994028\t40.751273\t-73.977827\t40.680942\t1.0\t2012-11-26 21:45:00.000000-73.99440.751340.6809-73.9778\t \n",
    "4\t35.3\t5\t11\t-73.97348\t40.751112\t-73.861642\t40.768322\t1.0\t2012-11-15 11:32:00.000000-73.973540.751140.7683-73.8616\t \n",
    "5\t25.5\t6\t12\t-73.969537\t40.760892\t-73.885182\t40.77218\t1.0\t2012-11-16 12:20:00.000000-73.969540.760940.7722-73.8852\t \n",
    "6\t24.5\t6\t11\t-73.9526\t40.772473\t-73.861593\t40.76822\t5.0\t2011-04-29 11:30:00.000000-73.952640.772540.7682-73.8616\t \n",
    "7\t30.3\t6\t5\t-73.945982\t40.8038\t-73.872017\t40.774515\t6.0\t2013-02-15 05:54:00.000000-73.94640.803840.7745-73.872\t \n",
    "8\t39.33\t7\t18\t-73.871122\t40.773887\t-73.986117\t40.75057\t1.0\t2014-10-18 18:17:00.000000-73.871140.773940.7506-73.9861\t \n",
    "9\t30.5\t6\t8\t-73.994405\t40.690012\t-73.984307\t40.763057\t1.0\t2014-09-26 08:45:00.000000-73.994440.6940.7631-73.9843\t \n",
    "10\t18.0\t7\t1\t-74.00346\t40.725322\t-73.97648\t40.78495\t2.0\t2012-12-01 01:39:00.000000-74.003540.725340.785-73.9765\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Preprocessing Dataflow job from BigQuery </h2>\n",
    "\n",
    "This code reads from BigQuery and saves the data as-is on Google Cloud Storage.  We can do additional preprocessing and cleanup inside Dataflow, but then we'll have to remember to repeat that prepreprocessing during inference. It is better to use tf.transform which will do this book-keeping for you, or to do preprocessing within your TensorFlow model. We will look at this in future notebooks. For now, we are simply moving data from BigQuery to CSV using Dataflow.\n",
    "\n",
    "While we could read from BQ directly from TensorFlow (See: https://www.tensorflow.org/api_docs/python/tf/contrib/cloud/BigQueryReader), it is quite convenient to export to CSV and do the training off CSV.  Let's use Dataflow to do this at scale.\n",
    "\n",
    "Because we are running this on the Cloud, you should go to the GCP Console (https://console.cloud.google.com/dataflow) to look at the status of the job. It will take several minutes for the preprocessing job to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil -m rm -rf gs://$BUCKET/taxifare/ch4/taxi_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -rowdict: Dictionary. The beam bigquery reader returns a PCollection in\n",
    "#     which each row is represented as a python dictionary\n",
    "# Returns:\n",
    "#   -rowstring: a comma separated string representation of the record with dayofweek\n",
    "#     converted from int to string (e.g. 3 --> Tue)\n",
    "####\n",
    "def to_csv(rowdict):\n",
    "  days = ['null', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "  CSV_COLUMNS = 'fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key'.split(',')\n",
    "  rowdict['dayofweek'] = days[rowdict['dayofweek']]\n",
    "  rowstring = ','.join([str(rowdict[k]) for k in CSV_COLUMNS])\n",
    "  return rowstring\n",
    "\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -EVERY_N: Integer. Sample one out of every N rows from the full dataset.\n",
    "#     Larger values will yield smaller sample\n",
    "#   -RUNNER: 'DirectRunner' or 'DataflowRunner'. Specfy to run the pipeline\n",
    "#     locally or on Google Cloud respectively. \n",
    "# Side-effects:\n",
    "#   -Creates and executes dataflow pipeline. \n",
    "#     See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
    "####\n",
    "def preprocess(EVERY_N, RUNNER):\n",
    "  job_name = 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "  print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "  OUTPUT_DIR = 'gs://{0}/taxifare/ch4/taxi_preproc/'.format(BUCKET)\n",
    "\n",
    "  #dictionary of pipeline options\n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': PROJECT,\n",
    "    'runner': RUNNER\n",
    "  }\n",
    "  #instantiate PipelineOptions object using options dictionary\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  #instantantiate Pipeline object using PipelineOptions\n",
    "  with beam.Pipeline(options=opts) as p:\n",
    "      for phase in ['train', 'valid']:\n",
    "        query = create_query(phase, EVERY_N) \n",
    "        outfile = os.path.join(OUTPUT_DIR, '{}.csv'.format(phase))\n",
    "        (\n",
    "          p | 'read_{}'.format(phase) >> beam.io.Read(beam.io.BigQuerySource(query = query)) ##TODO: read from BigQuery\n",
    "            | 'tocsv_{}'.format(phase) >> beam.Map(to_csv) ##TODO: apply the to_csv function to every row\n",
    "            | 'write_{}'.format(phase) >> beam.io.Write(beam.io.WriteToText(outfile)) ##TODO: write to outfile\n",
    "        )\n",
    "  print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-190207-075306 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:365: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  pipeline.replace_all(_get_transform_overrides(pipeline.options))\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/oauth2client/contrib/gce.py:99: UserWarning: You have requested explicit scopes to be used with a GCE service account.\n",
      "Using this argument will have no effect on the actual scopes for tokens\n",
      "requested. These scopes are set at VM instance creation time and\n",
      "can't be overridden in the request.\n",
      "\n",
      "  warnings.warn(_SCOPES_WARNING)\n",
      "WARNING:root:Dataset qwiklabs-gcp-0d074dd345826c6b:temp_dataset_f8ef8e2b3e714ba3b8b095729758ebde does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset qwiklabs-gcp-0d074dd345826c6b:temp_dataset_88f9316dee5e4fc4862ab6c6c4cd1968 does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "preprocess(EVERY_N = 50 * 10000, RUNNER = 'DirectRunner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/\n"
     ]
    }
   ],
   "source": [
    "print 'gs://{0}/taxifare/ch4/taxi_preproc/'.format(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00000-of-00005\r\n",
      "gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00001-of-00005\r\n",
      "gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00002-of-00005\r\n",
      "gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00003-of-00005\r\n",
      "gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00004-of-00005\r\n",
      "gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/valid.csv-00000-of-00002\r\n",
      "gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/valid.csv-00001-of-00002\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET/taxifare/ch4/taxi_preproc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipleline on cloud on a larger sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-190207-090124 ... hang on\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# preprocess(EVERY_N = 50 * 100, RUNNER = 'DataflowRunner')  ## time: 9 min 33 secs in the training environment\n",
    "# #change first arg to None to preprocess full dataset\n",
    "\n",
    "## changed to the same as locally:\n",
    "# preprocess(EVERY_N = 50 * 10000, RUNNER = 'DataflowRunner')  ## time: 7 min 8 secs\n",
    "\n",
    "## changed to 100 times more than locally:\n",
    "#preprocess(EVERY_N = 50 * 10000 * 100, RUNNER = 'DataflowRunner')  ## time: 6 min 59 sec\n",
    "\n",
    "## changed to 10000 times more than locally:\n",
    "preprocess(EVERY_N = 50 * 10000 * 10000, RUNNER = 'DataflowRunner')  ## time: \n",
    "\n",
    "## but something went wrong: the number of elements processed ('added') in\n",
    "## dataflow seems way to low..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job completes, observe the files created in Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2019-02-07T09:06:05Z  gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00000-of-00001\n",
      "    108804  2019-02-07T07:53:48Z  gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00000-of-00005\n",
      "    113893  2019-02-07T07:53:48Z  gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00001-of-00005\n",
      "    114335  2019-02-07T07:53:48Z  gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00002-of-00005\n",
      "    114722  2019-02-07T07:53:48Z  gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00003-of-00005\n",
      "    114927  2019-02-07T07:53:48Z  gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/train.csv-00004-of-00005\n",
      "         0  2019-02-07T09:06:18Z  gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/valid.csv-00000-of-00001\n",
      "    113578  2019-02-07T07:53:46Z  gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/valid.csv-00000-of-00002\n",
      "    106763  2019-02-07T07:53:46Z  gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/valid.csv-00001-of-00002\n",
      "                                 gs://qwiklabs-gcp-0d074dd345826c6b/taxifare/ch4/taxi_preproc/tmp/\n",
      "TOTAL: 9 objects, 787022 bytes (768.58 KiB)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://$BUCKET/taxifare/ch4/taxi_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1,Sat,3,-74.002516,40.750015,-73.984699,40.759539,1.0,2011-10-01 03:36:03.000000-74.002540.7540.7595-73.9847\n",
      "6.1,Thu,3,-74.00006,40.727142,-73.982435,40.72488,1.0,2012-06-14 03:17:00.000000-74.000140.727140.7249-73.9824\n",
      "6.1,Tue,4,-73.98821,40.764537,-73.978608,40.783388,2.0,2009-11-03 04:46:55.000000-73.988240.764540.7834-73.9786\n",
      "6.1,Mon,6,-73.964098,40.771065,-73.975158,40.752819,1.0,2012-06-25 06:01:48.000000-73.964140.771140.7528-73.9752\n",
      "6.1,Sat,6,-74.005195,40.706787,-73.988503,40.71854,3.0,2011-07-09 06:12:00.000000-74.005240.706840.7185-73.9885\n",
      "6.1,Tue,7,-73.952269,40.766104,-73.969499,40.769078,1.0,2009-03-31 07:51:25.000000-73.952340.766140.7691-73.9695\n",
      "6.1,Wed,7,-73.948706,40.782377,-73.963411,40.774713,1.0,2009-07-01 07:53:10.000000-73.948740.782440.7747-73.9634\n",
      "6.1,Fri,7,-74.003218,40.727604,-73.976228,40.719043,1.0,2010-01-15 07:30:05.000000-74.003240.727640.719-73.9762\n",
      "6.1,Tue,7,-73.992171,40.744027,-73.979134,40.756059,1.0,2012-03-13 07:35:19.000000-73.992240.74440.7561-73.9791\n",
      "6.1,Mon,7,-73.966526,40.761713,-73.982688,40.769111,1.0,2010-03-08 07:29:27.000000-73.966540.761740.7691-73.9827\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "#print first 10 lines of first shard of train.csv\n",
    "gsutil cat \"gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Develop model with new inputs </h2>\n",
    "\n",
    "Download the first shard of the preprocessed data to enable local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n",
      "CommandException: Destination URL must name a directory, bucket, or bucket\n",
      "subdirectory for the multiple source form of the cp command.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "mkdir sample\n",
    "gsutil cp \"gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*\" sample/train.csv\n",
    "gsutil cp \"gs://$BUCKET/taxifare/ch4/taxi_preproc/valid.csv-00000-of-*\" sample/valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the TODOs in taxifare/trainer/model.py so that the code below works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare/trainer/model.py:    # TODO: Define feature columns for dayofweek, hourofday, pickuplon, pickuplat, dropofflat, dropofflon, passengers\r\n",
      "taxifare/trainer/model.py:    # TODO: Add any engineered columns here\r\n",
      "taxifare/trainer/model.py:     TODO: Build an estimator starting from INPUT COLUMNS.\r\n",
      "taxifare/trainer/model.py:    return None # TODO: Add estimator definition here\r\n",
      "taxifare/trainer/model.py:    # TODO: Add any engineered features to the dict\r\n",
      "taxifare/trainer/model.py:        # TODO: What features will user provide? What will their types be?\r\n",
      "taxifare/trainer/model.py:    # TODO: Add any extra placeholders for inputs you'll generate\r\n",
      "taxifare/trainer/model.py:      features, # TODO: Wrap this with a call to add_engineered\r\n",
      "taxifare/trainer/model.py:            return features, label # TODO: Wrap this with a call to add_engineered\r\n"
     ]
    }
   ],
   "source": [
    "!grep TODO taxifare/trainer/*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    # TODO: Define feature columns for dayofweek, hourofday, pickuplon, pickuplat, dropofflat, d\n",
    "ropofflon, passengers\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    # TODO: Add any engineered columns here\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:     TODO: Build an estimator starting from INPUT COLUMNS.\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    return None # TODO: Add estimator definition here\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    # TODO: Add any engineered features to the dict\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:        # TODO: What features will user provide? What will their types be?\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:    # TODO: Add any extra placeholders for inputs you'll generate\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:      features, # TODO: Wrap this with a call to add_engineered\n",
    "04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py:            return features, label # TODO: Wrap this with a call to add_engineered\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 30, '_session_config': None, '_keep_checkpoint_max': 3, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe465215990>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxi_trained/', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxifare/trainer/task.py\", line 124, in <module>\n",
      "    model.train_and_evaluate(arguments)\n",
      "  File \"/content/datalab/gcp-ml-01-ml-with-tf-on-gcp/04-feature-engineering/taxifeateng/labs/taxifare/trainer/model.py\", line 209, in train_and_evaluate\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 439, in train_and_evaluate\n",
      "    executor.run()\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 518, in run\n",
      "    self.run_local()\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 650, in run_local\n",
      "    hooks=train_hooks)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 843, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 856, in _train_model_default\n",
      "    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 831, in _call_model_fn\n",
      "    model_fn_results = self._model_fn(features=features, **kwargs)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn_linear_combined.py\", line 568, in _model_fn\n",
      "    config=config)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn_linear_combined.py\", line 168, in _dnn_linear_combined_model_fn\n",
      "    dnn_logits = dnn_logit_fn(features=features, mode=mode)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 100, in dnn_logit_fn\n",
      "    name=hidden_layer_scope)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 253, in dense\n",
      "    return layer.apply(inputs)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 828, in apply\n",
      "    return self.__call__(inputs, *args, **kwargs)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 699, in __call__\n",
      "    self.build(input_shapes)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 138, in build\n",
      "    trainable=True)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 546, in add_variable\n",
      "    partitioner=partitioner)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/training/checkpointable.py\", line 436, in _add_variable_with_custom_getter\n",
      "    **kwargs_for_getter)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\n",
      "    constraint=constraint)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\n",
      "    constraint=constraint)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n",
      "    constraint=constraint)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 362, in _true_getter\n",
      "    constraint=constraint)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 521, in _get_partitioned_variable\n",
      "    shape = tensor_shape.as_shape(shape)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 940, in as_shape\n",
      "    return TensorShape(shape)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 538, in __init__\n",
      "    self._dims = [as_dimension(d) for d in dims_iter]\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 479, in as_dimension\n",
      "    return Dimension(value)\n",
      "  File \"/usr/local/envs/py2env/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 37, in __init__\n",
      "    self._value = int(value)\n",
      "ValueError: invalid literal for int() with base 10: ''\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "python -m trainer.task \\\n",
    "  --train_data_paths=${PWD}/sample/train.csv \\\n",
    "  --eval_data_paths=${PWD}/sample/valid.csv  \\\n",
    "  --output_dir=${PWD}/taxi_trained \\\n",
    "  --train_steps=1000 \\\n",
    "  --job-dir=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls taxi_trained/export/exporter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%writefile /tmp/test.json\n",
    "{\"dayofweek\": \"Sun\", \"hourofday\": 17, \"pickuplon\": -73.885262, \"pickuplat\": 40.773008, \"dropofflon\": -73.987232, \"dropofflat\": 40.732403, \"passengers\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "model_dir=$(ls ${PWD}/taxi_trained/export/exporter)\n",
    "gcloud ml-engine local predict \\\n",
    "  --model-dir=${PWD}/taxi_trained/export/exporter/${model_dir} \\\n",
    "  --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if gcloud ml-engine local predict fails, might need to update glcoud\n",
    "#!gcloud --quiet components update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Train on cloud </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/taxifare/ch4/taxi_trained\n",
    "JOBNAME=lab4a_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=${PWD}/taxifare/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=BASIC \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --train_data_paths=\"gs://$BUCKET/taxifare/ch4/taxi_preproc/train*\" \\\n",
    "  --eval_data_paths=\"gs://${BUCKET}/taxifare/ch4/taxi_preproc/valid*\"  \\\n",
    "  --train_steps=5000 \\\n",
    "  --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 6. Inspect with TensorBoard </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "OUTDIR='gs://{0}/taxifare/ch4/taxi_trained'.format(BUCKET)\n",
    "print OUTDIR\n",
    "TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your RMSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
