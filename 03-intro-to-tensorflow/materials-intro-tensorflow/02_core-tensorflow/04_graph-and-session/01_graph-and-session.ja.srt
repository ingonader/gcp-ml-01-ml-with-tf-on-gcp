1
00:00:00,000 --> 00:00:03,045
グラフとセッションについて
見ていきましょう

2
00:00:03,045 --> 00:00:05,370
TensorFlowにおける

3
00:00:05,370 --> 00:00:07,540
有向非巡回グラフ（DAG）は

4
00:00:07,540 --> 00:00:09,080
他のグラフと同様に

5
00:00:09,080 --> 00:00:11,955
エッジとノードで構成されます

6
00:00:11,955 --> 00:00:14,785
エッジはデータを表します

7
00:00:14,785 --> 00:00:20,075
これらはn次元の配列であるテンソルを表します

8
00:00:20,075 --> 00:00:24,920
ノードはテンソルに対する
TensorFlow演算を表します

9
00:00:24,920 --> 00:00:28,995
前のレッスンで考えた tf.addなどです

10
00:00:28,995 --> 00:00:36,350
TensorFlow DAGはテンソルと
それらのテンソルに対する演算で構成されます

11
00:00:36,350 --> 00:00:40,470
なぜTensorFlowは遅延評価を行うのでしょう

12
00:00:40,470 --> 00:00:44,380
それは グラフの実行時に
遅延評価によって

13
00:00:44,380 --> 00:00:48,645
柔軟性がもたらされ
最適化が可能になるからです

14
00:00:48,645 --> 00:00:52,240
TensorFlowではグラフの処理や
コンパイルのほか

15
00:00:52,240 --> 00:00:55,900
DAGの途中に
送受信ノードを挿入できます

16
00:00:55,900 --> 00:00:58,535
リモートでも実行できます

17
00:00:58,535 --> 00:01:02,550
I/Oバウンドかどうか
GPU機能が必要かどうかに応じて

18
00:01:02,550 --> 00:01:04,055
TensorFlowでは
DAGのさまざまなパーツを

19
00:01:04,055 --> 00:01:08,305
異なるデバイスに
割り当てることができます

20
00:01:08,305 --> 00:01:10,875
グラフが処理されている間は

21
00:01:10,875 --> 00:01:13,925
量子化やデータ型の追加

22
00:01:13,925 --> 00:01:15,800
デバッグノードの追加

23
00:01:15,800 --> 00:01:18,710
値を書き出すための
サマリー作成が可能です

24
00:01:18,710 --> 00:01:22,950
テンソルはこれらのほか
add、matmul、constants、

25
00:01:22,950 --> 00:01:27,755
variablesなどの演算を
読み取って処理します

26
00:01:27,755 --> 00:01:30,200
グラフのコンパイル時

27
00:01:30,200 --> 00:01:34,515
TensorFlowは2つの演算を結合して
パフォーマンスを改善します

28
00:01:34,515 --> 00:01:38,415
たとえば 連続する2つのaddノードを

29
00:01:38,415 --> 00:01:41,855
1つに結合できます

30
00:01:41,855 --> 00:01:44,800
TensorFlowのXLAコンパイラは

31
00:01:44,800 --> 00:01:49,640
DAGへの情報を使って
より速いコードを生成します

32
00:01:49,640 --> 00:01:55,865
これが 最適化のために
DAGを利用すべき理由の1つです

33
00:01:55,865 --> 00:01:58,030
最も素晴らしい点は

34
00:01:58,030 --> 00:02:02,630
リモートで DAGを実行し
デバイスに割り当てられることです

35
00:02:02,630 --> 00:02:08,500
この点でのメリットは
際立っています

36
00:02:08,500 --> 00:02:14,965
演算間の依存関係を表すために
明示的なエッジを使用することで

37
00:02:14,965 --> 00:02:20,980
同時に実行できる複数の演算が
簡単に識別されます

38
00:02:20,980 --> 00:02:26,230
演算間を流れる値を表すために
明示的なエッジを使用することで

39
00:02:26,230 --> 00:02:30,300
TensorFLowがプログラムを
パーティション化して

40
00:02:30,300 --> 00:02:34,135
CPU、GPU、TPUなど異なるマシンに接続された

41
00:02:34,135 --> 00:02:37,885
複数のデバイスに分けることができます

42
00:02:37,885 --> 00:02:43,760
TensorFlowは これらのデバイス間で
必要なやり取りや調整を挿入します

43
00:02:43,760 --> 00:02:46,030
この図の色分けに注目してください

44
00:02:46,030 --> 00:02:49,330
グラフのいくつかの部分は
異なるデバイス上に存在します

45
00:02:49,330 --> 00:02:52,770
GPUか複数のコンピュータかは
関係ありません

46
00:02:52,770 --> 00:02:55,290
このモデルの主なメリットの1つは

47
00:02:55,290 --> 00:03:00,730
たくさんのタイプの複数のコンピュータに
計算を分散できることです

48
00:03:00,730 --> 00:03:02,680
DAGでは これが可能です

49
00:03:02,680 --> 00:03:05,900
Pythonコードを書くだけで

50
00:03:05,900 --> 00:03:11,275
TensorFlow実行システムがグラフを
最適化、分散します

51
00:03:11,275 --> 00:03:18,995
セッションクラスは 記述するPythonプログラムと
C++ランタイムの間の接続を表します

52
00:03:18,995 --> 00:03:23,020
セッションオブジェクトは
分散TensorFlowランタイムを使用して

53
00:03:23,020 --> 00:03:25,969
ローカルコンピュータ上の
デバイスと

54
00:03:25,969 --> 00:03:29,395
リモートデバイスへの
アクセスを提供します

55
00:03:29,395 --> 00:03:32,450
また グラフ情報を
キャッシュに保存します

56
00:03:32,450 --> 00:03:36,275
そのため 何度も同じ計算を実行できます

57
00:03:36,275 --> 00:03:38,205
TensorFlowグラフを

58
00:03:38,205 --> 00:03:43,265
実行するには
tfセッションでの実行を呼び出します

59
00:03:43,265 --> 00:03:47,675
ここでは
評価するテンソルを指定します

60
00:03:47,675 --> 00:03:50,290
この例では2つのデータテンソル

61
00:03:50,290 --> 00:03:53,660
xとyを指定しています

62
00:03:53,660 --> 00:03:57,015
これらは定数で
1Dテンソルです

63
00:03:57,015 --> 00:04:05,465
テンソルzは xとyに対して
tf.addを呼び出した結果です

64
00:04:05,465 --> 00:04:07,415
評価する際に

65
00:04:07,415 --> 00:04:10,990
Zに対して session.runを呼び出します

66
00:04:10,990 --> 00:04:16,470
scssは
tf.sessionのインスタンスです

67
00:04:16,470 --> 00:04:19,130
Pythonの withステートメントを使うと

68
00:04:19,130 --> 00:04:23,990
完了時に セッションを
自動的に閉じることができます