1
00:00:00,050 --> 00:00:03,515
Hasta ahora, vimos
cómo escribir y ejecutar un grafo.

2
00:00:04,185 --> 00:00:07,850
Pero ocasionalmente
necesitará visualizar el grafo.

3
00:00:08,159 --> 00:00:10,015
Necesita ver las operaciones

4
00:00:10,155 --> 00:00:12,430
los datos que se ingresan, etcétera.

5
00:00:12,870 --> 00:00:16,390
O tal vez quiera visualizar
la arquitectura de sus redes neuronales.

6
00:00:17,290 --> 00:00:22,010
Para escribir el grafo,
use tf.summary.FileWriter.

7
00:00:22,590 --> 00:00:26,855
Esta operación le permitirá
ver el grafo de la sesión.

8
00:00:27,635 --> 00:00:29,615
Ahora, si visualizará el grafo

9
00:00:29,805 --> 00:00:33,445
probablemente no quiera ver nombres
de tensores generados automáticamente

10
00:00:33,445 --> 00:00:36,295
como Add_7, que vimos antes.

11
00:00:36,645 --> 00:00:39,410
Si escribe el grafo para visualizarlo

12
00:00:39,610 --> 00:00:43,110
asegúrese de asignar nombres
a los tensores de datos y las operaciones.

13
00:00:44,200 --> 00:00:47,180
Una vez que lo haga
y ejecute este programa

14
00:00:47,330 --> 00:00:49,915
aparecerá un nuevo directorio
llamado "summaries".

15
00:00:50,445 --> 00:00:52,780
Se llama "summaries"
porque lo especifiqué

16
00:00:52,940 --> 00:00:57,480
como primer argumento
de tf.summary.FileWriter.

17
00:00:57,770 --> 00:00:59,960
Puede nombrar al directorio como quiera.

18
00:01:00,190 --> 00:01:04,185
En ese directorio, 
hay un archivo con un grafo.

19
00:01:05,035 --> 00:01:08,090
El grafo está
en un formato binario documentado.

20
00:01:08,090 --> 00:01:09,815
No es realmente legible.

21
00:01:10,175 --> 00:01:13,545
¿Cómo puede visualizar
este archivo de salida?

22
00:01:14,645 --> 00:01:16,160
Para visualizar el grafo

23
00:01:16,420 --> 00:01:18,715
tendrá que usar
un programa llamado TensorBoard.

24
00:01:19,175 --> 00:01:21,260
Se incluye
en la distribución de TensorFlow

25
00:01:21,590 --> 00:01:23,495
por lo que puede llamarlo desde Datalab.

26
00:01:24,005 --> 00:01:27,450
Aquí está el código de Python
para invocar a TensorBoard

27
00:01:27,880 --> 00:01:31,015
y dirigirlo al directorio
de nivel superior llamado "summaries".

28
00:01:31,645 --> 00:01:33,470
Y luego, en la IU que aparece

29
00:01:33,720 --> 00:01:38,280
cambie a la sección de grafos
y verá una representación visual

30
00:01:38,280 --> 00:01:42,890
de su grafo con x, y, z1, z2 y z3

31
00:01:43,110 --> 00:01:45,005
y las operaciones que las conectan.

32
00:01:45,705 --> 00:01:47,767
Se puede hacer mucho más con TensorBoard.

33
00:01:47,927 --> 00:01:50,250
Podrá experimentar con él
en el siguiente lab

34
00:01:50,370 --> 00:01:52,655
y en otros labs de esta especialización.

35
00:01:54,445 --> 00:01:58,235
Puesto que se suele desarrollar
modelos de AA en Datalab

36
00:01:58,465 --> 00:02:03,210
poder iniciar TensorBoard directamente
desde Datalab es muy conveniente.

37
00:02:03,770 --> 00:02:06,870
Pero a veces
iniciará un trabajo de entrenamiento

38
00:02:06,940 --> 00:02:09,030
que quizá demore algunas horas

39
00:02:09,430 --> 00:02:11,490
y querrá supervisar el entrenamiento.

40
00:02:12,210 --> 00:02:16,230
No hace falta entrar
en Datalab para iniciar TensorBoard.

41
00:02:17,020 --> 00:02:20,085
Como dije, TensorBoard viene con TensorFlow.

42
00:02:20,705 --> 00:02:23,780
TensorFlow también
está instalado en Cloud Shell

43
00:02:24,460 --> 00:02:28,647
y si bien Cloud Shell no es tan robusto
como para realizar un trabajo de AA

44
00:02:29,485 --> 00:02:31,955
es suficiente para ejecutar TensorBoard.

45
00:02:32,435 --> 00:02:37,075
Aquí tiene las instrucciones
para iniciar TensorBoard desde Cloud Shell

46
00:02:37,395 --> 00:02:41,600
redirigiendo a un directorio de salida
de un modelo ubicado en Cloud Storage.

47
00:02:42,020 --> 00:02:45,714
Encuentre este documento en el sitio web
con el vínculo que se incluye en este video.