Now let's look at graph and session. The Directed Acyclic Graph, the DAG in TensorFlow, is like any graph. It consists of edges and nodes. The edges represent data, they represent tensors, which as we now know, are n-dimensional arrays. The nodes represent TensorFlow operations on those tensors. Things like tf.add that we solved in the previous lesson. A TensorFlow DAG consists of tensors and operations on those tensors. So, why does TensorFlow do lazy evaluation? It's because lazy evaluation allows for a lot of flexibility and optimization when you're running the graph. TensorFlow can now process the graph, compiler it, inserts send and receive nodes in the middle of the DAG, also that it can be remotely executed. Tensorflow can assign different parts of the DAG to different devices, depending on whether it's I/O bound, or whether it's going to require GPU capabilities. While the graph is being processed, TensorFlow can add quantization or data types, it can add debug nodes, it can create summaries to write values out, so tensor can read them besides computation like add, matmul, constants, variables all of these are ops and TensorFlow can work with them. When the graph is being compiled, TensorFlow can take two ops and fuse them to improve performance. For example, you may have two consecutive add nodes, and TensorFlow can fuse them into a single one. TensorFlow's XLA compiler can use the information into a Directed Acyclic Graph to generate faster code. So, that's one aspect of why you want to use a DAG for optimization. But the most exciting part is that the DAG can be remotely executed and assigned to devices. And that's where the benefits of the DAG approach become very evident. By using explicit edges to represent dependencies between operations, it's easy for the system to identify operations that can execute in parallel. And by using explicit edges to represent the values that flow between operations, it's possible for TensorFLow to partition your program across multiple devices; CPUs, GPUs, TPUs, etc that are attached even to different machines. TensorFlow inserts the necessary communication and coordination between these devices. So, note the colors in the diagram. Several parts of the graph can be on different devices, it doesn't matter whether it's GPU or different computers. So, one key benefit of this model to be able to distribute computation across many machines, and many types of machines, comes because of the DAG. We just write Python code and let the TensorFlow execution system optimize and distribute the graph. The session class represents this connection between the Python program that we write, and the C++ runtime. The session object provides access to the devices on the local machine, and to remote devices using the distributor TensorFlow runtime. It also caches information about the graph, so, the same computation can be run multiple times. As we saw, we execute TensorFlow graphs by calling run on a tf session, and when we do that, we specify a tensor that we want to evaluate. So, in this code example, I'm defining two data tensors X and Y. They're constants, they are 1D tensors. The tensor Z is a result of invoking tf.add on X and Y. When I want to evaluate, I call session.run on Z. Session here scss, is an instance of tf session, and the with statement in Python, is how we can ensure that the session is automatically closed when we are done.