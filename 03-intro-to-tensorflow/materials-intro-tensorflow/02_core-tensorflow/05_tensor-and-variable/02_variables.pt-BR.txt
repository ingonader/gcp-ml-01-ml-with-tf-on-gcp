Uma variável é um tensor em que o valor é inicializado e, em seguida, alterado
conforme um programa é executado. Vamos dar uma olhada neste exemplo. Tenho uma função chamada forward_pass, que recebe dois parâmetros, w e x, e os multiplica. Bem, é uma multiplicação de matrizes
porque são tensores, mas ela multiplica w e x. Na minha função train_loop, eu basicamente crio o tensor w, exceto que w não é uma constante como
os tensores que observamos até agora. W é uma variável. Isso tem nome: ponderação. A forma dela é 1,2, o que significa que tem
uma linha e duas colunas. É uma matriz 1x2. E quando w é inicializado, não estamos inicializando-o aqui
porque, lembre-se, o TensorFlow é
uma biblioteca de avaliação lenta e, portanto, estamos só
criando o gráfico. Ainda não o estamos executando. Quando w é inicializado, ele será inicializado por um
inicializador normal truncado. Este é um inicializador muito comum que você verá nos
programas de rede neural do TensorFlow. Ele inicializa uma variável
para números aleatórios, mas esses números
não são distribuídos de modo uniforme. Eles têm uma distribuição normal gaussiana
com média zero e variantes de unidade. Mas a normal gaussiana
tem uma cauda muito longa, e você pode ter valores atípicos extremos. É muito improvável, mas pode acontecer. Então, o que um normal truncado faz? Ele meio que trunca tudo
na multiplicação de soma do sigma. Por fim, dizemos que
a variável w é treinável. Uma variável treinável
pode ser alterada durante o treinamento. O objetivo de uma variável,
é claro, é poder mudá-la, então a maioria das variáveis ​
será treinável. Mas, de vez em quando, veremos isso
quando falarmos sobre redução do tamanho do modelo e depois
sobre o aprendizado transferido. De vez em quando, pode ser útil congelar um gráfico para que
as variáveis ​​não sejam alteradas. Essa sinalização booleana
nos permite fazer isso. Observe que estou chamando
tf.get_variable para criar w. Agora, você pode ver o código
do TensorFlow que cria diretamente uma variável chamando
o construtor tf.variable. Chamar diretamente o construtor
não é recomendado. Use tf.get_variable porque,
como veremos no curso 9, pode ser útil poder reutilizar variáveis
​​ou criá-las novamente dependendo de situações diferentes, e
podemos fazer isso usando tf.get_variable. Então, recomendo que você tenha
o hábito de usar tf.get_variable. Então, executamos forward_pass cinco vezes e armazenamos o resultado da
multiplicação da matriz em cada iteração. Depois de fazermos o produto,
mudamos a ponderação. Aqui, estamos adicionando 0.1 a ele. É como uma atualização de gradiente. Na realidade, claro,
na atualização de gradiente, escolhemos quais ponderações
serão alteradas e como mudá-las. Mas aqui,
apenas para fins de demonstração, adicionarei 0.1 às ponderações a cada vez. Agora, na sessão, chamamos train_loop passando x. X é uma matriz 2x3. Então, em forward_pass, multiplicamos w por esse x.
W é uma matriz 1x2. Multiplicar 1x2 por 2x3
nos dá uma matriz 1x3. Neste momento, o gráfico está pronto, mas
ainda precisamos inicializar as variáveis. Mas esse é o estágio de execução. Normalmente, apenas inicializamos
todas as variáveis no gráfico de uma só vez executando
o inicializador de variáveis ​​globais. Então, quando olhamos para o valor
do produto após cada etapa do loop, notamos que a matriz 1x3 está diferente,
conforme o esperado. Então, vamos resumir
o que acabamos de aprender. Número 1, crie uma variável
chamando get_variable. Eu pulei uma linha de código
quando passei por ela, a parte do escopo. Quando você cria uma variável, pode especificar o escopo. É aí que estou dizendo ao TensorFlow
para reutilizar a variável a cada vez, em vez de
sempre criar uma variável nova. Estou chamando train_loop apenas uma vez,
então não importa aqui, mas se eu fosse chamar
train_loop novamente, as ponderações seriam retomadas
de onde pararam. Vamos criar uma variável nova.
Poderíamos reutilizá-la. Você também está aprendendo aqui
que, quando cria uma variável, tem que decidir como inicializá-la. No treinamento da rede neural, o normal aleatório com truncamento
é uma escolha comum. Número 3, use a variável como qualquer
outro tensor ao criar o gráfico. Número 4, em sua sessão, lembre-se de inicializar a variável. Normalmente, você inicializará
todas as variáveis ​​juntas chamando o
inicializador de variáveis ​​globais. E, depois que as variáveis
​​são inicializadas, e este é o ponto número 5, você pode avaliar o tensor que quiser. Então, neste exemplo, estamos chamando o train_loop com x, mas x é uma constante. Isso é realista? Você fixa no código valores de entrada
em seus programas? Marcadores permitem
que você insira valores no gráfico. Por exemplo, você pode ler valores
de um arquivo de texto em uma lista do Python e depois alimentar
essa lista no gráfico do TensorFlow. Então, aqui, a é um marcador. Marca a posição
de um escalar. B é a multiplicado por 4. Se você imprimir a, receberá
a saída de depuração de um tensor. Você aprenderá que esse tensor específico
é um marcador que espera que números de ponto flutuante
sejam alimentados nele. Se você quiser avaliar b, poderá ajustar este session.run(b). Você precisa fornecer valores para os
marcadores dos quais b depende. Portanto, neste caso,
você precisa passar uma lista ou uma matriz NumPy de números
para o marcador a, então faça isso
usando feed_dict, um dicionário. O dicionário é formado
por pares de chave-valor. A chave é um marcador, neste caso, a. O valor é uma lista de matriz NumPy. E neste caso, é 1,2,3. É isso que alimentamos, e quando b é avaliado, você recebe o valor
de a multiplicado por 4, então recebemos 4,8,12.