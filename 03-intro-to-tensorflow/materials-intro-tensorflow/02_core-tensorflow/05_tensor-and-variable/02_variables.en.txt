A variable is a tensor whose value is initialized and then the value gets changed as a program runs. Let's take a close look at this example. I have a function called forward pass, which takes two parameters, w and x, and multiplies them. Well, it's a matrix multiply because these are tensors, but it multiplies w and x. In my train loop function, I basically create the tensor w except that w is not a constant like the tensors that we've been looking at so far. W is a variable. It has a name, weights. Its shape is 1,2, which means that it has one row and two columns. It's a 1 by 2 matrix. And when w is initialized, we are not initializing it here because remember, TensorFlow is a lazy evaluation framework and so we are only building the graph. We're not yet running it. When w is initialized, it will be initialized by a truncated normal initializer. Now this is a very common initializer that you will see in TensorFlow neural network programs. It initializes a variable to random numbers, but these random numbers are not uniformly distributed. Instead, they have a Gaussian normal distribution with zero mean and unit variants. But Gaussian normal has a very long tail and you might get extreme outliers. It's very unlikely but it could happen. So, what a truncated normal does, well, it kind of truncates things at sum multiplication of sigma. Finally, we say that the variable w is trainable. A trainable variable is a variable that can be changed during training. The point of a variable of course is to be able to change it so most variables will be trainable. But every once in a while, we'll talk about this when we talk about model size reduction and then we talk about transferred learning. Every once in a while, it can be helpful to freeze a graph to make it such that the variables are in changed. This Boolean flag lets us do that. Notice that I'm calling tf.get_variable to create w. Now, you might see TensorFlow code that directly creates a variable by calling the tf.variable constructor. Calling the constructor directly is not recommended. Use tf.get_variable because, as we'll see in course 9, it can be helpful to be able to reuse variables or create them afresh depending on different situations and using tf.get_variable let's us do so. So, I recommend that you get into the habit of using tf.get_variable. So, we then run the forward pass five times and store the result of the matrix multiply at each iteration. So, after we do the product, we change the weight. Here we are adding 0.1 to it. This is like a gradient update. In reality, of course, in gradient update, we will choose what weights to change and how to change them. But here, for just demo purposes, I'll just add 0.1 to the weights each time. Now, from the session, we call train loop by passing in x. The x is a 2 by 3 matrix. So in the forward pass, we multiply w by this x. W is a 1 by 2 matrix. Multiplying a 1 by 2 by 2 by 3 gives us a 1 by 3 matrix. So, at this point, the graph is done but we still need initialize the variables. But that's the run stage. We typically just initialized all the variables in the graph all at once by running the global variables initializer. So, when we now look at the value of the product after each step of the loop, we notice that the 1 by 3 matrix each time is different as you would expect. So, let's summarize what we have just learned. Number 1, create a variable by calling "get variable." Well, I skipped over one line of code when I went through it, the scope piece. When you create a variable, you can specify the scope. That's where I'm telling TensorFlow to reuse the variable each time instead of creating a new variable each time. I'm calling train loop only once so it doesn't matter here, but if I were to call train loop again, the weights would resume from where they left off. We will create a new variable. We would reuse it. So, second thing that you're learning here is that when you create a variable, you have to decide on how to initialize a variable. In neural network training, random normal with truncation is a typical choice. Number 3, use the variable just like any other tensor when building the graph. Number 4, in your session, remember to initialize the variable. Usually, you will initialize all the variables together by calling the global variables initializer. And after the variables are initialized, and this is point number 5, you can evaluate any tensor that you want to evaluate. So, in this example, we are calling the train loop with the x, but the x is a constant. How realistic is that? Do you hardcode input values into your programs? Placeholders allow you to feed in values into the graph. For example, you can read values from a text file into a Python list and then feed that list into the TensorFlow graph. So, here, a is a placeholder. It will hold a scalar. B is a multiplied by 4. If you print a, you will get the debug output of a tensor. You will learn that this particular tensor is a placeholder that expects floating point numbers to be fed into it. If you now want to evaluate b, you can adjust this session.run(b). You have to feed in values for the placeholders that b depends upon. So in this case, you have to pass in a list or a numpy array of numbers for the placeholder a, and you do this using a feed dict, a dictionary. Key value pairs is what a dictionary is. The key is a placeholder, in this case, a. The value is a list of numpy array. And in this case, it's 1,2,3. So that's what we feed in, and so when b is evaluated, you get the value of a multiply by 4, so we get 4,8,12.