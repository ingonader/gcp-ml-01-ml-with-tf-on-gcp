En las lecciones anteriores,
hablamos de qué es TensorFlow. Ahora veamos
la jerarquía de API de TensorFlow. Como muchas bibliotecas de software TensorFlow contiene
diversas capas de abstracción. La capa de abstracción más baja se implementa para orientarse
a diferentes plataformas de hardware. A menos que su empresa fabrique hardware es poco probable
que haga algo en esta capa. La siguiente capa
es la API de TensorFlow de C++. Así se escribe
una aplicación personalizada de TensorFlow. Implementa la función que desea en C++ y la registra
como una operación de TensorFlow. Consulte la documentación de TensorFlow
sobre cómo extender una aplicación. Luego, TensorFlow le da
un wrapper de Python que puede usar tal como
lo haría con una función existente. Sin embargo, en esta especialización supondremos que usted
no es un investigador de AA así que no tendrá que hacer esto. Pero si alguna vez necesita implementar
su propia aplicación personalizada lo haría con C++. No es difícil. TensorFlow es extensible de esta manera. La capa siguiente es
la API principal de Python. Contiene gran parte
del código de procesamiento numérico suma, resta, división,
multiplicación de matrices, etcétera. Crear variables y tensores obtener la forma
o la dimensión de un tensor todas las cuestiones básicas
de procesamiento numérico todo eso está en la API de Python. Luego, hay un conjunto
de módulos de Python con una representación de alto nivel de componentes útiles de redes neuronales. Por ejemplo, la manera de crear
una nueva capa de neuronas ocultas con una función de activación ReLU está en tf.layers. La forma de calcular
el error cuadrático medio de los datos a medida que llegan es tf.metrics. La manera de procesar
entropía cruzada con logits… Esto es común en problemas de clasificación
de medición de pérdida. La entropía cruzada
con logits: está en tf.losses. Estos módulos
proporcionan componentes útiles para compilar
modelos de NN personalizados. ¿Por qué hago hincapié
en que son modelos personalizados? Porque muy a menudo no se requiere
un modelo de red neuronal personalizado. Muchas veces es suficiente usar una manera relativamente estándar de entrenar, evaluar y entregar modelos. No hace falta
personalizar la manera de entrenar. Puede usar una familia de optimizadores
de descenso de gradientes y hacer una propagación inversa de los pesos
en un proceso iterativo. En ese caso no escriba un bucle de sesión de bajo nivel. Simplemente use un estimador. Estimator es la API
de alto nivel en TensorFlow. Sabe cómo realizar 
el entrenamiento distribuido cómo evaluar, cómo crear un control cómo guardar un modelo y cómo configurarlo para su entrega. Viene con todas las funciones
preparadas de manera adecuada que se adapta a la mayoría
de los modelos de AA en producción. Si encuentra un ejemplo
de código de TensorFlow en Internet que no usa la API de Estimator simplemente ignórelo. Olvídese de eso. No vale la pena. Tendría que escribir mucho código
para la asignación de dispositivos la administración de memoria y distribución. Deje que Estimator lo haga por usted. Esas son las capas
de abstracción de TensorFlow. Cloud ML Engine
es ortogonal a esta jerarquía. Sin importar la capa de abstracción
en la que escriba su código de TensorFlow CMLE le ofrece un servicio administrado. Es TensorFlow alojado. Así, puede ejecutar TensorFlow en la nube,
en un clúster de máquinas sin tener que instalar software
ni administrar servidores.