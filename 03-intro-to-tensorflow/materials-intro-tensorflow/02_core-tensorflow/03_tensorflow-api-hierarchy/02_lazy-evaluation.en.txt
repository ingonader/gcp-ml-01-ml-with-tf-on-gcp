So, let's look at the code on the slide. At first glance, this looks just like say NumPy, you want to add two tensors a and b. So, you write tf.add(a, b). It returns a tensor c. Unlike typical Python code though, running the tf,add doesn't execute it, it only builds the DAG. In the DAG in the directed acyclic graph, a, b, and c are tensors and add is an operation. In order to run this code, in order to execute the DAG, you need to run it and you run it as part of what is called a session. So, you say that you want a value of c and you ask the session, "Hey session, please evaluate c for me." So, that's what runs the DAG, and then you get back a traditional numeric array in Python that contains the values for c. Programming TensorFlow involves programming a DAG. So, there are two steps. First step, create the graph. Second step, run the graph. The graph definition is separate from the training loop because this is a lazy evaluation model. It minimizes the Python to C++ context switches and enable the computation to be very efficient. Conceptually, this is like writing a program, compiling it, and then running it on some data. But don't take that analogy too far. There is no explicit compile phase here. Note that c after you call tf.add, is not the actual values. You have to evaluate c in the context of a TensorFlow session to get a NumPy array of values, numpy_c. So, to reiterate, TensorFlow does lazy evaluation. You write a DAG and then you run the DAG in the context of a session to get results. Now, there is a different mode in which you can run TensorFlow. It's called tf.eager and in tf.eager, the evaluation is immediate and it's not lazy. But eager mode is typically not used in production programs. It's typically used only for development. We'll look at tf.eager a little bit later in this course, but for the most part, we'll focus on the lazy evaluation paradigm. And almost all the code that we write and we run in production will be in lazy evaluation mode. In NumPy, which is what the lion share of Python Numeric Software is written in, a and b are NumPy arrays. NumPy gets it's speed by being implemented in c, so when you call np.add, that add gets done in c. But it does get done when the CPU runs the code np.add (a, b) and the NumPy array c gets populated with the sums. So, when you print c, you get the 8, 2 and 10. 8 is the sum of 5 and 3, 3 and -1 you add to get 2, et cetera. The point is np.add is evaluated immediately. Unlike with NumPy, in TensorFlow c is not the actual values. Instead, c is a tensor and you have to evaluate c in the context of a TensorFlow session to get a NumPy array of values, the result. So, when the CPU or GPU or whatever hardware evaluates tf.add (a, b) a tensor gets created in the directed acyclic graph in the DAG. But the addition itself it's not carried out until session.run gets called. So, if we call print c, what gets printed out in the first box is the debug output of the tensor class. It includes a system assigned unique name for the node in the DAG, in this case, add_7 and the shape and the data type of the value that will show up when the DAG is run. After the session is run and c is evaluated in the context of a session, we can print the result and we will get 8, 2, and 10, just like we got with NumPy. So, there are two stages, a build stage and a run stage, but why? Why does TensorFlow do lazy evaluation? That is a next lesson.