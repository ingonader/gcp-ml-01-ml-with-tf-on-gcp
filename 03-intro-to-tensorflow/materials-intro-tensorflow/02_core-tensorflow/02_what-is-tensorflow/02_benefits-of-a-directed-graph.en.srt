1
00:00:00,250 --> 00:00:04,810
So why does TensorFlow use directed
acyclic graphs to represent computation?

2
00:00:06,010 --> 00:00:09,220
Portability, the directed acyclic graph,

3
00:00:09,220 --> 00:00:15,070
the DAG is a language-independent
representation of the code in your model.

4
00:00:15,070 --> 00:00:19,650
You can build a DAG in Python,
stored in a saved model, and

5
00:00:19,650 --> 00:00:24,500
restored in a C++ program for
low latency predictions.

6
00:00:24,500 --> 00:00:29,530
You can use the same Python code and
execute it on both CPUs and

7
00:00:29,530 --> 00:00:35,520
GPUs, so it gives you language and
hardware portability.

8
00:00:35,520 --> 00:00:40,000
In a lot of ways, this is similar to
how the Java Virtual Machine, the JVM,

9
00:00:40,000 --> 00:00:45,560
and its byte code representation,
help support ability of Java code.

10
00:00:45,560 --> 00:00:49,830
As developers, we get to write
code in a high level language,

11
00:00:49,830 --> 00:00:54,680
Java, and have it be executed in
different platforms by the JVM.

12
00:00:54,680 --> 00:01:00,940
The JVM itself is very efficient and
targeted towards the exact OS and

13
00:01:00,940 --> 00:01:04,780
the hardware, and
it's written in C or C++.

14
00:01:04,780 --> 00:01:07,000
Very similar thing with TensorFlow.

15
00:01:07,000 --> 00:01:12,480
As developers, we get to write code
in a high level language, Python, and

16
00:01:12,480 --> 00:01:17,893
have it be executed in different platforms
by the TensorFlow execution engine.

17
00:01:17,893 --> 00:01:22,585
The TensorFlow execution
engine is very efficient, and

18
00:01:22,585 --> 00:01:25,360
it's targeted towards
the exact hardware chip and

19
00:01:25,360 --> 00:01:29,720
its capabilities, and it's written in C++.

20
00:01:29,720 --> 00:01:35,660
Portability between devices enables
a lot of power and flexibility.

21
00:01:35,660 --> 00:01:38,600
For example, this is a common pattern.

22
00:01:38,600 --> 00:01:43,170
You can train a TensorFlow model on
the cloud, on lots and lots of powerful

23
00:01:43,170 --> 00:01:49,080
hardware, and then take that trained model
and put it on a device out at the edge.

24
00:01:49,080 --> 00:01:52,590
Perhaps a mobile phone or
even an embedded chip.

25
00:01:52,590 --> 00:01:56,830
And you can do predictions with
the model right on that device itself.

26
00:01:58,100 --> 00:02:01,620
Remember the Google Translate app that
we were talking about in the first

27
00:02:01,620 --> 00:02:03,750
course in this specialization?

28
00:02:03,750 --> 00:02:08,730
That app can work completely
offline because a trained

29
00:02:08,730 --> 00:02:14,800
translation model is stored on the phone
and is available for offline translation.

30
00:02:14,800 --> 00:02:18,736
It tends to be a smaller, less powerful
model than what's on the cloud,

31
00:02:18,736 --> 00:02:22,746
due to limitations of the processing
power that's available on a phone.

32
00:02:22,746 --> 00:02:27,824
But the fact that TensorFlow can do that,
very cool and possible

33
00:02:27,824 --> 00:02:35,290
only because of the portability provided
by the directed acyclic representation.

34
00:02:35,290 --> 00:02:36,880
These sorts of smaller,

35
00:02:36,880 --> 00:02:41,710
less powerful models are typically
implemented using TensorFlow Lite.

36
00:02:41,710 --> 00:02:44,150
I talked about the training
on the cloud and

37
00:02:44,150 --> 00:02:48,200
then doing prediction on a low
powered device such as a phone.

38
00:02:48,200 --> 00:02:51,930
Sure, but can you train
the model itself on the phone?

39
00:02:53,490 --> 00:02:59,430
Not today, because ML model training
tends to be an expensive operation.

40
00:02:59,430 --> 00:03:03,140
But increasingly, we're doing
something that's halfway there.

41
00:03:03,140 --> 00:03:07,180
Now this is very much the kind of
thing that only the most advanced

42
00:03:07,180 --> 00:03:11,110
ML players are doing,
not necessarily widespread.

43
00:03:11,110 --> 00:03:13,450
But what do I mean by halfway?

44
00:03:13,450 --> 00:03:19,520
One situation is that you train a model,
and then you deploy to a bunch of phones.

45
00:03:19,520 --> 00:03:21,060
And then when you make a prediction,

46
00:03:21,060 --> 00:03:26,340
the user says nope, this isn't right, or
please show me more results like this.

47
00:03:26,340 --> 00:03:27,720
And at that point,

48
00:03:27,720 --> 00:03:34,240
you want to update the weights of the
model to reflect that user's preferences.

49
00:03:34,240 --> 00:03:39,720
This sort of fine-tuning of a trained
model is definitely possible on a phone.

50
00:03:39,720 --> 00:03:45,740
The user's phone personalizes the model
locally based on their usage,

51
00:03:45,740 --> 00:03:47,530
so that's what is shown in A.

52
00:03:49,290 --> 00:03:54,150
However, here you are,
fine-tuning the model for each user.

53
00:03:54,150 --> 00:03:58,220
You may not want to send that user's
preferences back to your system,

54
00:03:58,220 --> 00:04:02,310
back to the cloud, because that
might be personally sensitive.

55
00:04:02,310 --> 00:04:05,650
But you can set up what is
called federated learning,

56
00:04:05,650 --> 00:04:10,440
where you aggregate many users' updates,
as shown in B.

57
00:04:11,520 --> 00:04:16,360
This aggregate is essentially like
a weight update on a batch of samples,

58
00:04:16,360 --> 00:04:18,380
except that it comes from different users.

59
00:04:18,380 --> 00:04:22,760
So it forms a consensus change, and
that's what we are showing in C, and

60
00:04:22,760 --> 00:04:27,140
this consensus change happens to
the shared model on the cloud.

61
00:04:27,140 --> 00:04:29,370
So you deploy the shared model,

62
00:04:29,370 --> 00:04:34,120
you fine tune it on different
users' devices, rinse and repeat.

63
00:04:34,120 --> 00:04:37,580
The TensorFlow is this portable, powerful,

64
00:04:37,580 --> 00:04:41,520
production ready software
to do numerical computing.

65
00:04:41,520 --> 00:04:43,880
It is particularly popular though for

66
00:04:43,880 --> 00:04:49,380
machine learning, the number one
repository for machine learning on GitHub.

67
00:04:49,380 --> 00:04:50,480
Why is it so popular?

68
00:04:51,560 --> 00:04:55,590
It's popular among deep learning
researchers because of the community

69
00:04:55,590 --> 00:05:00,620
around it and the ability to extend it and
do new cool things.

70
00:05:00,620 --> 00:05:03,360
It's popular among machine
learning engineers,

71
00:05:03,360 --> 00:05:07,700
because of the ability to productionize
models and do things at scale.

72
00:05:07,700 --> 00:05:12,350
The popularity among both these
groups feeds among each other.

73
00:05:12,350 --> 00:05:15,851
Researchers want to see their
methods being used widely, and

74
00:05:15,851 --> 00:05:19,093
implementing it in TensorFlow
is a way of ensuring that.

75
00:05:19,093 --> 00:05:23,770
ML engineers want to future-proof their
code so that they can use newer models as

76
00:05:23,770 --> 00:05:28,330
soon as they're invented, and
TensorFlow helps them do that.

77
00:05:28,330 --> 00:05:32,590
At Google, we open source TensorFlow,
because it can enable so

78
00:05:32,590 --> 00:05:33,950
many other companies, and

79
00:05:33,950 --> 00:05:37,910
because we saw the potential of this
kind of massive community support.

80
00:05:39,210 --> 00:05:44,280
The fact that TensorFlow is open
source gives you a key benefit.

81
00:05:44,280 --> 00:05:49,110
You are not logged in when you use
Cloud Machine Learning Engine on GCP,

82
00:05:49,110 --> 00:05:54,610
because the code that you write is in
TensorFlow, and TensorFlow is open source.