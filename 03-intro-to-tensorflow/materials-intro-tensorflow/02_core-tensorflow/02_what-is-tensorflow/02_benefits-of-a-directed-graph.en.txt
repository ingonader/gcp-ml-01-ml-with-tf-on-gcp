So why does TensorFlow use directed
acyclic graphs to represent computation? Portability, the directed acyclic graph, the DAG is a language-independent
representation of the code in your model. You can build a DAG in Python,
stored in a saved model, and restored in a C++ program for
low latency predictions. You can use the same Python code and
execute it on both CPUs and GPUs, so it gives you language and
hardware portability. In a lot of ways, this is similar to
how the Java Virtual Machine, the JVM, and its byte code representation,
help support ability of Java code. As developers, we get to write
code in a high level language, Java, and have it be executed in
different platforms by the JVM. The JVM itself is very efficient and
targeted towards the exact OS and the hardware, and
it's written in C or C++. Very similar thing with TensorFlow. As developers, we get to write code
in a high level language, Python, and have it be executed in different platforms
by the TensorFlow execution engine. The TensorFlow execution
engine is very efficient, and it's targeted towards
the exact hardware chip and its capabilities, and it's written in C++. Portability between devices enables
a lot of power and flexibility. For example, this is a common pattern. You can train a TensorFlow model on
the cloud, on lots and lots of powerful hardware, and then take that trained model
and put it on a device out at the edge. Perhaps a mobile phone or
even an embedded chip. And you can do predictions with
the model right on that device itself. Remember the Google Translate app that
we were talking about in the first course in this specialization? That app can work completely
offline because a trained translation model is stored on the phone
and is available for offline translation. It tends to be a smaller, less powerful
model than what's on the cloud, due to limitations of the processing
power that's available on a phone. But the fact that TensorFlow can do that,
very cool and possible only because of the portability provided
by the directed acyclic representation. These sorts of smaller, less powerful models are typically
implemented using TensorFlow Lite. I talked about the training
on the cloud and then doing prediction on a low
powered device such as a phone. Sure, but can you train
the model itself on the phone? Not today, because ML model training
tends to be an expensive operation. But increasingly, we're doing
something that's halfway there. Now this is very much the kind of
thing that only the most advanced ML players are doing,
not necessarily widespread. But what do I mean by halfway? One situation is that you train a model,
and then you deploy to a bunch of phones. And then when you make a prediction, the user says nope, this isn't right, or
please show me more results like this. And at that point, you want to update the weights of the
model to reflect that user's preferences. This sort of fine-tuning of a trained
model is definitely possible on a phone. The user's phone personalizes the model
locally based on their usage, so that's what is shown in A. However, here you are,
fine-tuning the model for each user. You may not want to send that user's
preferences back to your system, back to the cloud, because that
might be personally sensitive. But you can set up what is
called federated learning, where you aggregate many users' updates,
as shown in B. This aggregate is essentially like
a weight update on a batch of samples, except that it comes from different users. So it forms a consensus change, and
that's what we are showing in C, and this consensus change happens to
the shared model on the cloud. So you deploy the shared model, you fine tune it on different
users' devices, rinse and repeat. The TensorFlow is this portable, powerful, production ready software
to do numerical computing. It is particularly popular though for machine learning, the number one
repository for machine learning on GitHub. Why is it so popular? It's popular among deep learning
researchers because of the community around it and the ability to extend it and
do new cool things. It's popular among machine
learning engineers, because of the ability to productionize
models and do things at scale. The popularity among both these
groups feeds among each other. Researchers want to see their
methods being used widely, and implementing it in TensorFlow
is a way of ensuring that. ML engineers want to future-proof their
code so that they can use newer models as soon as they're invented, and
TensorFlow helps them do that. At Google, we open source TensorFlow,
because it can enable so many other companies, and because we saw the potential of this
kind of massive community support. The fact that TensorFlow is open
source gives you a key benefit. You are not logged in when you use
Cloud Machine Learning Engine on GCP, because the code that you write is in
TensorFlow, and TensorFlow is open source.