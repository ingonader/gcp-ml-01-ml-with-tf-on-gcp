¿Por qué TensorFlow usa
grafos acíclicos dirigidos para representar los cálculos? Por su portabilidad. El grafo acíclico dirigido o DAG es una representación del código
de su modelo independiente del lenguaje. Puede crear un DAG en Python almacenarlo en un modelo guardado y restaurarlo en un programa de C++
para obtener predicciones de baja latencia. Puede usar el mismo código de Python
y ejecutarlo tanto en CPU como en GPU. Ofrece portabilidad
en términos del hardware y del lenguaje. En muchos sentidos esto es similar
a cómo la máquina virtual Java, JVM y su representación de bytecode,
ayudan a la portabilidad del código de Java. Como desarrolladores,
nos permite escribir código en un lenguaje de alto nivel, Java y hacer que se ejecute
en diferentes plataformas con la JVM. La JVM en sí es muy eficiente. Está orientada a un SO
y un hardware en particular y está escrita en C o C++. Ocurre algo muy similar con TensorFlow. Como desarrolladores,
podemos escribimos código en un lenguaje
de alto nivel, en este caso, Python y hacer que se ejecute
en distintas plataformas con el motor de ejecución de TensorFlow. El motor de ejecución
de TensorFlow es muy eficiente. Está orientado hacia un chip
de hardware específico y sus capacidades y está escrito en C++. La portabilidad entre dispositivos
posibilita mucha potencia y flexibilidad. Por ejemplo, este es un patrón común. Puede entrenar un modelo de TensorFlow
en la nube en mucho hardware muy potente y trasladar el modelo entrenado
a un dispositivo afuera del perímetro como un teléfono celular
o incluso un chip integrado. Y puede hacer predicciones
con el modelo desde el mismo dispositivo. ¿Recuerda la aplicación
de Google Traductor de la que hablamos en el primer
curso de esta especialización? Esa aplicación puede funcionar
completamente sin conexión porque se almacena un modelo
de traducción entrenado en el teléfono y está disponible
para hacer traducciones sin conexión. Suele ser un modelo más pequeño
y menos potente que el de la nube debido a limitaciones
de la capacidad de procesamiento disponible en el teléfono. Pero el hecho de que TensorFlow
pueda hacerlo es una maravilla. Y eso es posible
solo gracias a la portabilidad que nos brinda
la representación acíclica dirigida. Estos modelos
más pequeños y menos potentes suelen implementarse con TensorFlow Lite. Hablé de entrenar en la nube y hacer predicciones en un dispositivo
de poca potencia, como un teléfono. Claro, pero ¿se puede entrenar
el modelo en sí en el teléfono? No en la actualidad porque entrenar
modelos de AA suele ser costoso. Pero cada vez hacemos más
algo que está a medio camino. Es algo que solo hacen
los actores más avanzados en el campo del AA. No está necesariamente generalizado. Pero ¿qué significa a medio camino? Un caso es que entrena un modelo
y lo implementa en muchos teléfonos. Cuando hace una predicción,
el usuario puede decir que no es buena o pedir más resultados como ese. En ese momento se pueden
actualizar los pesos del modelo para que reflejen
las preferencias de ese usuario. Estos ajustes de un modelo entrenado
pueden hacerse, sin dudas, en un teléfono. El teléfono del usuario personaliza
el modelo a nivel local según su uso. Eso es lo que se ve aquí en A. Sin embargo, aquí se está ajustando
el modelo para cada usuario. No es conveniente enviar
las preferencias de ese usuario de vuelta a su sistema en la nube ya que podría ser información personal. Pero puede configurar
lo que se llama aprendizaje federado en el que se agregan
las actualizaciones de muchos usuarios. Es lo que vemos aquí en B. Esta agregación es esencialmente una actualización
del peso de un lote de muestras solo que proviene de varios usuarios. Esto genera un cambio de consenso. Es lo que vemos en C. Este cambio ocurre
en el modelo compartido en la nube. Así, implementa el modelo compartido lo ajusta
en los dispositivos de distintos usuarios y se repite el proceso. TensorFlow es un software portable,
potente y listo para la producción para cálculos numéricos. Es particularmente popular
en el aprendizaje automático. Es el principal repositorio
de aprendizaje automático en GitHub. ¿Por qué es tan popular? Entre los investigadores
de aprendizaje profundo es popular por la comunidad conformada y su capacidad
para extenderlo y encontrar usos nuevos. Entre los ingenieros
de aprendizaje automático es popular por su capacidad para llevar
modelos a producción y trabajar a escala. La popularidad entre estos dos grupos
se complementa entre sí. Los investigadores quieren
que sus métodos se usen ampliamente. Implementarlos en TensorFlow
es una forma de lograrlo. Los ingenieros de AA quieren
que su código esté listo para el futuro de manera que puedan adoptar
modelos nuevos apenas se inventen. TensorFlow los ayuda a hacerlo. En Google, hicimos
TensorFlow de código abierto porque puede ser
una gran ayuda para muchas empresas y porque vimos el potencial
de enorme apoyo comunitario. El hecho de que TensorFlow sea
de código abierto le da un beneficio clave. No está obligado a usar siempre
Cloud Machine Learning Engine en GCP porque el código que escriba
estará en TensorFlow y TensorFlow es de código abierto.