1
00:00:00,250 --> 00:00:04,080
Warum nutzt TensorFlow
gerichtete azyklische Graphen

2
00:00:04,080 --> 00:00:05,910
zum Darstellen von Berechnungen?

3
00:00:06,010 --> 00:00:09,220
Wegen der Portabilität.
Der gerichtete azyklische Graph,

4
00:00:09,220 --> 00:00:15,070
der DAG, ist eine sprachunabhängige
Darstellung Ihres Codes im Modell.

5
00:00:15,070 --> 00:00:19,650
Sie können einen DAG in Python
erstellen, in einem Modell speichern

6
00:00:19,650 --> 00:00:24,500
und in einem C++-Programm für Vorhersagen
mit niedriger Latenz wiederherstellen.

7
00:00:24,500 --> 00:00:29,530
Sie können denselben Python-Code verwenden
und auf CPUs sowie GPUs ausführen.

8
00:00:29,530 --> 00:00:35,520
Sie profitieren also
von Sprach- und Hardwareportabilität.

9
00:00:35,520 --> 00:00:40,000
Das ist in vielerlei Hinsicht vergleichbar
mit der Java Virtual Machine, der JVM, und

10
00:00:40,000 --> 00:00:45,560
ihrer Byte-Code-Darstellung, die den
Möglichkeiten mit Java-Code zugutekommen.

11
00:00:45,560 --> 00:00:49,830
Als Entwickler schreiben wir Code
in einer höheren Programmiersprache,

12
00:00:49,830 --> 00:00:54,680
Java, und lassen ihn dann von der JVM
auf verschiedenen Plattformen ausführen.

13
00:00:54,680 --> 00:01:00,940
Die JVM selbst ist sehr effizient
und auf ein präzises Betriebssystem und

14
00:01:00,940 --> 00:01:04,780
die Hardware ausgerichtet.
Sie ist in C oder C++ geschrieben.

15
00:01:04,780 --> 00:01:07,000
Bei TensorFlow ist die Situation ähnlich.

16
00:01:07,000 --> 00:01:12,480
Als Entwickler schreiben wir den Code
in einer höheren Sprache, Python, und

17
00:01:12,480 --> 00:01:15,643
lassen ihn vom
TensorFlow-Ausführungsmodul

18
00:01:15,643 --> 00:01:17,893
auf unterschiedlichen
Plattformen ausführen.

19
00:01:17,893 --> 00:01:22,585
Das TensorFlow-Ausführungsmodul
ist sehr effizient und

20
00:01:22,585 --> 00:01:26,400
auf den präzisen Hardwarechip
und dessen Fähigkeiten ausgerichtet.

21
00:01:26,400 --> 00:01:29,720
Geschrieben ist es in C++.

22
00:01:29,720 --> 00:01:35,660
Portabilität zwischen Geräten ermöglicht
starke Leistung und Flexibilität.

23
00:01:35,660 --> 00:01:38,600
Das kommt beispielsweise häufig vor.

24
00:01:38,600 --> 00:01:41,340
Sie können ein TensorFlow-Modell
in der Cloud auf Unmengen

25
00:01:41,340 --> 00:01:43,390
leistungsstarker Hardware trainieren

26
00:01:43,390 --> 00:01:49,080
und das trainierte Modell dann
auf ein Gerät am Rand verlagern,

27
00:01:49,080 --> 00:01:52,590
z. B. ein Mobiltelefon
oder sogar ein integrierter Chip.

28
00:01:52,590 --> 00:01:57,010
Und Sie können direkt auf diesem Gerät
Vorhersagen mit dem Modell machen.

29
00:01:58,100 --> 00:02:01,060
Erinnern Sie sich an
die Google Translate-App, über die wir

30
00:02:01,060 --> 00:02:03,750
im ersten Kurs
dieser Spezialisierung gesprochen haben?

31
00:02:03,750 --> 00:02:08,729
Die App funktioniert komplett offline,
weil ein trainiertes Übersetzungsmodell

32
00:02:08,729 --> 00:02:14,800
auf dem Telefon gespeichert und
für Offline-Übersetzungen verfügbar ist.

33
00:02:14,800 --> 00:02:18,736
Das Modell ist wegen der begrenzten
Verarbeitungsleistung des Telefons

34
00:02:18,736 --> 00:02:22,746
tendenziell kleiner und weniger
leistungsfähig als das in der Cloud.

35
00:02:22,746 --> 00:02:27,824
Doch dass TensorFlow dazu imstande ist,
ist wirklich großartig.

36
00:02:27,824 --> 00:02:35,290
Und möglich ist das nur wegen
der vom DAG ermöglichten Portabilität.

37
00:02:35,290 --> 00:02:37,120
Diese Arten von kleineren, weniger

38
00:02:37,120 --> 00:02:41,710
leistungsfähigen Modellen werden in der
Regel mit TensorFlow Lite implementiert.

39
00:02:41,710 --> 00:02:43,930
Das Trainieren in der Cloud 
mit anschließenden Vorhersagen

40
00:02:43,930 --> 00:02:46,960
auf einem relativ leistungsschwachen 
Gerät wie einem Telefon

41
00:02:46,960 --> 00:02:48,200
habe ich bereits erwähnt.

42
00:02:48,200 --> 00:02:51,930
Aber kann man auch das Modell
selbst auf dem Telefon trainieren?

43
00:02:53,490 --> 00:02:59,430
Aktuell nicht, weil das Trainieren
von ML-Modellen eine eher teure Sache ist.

44
00:02:59,430 --> 00:03:03,140
Doch wir tun zunehmend etwas,
was dem halbwegs nahekommt.

45
00:03:03,140 --> 00:03:07,180
Das ist etwas, was eigentlich
nur die kompetentesten Akteure

46
00:03:07,180 --> 00:03:11,110
im ML-Bereich tun, und es ist
auch nicht unbedingt weit verbreitet.

47
00:03:11,110 --> 00:03:13,450
Was soll "halbwegs"
hier eigentlich heißen?

48
00:03:13,450 --> 00:03:17,820
Eine Situation ist das Trainieren
eines ML-Modells, das dann auf mehreren

49
00:03:17,820 --> 00:03:19,520
Telefonen bereitgestellt wird.

50
00:03:19,520 --> 00:03:22,730
Bei einer Vorhersage kann
der Nutzer dann z. B. sagen,

51
00:03:22,730 --> 00:03:26,340
dass das nicht stimmt oder er mehr
Ergebnisse wie dieses sehen möchte.

52
00:03:26,340 --> 00:03:31,090
An diesem Punkt werden dann
die Gewichtungen des Modells aktualisiert,

53
00:03:31,090 --> 00:03:34,240
um den Präferenzen
dieses Nutzers Rechnung zu tragen.

54
00:03:34,240 --> 00:03:39,720
Eine solche Anpassung eines trainierten
Modells ist auf einem Telefon möglich.

55
00:03:39,720 --> 00:03:45,740
Das Telefon des Nutzers personalisiert
das Modell entsprechend der Nutzung lokal.

56
00:03:45,740 --> 00:03:48,450
Das wird in A gezeigt.

57
00:03:49,290 --> 00:03:54,150
So passen Sie also das Modell
für jeden Nutzer an, aber die Präferenzen

58
00:03:54,150 --> 00:03:58,220
der jeweiligen Nutzer möchten Sie
nicht zurück an Ihr System senden

59
00:03:58,220 --> 00:04:02,310
oder zurück an die Cloud, da diese
persönlich und vertraulich sein können.

60
00:04:02,310 --> 00:04:05,650
Man kann aber sogenanntes
föderiertes Lernen einrichten.

61
00:04:05,650 --> 00:04:10,650
Dabei werden die Aktualisierungen vieler
Nutzer aggregiert, wie in B gezeigt.

62
00:04:11,520 --> 00:04:15,470
Diese Aggregation kommt im Grunde
einer Gewichtungsänderung bei einem Satz

63
00:04:15,470 --> 00:04:18,380
von Proben gleich,
nur dass diese von mehreren Nutzern kommt.

64
00:04:18,380 --> 00:04:22,760
Das bildet also
eine Konsensänderung, wie in C gezeigt.

65
00:04:22,760 --> 00:04:27,140
Diese Änderung wird am gemeinsamen
Modell in der Cloud vorgenommen.

66
00:04:27,140 --> 00:04:29,370
Das gemeinsame
Modell wird also bereitgestellt,

67
00:04:29,370 --> 00:04:34,120
auf den Geräten verschiedener Nutzer
angepasst und dann wird alles wiederholt.

68
00:04:34,120 --> 00:04:37,580
TensorFlow ist
eine portable, leistungsstarke

69
00:04:37,580 --> 00:04:41,520
und für den Produktionseinsatz geeignete
Software für numerische Berechnungen.

70
00:04:41,520 --> 00:04:44,430
Besonders beliebt ist sie
im Bereich maschinelles Lernen.

71
00:04:44,430 --> 00:04:48,990
Sie ist das führende Repository
für maschinelles Lernen auf GitHub.

72
00:04:48,990 --> 00:04:50,750
Warum ist TensorFlow so beliebt?

73
00:04:51,560 --> 00:04:55,590
Bei Deep-Learning-Forschern
liegt das an der Community und

74
00:04:55,590 --> 00:05:00,620
der Möglichkeit, Erweiterungen
und neue, tolle Sachen vorzunehmen.

75
00:05:00,620 --> 00:05:03,360
Bei ML-Ingenieuren
liegt es an der Möglichkeit

76
00:05:03,360 --> 00:05:07,700
zur Nutzung von Modellen in
der Produktion und an der Skalierbarkeit.

77
00:05:07,700 --> 00:05:12,350
Die Beliebtheit bei diesen Gruppen
steht in einer Wechselbeziehung.

78
00:05:12,350 --> 00:05:15,731
Forscher möchten, dass ihre Methoden
weitreichend verwendet werden und

79
00:05:15,731 --> 00:05:19,093
durch die Implementierung in
TensorFlow kann das gewährleistet werden.

80
00:05:19,093 --> 00:05:23,770
ML-Ingenieure möchten ihren Code
zukunftssicher machen, sodass neue Modelle

81
00:05:23,770 --> 00:05:28,330
direkt bei Verfügbarkeit genutzt werden
können, und TensorFlow hilft ihnen dabei.

82
00:05:28,330 --> 00:05:31,340
Google hat TensorFlow als
Open-Source-Software konzipiert,

83
00:05:31,340 --> 00:05:33,950
weil sie so vielen
anderen Unternehmen helfen kann,

84
00:05:33,950 --> 00:05:37,910
und weil wir das Potenzial eines massiven
Community-Supports gesehen haben.

85
00:05:39,210 --> 00:05:44,280
Dass TensorFlow eine Open-Source-Software
ist, ist ein großer Vorteil.

86
00:05:44,280 --> 00:05:48,110
Sie sind bei der Nutzung von
Cloud Machine Learning Engine auf der GCP

87
00:05:48,110 --> 00:05:51,300
völlig ungebunden,
da sich der Code, den Sie schreiben,

88
00:05:51,300 --> 00:05:54,917
in TensorFlow befindet, 
und TensorFlow ist Open Source.