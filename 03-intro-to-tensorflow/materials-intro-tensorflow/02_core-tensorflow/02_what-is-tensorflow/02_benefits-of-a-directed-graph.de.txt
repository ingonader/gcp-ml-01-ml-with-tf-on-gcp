Warum nutzt TensorFlow
gerichtete azyklische Graphen zum Darstellen von Berechnungen? Wegen der Portabilität.
Der gerichtete azyklische Graph, der DAG, ist eine sprachunabhängige
Darstellung Ihres Codes im Modell. Sie können einen DAG in Python
erstellen, in einem Modell speichern und in einem C++-Programm für Vorhersagen
mit niedriger Latenz wiederherstellen. Sie können denselben Python-Code verwenden
und auf CPUs sowie GPUs ausführen. Sie profitieren also
von Sprach- und Hardwareportabilität. Das ist in vielerlei Hinsicht vergleichbar
mit der Java Virtual Machine, der JVM, und ihrer Byte-Code-Darstellung, die den
Möglichkeiten mit Java-Code zugutekommen. Als Entwickler schreiben wir Code
in einer höheren Programmiersprache, Java, und lassen ihn dann von der JVM
auf verschiedenen Plattformen ausführen. Die JVM selbst ist sehr effizient
und auf ein präzises Betriebssystem und die Hardware ausgerichtet.
Sie ist in C oder C++ geschrieben. Bei TensorFlow ist die Situation ähnlich. Als Entwickler schreiben wir den Code
in einer höheren Sprache, Python, und lassen ihn vom
TensorFlow-Ausführungsmodul auf unterschiedlichen
Plattformen ausführen. Das TensorFlow-Ausführungsmodul
ist sehr effizient und auf den präzisen Hardwarechip
und dessen Fähigkeiten ausgerichtet. Geschrieben ist es in C++. Portabilität zwischen Geräten ermöglicht
starke Leistung und Flexibilität. Das kommt beispielsweise häufig vor. Sie können ein TensorFlow-Modell
in der Cloud auf Unmengen leistungsstarker Hardware trainieren und das trainierte Modell dann
auf ein Gerät am Rand verlagern, z. B. ein Mobiltelefon
oder sogar ein integrierter Chip. Und Sie können direkt auf diesem Gerät
Vorhersagen mit dem Modell machen. Erinnern Sie sich an
die Google Translate-App, über die wir im ersten Kurs
dieser Spezialisierung gesprochen haben? Die App funktioniert komplett offline,
weil ein trainiertes Übersetzungsmodell auf dem Telefon gespeichert und
für Offline-Übersetzungen verfügbar ist. Das Modell ist wegen der begrenzten
Verarbeitungsleistung des Telefons tendenziell kleiner und weniger
leistungsfähig als das in der Cloud. Doch dass TensorFlow dazu imstande ist,
ist wirklich großartig. Und möglich ist das nur wegen
der vom DAG ermöglichten Portabilität. Diese Arten von kleineren, weniger leistungsfähigen Modellen werden in der
Regel mit TensorFlow Lite implementiert. Das Trainieren in der Cloud 
mit anschließenden Vorhersagen auf einem relativ leistungsschwachen 
Gerät wie einem Telefon habe ich bereits erwähnt. Aber kann man auch das Modell
selbst auf dem Telefon trainieren? Aktuell nicht, weil das Trainieren
von ML-Modellen eine eher teure Sache ist. Doch wir tun zunehmend etwas,
was dem halbwegs nahekommt. Das ist etwas, was eigentlich
nur die kompetentesten Akteure im ML-Bereich tun, und es ist
auch nicht unbedingt weit verbreitet. Was soll "halbwegs"
hier eigentlich heißen? Eine Situation ist das Trainieren
eines ML-Modells, das dann auf mehreren Telefonen bereitgestellt wird. Bei einer Vorhersage kann
der Nutzer dann z. B. sagen, dass das nicht stimmt oder er mehr
Ergebnisse wie dieses sehen möchte. An diesem Punkt werden dann
die Gewichtungen des Modells aktualisiert, um den Präferenzen
dieses Nutzers Rechnung zu tragen. Eine solche Anpassung eines trainierten
Modells ist auf einem Telefon möglich. Das Telefon des Nutzers personalisiert
das Modell entsprechend der Nutzung lokal. Das wird in A gezeigt. So passen Sie also das Modell
für jeden Nutzer an, aber die Präferenzen der jeweiligen Nutzer möchten Sie
nicht zurück an Ihr System senden oder zurück an die Cloud, da diese
persönlich und vertraulich sein können. Man kann aber sogenanntes
föderiertes Lernen einrichten. Dabei werden die Aktualisierungen vieler
Nutzer aggregiert, wie in B gezeigt. Diese Aggregation kommt im Grunde
einer Gewichtungsänderung bei einem Satz von Proben gleich,
nur dass diese von mehreren Nutzern kommt. Das bildet also
eine Konsensänderung, wie in C gezeigt. Diese Änderung wird am gemeinsamen
Modell in der Cloud vorgenommen. Das gemeinsame
Modell wird also bereitgestellt, auf den Geräten verschiedener Nutzer
angepasst und dann wird alles wiederholt. TensorFlow ist
eine portable, leistungsstarke und für den Produktionseinsatz geeignete
Software für numerische Berechnungen. Besonders beliebt ist sie
im Bereich maschinelles Lernen. Sie ist das führende Repository
für maschinelles Lernen auf GitHub. Warum ist TensorFlow so beliebt? Bei Deep-Learning-Forschern
liegt das an der Community und der Möglichkeit, Erweiterungen
und neue, tolle Sachen vorzunehmen. Bei ML-Ingenieuren
liegt es an der Möglichkeit zur Nutzung von Modellen in
der Produktion und an der Skalierbarkeit. Die Beliebtheit bei diesen Gruppen
steht in einer Wechselbeziehung. Forscher möchten, dass ihre Methoden
weitreichend verwendet werden und durch die Implementierung in
TensorFlow kann das gewährleistet werden. ML-Ingenieure möchten ihren Code
zukunftssicher machen, sodass neue Modelle direkt bei Verfügbarkeit genutzt werden
können, und TensorFlow hilft ihnen dabei. Google hat TensorFlow als
Open-Source-Software konzipiert, weil sie so vielen
anderen Unternehmen helfen kann, und weil wir das Potenzial eines massiven
Community-Supports gesehen haben. Dass TensorFlow eine Open-Source-Software
ist, ist ein großer Vorteil. Sie sind bei der Nutzung von
Cloud Machine Learning Engine auf der GCP völlig ungebunden,
da sich der Code, den Sie schreiben, in TensorFlow befindet, 
und TensorFlow ist Open Source.