Let's start by explaining what is TensorFlow. TensorFlow is an open source, high performance, library for numerical computation. It's not just about machine learning. It's about any numeric computation. Infact,, people have used TensorFlow for all kinds of GPU computing. For example, you can use TensorFlow to solve partial differential equations. These are useful in domains like fluid dynamics. Tensorflow as a numeric program library is appealing, because you can write your competition code in a high level language, Python for example, and have it be executed in a fast way. The way TensorFlow works is that you create a directed acyclic graph, a DAG. To represent your computation. In this schematic, the nodes represent mathematical operations. Things like adding, subtracting, multiplying et cetera. Also more complex functions. Here for example you see soft max matrix multiplication. These are all mathematical operations that are part of the directed acyclic graph, the DAG. Connecting the nodes in the DAG are the edges, the input and the output of mathematical operations. The edges represent arrays of data. Essentially the result of computing across entropy is one of the three inputs to the bias add operation and the output of the bias and operation is sent along the matrix multiplication operation, matmul in the diagram. The other input to matmul, you need to input your matrix multiplication. The other input is a variable, the weight. So where does the name TensorFlow come from. In math a simple number like three or five is called a scalar. A vector is a one dimensional array of such numbers. In physics a vector is something magnitude and direction, but in computer science we've used vector to mean 1D arrays. A two-dimensional array is a matrix, but the three-dimensional array, we just call it a 3D tensor. So scalar, vector, matrix 3D tensor, 4D tensor et cetera. A tensor is an n dimensional array of data. So your data in TensorFlow, they are tensors. They flow through the directed acyclic graph, hence TensorFlow.