1
00:00:00,250 --> 00:00:03,580
Alors, pourquoi TensorFlow utilise-t-il
des graphes orientés acycliques

2
00:00:03,580 --> 00:00:06,040
pour représenter les calculs ?

3
00:00:06,040 --> 00:00:07,990
La réponse tient en un mot : portabilité.

4
00:00:07,990 --> 00:00:12,145
Le graphe orienté acyclique (DAG) est
une représentation du code de votre modèle

5
00:00:12,145 --> 00:00:15,070
qui est indépendante du langage.

6
00:00:15,070 --> 00:00:19,480
Vous pouvez créer un DAG en Python,
le stocker dans un modèle enregistré

7
00:00:19,480 --> 00:00:22,075
et le restaurer dans un programme C++

8
00:00:22,075 --> 00:00:24,500
pour établir des prédictions
à faible latence.

9
00:00:24,500 --> 00:00:30,830
Et ce même code Python peut être exécuté
sur des processeurs et sur des GPU,

10
00:00:30,830 --> 00:00:35,450
de sorte que la portabilité concerne
aussi bien le langage que le matériel.

11
00:00:35,450 --> 00:00:37,030
De bien des façons,

12
00:00:37,030 --> 00:00:40,790
ce mode de fonctionnement s'apparente
à celui de la machine virtuelle Java (JVM)

13
00:00:40,790 --> 00:00:45,560
dont la représentation du bytecode
facilite la portabilité du code Java.

14
00:00:45,560 --> 00:00:47,165
En tant que développeurs,

15
00:00:47,165 --> 00:00:50,710
nous avons à écrire du code en Java,
un langage de haut niveau,

16
00:00:50,710 --> 00:00:54,680
et à faire en sorte qu'il soit exécuté
sur différentes plates-formes via la JVM.

17
00:00:54,680 --> 00:00:58,880
Très performante, la JVM est
spécifiquement développée

18
00:00:58,880 --> 00:01:01,990
pour le système d'exploitation
et le matériel utilisés.

19
00:01:01,990 --> 00:01:04,780
Elle est écrite en C ou en C++.

20
00:01:04,780 --> 00:01:07,500
C'est très proche du mode
de fonctionnement de TensorFlow.

21
00:01:07,500 --> 00:01:08,690
En tant que développeurs,

22
00:01:08,690 --> 00:01:12,360
nous avons à écrire du code en Python,
un langage de haut niveau,

23
00:01:12,360 --> 00:01:16,026
et à faire en sorte qu'il soit exécuté
sur différentes plates-formes

24
00:01:16,026 --> 00:01:18,713
via le moteur d'exécution de TensorFlow.

25
00:01:18,713 --> 00:01:23,355
Très performant, ce moteur d'exécution
est spécifiquement développé

26
00:01:23,355 --> 00:01:27,070
pour la puce matérielle utilisée afin
d'en exploiter au mieux les possibilités.

27
00:01:27,070 --> 00:01:29,780
Il est écrit en C++.

28
00:01:29,780 --> 00:01:35,740
La portabilité entre les appareils apporte
beaucoup de puissance et de flexibilité.

29
00:01:35,740 --> 00:01:41,500
Par exemple, il est courant que vous puissiez
entraîner un modèle TensorFlow dans le cloud

30
00:01:41,500 --> 00:01:44,210
sur une multitude de machines puissantes,

31
00:01:44,210 --> 00:01:46,405
puis que vous fassiez passer
le modèle entraîné

32
00:01:46,405 --> 00:01:49,170
sur un appareil situé
en périphérie du réseau.

33
00:01:49,170 --> 00:01:52,530
Il peut s'agir d'un téléphone mobile,
ou même d'une puce intégrée.

34
00:01:52,530 --> 00:01:56,830
Et vous pouvez établir des prédictions
avec le modèle, directement sur cet appareil.

35
00:01:58,100 --> 00:02:00,580
Vous vous souvenez
de l'application Google Traduction

36
00:02:00,580 --> 00:02:03,860
dont il a été question dans
le premier cours de cette spécialisation ?

37
00:02:03,860 --> 00:02:07,429
Elle peut fonctionner tout en étant
totalement hors connexion

38
00:02:07,429 --> 00:02:11,494
avec un modèle de traduction entraîné,
stocké sur le téléphone,

39
00:02:11,494 --> 00:02:14,800
qui est opérationnel sans connexion Internet.

40
00:02:14,800 --> 00:02:19,236
Il a tendance à être plus petit et moins
puissant que celui utilisé dans le cloud,

41
00:02:19,236 --> 00:02:23,026
ceci en raison des limites liées
à la puissance de traitement du téléphone.

42
00:02:23,026 --> 00:02:26,185
Mais si TensorFlow peut faire ça,

43
00:02:26,185 --> 00:02:29,174
et c'est bien pratique,

44
00:02:29,174 --> 00:02:35,290
c'est grâce à la portabilité apportée
par la représentation orientée acyclique.

45
00:02:35,290 --> 00:02:38,310
Ces types de modèles,
plus petits et moins puissants,

46
00:02:38,310 --> 00:02:41,680
sont généralement mis en œuvre
avec TensorFlow Lite.

47
00:02:41,680 --> 00:02:44,060
J'ai parlé de l'entraînement
effectué dans le cloud

48
00:02:44,060 --> 00:02:48,340
et de prédictions établies sur un appareil
de faible puissance tel qu'un téléphone.

49
00:02:48,340 --> 00:02:53,530
Oui, mais est-il possible d'entraîner
le modèle sur le téléphone ?

50
00:02:53,530 --> 00:02:54,580
Pas actuellement,

51
00:02:54,580 --> 00:02:59,430
car l'entrainement d'un modèle de ML
a tendance à être une opération onéreuse.

52
00:02:59,430 --> 00:03:03,190
Mais nous en prenons de plus en plus
le chemin en adoptant une approche mixte.

53
00:03:03,190 --> 00:03:11,150
C'est une pratique réservée aux acteurs
les plus avancés du ML, et peu répandue.

54
00:03:11,150 --> 00:03:14,200
Mais qu'est-ce que je veux dire
quand je parle d'approche mixte ?

55
00:03:14,200 --> 00:03:17,055
Imaginez une situation dans laquelle
vous entraînez un modèle

56
00:03:17,055 --> 00:03:19,520
avant de le déployer
sur de nombreux téléphones.

57
00:03:19,520 --> 00:03:21,240
Et lorsqu'il obtient une prédiction,

58
00:03:21,240 --> 00:03:23,610
un utilisateur considère
qu'elle est incorrecte

59
00:03:23,610 --> 00:03:26,340
ou souhaite voir davantage
de résultats du même type.

60
00:03:26,340 --> 00:03:30,620
Et à ce stade, vous voulez
mettre à jour les pondérations du modèle

61
00:03:30,620 --> 00:03:34,160
en fonction des préférences
de cet utilisateur.

62
00:03:34,160 --> 00:03:39,820
Ce type d'optimisation d'un modèle entraîné
est tout à fait possible sur un téléphone.

63
00:03:39,820 --> 00:03:45,740
L'appareil personnalise le modèle en local
en fonction des habitudes de l'utilisateur

64
00:03:45,740 --> 00:03:49,290
(A sur le schéma).

65
00:03:49,290 --> 00:03:54,150
Cela revient toutefois à optimiser le modèle
pour chacun des utilisateurs.

66
00:03:54,150 --> 00:03:57,085
Vous pouvez souhaiter
que les préférences des utilisateurs,

67
00:03:57,085 --> 00:03:59,990
ne soient pas renvoyées
sur votre système situé dans le cloud,

68
00:03:59,990 --> 00:04:02,700
car il s'agit peut-être
de données personnelles sensibles.

69
00:04:02,700 --> 00:04:05,650
Vous pouvez alors avoir recours
à l'apprentissage fédéré,

70
00:04:05,650 --> 00:04:09,745
qui consiste en l'agrégation
des mises à jour de nombreux utilisateurs

71
00:04:09,745 --> 00:04:11,530
(B sur le schéma).

72
00:04:11,530 --> 00:04:16,360
Cela revient pour l'essentiel à mettre à jour
les pondérations d'un lot d'échantillons,

73
00:04:16,360 --> 00:04:19,500
à ceci près que les données proviennent
de différents utilisateurs.

74
00:04:19,500 --> 00:04:22,740
L'opération (C sur le schéma)
fait ainsi consensus,

75
00:04:22,740 --> 00:04:27,140
et s'effectue au niveau
du modèle partagé stocké dans le cloud.

76
00:04:27,140 --> 00:04:29,370
Donc, vous déployez le modèle partagé,

77
00:04:29,370 --> 00:04:32,765
vous l'optimisez sur les appareils
de différents utilisateurs,

78
00:04:32,765 --> 00:04:34,140
et vous recommencez.

79
00:04:34,140 --> 00:04:39,390
TensorFlow est donc un logiciel portable,
puissant et prêt à être utilisé en production

80
00:04:39,390 --> 00:04:41,530
qui permet d'effectuer
des calculs numériques.

81
00:04:41,530 --> 00:04:45,240
Et il est particulièrement prisé
pour le machine learning.

82
00:04:45,240 --> 00:04:49,380
C'est le dépôt le plus utilisé
pour le machine learning sur GitHub.

83
00:04:49,380 --> 00:04:51,530
Pourquoi ?

84
00:04:51,530 --> 00:04:56,550
Les chercheurs en deep learning l'apprécient
pour sa communauté d'utilisateurs

85
00:04:56,550 --> 00:05:00,620
et les possibilités d'extension
qui ouvrent la voie à de nouveaux usages.

86
00:05:00,620 --> 00:05:05,730
Les ingénieurs en ML l'apprécient
pour sa capacité à produire des modèles

87
00:05:05,730 --> 00:05:07,700
et pour son évolutivité.

88
00:05:07,700 --> 00:05:12,350
Les raisons de ces deux groupes
se nourrissent les unes des autres.

89
00:05:12,350 --> 00:05:15,931
Les chercheurs veulent que leurs méthodes
soient largement utilisées,

90
00:05:15,931 --> 00:05:19,553
et le fait de les mettre en œuvre
dans TensorFlow est une façon d'y parvenir.

91
00:05:19,553 --> 00:05:22,181
Les ingénieurs en ML veulent
que leur code soit évolutif

92
00:05:22,181 --> 00:05:25,990
et leur permette d'utiliser des modèles
plus récents dès leur invention,

93
00:05:25,990 --> 00:05:28,330
et TensorFlow les aide à y parvenir.

94
00:05:28,330 --> 00:05:29,460
Chez Google,

95
00:05:29,460 --> 00:05:31,940
nous avons fait de TensorFlow
un logiciel Open Source

96
00:05:31,940 --> 00:05:34,420
parce qu'il peut aider
de nombreuses autres sociétés,

97
00:05:34,420 --> 00:05:36,610
mais aussi parce que
nous avons vu le potentiel

98
00:05:36,610 --> 00:05:39,320
de cette sorte d'aide massive
apportée par une communauté.

99
00:05:39,320 --> 00:05:44,280
Le fait que TensorFlow soit Open Source
vous donne un avantage essentiel.

100
00:05:44,280 --> 00:05:49,110
Vous n'êtes pas captifs lorsque vous utilisez
Cloud Machine Learning Engine sur GCP :

101
00:05:49,110 --> 00:05:54,610
le code que vous écrivez est dans TensorFlow,
et TensorFlow est Open Source.