Now. Let's look at how training the model machine learning engine works. Before you begin training, though, be sure to, one, gather and prepare your training data, clean, split, engineer features pre-processed features. And two, put that training data in an online source that Cloud Machine Learning Engine can access for example, Cloud storage. When sending training jobs to Cloud Machine Learning Engine, it's common to split most of the logic into a task.py file and a model.py file. Task.py is the entry point to your code that seemingly will start in those job level details like, how to press command line arguments, how long to run, where to write the outputs, how to interface with hyper parameter tuning, and so on. To do the core ML, task.py will then invoke model.py. Model.py focuses more on the core ML tasks like fetching the data, defining the features, configuring the service signature, and of course, the actual train and eval loop. Sharing code between computers always involves some type of packaging. Sending a model to CMLE for training is no different. Tensor flow, and python in particular, require very specific with standardized package and structure shown here. It's a great practice to do a quick local test that your packaging works as expected. Try calling it directly with Python -m, to check all the imports are in good shape. Next, let's use the GCloud to locally test our code. This will do some quick sanity checks that our packaged structure is correct. When satisfied, we can submit a training job to send the task to cloud to scale out. The key command line adds here are package path, to specify where the code is located, the module name, to specify which of the files in the package to execute, and scale tier, to specify what kind of hardware you want the code to be executed on. You would specify scale tier equals basic to run one machine, scale two equals standard to run a smallish cluster. Scale two equals basic GPU, to run on a single GPU. You want to run on a tpu? You guessed it. Scale three equals basic TPU. You can also specify custom tiers and define each machine type. The scale tiers keep expanding. Look up the Cloud Machine Learning Engine documentation for all your current options. Just a pro tip here, to get the best performance for ML jobs, make sure you select a single-region bucket in Google Cloud storage. The default is multi-region, which is better suited for web serving than ML training.