1
00:00:00,890 --> 00:00:02,990
Ya debe haber visto este diagrama.

2
00:00:02,990 --> 00:00:06,340
TensorFlow se puede ejecutar en
diferentes configuraciones de hardware.

3
00:00:06,340 --> 00:00:09,350
Puede programarlo
en una API de C++ de bajo nivel

4
00:00:09,350 --> 00:00:13,155
y lo más probable es que use la API
de Python como se practica en este curso.

5
00:00:13,415 --> 00:00:14,790
Ya comenzó a ver

6
00:00:14,790 --> 00:00:18,070
las diferentes capas de abstracción
para el entrenamiento distribuido.

7
00:00:18,310 --> 00:00:21,835
Pero, ¿ejecuta entrenamiento
distribuido a escala en la producción?

8
00:00:22,215 --> 00:00:25,460
Para hacerlo, presentaremos
Cloud Machine Learning Engine.

9
00:00:25,910 --> 00:00:30,030
Para iniciarse en el AA,
es frecuente comenzar, pero no terminar

10
00:00:30,030 --> 00:00:32,580
con pequeños conjuntos
de datos que caben en memoria.

11
00:00:32,580 --> 00:00:36,495
Para estos conjuntos sencillos,
casi cualquier marco de AA es suficiente.

12
00:00:36,510 --> 00:00:40,999
R, Python y otros lenguajes
tienen paquetes estadísticos

13
00:00:40,999 --> 00:00:45,505
que solo necesitan
3 o 4 líneas de código para ejecutarse.

14
00:00:45,630 --> 00:00:48,810
El Estimador de TensorFlow
tiene una API similar a Scikit-learn

15
00:00:48,810 --> 00:00:52,440
que es fácil y funciona bien
con conjuntos de datos pequeños.

16
00:00:52,440 --> 00:00:54,474
Pero, claro, lo que queremos en realidad

17
00:00:54,474 --> 00:00:58,350
es usar conjuntos de datos
de producción de escala empresarial.

18
00:00:58,350 --> 00:01:00,765
Conjuntos tan grandes,
que no caben en la memoria.

19
00:01:01,015 --> 00:01:04,545
Se hace necesario escalar
a paquetes más sofisticados.

20
00:01:04,545 --> 00:01:07,490
Ahora que el conjunto de datos
ya no cabe en la memoria

21
00:01:07,490 --> 00:01:10,940
tendremos que iterar
muchas veces durante los entrenamientos.

22
00:01:10,990 --> 00:01:14,180
Esto puede hacerse
con una sola máquina, pero no es lo ideal.

23
00:01:14,265 --> 00:01:18,270
¿Imagina esperar semanas solo para ver
si el entrenamiento es convergente o no?

24
00:01:18,720 --> 00:01:22,155
Necesitamos distribuir
el entrenamiento en varias máquinas.

25
00:01:22,155 --> 00:01:26,300
No es tan sencillo como con MapReduce,
donde todo es terriblemente paralelo.

26
00:01:26,590 --> 00:01:30,524
La optimización por descenso de gradientes
y otros algoritmos no son tan sencillos

27
00:01:30,540 --> 00:01:34,725
y requieren servidores de parámetros
que ayuden a los trabajadores.

28
00:01:34,725 --> 00:01:38,185
Estos servidores de parámetros
forman una suerte de memoria compartida

29
00:01:38,185 --> 00:01:40,720
y permiten que cada
entrenador aprenda de los demás.

30
00:01:41,100 --> 00:01:44,010
Suena tentador evitar
el entrenamiento distribuido

31
00:01:44,010 --> 00:01:47,340
usando una máquina gigante con muchos GPU.

32
00:01:47,340 --> 00:01:50,550
Sin embargo,
esto casi nunca resulta a la larga.

33
00:01:50,550 --> 00:01:54,450
Los conjuntos de datos suelen crecer
más rápido que la capacidad de la máquina.

34
00:01:54,450 --> 00:01:57,765
La solución es usar
escalamiento horizontal, no vertical.

35
00:01:57,765 --> 00:02:01,360
Otro atajo que muchos intentan
es tomar muestras de los datos

36
00:02:01,360 --> 00:02:04,875
de un tamaño que les permita
realizar AA con el hardware que tienen.

37
00:02:04,875 --> 00:02:08,264
Con este enfoque, se sacrifica
un rendimiento potencial considerable.

38
00:02:08,264 --> 00:02:10,229
Usar todos los datos disponibles

39
00:02:10,229 --> 00:02:13,170
y elaborar un plan
para recopilar diez veces más

40
00:02:13,170 --> 00:02:16,350
suele ser la diferencia
entre el AA que se desempeña bien

41
00:02:16,350 --> 00:02:17,930
y el que no.

42
00:02:18,010 --> 00:02:21,910
A menudo creamos modelos de AA
en dominios en que la información humana

43
00:02:21,910 --> 00:02:23,285
puede mejorar el rendimiento

44
00:02:23,285 --> 00:02:25,695
más que el entrenamiento
con los datos por sí solo.

45
00:02:25,695 --> 00:02:27,350
Solemos usar esta información

46
00:02:27,350 --> 00:02:29,790
sobre lo que los expertos
ya saben del problema

47
00:02:29,790 --> 00:02:31,570
en forma de nuevas funciones.

48
00:02:31,570 --> 00:02:35,175
Estas funciones se agregan
después del preprocesamiento de los datos.

49
00:02:35,175 --> 00:02:38,840
Es decir, cuando los escalamos,
los codificamos y demás.

50
00:02:39,165 --> 00:02:42,820
Otra vez, el tamaño de los conjuntos
de datos que queremos utilizar

51
00:02:42,870 --> 00:02:46,335
es necesario distribuir
estos procesos y realizarlos en la nube.

52
00:02:46,335 --> 00:02:50,630
Cuando realizamos AA, a menudo tenemos
que realizar elecciones algo arbitrarias

53
00:02:50,630 --> 00:02:52,935
como la cantidad
de nodos, las incorporaciones

54
00:02:52,935 --> 00:02:55,210
o el tamaño del paso
de una capa convolucional.

55
00:02:55,210 --> 00:02:57,410
A medida que sus modelos se complejizan

56
00:02:57,410 --> 00:03:00,210
comenzará a preguntarse
si eligió los valores adecuados.

57
00:03:00,210 --> 00:03:02,010
Ya sea de forma manual o automática

58
00:03:02,010 --> 00:03:05,360
tendrá que realizar una búsqueda
en el espacio de hiperparámetros

59
00:03:05,360 --> 00:03:08,685
para ver si sería mejor
utilizar otros valores.

60
00:03:08,845 --> 00:03:12,200
La cantidad de capas o de nodos
son hiperparámetros obvios.

61
00:03:12,200 --> 00:03:13,770
Pero, como verá en este curso

62
00:03:13,770 --> 00:03:16,375
es conveniente tomar
las opciones de preprocesamiento

63
00:03:16,375 --> 00:03:19,850
como la cantidad de depósitos,
y también tratarlas como hiperparámetros.

64
00:03:20,340 --> 00:03:22,440
Hasta ahora, hablamos del entrenamiento.

65
00:03:22,440 --> 00:03:26,300
Pero ¿de qué sirve un modelo entrenado
si no lo puede usar para las inferencias?

66
00:03:26,300 --> 00:03:29,780
No es conveniente, y a menudo
no es posible, integrar directamente

67
00:03:29,780 --> 00:03:33,410
nuestro modelo de AA a la aplicación
que necesita las funciones predichas.

68
00:03:33,410 --> 00:03:37,290
Una buena forma de manejar esto es
envolver el modelo en un microservicio

69
00:03:37,290 --> 00:03:39,810
y hacer que otros
microservicios se comuniquen con él

70
00:03:39,810 --> 00:03:41,775
al igual que cualquier aplicación web.

71
00:03:41,775 --> 00:03:45,105
Esto tiene la ventaja
de que nos permite actualizar el modelo

72
00:03:45,105 --> 00:03:49,030
y ejecutar pruebas A/B
sin cambiar la lógica de la aplicación.

73
00:03:49,030 --> 00:03:51,340
Solo hay que cambiar los microservicios.

74
00:03:51,670 --> 00:03:55,205
¿Cómo aprovisionamos la cantidad
adecuada de hardware para este modelo?

75
00:03:55,205 --> 00:03:57,210
Los sistemas eficaces
usan autoescalamiento

76
00:03:57,210 --> 00:03:59,800
para adaptarse
a sus necesidades en cada momento.

77
00:03:59,800 --> 00:04:02,070
En Cloud, puede escalar a cero máquinas

78
00:04:02,070 --> 00:04:05,250
o a las que necesite para manejar
muchas consultas por segundo.

79
00:04:05,250 --> 00:04:07,770
Permítame ahorrarle dolores de cabeza.

80
00:04:07,770 --> 00:04:11,540
¿Recuerda que hablamos sobre procesar
sus ejemplos antes del entrenamiento?

81
00:04:11,540 --> 00:04:13,065
Bueno, tenga cuidado.

82
00:04:13,065 --> 00:04:17,864
Asegúrese de que ese procesamiento previo
también se realice para las predicciones.

83
00:04:17,864 --> 00:04:21,150
Además del procesamiento previo,
hay otras diferencias potenciales

84
00:04:21,150 --> 00:04:24,140
entre sus modelos
de entrenamiento y predicción.

85
00:04:24,140 --> 00:04:28,680
Usar un estándar como Cloud MLE
lo ayudará a eliminar estos problemas.

86
00:04:29,570 --> 00:04:30,980
Aunque no se menciona mucho

87
00:04:30,980 --> 00:04:32,790
sus entradas de predicción serán

88
00:04:32,790 --> 00:04:35,700
sistemáticamente
diferentes a las del entrenamiento.

89
00:04:35,700 --> 00:04:38,040
De maneras sutiles
y difíciles de detectar.

90
00:04:38,040 --> 00:04:40,430
Tal vez se modificó
el promedio de alguna columna

91
00:04:40,430 --> 00:04:42,330
o la varianza aumentó con el tiempo.

92
00:04:42,330 --> 00:04:44,260
Esto se denomina distorsión.

93
00:04:44,260 --> 00:04:48,930
Para detectarla, hay que recopilar datos
y reexaminarlos en forma continua.

94
00:04:48,930 --> 00:04:51,540
Usar TensorFlow básico
por su cuenta no es fácil.

95
00:04:51,540 --> 00:04:54,535
Tiene que instalar controladores,
tener las máquinas adecuadas

96
00:04:54,535 --> 00:04:58,095
hacer un seguimiento
del orden del procesamiento previo

97
00:04:58,095 --> 00:05:00,205
los parámetros de escalamiento, etcétera.

98
00:05:00,205 --> 00:05:01,965
Pero Google Cloud puede ayudar.

99
00:05:01,965 --> 00:05:04,155
Ofrecemos varios servicios de macrodatos.

100
00:05:04,155 --> 00:05:09,320
Hoy me quiero enfocar
en Cloud Machine Learning Engine o CMLE.

101
00:05:09,345 --> 00:05:12,340
Le brinda las máquinas
que necesita en el momento justo.

102
00:05:12,340 --> 00:05:14,530
Simplifica la contabilidad y garantiza

103
00:05:14,530 --> 00:05:18,045
que el modelo entrenado sea
lo que se ejecuta durante la predicción.

104
00:05:18,045 --> 00:05:22,305
Es un servicio muy escalable que facilita
la entrega y el entrenamiento distribuido.

105
00:05:22,305 --> 00:05:25,635
CMLE ayuda a distribuir
el procesamiento previo

106
00:05:25,635 --> 00:05:27,300
muestra los
servidores de parámetros

107
00:05:27,300 --> 00:05:29,130
y ajusta los hiperparámetros.

108
00:05:29,130 --> 00:05:32,690
Para las predicciones, se puede acceder
al modelo de AA con una API de REST

109
00:05:32,690 --> 00:05:36,190
que incluye la creación de funciones
de procesamiento que pondría usted.

110
00:05:36,190 --> 00:05:39,660
Así, el código del cliente puede
proporcionar las variables de entrada

111
00:05:39,660 --> 00:05:43,160
con lo que recopiló del archivo
de registro, el sensor o la base de datos

112
00:05:43,160 --> 00:05:45,145
y obtener una predicción.

113
00:05:45,145 --> 00:05:48,655
CMLE también escala su servicio
con la cantidad de máquinas que necesite

114
00:05:48,655 --> 00:05:52,790
para alcanzar más consultas
por segundo. Y esto es importante.

115
00:05:52,790 --> 00:05:56,875
Necesita una ejecución de calidad
durante el entrenamiento y la predicción.

116
00:05:57,435 --> 00:06:00,820
Los cálculos de los modelos
de TensorFlow son relativamente baratos

117
00:06:00,820 --> 00:06:04,435
pero la clave es obtener
muchas predicciones de su modelo de AA.

118
00:06:05,955 --> 00:06:09,285
Los notebooks
como Cloud Datalab o Kaggle Kernels

119
00:06:09,285 --> 00:06:13,470
son una gran forma de comenzar y reforzar
rápidamente el desarrollo de su modelo.

120
00:06:13,470 --> 00:06:16,190
Le permiten explorar
los datos de forma interactiva

121
00:06:16,190 --> 00:06:18,275
para encontrar y explorar nuevas funciones

122
00:06:18,275 --> 00:06:20,805
o hacer trabajos
de entrenamiento y evaluación.

123
00:06:20,805 --> 00:06:22,910
La interfaz combina
el código, los resultados

124
00:06:22,910 --> 00:06:25,645
y la documentación en un formato legible.

125
00:06:25,865 --> 00:06:29,400
Y como está en Cloud,
tiene funciones para compatir y colaborar

126
00:06:29,400 --> 00:06:31,475
además de instructivos.

127
00:06:31,495 --> 00:06:33,890
Datalab nos ofrece una gran ventaja

128
00:06:33,890 --> 00:06:37,040
y una transición sencilla
para escalar el procesamiento

129
00:06:37,040 --> 00:06:39,460
con diversos servicios de Google Cloud.

130
00:06:39,700 --> 00:06:42,925
En este ejemplo, puede ver
que lanzamos un trabajo de Apache Beam

131
00:06:42,925 --> 00:06:46,420
en Dataflow, que puede
distribuirse a muchas VM.