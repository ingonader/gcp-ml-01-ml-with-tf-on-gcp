Now this diagram, you've already seen before. Recall the TensorFlow can run on different hardware. You could program it on a low-level C++ API, but more likely you'll use the Python API as we practice in this course. And you've already started to see the different abstraction layers for distributed training. But you actually run distributed training at scale in production. For that, let's introduce Cloud Machine Learning Engine. When we first approach ML, we typically start but do not finish with small data sets that fit in memory. With these warm-up data sets, pretty much any ML framework will suffice. Our python and many other languages, all have statistical packages that typically need three or four lines of code to get you up and running. TensorFlow estimator also has an API that can decide learn which is easy and works great on the small data sets. But of course, what we really want is to have a production, enterprise size, data sets. When so big, they cannot fit into memory. At this point, we'll need to scale up to more sophisticated packages. Now that our data set is too big to fit into memory, we'll have to iterate through perhaps many time during trainings. While it's possible that with a single machine, it's far from ideal. Can you imagine having to wait weeks just to see if training converged or not? We needed to distribute training over many machines. This is not as simple as mass produce where things are embarrassingly parallel. Algorithms, like grading descent optimization, are not so easy, we'll need help from so-called parameter servers to assist a pool of training workers. This parameters servers form a type of shared memory, and let each trainer learn from all the others. It's tempting to try to escape distributed training by using a single giant machine with that of GPUs. This, however, is ultimately shortsighted for most because data sets often grow faster than any single machine's capabilities. Scaling out, not up, solves us. Another common shortcut people take is to try to sample a data. So, it's small enough to do ML on the hardware they happen to have. This leaves substantial performance games on the table. Using all the available data, and devising a plan to collect 10x more than that, is often the difference between ML that performs magically, and ML that doesn't. Oftentimes you are building machine learning models in a domain where human insights can add performance beyond training, just on the raw data. We typically bring this insight namely when experts already know about the problem in the form of new features. These features are added right after we've preprocessed the raw data. You know, when we do things like scaling it, and coding it, and so on. And again, for the size of data sets, we are really excited to work with, these two things that need to be distributed and done on cloud. When you do ML, you often have to pick a number of things, somewhat arbitrarily, the number of nodes, the embedding, the stride size of a convolutional layer. As your models get more complex, you're going to start to wonder whether you picked the right values, either manually or automatically, you'll have to do some kind of search on the hyperparameter space, to see if there are better choices you could have made. How many layers or how many nodes are some obvious hyperparameters. But as you'll see in this course, it's good to take the preprocessing nobs, such as the number of buckets, and treat them as hyperparameters too. So far, we've just talked about training. But what good is a trained model if you cannot use it for inference? We don't want to and often cannot directly embed our ML model into the application that needs the predicted features. An excellent way to handle this is to wrap the model its own micro service, and have other micro-services communicate with it, just like any other web app. Now you also are in this great situation where you can update your model, run AP tests, all without changing your core application logic. Just change the micro-servers. Bu how do you provision the right amount of hardware for this model serving? Great systems autoscale to provide the number of machines you need, when you need them. On cloud, we can scale to zero machines or as many as you need to handle beaucoup queries per second. Let me try to spare you some future heartache. Remember how we talked about preprocessing your examples before training? Well, watch out. Because you have to make sure that the same preprocessing happens at prediction time too. Beyond just preprocessing, there are a variety of ways your trained model could be a bit different than your prediction one. But using a standard like Cloud Machine Learning Engine helps remove these issues. Well it's rarely talked about, your prediction inputs will commonly be systematically different than the ones that training. In subtle and hard to detect ways. Maybe the average of some column has shifted, or the variance has grown over time. This is called the training settings skills, and detecting it requires continued data collection and re-examination. Using bare TensorFlow yourself can be a pain. You have to install drivers, get the right machines, keep track of things like the preprocessing order of operations, scaling parameters, you name it. But Google Cloud can help here. We offer several Big Data services. But today I want to focus in on Cloud Machine Learning Engine, or CMLE for short. CMLE gets you the machines you need when you need them. Simplifies bookkeeping and ensures that the trained model is what you actually run at prediction time. It's a highly scalable service and will make distributed training and serving easy. Cloud Machine Learning Engine will help distribute preprocessing, bring up perimeter servers, and even hyperparameter tune. For predictions, the ML model is accessible via a rest API and includes all the preprocessing feature creation that you do. So the client code can simply provide the raw input variables. Exactly what you collected out of the log files, sensor, database, and get back a prediction. CMLE will also scale your service with as many machines as you need to reach a higher number of queries per second. And this is important. You need high-quality execution both at training and prediction time. Computation of TensorFlow model is relatively cheap. Value comes from getting lots of predictions out of your ML model. Notebooks like Google Cloud's data lab or Kaggle Kernels are a great way to get started and underlay quickly with developing your model. Notebooks let you interactively explore the data, to find and probe new features, even large training in evolve jobs. The interface combines code, result, docs all into a human-readable format. And since you're on cloud, you have a great sharing and collaboration support and a rich set of tutorial. Datalab gives us both a great head start, and a smooth transition into scaling out our computation, with a variety of Google cloud services. In this example, you can see we're launching an Apache Beam job, on data flow which can distribute to many, many VMs.