今回はQwiklabsにログインして
ラボを開始します 自分のユーザー名とパスワードで
GCP Consoleにログインします Datalabを開始して動作させます 最初にすべてのノートブックがある
リポジトリのクローンを作成します 上のGitアイコンを使うと簡単ですが 新しいノートブックを作成して
bash機能を使う方法もあります では「bash git clone」と
入力していきます training-data-analystリポジトリを
git cloneします このラボで使いたいノートブックが
含まれています ここに先ほどのtraining-data-analystがあります training-data-analystに入って
フォルダに移動します ノートブックが入っているので確認します 3番目のコースでCloud ML Engineを確認します これがCloud ML Engineです
このラボでは TensorFlowモデルをスケールアップします すでに作ったモデルと同じですが
今度はPythonモデルで作り ML Engineで実行します クラウドで実行するので 最初に 構築するプロジェクトを指定します QwiklabsからもらったプロジェクトIDはこれです このプロジェクトIDを
プロジェクトの作成に使用します そしてバケットです
バケットとは何でしょうか？ バケットを作成しなければなりません そこでGCP Consoleで
[Storage] [ブラウザ]の順に移動します バケットがあるか確認して
なければ作成します バケットの名前は一意にしてください
それにはどうすればいいでしょうか？ 1つの方法として
バケット名をプロジェクトと同じ名前にします よほど運が悪くなければ
この名前のバケットは作成されていません バケットの名前を作成し
マルチリージョンバケットを作成できます ここでバケットができました バケットはプロジェクトと同じ名前なので
とても簡単です それから バケットが必要になったら
バケット名とリージョンを指定します リージョンはとても重要です リージョンとはML Engineジョブの送信先です 単一リージョンのバケットを
使っていた場合は 同じリージョンに指定したいでしょう これはマルチリージョンのバケットですが
どちらでもかまいません コンピューティングに
使いたいリージョンを使えるので us-centralのままにします これはDatalabを始めたときと
同じリージョンですが DatalabインスタンスとML Engineジョブは 別のリージョンでも実行できます ここではジョブ送信と
ジョブ実行のために作成するマシンは すべてus-central1にします ノートブックを保存しておきます 私のプロジェクトで[Run]をクリックします 今度からはShift+Enterを押します
これでも動作します これでPROJECT、BUCKET、REGIONの変数が
Pythonで作成されます 次のセルは同じ変数を設定します
ただしbashです Pythonのコマンドos.environで
bashの変数を設定します これで
ノートブックで$PROJECTや $BUCKETを使えば
bashスクリプトで該当する変数が取得できます ここでやっていることは
gcloudはフェイクが使えるので プロジェクトが設定済みの
このプロジェクトだと設定し コンピューティングリージョンも
設定済みのリージョンだと設定します コアプロジェクトとコンピューティング リージョンが
更新されました ここでCloud ML Engineは
シャドウプロジェクトで動作し 次に全データファイルへのアクセス権を
ML Engineに付与します ML Engineは人間ではなく
自動化ロボットアカウントです サービスアカウントであり
このサービスアカウントに バケット内のファイルへの
読み取り権限を付与します つまりここでは ML Engineにバケット内の
既存のファイルと 新規作成するファイルへの
アクセス権を付与します 書き込み権限も付与します このバケットにチェックポイントや モデルの出力なども格納するからです バケットには必要なデータだけを入れてください ML Engineがアクセスして読み取れるデータです 通常はすべてのデータを保存するバケットは
作成しません 機械学習固有のバケットを作成し そのファイルだけを保存すると
セキュリティを向上できます ここで ML Engineにこのバケットへの
読み書き権限を付与します 付与したらML Engineサービスアカウントを
承認します アカウントはservice-の後に プロジェクトIDです プロジェクトIDを見つけるには
GCP Consoleに移動します [Home]に移動するとプロジェクトIDがあります プロジェクト番号はここです
同じプロジェクト番号です これは覚えなくてかまいません スクリプトで取得できます JSON呼び出し response['serviceAccount'] です 次はコードを取り出します コードは前のラボの実験で
ノートブックに書いたからです 今回は大規模実行するので
コードを送信して実行したいときは Pythonパッケージにします それを今から行いましょう Pythonパッケージを作成します パッケージtaxifareにこのファイルすべてを入れます Datalabで確認できます Datalabに移動すると
taxifareの中にtrainerフォルダがあります trainerには2つのファイルがあります task.pyとmodel.pyです task.pyにはmainが含まれ 同期のたびにコマンドラインをすべて読み train_data_pathsとtrain_batch_sizeなどを探します これはコマンドラインから来ます model.pyにはモデルのコアが含まれます これは適切なリグレッサーを作成し データを読み取る入力関数などがあります ここにパッケージがあります
Pythonのパッケージは 必要なファイルがすべて含まれる
フォルダ構造です ではmodel.pyを確認しましょう これは基本的に 以前Datalabの
ノートブックにあったコードで これをPythonパッケージにします ここで問題は
どうやってPythonコードを取り出して パッケージにするかです 1つ簡単な方法があります
Pythonが書かれたものを見てみましょう このコードをファイルに書き込みます 簡単なのはJupyter Magicの
writefileを使うことです writefile tensorboard.pyと入力して
実行すると ここにあるコードが
すべてtensorboard.pyに書き込まれます このように簡単に
Pythonノートブックのコードを取得して 別のPythonファイルに書き出して
Pythonパッケージに入れられます writefileは追加のためのオプションもあり 必要に応じてPython .pyに
行を追加できます これは削除します 書き込まれたtensorboard.pyを見るため ディレクトリに戻りましょう 03_tensorflowにtensorboard.pyがあります これが%writefileで書き出したファイルです では元の場所に戻りましょう ここではPythonパッケージを作成して データファイルがあることを確認できます これがデータファイルで
Datalabのすべてが/contentにマップされ このディレクトリに入っています トレーニングの入力ファイルの1行と 検証入力ファイルの1行を出力しています Pythonパッケージがあるので
実行してみましょう Pythonパッケージの実行は
ML Engineと関係ありません Pythonパッケージを実行するには python -mにモジュールを渡します モジュール名はtaskですが
trainerパッケージにあるので Pythonが探す場所を
PYTHONPATHで設定します それを現在のディレクトリの
/taxifareに設定します ここにtrainerがあるからです
PYTHONPATHを指定して pythonプログラムに
taxi-train*とtaxi-validを渡して実行します このコマンドラインパスが正しいことを
確認します 出力ディレクトリと
少数のトレーニング手順を指定します 手順10個だけでも指定できます これでShift+Enterで実行できます ここでPythonモジュールを実行して 動くことを確認します
動いたら 何が書き出されたかを確認できます 全部実行されて 保存されたモデルに書き込まれました
これが重要です トレーニングが行われて 保存されたモデルがあることを確認します それにはexport/exporterに保存されたモデルが
存在することを確認します このディレクトリに存在しています ここで すべて動作するか確認できます ML Engineはまったく実行していません
まだDatalabの中で実行しています 確認しているのはPythonモジュールの動作です test.jsonにここで使ったwritefileで この行を書き込みます gcloudコマンドを使って ローカルディレクトリをエクスポート先にして test.jsonを渡し エクスポートが動作し
予測が動作することを確認します このシーケンス全体がPythonモジュールとして
ローカルで動作します 予測はあまり正確ではありません
10の手順でしかトレーニングしていません ただしコードは全部動作します
モデルをトレーニングしてエクスポートし JSON入力を渡し
それを使って予測できます ここでgcloud ml-engineを使って ローカルでトレーニングできます python -mとまったく同じです ただモジュール名とパッケージのパスの指定が 少し違うくらいです Pythonのパス指定は不要です
ML Engineが知っているからです これらのパラメータをすべて指定すると モデルが受け取ります その後 gcloudとpython -mの
どちらを使った場合でも TensorBoardを実行して
モデルを可視化できます ではTensorBoardを開始しましょう
ここにあります 現在のディレクトリを渡します 後ろは削除して これを開始します TensorBoardが開始されたので クリックしてTensorBoardにアクセスできます
ここに表示されています 10個の手順しか実行していませんが
ここに損失の変化が表示されます これが役立つのは
戻ってML Engineで実行したときです Google Cloud Storageディレクトリでも
ポイントでき トレーニング中に変化した
最後の関数を確認できます では下に移動して停止しましょう ただローカルでも使えることを
見せたいだけなので 4122で停止しました 今度はクラウド上で実行しましょう
その際に重要なことがあります データもクラウドに置いてください これから入力ファイルをクラウドにコピーします 今CSVファイルをクラウドにコピーしています ファイルのコピーがすべて終わったら トレーニングジョブを
ML Engineに送信できます 入力でさらに多くの手順を指定して
ML Engineに送信します ジョブがキューに入れられました GCP Consoleに戻って ML Engineがある場所まで
スクロールします [ML Engine]をクリックし
[ジョブ]を見てください ジョブが開始プロセスに入っています ジョブの実行中にログと 実行中に生成されるものを
確認できます 最後にこのモデルをデプロイできます ローカルと同じように予測に使用できます ただし今度は
完全にトレーニングされたモデルです 複数の手順でトレーニングされ
準備が整いました デプロイしたら
gcloud ml-engineで予測を試せます ただしクライアントプログラムのやり方で
実行します クライアントプログラムのやり方は
基本的にJSONの入力を Pythonプログラムから作成します Python APIを使って予測関数を呼び出し
レスポンスを取得します ここではまだ優れたモデルではありません ただ未加工データがあって
モデルに入れているだけです 次のコースでは モデルを
特性エンジニアリングで改良します このパフォーマンスをお見せするために ターゲットデータセットで
トレーニングできますが あまり役に立ちません 優れたモデルではなく
人が分析していません クラウドのトレーニングを
もっと大きなデータセットでも実行できます これらはまったく同じものを実行するので
すべて省略します 時間があったら試してください そして前の演習の問題の解決策を
変更してみてください 練習問題はぜひ実際に試してください そして戻ってCourseraフォーラムで
話し合いましょう