1
00:00:00,000 --> 00:00:04,483
今回はQwiklabsにログインして
ラボを開始します

2
00:00:04,483 --> 00:00:09,281
自分のユーザー名とパスワードで
GCP Consoleにログインします

3
00:00:09,281 --> 00:00:13,388
Datalabを開始して動作させます

4
00:00:13,388 --> 00:00:19,880
最初にすべてのノートブックがある
リポジトリのクローンを作成します

5
00:00:19,890 --> 00:00:24,700
上のGitアイコンを使うと簡単ですが

6
00:00:24,700 --> 00:00:31,273
新しいノートブックを作成して
bash機能を使う方法もあります

7
00:00:31,300 --> 00:00:36,780
では「bash git clone」と
入力していきます

8
00:00:48,392 --> 00:00:51,554
training-data-analystリポジトリを
git cloneします

9
00:00:51,554 --> 00:00:55,280
このラボで使いたいノートブックが
含まれています

10
00:00:59,647 --> 00:01:04,919
ここに先ほどのtraining-data-analystがあります

11
00:01:04,929 --> 00:01:10,580
training-data-analystに入って
フォルダに移動します

12
00:01:10,580 --> 00:01:16,260
ノートブックが入っているので確認します

13
00:01:16,260 --> 00:01:21,430
3番目のコースでCloud ML Engineを確認します

14
00:01:22,810 --> 00:01:26,880
これがCloud ML Engineです
このラボでは

15
00:01:26,880 --> 00:01:30,300
TensorFlowモデルをスケールアップします

16
00:01:30,300 --> 00:01:34,841
すでに作ったモデルと同じですが
今度はPythonモデルで作り

17
00:01:34,841 --> 00:01:37,379
ML Engineで実行します

18
00:01:37,379 --> 00:01:41,799
クラウドで実行するので 最初に

19
00:01:41,799 --> 00:01:46,535
構築するプロジェクトを指定します

20
00:01:46,535 --> 00:01:51,164
QwiklabsからもらったプロジェクトIDはこれです

21
00:01:51,164 --> 00:01:56,353
このプロジェクトIDを
プロジェクトの作成に使用します

22
00:01:56,353 --> 00:01:58,780
そしてバケットです
バケットとは何でしょうか？

23
00:01:58,780 --> 00:02:02,750
バケットを作成しなければなりません

24
00:02:02,750 --> 00:02:08,850
そこでGCP Consoleで
[Storage] [ブラウザ]の順に移動します

25
00:02:10,300 --> 00:02:15,820
バケットがあるか確認して
なければ作成します

26
00:02:15,820 --> 00:02:21,460
バケットの名前は一意にしてください
それにはどうすればいいでしょうか？

27
00:02:21,460 --> 00:02:26,690
1つの方法として
バケット名をプロジェクトと同じ名前にします

28
00:02:26,690 --> 00:02:31,290
よほど運が悪くなければ
この名前のバケットは作成されていません

29
00:02:31,290 --> 00:02:36,730
バケットの名前を作成し
マルチリージョンバケットを作成できます

30
00:02:36,730 --> 00:02:41,690
ここでバケットができました

31
00:02:41,690 --> 00:02:46,270
バケットはプロジェクトと同じ名前なので
とても簡単です

32
00:02:46,270 --> 00:02:47,880
それから

33
00:02:47,880 --> 00:02:53,170
バケットが必要になったら
バケット名とリージョンを指定します

34
00:02:53,170 --> 00:02:55,280
リージョンはとても重要です

35
00:02:55,280 --> 00:02:59,910
リージョンとはML Engineジョブの送信先です

36
00:02:59,910 --> 00:03:02,340
単一リージョンのバケットを
使っていた場合は

37
00:03:02,340 --> 00:03:05,220
同じリージョンに指定したいでしょう

38
00:03:05,220 --> 00:03:08,900
これはマルチリージョンのバケットですが
どちらでもかまいません

39
00:03:08,900 --> 00:03:12,590
コンピューティングに
使いたいリージョンを使えるので

40
00:03:12,590 --> 00:03:14,340
us-centralのままにします

41
00:03:14,340 --> 00:03:17,800
これはDatalabを始めたときと
同じリージョンですが

42
00:03:17,800 --> 00:03:22,420
DatalabインスタンスとML Engineジョブは

43
00:03:22,420 --> 00:03:25,300
別のリージョンでも実行できます

44
00:03:25,300 --> 00:03:29,310
ここではジョブ送信と
ジョブ実行のために作成するマシンは

45
00:03:29,310 --> 00:03:33,656
すべてus-central1にします

46
00:03:33,656 --> 00:03:36,410
ノートブックを保存しておきます

47
00:03:36,410 --> 00:03:41,360
私のプロジェクトで[Run]をクリックします

48
00:03:41,360 --> 00:03:45,350
今度からはShift+Enterを押します
これでも動作します

49
00:03:45,350 --> 00:03:49,820
これでPROJECT、BUCKET、REGIONの変数が
Pythonで作成されます

50
00:03:49,820 --> 00:03:57,310
次のセルは同じ変数を設定します
ただしbashです

51
00:03:57,310 --> 00:04:03,150
Pythonのコマンドos.environで
bashの変数を設定します

52
00:04:03,150 --> 00:04:07,240
これで
ノートブックで$PROJECTや

53
00:04:07,240 --> 00:04:13,870
$BUCKETを使えば
bashスクリプトで該当する変数が取得できます

54
00:04:13,870 --> 00:04:17,519
ここでやっていることは
gcloudはフェイクが使えるので

55
00:04:17,519 --> 00:04:19,950
プロジェクトが設定済みの
このプロジェクトだと設定し

56
00:04:19,950 --> 00:04:23,960
コンピューティングリージョンも
設定済みのリージョンだと設定します

57
00:04:23,960 --> 00:04:29,010
コアプロジェクトとコンピューティング リージョンが
更新されました

58
00:04:29,010 --> 00:04:35,040
ここでCloud ML Engineは
シャドウプロジェクトで動作し

59
00:04:35,040 --> 00:04:40,990
次に全データファイルへのアクセス権を
ML Engineに付与します

60
00:04:40,990 --> 00:04:45,680
ML Engineは人間ではなく
自動化ロボットアカウントです

61
00:04:45,680 --> 00:04:50,710
サービスアカウントであり
このサービスアカウントに

62
00:04:50,710 --> 00:04:55,270
バケット内のファイルへの
読み取り権限を付与します

63
00:04:55,270 --> 00:04:57,570
つまりここでは

64
00:04:57,570 --> 00:05:02,220
ML Engineにバケット内の
既存のファイルと

65
00:05:02,220 --> 00:05:05,540
新規作成するファイルへの
アクセス権を付与します

66
00:05:05,540 --> 00:05:08,070
書き込み権限も付与します

67
00:05:08,070 --> 00:05:11,210
このバケットにチェックポイントや

68
00:05:11,210 --> 00:05:15,000
モデルの出力なども格納するからです

69
00:05:15,000 --> 00:05:20,650
バケットには必要なデータだけを入れてください

70
00:05:20,650 --> 00:05:25,590
ML Engineがアクセスして読み取れるデータです

71
00:05:25,620 --> 00:05:30,510
通常はすべてのデータを保存するバケットは
作成しません

72
00:05:30,510 --> 00:05:34,370
機械学習固有のバケットを作成し

73
00:05:34,370 --> 00:05:39,123
そのファイルだけを保存すると
セキュリティを向上できます

74
00:05:39,123 --> 00:05:41,420
ここで

75
00:05:41,420 --> 00:05:46,960
ML Engineにこのバケットへの
読み書き権限を付与します

76
00:05:50,203 --> 00:05:55,011
付与したらML Engineサービスアカウントを
承認します

77
00:05:55,011 --> 00:06:00,280
アカウントはservice-の後に

78
00:06:00,280 --> 00:06:01,980
プロジェクトIDです

79
00:06:01,980 --> 00:06:06,520
プロジェクトIDを見つけるには
GCP Consoleに移動します

80
00:06:06,520 --> 00:06:10,960
[Home]に移動するとプロジェクトIDがあります

81
00:06:10,960 --> 00:06:13,630
プロジェクト番号はここです
同じプロジェクト番号です

82
00:06:13,630 --> 00:06:17,420
これは覚えなくてかまいません

83
00:06:17,420 --> 00:06:19,330
スクリプトで取得できます

84
00:06:19,330 --> 00:06:27,150
JSON呼び出し response['serviceAccount'] です

85
00:06:28,900 --> 00:06:33,042
次はコードを取り出します

86
00:06:33,042 --> 00:06:39,071
コードは前のラボの実験で
ノートブックに書いたからです

87
00:06:39,104 --> 00:06:45,496
今回は大規模実行するので
コードを送信して実行したいときは

88
00:06:45,499 --> 00:06:48,650
Pythonパッケージにします

89
00:06:48,650 --> 00:06:50,400
それを今から行いましょう

90
00:06:50,400 --> 00:06:53,830
Pythonパッケージを作成します

91
00:06:53,830 --> 00:06:58,110
パッケージtaxifareにこのファイルすべてを入れます

92
00:06:58,110 --> 00:07:01,090
Datalabで確認できます

93
00:07:01,090 --> 00:07:09,450
Datalabに移動すると
taxifareの中にtrainerフォルダがあります

94
00:07:09,450 --> 00:07:13,866
trainerには2つのファイルがあります

95
00:07:13,866 --> 00:07:17,150
task.pyとmodel.pyです

96
00:07:17,150 --> 00:07:19,810
task.pyにはmainが含まれ

97
00:07:19,810 --> 00:07:23,630
同期のたびにコマンドラインをすべて読み

98
00:07:23,630 --> 00:07:27,560
train_data_pathsとtrain_batch_sizeなどを探します

99
00:07:27,560 --> 00:07:30,230
これはコマンドラインから来ます

100
00:07:30,230 --> 00:07:33,430
model.pyにはモデルのコアが含まれます

101
00:07:33,430 --> 00:07:36,730
これは適切なリグレッサーを作成し

102
00:07:36,730 --> 00:07:40,450
データを読み取る入力関数などがあります

103
00:07:41,280 --> 00:07:47,090
ここにパッケージがあります
Pythonのパッケージは

104
00:07:47,090 --> 00:07:52,040
必要なファイルがすべて含まれる
フォルダ構造です

105
00:07:52,040 --> 00:07:54,880
ではmodel.pyを確認しましょう

106
00:07:54,880 --> 00:07:59,980
これは基本的に 以前Datalabの
ノートブックにあったコードで

107
00:07:59,980 --> 00:08:04,350
これをPythonパッケージにします

108
00:08:04,350 --> 00:08:08,380
ここで問題は
どうやってPythonコードを取り出して

109
00:08:08,380 --> 00:08:10,280
パッケージにするかです

110
00:08:10,280 --> 00:08:14,950
1つ簡単な方法があります
Pythonが書かれたものを見てみましょう

111
00:08:14,950 --> 00:08:18,840
このコードをファイルに書き込みます

112
00:08:18,840 --> 00:08:22,700
簡単なのはJupyter Magicの
writefileを使うことです

113
00:08:22,700 --> 00:08:27,190
writefile tensorboard.pyと入力して
実行すると

114
00:08:27,190 --> 00:08:32,940
ここにあるコードが
すべてtensorboard.pyに書き込まれます

115
00:08:32,940 --> 00:08:38,409
このように簡単に
Pythonノートブックのコードを取得して

116
00:08:38,409 --> 00:08:42,820
別のPythonファイルに書き出して
Pythonパッケージに入れられます

117
00:08:42,820 --> 00:08:45,910
writefileは追加のためのオプションもあり

118
00:08:45,910 --> 00:08:50,820
必要に応じてPython .pyに
行を追加できます

119
00:08:50,820 --> 00:08:53,540
これは削除します

120
00:08:53,540 --> 00:08:56,670
書き込まれたtensorboard.pyを見るため

121
00:08:56,670 --> 00:09:00,846
ディレクトリに戻りましょう

122
00:09:00,846 --> 00:09:06,760
03_tensorflowにtensorboard.pyがあります

123
00:09:06,760 --> 00:09:12,802
これが%writefileで書き出したファイルです

124
00:09:12,802 --> 00:09:15,960
では元の場所に戻りましょう

125
00:09:15,960 --> 00:09:20,240
ここではPythonパッケージを作成して

126
00:09:20,240 --> 00:09:24,500
データファイルがあることを確認できます

127
00:09:24,500 --> 00:09:29,370
これがデータファイルで
Datalabのすべてが/contentにマップされ

128
00:09:29,370 --> 00:09:31,770
このディレクトリに入っています

129
00:09:31,770 --> 00:09:35,765
トレーニングの入力ファイルの1行と

130
00:09:35,765 --> 00:09:38,640
検証入力ファイルの1行を出力しています

131
00:09:38,640 --> 00:09:44,850
Pythonパッケージがあるので
実行してみましょう

132
00:09:44,850 --> 00:09:48,190
Pythonパッケージの実行は
ML Engineと関係ありません

133
00:09:48,190 --> 00:09:51,450
Pythonパッケージを実行するには

134
00:09:51,450 --> 00:09:56,310
python -mにモジュールを渡します

135
00:09:56,310 --> 00:10:00,390
モジュール名はtaskですが
trainerパッケージにあるので

136
00:10:00,390 --> 00:10:06,620
Pythonが探す場所を
PYTHONPATHで設定します

137
00:10:06,650 --> 00:10:10,750
それを現在のディレクトリの
/taxifareに設定します

138
00:10:10,750 --> 00:10:14,810
ここにtrainerがあるからです
PYTHONPATHを指定して

139
00:10:14,810 --> 00:10:20,120
pythonプログラムに
taxi-train*とtaxi-validを渡して実行します

140
00:10:20,120 --> 00:10:25,460
このコマンドラインパスが正しいことを
確認します

141
00:10:25,460 --> 00:10:29,760
出力ディレクトリと
少数のトレーニング手順を指定します

142
00:10:29,760 --> 00:10:32,710
手順10個だけでも指定できます

143
00:10:32,710 --> 00:10:35,830
これでShift+Enterで実行できます

144
00:10:35,830 --> 00:10:39,710
ここでPythonモジュールを実行して

145
00:10:39,710 --> 00:10:43,160
動くことを確認します
動いたら

146
00:10:44,180 --> 00:10:48,500
何が書き出されたかを確認できます

147
00:10:48,500 --> 00:10:50,730
全部実行されて

148
00:10:50,730 --> 00:10:55,680
保存されたモデルに書き込まれました
これが重要です

149
00:10:55,680 --> 00:10:59,225
トレーニングが行われて

150
00:10:59,225 --> 00:11:02,380
保存されたモデルがあることを確認します

151
00:11:02,380 --> 00:11:07,720
それにはexport/exporterに保存されたモデルが
存在することを確認します

152
00:11:07,720 --> 00:11:11,406
このディレクトリに存在しています

153
00:11:11,406 --> 00:11:14,819
ここで すべて動作するか確認できます

154
00:11:14,819 --> 00:11:19,433
ML Engineはまったく実行していません
まだDatalabの中で実行しています

155
00:11:19,433 --> 00:11:23,431
確認しているのはPythonモジュールの動作です

156
00:11:23,431 --> 00:11:28,344
test.jsonにここで使ったwritefileで

157
00:11:28,344 --> 00:11:32,030
この行を書き込みます

158
00:11:32,030 --> 00:11:36,370
gcloudコマンドを使って

159
00:11:36,370 --> 00:11:41,080
ローカルディレクトリをエクスポート先にして

160
00:11:41,080 --> 00:11:47,110
test.jsonを渡し エクスポートが動作し
予測が動作することを確認します

161
00:11:47,110 --> 00:11:53,270
このシーケンス全体がPythonモジュールとして
ローカルで動作します

162
00:11:53,270 --> 00:11:56,620
予測はあまり正確ではありません
10の手順でしかトレーニングしていません

163
00:11:56,620 --> 00:12:01,530
ただしコードは全部動作します
モデルをトレーニングしてエクスポートし

164
00:12:01,530 --> 00:12:06,760
JSON入力を渡し
それを使って予測できます

165
00:12:06,760 --> 00:12:10,446
ここでgcloud ml-engineを使って

166
00:12:10,446 --> 00:12:14,090
ローカルでトレーニングできます

167
00:12:14,090 --> 00:12:18,720
python -mとまったく同じです

168
00:12:18,720 --> 00:12:22,200
ただモジュール名とパッケージのパスの指定が

169
00:12:22,200 --> 00:12:25,230
少し違うくらいです

170
00:12:25,230 --> 00:12:29,520
Pythonのパス指定は不要です
ML Engineが知っているからです

171
00:12:29,520 --> 00:12:32,990
これらのパラメータをすべて指定すると

172
00:12:32,990 --> 00:12:36,440
モデルが受け取ります

173
00:12:36,440 --> 00:12:41,340
その後 gcloudとpython -mの
どちらを使った場合でも

174
00:12:41,340 --> 00:12:47,140
TensorBoardを実行して
モデルを可視化できます

175
00:12:47,140 --> 00:12:54,570
ではTensorBoardを開始しましょう
ここにあります

176
00:12:55,690 --> 00:13:02,241
現在のディレクトリを渡します

177
00:13:02,241 --> 00:13:04,639
後ろは削除して

178
00:13:10,398 --> 00:13:14,950
これを開始します

179
00:13:14,950 --> 00:13:18,267
TensorBoardが開始されたので

180
00:13:18,267 --> 00:13:23,203
クリックしてTensorBoardにアクセスできます
ここに表示されています

181
00:13:23,203 --> 00:13:27,800
10個の手順しか実行していませんが
ここに損失の変化が表示されます

182
00:13:27,800 --> 00:13:32,460
これが役立つのは
戻ってML Engineで実行したときです

183
00:13:32,460 --> 00:13:36,290
Google Cloud Storageディレクトリでも
ポイントでき

184
00:13:36,290 --> 00:13:40,820
トレーニング中に変化した
最後の関数を確認できます

185
00:13:40,820 --> 00:13:44,230
では下に移動して停止しましょう

186
00:13:44,230 --> 00:13:48,300
ただローカルでも使えることを
見せたいだけなので

187
00:13:48,300 --> 00:13:52,920
4122で停止しました

188
00:13:52,920 --> 00:13:57,400
今度はクラウド上で実行しましょう
その際に重要なことがあります

189
00:13:57,400 --> 00:14:01,370
データもクラウドに置いてください

190
00:14:01,370 --> 00:14:04,280
これから入力ファイルをクラウドにコピーします

191
00:14:04,280 --> 00:14:10,960
今CSVファイルをクラウドにコピーしています

192
00:14:10,990 --> 00:14:15,180
ファイルのコピーがすべて終わったら

193
00:14:15,180 --> 00:14:19,080
トレーニングジョブを
ML Engineに送信できます

194
00:14:19,080 --> 00:14:26,300
入力でさらに多くの手順を指定して
ML Engineに送信します

195
00:14:26,300 --> 00:14:29,470
ジョブがキューに入れられました

196
00:14:29,470 --> 00:14:35,510
GCP Consoleに戻って

197
00:14:35,510 --> 00:14:38,770
ML Engineがある場所まで
スクロールします

198
00:14:38,770 --> 00:14:42,410
[ML Engine]をクリックし
[ジョブ]を見てください

199
00:14:42,410 --> 00:14:48,080
ジョブが開始プロセスに入っています

200
00:14:48,080 --> 00:14:52,070
ジョブの実行中にログと

201
00:14:52,070 --> 00:14:58,210
実行中に生成されるものを
確認できます

202
00:14:58,210 --> 00:15:03,290
最後にこのモデルをデプロイできます

203
00:15:03,290 --> 00:15:07,270
ローカルと同じように予測に使用できます

204
00:15:07,270 --> 00:15:09,940
ただし今度は
完全にトレーニングされたモデルです

205
00:15:09,940 --> 00:15:13,260
複数の手順でトレーニングされ
準備が整いました

206
00:15:14,540 --> 00:15:19,840
デプロイしたら
gcloud ml-engineで予測を試せます

207
00:15:19,840 --> 00:15:22,530
ただしクライアントプログラムのやり方で
実行します

208
00:15:22,530 --> 00:15:27,700
クライアントプログラムのやり方は
基本的にJSONの入力を

209
00:15:27,700 --> 00:15:30,580
Pythonプログラムから作成します

210
00:15:30,580 --> 00:15:36,870
Python APIを使って予測関数を呼び出し
レスポンスを取得します

211
00:15:36,870 --> 00:15:42,070
ここではまだ優れたモデルではありません

212
00:15:42,070 --> 00:15:45,850
ただ未加工データがあって
モデルに入れているだけです

213
00:15:45,850 --> 00:15:49,690
次のコースでは モデルを
特性エンジニアリングで改良します

214
00:15:49,690 --> 00:15:52,880
このパフォーマンスをお見せするために

215
00:15:52,880 --> 00:15:56,730
ターゲットデータセットで
トレーニングできますが あまり役に立ちません

216
00:15:56,730 --> 00:16:00,240
優れたモデルではなく
人が分析していません

217
00:16:00,240 --> 00:16:03,810
クラウドのトレーニングを
もっと大きなデータセットでも実行できます

218
00:16:03,810 --> 00:16:07,710
これらはまったく同じものを実行するので
すべて省略します

219
00:16:07,710 --> 00:16:11,980
時間があったら試してください

220
00:16:11,980 --> 00:16:16,550
そして前の演習の問題の解決策を
変更してみてください

221
00:16:16,550 --> 00:16:21,460
練習問題はぜひ実際に試してください

222
00:16:21,460 --> 00:16:24,930
そして戻ってCourseraフォーラムで
話し合いましょう