Let's take a few minutes to discuss how monitoring our jobs works. Once you've submitted a job to execute on the Cloud Machine Learning Engine, there's a variety of ways to check in on it. The simplest one is to just get its current state. This will tell you if it's pending, running, or already done. Of course, once it's running, you might want to inspect the most recent log entries from that job, which you can also do with GCloud. Finally, when you're submitting many jobs to run in parallel, you should try out GCloud's ability to list and filter jobs. The GCP Web console has a great UI for monitoring your jobs. You can see exactly how they were invoked, check out their logs, and see how much CPU and memory they are consuming. While inspecting log entries may help you debug technical issues like an exception, it's really not the right tool to investigate the ML performance. TensorBoard however, is a great tool. To use it, make sure your job saves summary data the Google cloud storage location, and then when you start TensorBoard, simply provide that directory. You can even handle multiple jobs per folder. Now that we've got a model, let's see what we can do with it. Once our training job completes, we'll have a tensorflow model ready to serve for predictions. Cloud ML engine provides a great infrastructure for this. CMLE we'll build you a production ready Web app out of your train model, and offer a batch service for your less latency sensitive predictions. Since these are both rest APIs, you'll be able to make scalable secure inferences from whatever language you want to write the client in. So, to send your TF model artifact to cloud for serving, we need to create a CMLE model inversion resource. The individual TF trained model file you have will correspond to a specific version. On CMLE, a model is actually a group of these versions that has a default version as well. This extra layer of abstraction and grouping allows us to seamlessly migrate traffic from one TF model version to the next. Just need to change in models default version. Here's a simple example of how to use the remotely deployed model for predictions with the rest call. CMLE online prediction is a completely serverless system so you don't have to worry about any resource allocations. It will just scale for you.