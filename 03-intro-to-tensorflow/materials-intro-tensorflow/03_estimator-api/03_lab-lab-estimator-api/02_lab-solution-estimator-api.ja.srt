1
00:00:00,190 --> 00:00:03,732
最初のEstimator Codelabに
取りかかりましょう

2
00:00:03,732 --> 00:00:07,920
まず Codelabの
ファイルがある場所を見つけます

3
00:00:07,920 --> 00:00:09,980
その方法をお見せしましょう

4
00:00:09,980 --> 00:00:18,410
cloud.google.com/consoleに移動し
ラボのアカウントでこのようにログインします

5
00:00:30,700 --> 00:00:33,550
次に プロジェクトを選択します

6
00:00:33,550 --> 00:00:35,980
すでに選択済みの場合もあります

7
00:00:36,650 --> 00:00:40,280
Cloud Shellを開きます
このアイコンです

8
00:00:44,109 --> 00:00:51,326
Cloud Shellで「datalab create」
（データラボの作成）と入力します

9
00:00:54,831 --> 00:01:00,060
Datalabインスタンスに
「mylab」という名前を付けましょう

10
00:01:01,110 --> 00:01:07,644
これをus-central1-aゾーンに作成します

11
00:01:09,494 --> 00:01:13,410
これで最初のDatalabインスタンスが
作成されます

12
00:01:14,750 --> 00:01:18,722
初めてのときは 少し時間がかかります

13
00:01:18,722 --> 00:01:20,822
（数分後）

14
00:01:21,886 --> 00:01:26,208
次回はインスタンスのデータを
再作成する必要はなく

15
00:01:26,208 --> 00:01:30,815
再接続するだけです
それには「datalab connect」と入力し

16
00:01:30,815 --> 00:01:34,579
インスタンスのデータを入力します
ここでは「mylab」です

17
00:01:47,098 --> 00:01:52,030
ここに[ウェブでプレビュー]ボタンがあります

18
00:01:53,140 --> 00:01:57,919
[ポートの変更]でポートを8081に変更します
Datalabはこれを使います

19
00:01:58,380 --> 00:02:00,370
[変更してプレビュー]をクリックすると

20
00:02:00,370 --> 00:02:03,960
おなじみのノートブック
インターフェースが開きます

21
00:02:06,908 --> 00:02:10,779
ここからGitHubのコードを
取得する必要があります

22
00:02:10,779 --> 00:02:17,385
それで ノートブックを開いて
bashコマンドを入力します

23
00:02:19,015 --> 00:02:21,720
%bash

24
00:02:21,720 --> 00:02:24,539
さらにgit cloneで

25
00:02:24,539 --> 00:02:33,541
Codelabのリポジトリを
ローカルディレクトリにコピーします

26
00:02:33,541 --> 00:02:36,650
完了するとすぐに

27
00:02:36,650 --> 00:02:41,189
ローカルディレクトリがここに表示されます

28
00:02:47,488 --> 00:02:49,160
できました

29
00:02:49,920 --> 00:02:52,884
training-data-analystという名前で

30
00:02:52,884 --> 00:02:59,344
この中にcoursesという
ディレクトリが見つかるはずです

31
00:02:59,344 --> 00:03:04,584
さらにその中にmachine_learning、
deepdive、

32
00:03:05,664 --> 00:03:09,460
tensorflowがあります

33
00:03:09,460 --> 00:03:15,104
最初のラボは「b_estimator.ipynb」です

34
00:03:16,707 --> 00:03:21,803
この例ではpandasを使って
CSVファイルからデータを読み込みます

35
00:03:21,803 --> 00:03:25,260
そこにはタクシーの乗車情報が含まれています

36
00:03:25,260 --> 00:03:29,630
乗車の場所、降車の場所、乗客の数です

37
00:03:29,630 --> 00:03:33,309
モデルをトレーニングして
タクシー料金を予測します

38
00:03:34,580 --> 00:03:36,520
では取りかかりましょう

39
00:03:37,816 --> 00:03:40,889
まず列名をここで定義します

40
00:03:40,889 --> 00:03:44,370
fare_amount、pickuplon、pickuplatなどです

41
00:03:46,167 --> 00:03:52,890
pandasを使ってこのデータを
CSVファイルから読み込みます

42
00:03:52,890 --> 00:03:57,340
トレーニング用データセットが1つ
検証用データセットが1つです

43
00:04:00,880 --> 00:04:05,201
次にEstimatorの組み込み機能を使って

44
00:04:05,201 --> 00:04:10,547
pandasからinput関数を作成します

45
00:04:10,547 --> 00:04:15,594
tf.estimators.inputs.pandas_input_fn
という名前です

46
00:04:15,594 --> 00:04:20,702
この関数でfeaturesをxとして
このように指定し

47
00:04:20,702 --> 00:04:24,920
ターゲットとなるlabelをyとして
指定できます

48
00:04:24,920 --> 00:04:28,750
また トレーニング データセット用の
標準設定も揃っています

49
00:04:28,750 --> 00:04:32,497
バッチサイズ、エポック数、シャッフルです

50
00:04:32,497 --> 00:04:37,430
queue_capacityは
シャッフルキューのバッファです

51
00:04:38,370 --> 00:04:40,180
これを実行しましょう

52
00:04:41,410 --> 00:04:44,730
次にfeature column（特徴列）を作成します

53
00:04:44,730 --> 00:04:46,580
すべて数値列です

54
00:04:47,275 --> 00:04:51,170
それぞれに対して
tf.feature_column.numeric_columnを呼び出します

55
00:04:51,170 --> 00:04:53,570
feature column（特徴列）の一覧では

56
00:04:53,570 --> 00:04:57,460
データを入力ベクターに入れる方法を
モデルに指示します

57
00:05:00,069 --> 00:05:02,875
モデルはここでインスタンス化されます

58
00:05:02,875 --> 00:05:04,505
ここです

59
00:05:05,440 --> 00:05:09,560
feature_columnsの一覧と
ディレクトリを指定します

60
00:05:09,560 --> 00:05:12,730
そこに すべての出力データが書き込まれます

61
00:05:14,250 --> 00:05:16,320
モデルをトレーニングするには

62
00:05:16,320 --> 00:05:20,030
train関数を指定し
データ入力関数inputを渡します

63
00:05:20,030 --> 00:05:23,742
train ... そしてinput関数

64
00:05:24,689 --> 00:05:29,519
これでpandasのデータフレームから
データをモデルに取得します

65
00:05:33,596 --> 00:05:36,760
モデルがエポック数10回で動作しています

66
00:05:36,760 --> 00:05:40,220
トレーニングのログはこれです

67
00:05:40,220 --> 00:05:42,180
トレーニングが終わりました

68
00:05:42,180 --> 00:05:43,792
出来はどうでしょうか？

69
00:05:43,792 --> 00:05:46,710
検証用のデータセットで試しましょう

70
00:05:46,710 --> 00:05:52,371
そのためにmodel.evaluateを
このように呼び出します

71
00:05:52,371 --> 00:05:56,107
解析しています
今回はデータ入力関数inputが

72
00:05:56,107 --> 00:06:01,800
df_validつまり検証用pandasデータフレームから
データを取得します

73
00:06:01,800 --> 00:06:03,860
ここで解析します

74
00:06:07,756 --> 00:06:10,770
結果が出ました

75
00:06:10,770 --> 00:06:17,428
最後のRMSE（平均二乗平方根誤差）は
$10です

76
00:06:17,428 --> 00:06:20,740
$10はタクシー料金としては大きな誤差です

77
00:06:20,740 --> 00:06:23,800
前のベンチマークの$6からも遠いです

78
00:06:23,800 --> 00:06:29,790
あとでこれを改善します
今は この手元のコードを使って

79
00:06:29,790 --> 00:06:33,199
このモデルを予測に使えるか見てみましょう

80
00:06:38,591 --> 00:06:41,072
モデルを再びインスタンス化するとき

81
00:06:41,072 --> 00:06:45,650
modelディレクトリ内のチェックポイントを探し
そこからリロードします

82
00:06:45,650 --> 00:06:49,780
モデルのトレーニングが完了し
チェックポイントができたので

83
00:06:49,780 --> 00:06:51,819
予測の準備ができました

84
00:06:53,250 --> 00:06:57,890
ここでインスタンス化して
同じ出力ディレクトリを渡します

85
00:07:01,852 --> 00:07:09,300
predict関数をここで呼び出して
Pythonジェネレーターを返します

86
00:07:09,300 --> 00:07:14,214
これをループの中で呼び出して
予測した料金を取得します

87
00:07:14,214 --> 00:07:18,570
ここで予測した料金を確認できます

88
00:07:18,570 --> 00:07:22,690
なぜRMSEが高かったのか
これでわかるでしょう

89
00:07:22,690 --> 00:07:26,550
モデルは基本的に すべての乗車で
同じ金額を予測します

90
00:07:26,550 --> 00:07:29,030
複雑なモデルなら役立つでしょうか？

91
00:07:29,030 --> 00:07:31,600
DNNを使って
試してみましょう

92
00:07:31,600 --> 00:07:35,680
feature_columns、input関数などを
全部このままにします

93
00:07:35,680 --> 00:07:38,740
モデルをlinearRegressorから

94
00:07:38,740 --> 00:07:42,660
3つの隠れレイヤを持つ
DNNRegressorに変更します

95
00:07:45,930 --> 00:07:47,990
やってみましょう

96
00:07:47,990 --> 00:07:53,822
ここをDNNRegressorに変更し
ここで隠れレイヤを構成します

97
00:07:53,822 --> 00:07:57,999
最初のは32ノード
2番目は8ノード、最後は2ノードです

98
00:08:00,022 --> 00:08:01,740
トレーニングしましょう

99
00:08:01,740 --> 00:08:04,868
10回のエポック数でトレーニングします

100
00:08:05,931 --> 00:08:13,584
最後にmodel.predict関数を再び

101
00:08:13,584 --> 00:08:17,609
print_rmseヘルパーから呼び出します

102
00:08:25,165 --> 00:08:27,600
トレーニングがずっと続きます

103
00:08:32,106 --> 00:08:34,189
今 検証しています

104
00:08:35,634 --> 00:08:41,196
検証用データセットのRMSEは
今回は$11です

105
00:08:41,196 --> 00:08:43,610
まだ悪いですね

106
00:08:43,610 --> 00:08:47,010
どちらのモデルも
ベンチマークモデルに負けています

107
00:08:47,010 --> 00:08:48,380
なぜ？

108
00:08:48,380 --> 00:08:51,227
TensorFlowを機械学習に使っていますが

109
00:08:51,227 --> 00:08:53,583
たぶん使い方が悪いのでしょう

110
00:08:53,583 --> 00:08:56,980
このコースの残りの部分で
これを取り上げます

111
00:08:56,980 --> 00:09:00,800
でも 記録用に2つのモデルの
どちらかを選ぶ必要があるなら

112
00:09:00,800 --> 00:09:05,150
検証エラーが低いものを選ぶでしょう

113
00:09:05,150 --> 00:09:10,650
そして最後に 選択したモデルを使って
テストデータでRMSEを測定するでしょう

114
00:09:10,650 --> 00:09:16,010
この最終RMSEを 最適モデルの
目標パフォーマンスとして公表できます

115
00:09:16,010 --> 00:09:20,930
データサイエンスの標準的な手順
つまりトレーニング、評価、テストでは

116
00:09:20,930 --> 00:09:24,060
それぞれ別個のデータセットを使用します

117
00:09:24,060 --> 00:09:27,101
ベンチマーク用のデータセットで試しましょう

118
00:09:27,101 --> 00:09:35,478
ベンチマーク用データセットの
RMSEは9.41です

119
00:09:57,647 --> 00:10:03,794
このように ベンチマーク用データセットの
RMSEは$10.5です

120
00:10:03,794 --> 00:10:08,914
これは元のベンチマーク
$6よりかなり多く しかも

121
00:10:08,914 --> 00:10:14,264
距離ベースのルールRMSE
$8より悪いです

122
00:10:14,264 --> 00:10:18,550
ここまでTensorFlowのモデルの
作成方法を学びました

123
00:10:18,550 --> 00:10:20,900
しかし モデルを改善して

124
00:10:20,900 --> 00:10:24,670
パフォーマンスを向上させる必要があります

125
00:10:24,670 --> 00:10:27,090
それについては
後の章で見ていきます

126
00:10:27,090 --> 00:10:30,115
この章では
そのような改善を可能にするために

127
00:10:30,115 --> 00:10:33,415
TensorFlowモデルを準備していきます