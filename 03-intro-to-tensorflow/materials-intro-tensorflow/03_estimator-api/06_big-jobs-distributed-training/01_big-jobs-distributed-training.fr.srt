1
00:00:00,570 --> 00:00:03,430
Nous allons voir comment traiter
les tâches de ML importantes

2
00:00:03,430 --> 00:00:06,440
et nous allons parler des avantages
de l'entraînement distribué.

3
00:00:06,440 --> 00:00:09,220
Nous pouvons cocher
le premier élément de la liste

4
00:00:09,220 --> 00:00:14,510
des problèmes liés aux modèles réels,
et passer au deuxième point, la vitesse.

5
00:00:14,655 --> 00:00:17,700
L'entraînement des modèles réels
durent en effet très longtemps :

6
00:00:17,700 --> 00:00:20,370
des heures, des jours, voire des semaines.

7
00:00:20,370 --> 00:00:24,145
Chez Google, lorsque l'entraînement prend
plusieurs semaines, on abandonne.

8
00:00:24,145 --> 00:00:28,125
Ce n'est pas possible d'optimiser
un modèle dans ces conditions.

9
00:00:28,125 --> 00:00:32,700
Il faut le distribuer sur un cluster
pour accélérer le processus.

10
00:00:33,250 --> 00:00:36,570
Cela peut paraître compliqué
de passer d'une à plusieurs machines.

11
00:00:36,570 --> 00:00:40,355
Mais comme nous allons le voir,
grâce à l'API Estimator et à ML Engine

12
00:00:40,355 --> 00:00:42,275
qui gèrent le cluster automatiquement,

13
00:00:42,275 --> 00:00:44,920
vous profitez
d'une distribution prête à l'emploi.

14
00:00:45,270 --> 00:00:48,740
La fonction qui implémente
l'entraînement distribué s'appelle

15
00:00:48,740 --> 00:00:51,560
"estimator.train_and_evaluate".

16
00:00:51,765 --> 00:00:55,065
Le nom de cette fonction montre
aussi qu'il est important d'évaluer

17
00:00:55,065 --> 00:00:58,695
et de surveiller les tâches
d'entraînement à grande échelle.

18
00:00:58,905 --> 00:01:00,435
Nous en reparlerons plus tard.

19
00:01:00,435 --> 00:01:02,885
Concentrons-nous
pour l'instant sur la distribution.

20
00:01:03,445 --> 00:01:06,690
Le modèle de distribution traditionnel
pour les réseaux de neurones

21
00:01:06,690 --> 00:01:09,285
d'entraînement s'appelle
le "parallélisme de donnée".

22
00:01:09,285 --> 00:01:13,170
Votre modèle est répliqué
sur plusieurs nœuds de calcul.

23
00:01:13,170 --> 00:01:16,665
À chaque étape d'entraînement,
ces nœuds chargent un lot

24
00:01:16,665 --> 00:01:19,280
de données d'entraînement
à chaque fois différent,

25
00:01:19,280 --> 00:01:24,280
calculent son gradient,
puis l'envoient à un ou plusieurs serveurs

26
00:01:24,280 --> 00:01:28,085
de paramètres centraux, qui contiennent
toutes les pondérations et tous les biais

27
00:01:28,085 --> 00:01:29,770
du modèle de réseau de neurones.

28
00:01:29,770 --> 00:01:32,095
Les gradients sont appliqués
lorsqu'ils arrivent,

29
00:01:32,095 --> 00:01:34,130
et modifient
les pondérations et les biais.

30
00:01:34,130 --> 00:01:37,200
Puis le modèle mis à jour
est renvoyé aux nœuds

31
00:01:37,200 --> 00:01:39,140
pour la prochaine étape d'entraînement.

32
00:01:39,495 --> 00:01:42,445
De nombreuses étapes
sont nécessaires pour arriver là.

33
00:01:42,530 --> 00:01:45,730
Les nœuds doivent être démarrés,
puis recevoir leur copie du modèle.

34
00:01:45,730 --> 00:01:47,420
Les flux de données entre les nœuds

35
00:01:47,420 --> 00:01:49,800
et les serveurs de paramètres
doivent être établis.

36
00:01:49,800 --> 00:01:52,180
Le système doit gérer
les exceptions et les échecs,

37
00:01:52,180 --> 00:01:56,990
et redémarrer les nœuds de terrain là
où ils se sont arrêtés en cas d'incident.

38
00:01:57,460 --> 00:02:02,005
Les points de contrôle sont aussi
plus complexes avec tout cela.

39
00:02:02,440 --> 00:02:06,085
Heureusement pour vous, il vous suffira
d'écrire un fichier de configuration

40
00:02:06,085 --> 00:02:07,470
pour assurer la distribution.

41
00:02:07,725 --> 00:02:11,650
Tout le code récurrent
est déjà écrit dans l'API Estimator

42
00:02:11,650 --> 00:02:15,495
et dans la fonction
"estimator.train_and_evaluate".

43
00:02:15,955 --> 00:02:17,805
Vous avez quatre choses à faire :

44
00:02:17,805 --> 00:02:21,250
choisir votre estimateur,
fournir une configuration d'exécution,

45
00:02:21,250 --> 00:02:24,975
assurer l'entraînement et tester
les données via les commandes

46
00:02:24,975 --> 00:02:26,520
"train_spec" et "eval_spec".

47
00:02:26,520 --> 00:02:30,195
Une fois tout cela configuré, il vous
suffit d'appeler "train_and_evaluate".

48
00:02:30,195 --> 00:02:31,925
Si vous êtes sur ML Engine

49
00:02:31,925 --> 00:02:34,525
et si vous avez spécifié
la taille du cluster,

50
00:02:34,525 --> 00:02:37,050
l'entraînement distribué se lancera.

51
00:02:37,250 --> 00:02:40,795
Voyons cela plus en détail, en commençant
par la configuration d'exécution.

52
00:02:40,795 --> 00:02:44,595
C'est là que vous spécifiez le répertoire
de sortie pour les points de contrôle.

53
00:02:44,595 --> 00:02:48,490
Vous pouvez la configurer directement
lors de l'instanciation de l'estimateur.

54
00:02:48,490 --> 00:02:50,665
Mais c'est plus simple de l'avoir ici

55
00:02:50,665 --> 00:02:53,060
avec les autres paramètres
de points de contrôle.

56
00:02:53,270 --> 00:02:56,690
En effet, c'est ici aussi
que vous allez définir la fréquence

57
00:02:56,690 --> 00:02:59,915
des points de contrôle,
mais aussi la fréquence

58
00:02:59,915 --> 00:03:02,155
des journaux d'entraînement
ou de vos résumés.

59
00:03:02,155 --> 00:03:03,750
Nous y reviendrons plus tard.

60
00:03:04,430 --> 00:03:08,140
"train_spec" vous permet de transmettre
votre fonction d'entrée de données

61
00:03:08,140 --> 00:03:09,710
pour les données d'entraînement.

62
00:03:09,835 --> 00:03:12,705
Utilisez l'API Dataset
pour le configurer correctement.

63
00:03:13,270 --> 00:03:17,380
Vous pouvez aussi limiter l'entraînement
à un certain nombre d'étapes.

64
00:03:17,380 --> 00:03:19,465
Par défaut, l'entraînement continue

65
00:03:19,465 --> 00:03:22,175
jusqu'à ce que l'ensemble
de données d'entrée soit épuisé,

66
00:03:22,175 --> 00:03:24,375
ce qui peut arriver
après plusieurs itérations,

67
00:03:24,375 --> 00:03:26,280
si vous avez
choisi cette configuration.

68
00:03:27,035 --> 00:03:30,885
"eval_spec" vous permet d'associer
votre ensemble de données de test.

69
00:03:31,265 --> 00:03:34,420
En effet, si vous voulez analyser
les performances de votre modèle,

70
00:03:34,420 --> 00:03:39,685
vous devez utiliser un ensemble
qu'il n'a pas utilisé à l'entraînement,

71
00:03:39,685 --> 00:03:43,960
généralement un sous-ensemble
de vos données réservé au test.

72
00:03:43,960 --> 00:03:47,270
Les données de test sont intégrées
via une fonction d'entrée "eval".

73
00:03:47,270 --> 00:03:50,725
Ici encore, utilisez l'API Dataset.

74
00:03:50,725 --> 00:03:55,064
Vous devez aussi spécifier combien de lots
de données de test vous voulez utiliser

75
00:03:55,064 --> 00:03:59,180
pour l'évaluation,
ainsi que la fréquence des évaluations.

76
00:03:59,760 --> 00:04:04,095
N'oubliez pas
qu'avec l'entraînement distribué,

77
00:04:04,095 --> 00:04:07,575
l'évaluation
se produit sur un serveur dédié,

78
00:04:07,575 --> 00:04:11,095
qui répond au modèle
à partir du dernier point de contrôle,

79
00:04:11,095 --> 00:04:12,785
puis exécute l'évaluation.

80
00:04:12,785 --> 00:04:16,555
La fréquence des évaluations
ne peut pas être supérieure

81
00:04:16,555 --> 00:04:18,674
à la fréquence des points de contrôle

82
00:04:18,674 --> 00:04:20,844
spécifiée
dans la configuration d'exécution.

83
00:04:20,844 --> 00:04:23,310
Vous pouvez en revanche
diminuer la fréquence

84
00:04:23,310 --> 00:04:27,030
en ajoutant le paramètre de limitation
dans la commande "eval-spec".

85
00:04:28,310 --> 00:04:32,195
Vous pouvez voir que cette commande
possède aussi un paramètre "exporters".

86
00:04:32,625 --> 00:04:35,270
Ce paramètre contrôle
l'exportation d'un modèle

87
00:04:35,270 --> 00:04:36,890
pour le déploiement en production.

88
00:04:36,890 --> 00:04:39,510
Nous en parlerons
dans le prochain chapitre.

89
00:04:40,120 --> 00:04:41,845
Récapitulons.

90
00:04:41,845 --> 00:04:43,865
Vous instanciez un estimateur,

91
00:04:43,865 --> 00:04:45,425
vous configurez son exécution

92
00:04:45,425 --> 00:04:48,860
qui vous permet de définir
la fréquence et l'emplacement

93
00:04:48,860 --> 00:04:52,120
d'écriture des points de contrôle
et des autres données de contrôle,

94
00:04:52,120 --> 00:04:56,735
puis vous configurez un ensemble de
données d'entraînement et d'évaluation

95
00:04:56,735 --> 00:05:03,390
via les fonctions d'entrée de données
de "train_spec" et "eval_spec".

96
00:05:03,390 --> 00:05:06,775
Vous êtes maintenant prêt à passer
à l'entraînement et à l'évaluation.

97
00:05:07,165 --> 00:05:11,140
J'aimerais m'arrêter un instant
sur un point pratique important :

98
00:05:11,140 --> 00:05:12,380
le brassage de données.

99
00:05:12,790 --> 00:05:15,150
L'algorithme de descente
de gradient stochastique

100
00:05:15,150 --> 00:05:18,570
que les réseaux de neurones utilisent
pour l'entraînement ne fonctionne

101
00:05:18,570 --> 00:05:20,435
que sur les données brassées.

102
00:05:20,435 --> 00:05:23,790
L'API Dataset possède une fonction
de brassage qui peut être utile,

103
00:05:23,790 --> 00:05:26,410
mais certaines personnes
pensent ne pas en avoir besoin,

104
00:05:26,410 --> 00:05:30,170
car elles estiment que leur ensemble de
données est déjà bien brassé sur disque.

105
00:05:30,170 --> 00:05:32,400
Avec l'entraînement distribué, attention !

106
00:05:32,400 --> 00:05:35,490
Même avec un ensemble de données
bien brassé sur disque,

107
00:05:35,490 --> 00:05:39,660
si tous vos nœuds de calcul sont chargés
à partir de cet ensemble de données,

108
00:05:39,660 --> 00:05:42,935
ils verront le même lot de données
au même moment,

109
00:05:42,935 --> 00:05:45,245
et ils produiront les mêmes gradients.

110
00:05:45,535 --> 00:05:48,360
Les avantages de l'entraînement
distribué sont alors perdus.

111
00:05:48,360 --> 00:05:52,350
Vos différents nœuds de calcul
font exactement la même chose.

112
00:05:52,350 --> 00:05:56,394
Avec "dataset.shuffle",
le brassage se produit indépendamment

113
00:05:56,394 --> 00:05:58,640
sur chaque nœud
à l'aide d'une source aléatoire.

114
00:05:58,640 --> 00:05:59,985
Préférez donc cette méthode,

115
00:05:59,985 --> 00:06:02,955
même si vos données
sont déjà brassées sur disque.

116
00:06:03,545 --> 00:06:08,495
Et pour être sûr de vous, vous pouvez
aussi brasser la liste de noms de fichiers

117
00:06:08,495 --> 00:06:10,400
dans votre ensemble de données segmenté.

118
00:06:10,400 --> 00:06:13,620
"list_files" renvoie un ensemble
de données de noms de fichiers.

119
00:06:13,620 --> 00:06:15,970
Il vous suffit donc
d'appeler "shuffle" dessus.