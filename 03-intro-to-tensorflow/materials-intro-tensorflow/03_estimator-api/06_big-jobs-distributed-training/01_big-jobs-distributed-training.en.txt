Now, we'll discuss what to do about large ML jobs and the benefits of distributed training. We can check off the first item in our checklist of annoying problems that the real world models have, and move onto the second one, speed. Yes. Real models train for ages, hours, days, weeks. When it gets to multiple weeks of training, at Google we actually give up. Optimizing a model in these conditions, is not workable. You have to distribute it on a cluster to make it faster. Going from one machine to many, might sound complicated, but as we will see, with the estimator API and ML engine managing the cluster automatically, you get distribution out of the box. The function that implements distributed training is called estimator.train and evaluate. The name of the function also highlights that evaluating and monitoring a large training job will be important. We will see that later. Let's focus for now on distribution. The traditional distribution model for training neural networks is called data parallelism. Your model is replicated on multiple workers. At each training steps, these load a batch of training data, hopefully a different one each, computer gradients and send them to one or several central parameter servers, which hold all the weights and biases of the neural network model. The gradients are applied as they arrive. Change the weights and biases, and the updated model is then sent back to workers for the next step of training. There is a lot of work to do to make this happen. Workers must be started, then receive their copy of the model, data flows between workers, and parameter servers must be established, the system must also handle exceptions, and failures and restart fieldworkers from where they left off and if an incident occurs and, check pointing also becomes a bit more complicated, when all this is going on. Fortunately for you, distribution will be as simple as writing a config file. All the boilerplate code is already written in the estimator API, and the estimator.train and evaluate function. You will need to do four things. Choose your estimator, provide a run configuration, and provide training, and test data through a TrainSpec and an EvalSpec. Once that is set up, call train and evaluate. And if you are running on ML engine, and have specified the cluster size, distributed training will kick in. Let's have a closer look. The run config first. This is where you specify the output directory for checkpoints. You can still set it directly, when instantiating the estimator, but it's cleaner to have it here, along with other checkpoint settings. Indeed, this is also where you set the frequency at which you want to see checkpoints, and also the frequency of your trading logs or summaries. We will come to that later. The TrainSpec, is where you pass in your data input function for training data. Please use the data set API to set it up properly. Optionally, you can limit the training to a given number of steps. By default, training proceeds until the input data set is exhausted. Which might happen after multiple e-bugs if that's how you set it up. The EvalSpec, is where you plug in your test data set. Yes, if you want to see how well your model is doing, you have to measure that on a data set that it has not seen during training. Usually a subset of your data that you set aside for testing. The test data comes in through an eval input function, and again, please use the data set API to get it. You also get to specify on how many batches of test data you want to evaluate, and how frequently evaluations happen. One implementation detail to bear in mind, in distributed training, evaluation happens on a dedicated server, which responds the model from the latest checkpoint and then runs eval. So, you cannot get evaluations more frequently than the check points frequency you entered in your run config. You can, however, get them less frequently, by adding the throttling parameter in the EvalSpec. You notice that the EvalSpec also has a parameter for exporters. They control how a model is exported for deployment to production, and we will cover them in the next chapter. Here is what we have so far. You instantiate an estimator, give it a run config where you can set how often and in which folder are you want to write checkpoints and other monitoring data, you then set up a training and an evaluation data set, which you pipe in through the data input functions in TrainSpec and EvalSpec, you are then ready to train and evaluate. I want to say a couple of words about an important practical consideration, data shuffling. The stochastic gradient descent algorithm that neural networks use for training, only works on well-shuffled data. The data set API has a shuffle function that can help there, but some people might not use it if they think their data set is already well shuffled on disk. With distributed training, beware. Even with a well-shuffled data set on disk, if all your workers are loading straight from this data set, they will be seeing the same batch of data, at the same time, and produce the same gradients. The benefits of distributed training are wasted. Your multiple workers all do the exact same things. With data set that shuffle, the shuffling happens independently on each worker using a different random seed, so please use it. Even if your data comes already shuffled on disk. And if you want to be extra sure, you can also shuffle the list of filenames in your shorter data set. List files, returns a data set of filenames, so just call shuffle lines.