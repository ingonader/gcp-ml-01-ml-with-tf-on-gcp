Dataset APIをご紹介します これはTensorFlowに含まれ
Estimatorとともに使用できます どんな目的で
どんなときに使うか見ていきます トレーニングと予測のためにモデルを選んで メモリ内のデータを入力する方法を
すでに学びました しかし現実世界のモデルでは まだ解決すべき問題がいくつかあります ここでもEstimator APIが役立ちます 最初の問題は データサイズです 現実世界ではトレーニングデータが
メモリに収まることは稀です トレーニング中にディスクから
順次読み込むことになるでしょう Estimatorでinput関数を使って
これを実行するには Datasetを使います
tf.data.Datasetなどです 大きなデータセットは
しばしば多数の小ファイルに分かれ それらを順次読み込みます すでに見たとおり トレーニングでは
データのミニバッチを使います データセット全体を
メモリに入れる必要はありません トレーニングの1ステップで必要なのは
1つのミニバッチだけです これがDataset APIです データを順次読み込む
input関数を作成できます 読み込むデータごとに
専用のDatasetクラスがあります CSVなどのテキストファイル用 TensorFlowレコード用
固定長レコードファイル用などです 他のデータに関しては 汎用のdataset句を使って
独自のデコード コードを追加できます この例ではTextLineDatasetを使って
CSVファイルからデータを読み込みます 別の部分を見てみましょう ここではトレーニングバッチを編成する
方法をdatasetクラスに指示しています バッチは128個からなり エポック数15回で繰り返し（repeat） shuffleでシャッフルし
バッファには1,000要素を指定します ここでファイル名から
TextLineDatasetをインスタンス化します これはファイルを読み込んで行に分割します 結果のデータセットは
テキスト行からなります 次にmap関数を使って行を変換できます この場合は 各行をデータ項目に分割します mapは データセットの項目ごとに
個別に関数を適用します この関数ではtf.decode_csv関数を使い テキスト行からカンマ区切り値を抽出します そしてモデルが想定する
featuresとlabelにフォーマットします mapの後にはfeaturesとlabelに基づく
データセットができます 最後にモデル用のinput関数を作成します この定型コードがそれを行います でも不思議に思うかもしれません なぜiteratorやget_nextという名前？ トレーニングはどのように動作するのか？ さらに詳しく見ていくと 理解しやすくなります TensorFlowは遅延実行の原則で
動作します Pythonで作成したすべての tf.**コマンドは
実はデータを処理するのではなく 操作グラフをメモリ内に構築します このグラフがトレーニングや
予測のときに実行されます LinearRegresorなどのEstimatorを
インスタンス化するときにも 同じことが起きます モデルを表現する TensorFlowグラフが
メモリ内に作成されます そこでデータソースへの接続が問題になり その際にinput関数が役立ちます input関数の役割は モデルが想定するfeaturesとlabelを表す
1つのTensorFlowノードを 返すことです このノードがモデルの入力に接続され トレーニングや予測の際に 新しいバッチが実行されるたびに
データを提供します Dataset APIはその際に役立ちます ステップごとに1つのバッチを提供する
inputノードが自動生成されます またデータの順次読み込みが確実に行われ
メモリの飽和を防ぎます さてdataset.make_iterator.get_nextを
呼び出すと 実際にはデータセット内の
次の要素が得られるのではなく TensorFlowノードが得られます そのノードをトレーニング中に実行するたびに データのバッチが返されます まとめると モデルのインスタンス化の際に
input関数が呼び出されます TensorFlowノードのペアが返されて
モデルの入力に追加され これらのノードによって
トレーニングや予測の時に データをモデルに送り込みます input関数について いくつかの誤解を解きたいと思います モデルでデータが必要なときに
input関数を毎回呼び出すのではありません モデルの作成時に1回だけ呼び出します input関数は実際のデータを
返すものではありません 書いている時にそう見えても 違います これはTensorFlowノードを返し これらのノードが実行時にデータを返すのです 実際 任意の複雑なコードを
input関数の中に置いて データを変換することができます ただし1回だけ実行される
ことに注意してください どんな変換を適用する場合でも Dataset APIの使用の有無に関係なく TensorFlowグラフを生成する
tf.**コマンドで 必ず表現してください データがモデルに読み込まれるとき
この方法で データの各バッチに変換が適用されます ただし input関数自体は
1回しか呼び出されません コード全体をもう一度 見ていきましょう 下から始めて上に行きます model.trainが
トレーニングループを起動します モデルはinputノード、features、labelから
データを受け取ります input関数の定義どおりです これらのノードはデータセットを繰り返し トレーニングループで実行される度に
データのバッチを1つ返します 呼び出すDataset APIの名前が dataset.make_one_short_iterator.get_next
である理由がわかるでしょう datasetはデータをシャッフルし 15回のエポック数を繰り返し 128個の要素からなるミニバッチにします データセットを生成するために
テキストファイルから行が読み込まれ そこからカンマ区切り値をデコードします map操作で テキストの行のデータセットを featuresとlabelのデータセットに変換します では最後に「多数の小さなファイルから
大きなデータセットを読み込む」という 最初の懸念を解決しましょう コードを1行追加するだけでうまくいきます 最初にディスクをスキャンして
複数ファイル名のデータセットを読み込むために Dataset.list_files関数を使います これは glob のような構文をサポートし
共通パターンでファイル名を照合します 次に TextLineDatasetを使って
それらのファイルを読み込み 各ファイル名を テキスト行からなる
1つのデータセットに変えます flat_mapでそれらを全部
1つのデータセットにします それぞれのテキスト行に対して mapを使ってCSV解析アルゴリズムを適用し featuresとlabelからなる
データセットを取得します なぜこの2つのマッピング関数 mapとflat_mapなのでしょう？ このうち一つは1対1の変換で もう一つは1対多の変換です テキスト行の解析は1対1の変換です ですからmapを適用します ファイルをTextLineDatasetで読み込むとき 1つのファイル名が
テキスト行のコレクションに変わります ですから1対多の変換で flat_mapを適用して複数のテキスト行を
1つのデータセットに平準化します このような方法で データセットを使って モデルに適したinput関数を生成し メモリに収まらない大規模な
データセットでトレーニングできます Datasetにはデータを処理して変換する
豊富なAPIもあります ぜひ活用してください