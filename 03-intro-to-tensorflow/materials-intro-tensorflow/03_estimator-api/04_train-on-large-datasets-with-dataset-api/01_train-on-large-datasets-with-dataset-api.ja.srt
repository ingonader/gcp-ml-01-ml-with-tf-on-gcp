1
00:00:00,000 --> 00:00:03,135
Dataset APIをご紹介します

2
00:00:03,135 --> 00:00:07,315
これはTensorFlowに含まれ
Estimatorとともに使用できます

3
00:00:07,315 --> 00:00:10,320
どんな目的で
どんなときに使うか見ていきます

4
00:00:10,320 --> 00:00:13,030
トレーニングと予測のためにモデルを選んで

5
00:00:13,030 --> 00:00:16,184
メモリ内のデータを入力する方法を
すでに学びました

6
00:00:16,184 --> 00:00:18,120
しかし現実世界のモデルでは

7
00:00:18,120 --> 00:00:21,645
まだ解決すべき問題がいくつかあります

8
00:00:21,645 --> 00:00:24,125
ここでもEstimator APIが役立ちます

9
00:00:24,125 --> 00:00:26,005
最初の問題は

10
00:00:26,005 --> 00:00:27,730
データサイズです

11
00:00:27,730 --> 00:00:31,875
現実世界ではトレーニングデータが
メモリに収まることは稀です

12
00:00:31,875 --> 00:00:35,965
トレーニング中にディスクから
順次読み込むことになるでしょう

13
00:00:35,965 --> 00:00:39,559
Estimatorでinput関数を使って
これを実行するには

14
00:00:39,559 --> 00:00:43,710
Datasetを使います
tf.data.Datasetなどです

15
00:00:43,710 --> 00:00:47,520
大きなデータセットは
しばしば多数の小ファイルに分かれ

16
00:00:47,520 --> 00:00:49,410
それらを順次読み込みます

17
00:00:49,410 --> 00:00:53,180
すでに見たとおり トレーニングでは
データのミニバッチを使います

18
00:00:53,180 --> 00:00:56,430
データセット全体を
メモリに入れる必要はありません

19
00:00:56,430 --> 00:01:00,940
トレーニングの1ステップで必要なのは
1つのミニバッチだけです

20
00:01:00,940 --> 00:01:03,460
これがDataset APIです

21
00:01:03,460 --> 00:01:08,505
データを順次読み込む
input関数を作成できます

22
00:01:08,505 --> 00:01:12,470
読み込むデータごとに
専用のDatasetクラスがあります

23
00:01:12,470 --> 00:01:14,520
CSVなどのテキストファイル用

24
00:01:14,520 --> 00:01:18,195
TensorFlowレコード用
固定長レコードファイル用などです

25
00:01:18,195 --> 00:01:19,980
他のデータに関しては

26
00:01:19,980 --> 00:01:24,685
汎用のdataset句を使って
独自のデコード コードを追加できます

27
00:01:24,685 --> 00:01:30,760
この例ではTextLineDatasetを使って
CSVファイルからデータを読み込みます

28
00:01:30,760 --> 00:01:33,360
別の部分を見てみましょう

29
00:01:33,360 --> 00:01:39,495
ここではトレーニングバッチを編成する
方法をdatasetクラスに指示しています

30
00:01:39,495 --> 00:01:41,399
バッチは128個からなり

31
00:01:41,399 --> 00:01:44,565
エポック数15回で繰り返し（repeat）

32
00:01:44,565 --> 00:01:49,480
shuffleでシャッフルし
バッファには1,000要素を指定します

33
00:01:49,480 --> 00:01:55,202
ここでファイル名から
TextLineDatasetをインスタンス化します

34
00:01:55,202 --> 00:01:58,650
これはファイルを読み込んで行に分割します

35
00:01:58,650 --> 00:02:02,100
結果のデータセットは
テキスト行からなります

36
00:02:02,100 --> 00:02:06,330
次にmap関数を使って行を変換できます

37
00:02:06,330 --> 00:02:10,220
この場合は 各行をデータ項目に分割します

38
00:02:10,220 --> 00:02:14,970
mapは データセットの項目ごとに
個別に関数を適用します

39
00:02:14,970 --> 00:02:20,955
この関数ではtf.decode_csv関数を使い

40
00:02:20,955 --> 00:02:24,885
テキスト行からカンマ区切り値を抽出します

41
00:02:24,885 --> 00:02:30,950
そしてモデルが想定する
featuresとlabelにフォーマットします

42
00:02:30,950 --> 00:02:34,845
mapの後にはfeaturesとlabelに基づく
データセットができます

43
00:02:34,845 --> 00:02:39,120
最後にモデル用のinput関数を作成します

44
00:02:39,120 --> 00:02:42,555
この定型コードがそれを行います

45
00:02:42,555 --> 00:02:44,585
でも不思議に思うかもしれません

46
00:02:44,585 --> 00:02:47,685
なぜiteratorやget_nextという名前？

47
00:02:47,685 --> 00:02:50,525
トレーニングはどのように動作するのか？

48
00:02:50,525 --> 00:02:52,680
さらに詳しく見ていくと

49
00:02:52,680 --> 00:02:55,495
理解しやすくなります

50
00:02:55,495 --> 00:02:59,655
TensorFlowは遅延実行の原則で
動作します

51
00:02:59,655 --> 00:03:05,340
Pythonで作成したすべての tf.**コマンドは
実はデータを処理するのではなく

52
00:03:05,340 --> 00:03:08,415
操作グラフをメモリ内に構築します

53
00:03:08,415 --> 00:03:12,265
このグラフがトレーニングや
予測のときに実行されます

54
00:03:12,265 --> 00:03:15,924
LinearRegresorなどのEstimatorを
インスタンス化するときにも

55
00:03:15,924 --> 00:03:17,470
同じことが起きます

56
00:03:17,470 --> 00:03:22,140
モデルを表現する TensorFlowグラフが
メモリ内に作成されます

57
00:03:22,140 --> 00:03:26,400
そこでデータソースへの接続が問題になり

58
00:03:26,400 --> 00:03:29,175
その際にinput関数が役立ちます

59
00:03:29,175 --> 00:03:31,180
input関数の役割は

60
00:03:31,180 --> 00:03:36,000
モデルが想定するfeaturesとlabelを表す
1つのTensorFlowノードを

61
00:03:36,000 --> 00:03:37,905
返すことです

62
00:03:37,905 --> 00:03:41,760
このノードがモデルの入力に接続され

63
00:03:41,760 --> 00:03:44,120
トレーニングや予測の際に

64
00:03:44,120 --> 00:03:49,680
新しいバッチが実行されるたびに
データを提供します

65
00:03:49,680 --> 00:03:53,415
Dataset APIはその際に役立ちます

66
00:03:53,415 --> 00:04:00,057
ステップごとに1つのバッチを提供する
inputノードが自動生成されます

67
00:04:00,060 --> 00:04:06,435
またデータの順次読み込みが確実に行われ
メモリの飽和を防ぎます

68
00:04:06,435 --> 00:04:11,760
さてdataset.make_iterator.get_nextを
呼び出すと

69
00:04:11,760 --> 00:04:15,755
実際にはデータセット内の
次の要素が得られるのではなく

70
00:04:15,755 --> 00:04:18,165
TensorFlowノードが得られます

71
00:04:18,165 --> 00:04:21,310
そのノードをトレーニング中に実行するたびに

72
00:04:21,310 --> 00:04:24,290
データのバッチが返されます

73
00:04:24,290 --> 00:04:30,425
まとめると モデルのインスタンス化の際に
input関数が呼び出されます

74
00:04:30,425 --> 00:04:35,840
TensorFlowノードのペアが返されて
モデルの入力に追加され

75
00:04:35,840 --> 00:04:39,065
これらのノードによって
トレーニングや予測の時に

76
00:04:39,065 --> 00:04:42,625
データをモデルに送り込みます

77
00:04:42,625 --> 00:04:44,705
input関数について

78
00:04:44,705 --> 00:04:48,330
いくつかの誤解を解きたいと思います

79
00:04:48,330 --> 00:04:53,120
モデルでデータが必要なときに
input関数を毎回呼び出すのではありません

80
00:04:53,120 --> 00:04:56,880
モデルの作成時に1回だけ呼び出します

81
00:04:56,880 --> 00:05:00,750
input関数は実際のデータを
返すものではありません

82
00:05:00,750 --> 00:05:04,130
書いている時にそう見えても 違います

83
00:05:04,130 --> 00:05:06,530
これはTensorFlowノードを返し

84
00:05:06,530 --> 00:05:11,030
これらのノードが実行時にデータを返すのです

85
00:05:11,030 --> 00:05:14,645
実際 任意の複雑なコードを
input関数の中に置いて

86
00:05:14,645 --> 00:05:17,390
データを変換することができます

87
00:05:17,390 --> 00:05:20,910
ただし1回だけ実行される
ことに注意してください

88
00:05:20,910 --> 00:05:23,730
どんな変換を適用する場合でも

89
00:05:23,730 --> 00:05:26,920
Dataset APIの使用の有無に関係なく

90
00:05:26,920 --> 00:05:32,555
TensorFlowグラフを生成する
tf.**コマンドで 必ず表現してください

91
00:05:32,555 --> 00:05:36,595
データがモデルに読み込まれるとき
この方法で

92
00:05:36,595 --> 00:05:40,555
データの各バッチに変換が適用されます

93
00:05:40,555 --> 00:05:44,425
ただし input関数自体は
1回しか呼び出されません

94
00:05:44,425 --> 00:05:46,430
コード全体をもう一度

95
00:05:46,430 --> 00:05:48,830
見ていきましょう

96
00:05:48,830 --> 00:05:51,965
下から始めて上に行きます

97
00:05:51,965 --> 00:05:55,520
model.trainが
トレーニングループを起動します

98
00:05:55,520 --> 00:06:01,320
モデルはinputノード、features、labelから
データを受け取ります

99
00:06:01,320 --> 00:06:03,560
input関数の定義どおりです

100
00:06:03,560 --> 00:06:07,120
これらのノードはデータセットを繰り返し

101
00:06:07,120 --> 00:06:12,620
トレーニングループで実行される度に
データのバッチを1つ返します

102
00:06:12,620 --> 00:06:15,050
呼び出すDataset APIの名前が

103
00:06:15,050 --> 00:06:21,075
dataset.make_one_short_iterator.get_next
である理由がわかるでしょう

104
00:06:21,075 --> 00:06:23,535
datasetはデータをシャッフルし

105
00:06:23,545 --> 00:06:25,695
15回のエポック数を繰り返し

106
00:06:25,695 --> 00:06:29,415
128個の要素からなるミニバッチにします

107
00:06:29,415 --> 00:06:33,880
データセットを生成するために
テキストファイルから行が読み込まれ

108
00:06:33,880 --> 00:06:37,970
そこからカンマ区切り値をデコードします

109
00:06:37,970 --> 00:06:41,555
map操作で テキストの行のデータセットを

110
00:06:41,555 --> 00:06:45,440
featuresとlabelのデータセットに変換します

111
00:06:45,440 --> 00:06:51,195
では最後に「多数の小さなファイルから
大きなデータセットを読み込む」という

112
00:06:51,195 --> 00:06:54,255
最初の懸念を解決しましょう

113
00:06:54,255 --> 00:06:57,360
コードを1行追加するだけでうまくいきます

114
00:06:57,360 --> 00:07:02,510
最初にディスクをスキャンして
複数ファイル名のデータセットを読み込むために

115
00:07:02,510 --> 00:07:05,500
Dataset.list_files関数を使います

116
00:07:05,500 --> 00:07:11,515
これは glob のような構文をサポートし
共通パターンでファイル名を照合します

117
00:07:11,515 --> 00:07:15,165
次に TextLineDatasetを使って
それらのファイルを読み込み

118
00:07:15,165 --> 00:07:19,490
各ファイル名を テキスト行からなる
1つのデータセットに変えます

119
00:07:19,490 --> 00:07:23,435
flat_mapでそれらを全部
1つのデータセットにします

120
00:07:23,435 --> 00:07:25,955
それぞれのテキスト行に対して

121
00:07:25,955 --> 00:07:29,060
mapを使ってCSV解析アルゴリズムを適用し

122
00:07:29,060 --> 00:07:33,840
featuresとlabelからなる
データセットを取得します

123
00:07:33,840 --> 00:07:36,095
なぜこの2つのマッピング関数

124
00:07:36,095 --> 00:07:38,350
mapとflat_mapなのでしょう？

125
00:07:38,350 --> 00:07:40,760
このうち一つは1対1の変換で

126
00:07:40,760 --> 00:07:45,525
もう一つは1対多の変換です

127
00:07:45,525 --> 00:07:50,485
テキスト行の解析は1対1の変換です

128
00:07:50,485 --> 00:07:52,535
ですからmapを適用します

129
00:07:52,535 --> 00:07:55,925
ファイルをTextLineDatasetで読み込むとき

130
00:07:55,925 --> 00:08:00,250
1つのファイル名が
テキスト行のコレクションに変わります

131
00:08:00,250 --> 00:08:03,655
ですから1対多の変換で

132
00:08:03,655 --> 00:08:11,450
flat_mapを適用して複数のテキスト行を
1つのデータセットに平準化します

133
00:08:11,450 --> 00:08:14,525
このような方法で データセットを使って

134
00:08:14,525 --> 00:08:16,740
モデルに適したinput関数を生成し

135
00:08:16,740 --> 00:08:20,935
メモリに収まらない大規模な
データセットでトレーニングできます

136
00:08:20,935 --> 00:08:26,695
Datasetにはデータを処理して変換する
豊富なAPIもあります

137
00:08:26,695 --> 00:08:29,040
ぜひ活用してください