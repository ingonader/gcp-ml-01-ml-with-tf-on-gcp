Here, we introduce the data set API, which comes with TensorFlow and is designed to be used with estimators. Let's see why and when you'll need to use it. We now know how to pick a model and how to feed it data from memory, for training and predictions. But for real world models, we still need to solve a couple of practical issues. The estimator API can help there too. Let's start with the first one. Data size, yes, in the real life, your training data will readily fit in memory and you will load it progressively from disk during training. How can you write an input function for your estimator, that does that by using data sets, as in tf.data.Dataset. Large data sets tend to be sharded into multiple files, which can be loaded progressively. Remember, that we train on mini batches of data. We do not need to have the entire data set in memory. One mini batch is all we need for one training step. Here is the data set API. It will help us create input functions for our model that load data progressively. There are a specialized data set classes, that can read data from text files like CSVs, TensorFlow records, or fixed length record files. For anything else, you can use the generic data set clause and add your own decoding code. Here is an example where we use text line data set to load data from a CSV file. Let's go through the different parts. This part of the code tells the data set class how to organize the data into training batches of 128, repeated for 15 epoch, and of course, shuffled with a shuffle buffer of 1000 elements. Here, we instantiate the text line data set from a filename. This loads the file and splits it into lines. The resulting data set is a set of text lines. We can now use the map function to transform the lines. In this case, we want to split each line into data items. Map, applies a function to each item in the data set independently. And in this function we use the TF decode CSV function to extract the comma separated values from the text lines, and format them into features and labels as our model expects them. After the map, we have a data set of featured and label base. Finally, we create the input function for our model. This piece of boilerplate code does the trick. But you might be wondering why is it called an iterator, and why get next, and what is actually going on when we train? Let's dive under the hood again. It will help us understand. TensorFlow, works with a deferred execution principle. Older tf.something commands, that you write in Python do not actually process data. They build a graph of operations in memory. This graph will be executed when we train or predict. When we instantiate an estimator, like linear or regresor the same thing happens. A TensorFlow graph is created in memory representing our model. Now, the problem is to connect it to a data source. That is what input functions are for. The contract for an input function, is to return one TensorFlow node, representing the features and labels expected by the model. This node will be connected to the inputs of the model, and its responsibility is to deliver a fresh batch of data every time it is executed, during training or inference. That is what the data set API is useful for. It generates for you input nodes that deliver one batch of data at each training step. And they also make sure the data is loaded progressively and never saturates the memory. When you call dataset.makeiterator.getnext, you're not really getting the next element in the data set, you are getting a TensorFlow node, that each time it gets executed during training returns a batch of training data. Let's recap. Input functions are called when a model is instantiated. They return a pair of TensorFlow nodes to be attached to the inputs of your model and these nodes are responsible for pumping data into your model during training or inference. There are a couple of misconceptions about input functions, that I would like to clear. Now, an input function is not called every time your model needs data. It is called only once, at model creation time. And no, input functions are not expected to return actual data, even if that is what it looks like when you write them. They return TensorFlow nodes, and these nodes return data when they get executed. You can actually place arbitrarily complex code in the input function to transform your data, as long as you bear in mind that it will run only once. Whatever transformations you want to apply, and whether you use the data set API or not, make sure they are expressed in tf.something commands that generate a TensorFlow graph. That is how you get your transformations to be applied to each batch of data, as it is loaded into your model. Even if the input function itself is only called code once. Here's the full code again. Let us go through it one more time. Starting at the bottom and going up. Muddle the train launches the training loop. The model receives data from its input nodes, features and labels as defined in the input function. These nodes iterate on the data set and return one batch of data every time that they get executed in the training loop. That explains why the name of the data set API you call to give them is data set, make one short iterator, get next. The data set shuffles the data, repeats it for 15 epochs, and batches into mini batches of 128 elements. The data set has been produced by reading lines from a text file and decoding the comma separated values from them. The map operation transforms a data set of text lines into a data set of features and labels. Finally, we have to address our initial concerns, loading large data sets from a set of sharded files. One extra line of code will do. We first scan the disk and load a data set of filenames using the data set that list files functions. It supports a globe like syntax which stars to match filenames with a common pattern. Then, we use text line data set to load these files and turn each filename into a data set of text lines. We flat map all of them together into a single data set. And then for each line of text, we use map to apply the CSV parsing algorithm and obtain a data set of features and labels. Why two mapping functions, map and flat map? Well, one of them is simply for one to one transformations and the other one for one to many transformations. Parsing a line of text int- is one to one transformation, so we apply it with map. When loading a file with text line data set, one filename becomes a collection of text lines. So, that's a one to many transformations and it is applied with flat map to flatten all the resulting text lines into one data set. Now you know how to use data sets to generate proper input functions for your models and get them training on large, out of memory data sets. But data sets also offer a rich API for working on and transforming your data. Take advantage of it.