1
00:00:00,000 --> 00:00:03,225
Here, we introduce the data set API,

2
00:00:03,225 --> 00:00:07,445
which comes with TensorFlow and is designed to be used with estimators.

3
00:00:07,445 --> 00:00:10,320
Let's see why and when you'll need to use it.

4
00:00:10,320 --> 00:00:14,460
We now know how to pick a model and how to feed it data from memory,

5
00:00:14,460 --> 00:00:16,185
for training and predictions.

6
00:00:16,185 --> 00:00:18,120
But for real world models,

7
00:00:18,120 --> 00:00:21,645
we still need to solve a couple of practical issues.

8
00:00:21,645 --> 00:00:24,125
The estimator API can help there too.

9
00:00:24,125 --> 00:00:26,145
Let's start with the first one.

10
00:00:26,145 --> 00:00:28,970
Data size, yes, in the real life,

11
00:00:28,970 --> 00:00:31,875
your training data will readily fit in memory

12
00:00:31,875 --> 00:00:35,445
and you will load it progressively from disk during training.

13
00:00:35,445 --> 00:00:38,549
How can you write an input function for your estimator,

14
00:00:38,549 --> 00:00:43,710
that does that by using data sets, as in tf.data.Dataset.

15
00:00:43,710 --> 00:00:47,520
Large data sets tend to be sharded into multiple files,

16
00:00:47,520 --> 00:00:49,410
which can be loaded progressively.

17
00:00:49,410 --> 00:00:52,890
Remember, that we train on mini batches of data.

18
00:00:52,890 --> 00:00:56,430
We do not need to have the entire data set in memory.

19
00:00:56,430 --> 00:01:00,940
One mini batch is all we need for one training step.

20
00:01:00,940 --> 00:01:02,910
Here is the data set API.

21
00:01:02,910 --> 00:01:08,505
It will help us create input functions for our model that load data progressively.

22
00:01:08,505 --> 00:01:11,400
There are a specialized data set classes,

23
00:01:11,400 --> 00:01:14,520
that can read data from text files like CSVs,

24
00:01:14,520 --> 00:01:18,195
TensorFlow records, or fixed length record files.

25
00:01:18,195 --> 00:01:19,950
For anything else, you can use

26
00:01:19,950 --> 00:01:23,735
the generic data set clause and add your own decoding code.

27
00:01:23,735 --> 00:01:30,760
Here is an example where we use text line data set to load data from a CSV file.

28
00:01:30,760 --> 00:01:33,360
Let's go through the different parts.

29
00:01:33,360 --> 00:01:36,885
This part of the code tells the data set class how to

30
00:01:36,885 --> 00:01:41,399
organize the data into training batches of 128,

31
00:01:41,399 --> 00:01:44,565
repeated for 15 epoch,

32
00:01:44,565 --> 00:01:49,480
and of course, shuffled with a shuffle buffer of 1000 elements.

33
00:01:49,480 --> 00:01:55,165
Here, we instantiate the text line data set from a filename.

34
00:01:55,165 --> 00:01:58,650
This loads the file and splits it into lines.

35
00:01:58,650 --> 00:02:02,100
The resulting data set is a set of text lines.

36
00:02:02,100 --> 00:02:06,330
We can now use the map function to transform the lines.

37
00:02:06,330 --> 00:02:10,220
In this case, we want to split each line into data items.

38
00:02:10,220 --> 00:02:14,970
Map, applies a function to each item in the data set independently.

39
00:02:14,970 --> 00:02:18,605
And in this function we use the TF decode

40
00:02:18,605 --> 00:02:24,885
CSV function to extract the comma separated values from the text lines,

41
00:02:24,885 --> 00:02:29,940
and format them into features and labels as our model expects them.

42
00:02:29,940 --> 00:02:34,845
After the map, we have a data set of featured and label base.

43
00:02:34,845 --> 00:02:39,120
Finally, we create the input function for our model.

44
00:02:39,120 --> 00:02:42,735
This piece of boilerplate code does the trick.

45
00:02:42,735 --> 00:02:46,245
But you might be wondering why is it called an iterator,

46
00:02:46,245 --> 00:02:47,685
and why get next,

47
00:02:47,685 --> 00:02:50,415
and what is actually going on when we train?

48
00:02:50,415 --> 00:02:52,680
Let's dive under the hood again.

49
00:02:52,680 --> 00:02:55,495
It will help us understand.

50
00:02:55,495 --> 00:02:59,655
TensorFlow, works with a deferred execution principle.

51
00:02:59,655 --> 00:03:04,970
Older tf.something commands, that you write in Python do not actually process data.

52
00:03:04,970 --> 00:03:08,415
They build a graph of operations in memory.

53
00:03:08,415 --> 00:03:12,455
This graph will be executed when we train or predict.

54
00:03:12,455 --> 00:03:14,704
When we instantiate an estimator,

55
00:03:14,704 --> 00:03:17,450
like linear or regresor the same thing happens.

56
00:03:17,450 --> 00:03:22,140
A TensorFlow graph is created in memory representing our model.

57
00:03:22,140 --> 00:03:26,520
Now, the problem is to connect it to a data source.

58
00:03:26,520 --> 00:03:29,065
That is what input functions are for.

59
00:03:29,065 --> 00:03:31,670
The contract for an input function,

60
00:03:31,670 --> 00:03:34,590
is to return one TensorFlow node,

61
00:03:34,590 --> 00:03:37,905
representing the features and labels expected by the model.

62
00:03:37,905 --> 00:03:41,760
This node will be connected to the inputs of the model,

63
00:03:41,760 --> 00:03:47,805
and its responsibility is to deliver a fresh batch of data every time it is executed,

64
00:03:47,805 --> 00:03:49,680
during training or inference.

65
00:03:49,680 --> 00:03:53,415
That is what the data set API is useful for.

66
00:03:53,415 --> 00:04:00,060
It generates for you input nodes that deliver one batch of data at each training step.

67
00:04:00,060 --> 00:04:06,435
And they also make sure the data is loaded progressively and never saturates the memory.

68
00:04:06,435 --> 00:04:11,760
When you call dataset.makeiterator.getnext,

69
00:04:11,760 --> 00:04:15,755
you're not really getting the next element in the data set,

70
00:04:15,755 --> 00:04:18,015
you are getting a TensorFlow node,

71
00:04:18,015 --> 00:04:24,290
that each time it gets executed during training returns a batch of training data.

72
00:04:24,290 --> 00:04:30,425
Let's recap. Input functions are called when a model is instantiated.

73
00:04:30,425 --> 00:04:36,060
They return a pair of TensorFlow nodes to be attached to the inputs of your model and

74
00:04:36,060 --> 00:04:38,925
these nodes are responsible for pumping data

75
00:04:38,925 --> 00:04:42,625
into your model during training or inference.

76
00:04:42,625 --> 00:04:46,595
There are a couple of misconceptions about input functions,

77
00:04:46,595 --> 00:04:48,410
that I would like to clear.

78
00:04:48,410 --> 00:04:53,120
Now, an input function is not called every time your model needs data.

79
00:04:53,120 --> 00:04:54,735
It is called only once,

80
00:04:54,735 --> 00:04:56,970
at model creation time.

81
00:04:56,970 --> 00:05:00,750
And no, input functions are not expected to return actual data,

82
00:05:00,750 --> 00:05:04,130
even if that is what it looks like when you write them.

83
00:05:04,130 --> 00:05:06,490
They return TensorFlow nodes,

84
00:05:06,490 --> 00:05:11,030
and these nodes return data when they get executed.

85
00:05:11,030 --> 00:05:14,435
You can actually place arbitrarily complex code

86
00:05:14,435 --> 00:05:17,390
in the input function to transform your data,

87
00:05:17,390 --> 00:05:20,910
as long as you bear in mind that it will run only once.

88
00:05:20,910 --> 00:05:23,730
Whatever transformations you want to apply,

89
00:05:23,730 --> 00:05:26,920
and whether you use the data set API or not,

90
00:05:26,920 --> 00:05:32,555
make sure they are expressed in tf.something commands that generate a TensorFlow graph.

91
00:05:32,555 --> 00:05:38,025
That is how you get your transformations to be applied to each batch of data,

92
00:05:38,025 --> 00:05:40,555
as it is loaded into your model.

93
00:05:40,555 --> 00:05:44,425
Even if the input function itself is only called code once.

94
00:05:44,425 --> 00:05:46,430
Here's the full code again.

95
00:05:46,430 --> 00:05:48,830
Let us go through it one more time.

96
00:05:48,830 --> 00:05:51,965
Starting at the bottom and going up.

97
00:05:51,965 --> 00:05:55,520
Muddle the train launches the training loop.

98
00:05:55,520 --> 00:05:59,960
The model receives data from its input nodes,

99
00:05:59,960 --> 00:06:03,560
features and labels as defined in the input function.

100
00:06:03,560 --> 00:06:08,030
These nodes iterate on the data set and return one batch of

101
00:06:08,030 --> 00:06:12,620
data every time that they get executed in the training loop.

102
00:06:12,620 --> 00:06:18,690
That explains why the name of the data set API you call to give them is data set,

103
00:06:18,690 --> 00:06:21,065
make one short iterator, get next.

104
00:06:21,065 --> 00:06:23,545
The data set shuffles the data,

105
00:06:23,545 --> 00:06:25,695
repeats it for 15 epochs,

106
00:06:25,695 --> 00:06:29,415
and batches into mini batches of 128 elements.

107
00:06:29,415 --> 00:06:33,170
The data set has been produced by reading lines from

108
00:06:33,170 --> 00:06:37,970
a text file and decoding the comma separated values from them.

109
00:06:37,970 --> 00:06:41,555
The map operation transforms a data set of

110
00:06:41,555 --> 00:06:45,440
text lines into a data set of features and labels.

111
00:06:45,440 --> 00:06:49,685
Finally, we have to address our initial concerns,

112
00:06:49,685 --> 00:06:54,255
loading large data sets from a set of sharded files.

113
00:06:54,255 --> 00:06:57,360
One extra line of code will do.

114
00:06:57,360 --> 00:07:00,500
We first scan the disk and load a data set of

115
00:07:00,500 --> 00:07:05,500
filenames using the data set that list files functions.

116
00:07:05,500 --> 00:07:11,705
It supports a globe like syntax which stars to match filenames with a common pattern.

117
00:07:11,705 --> 00:07:15,095
Then, we use text line data set to load these files

118
00:07:15,095 --> 00:07:19,370
and turn each filename into a data set of text lines.

119
00:07:19,370 --> 00:07:23,435
We flat map all of them together into a single data set.

120
00:07:23,435 --> 00:07:25,955
And then for each line of text,

121
00:07:25,955 --> 00:07:28,040
we use map to apply

122
00:07:28,040 --> 00:07:33,840
the CSV parsing algorithm and obtain a data set of features and labels.

123
00:07:33,840 --> 00:07:36,455
Why two mapping functions,

124
00:07:36,455 --> 00:07:38,350
map and flat map?

125
00:07:38,350 --> 00:07:40,760
Well, one of them is simply for

126
00:07:40,760 --> 00:07:45,525
one to one transformations and the other one for one to many transformations.

127
00:07:45,525 --> 00:07:50,485
Parsing a line of text int- is one to one transformation,

128
00:07:50,485 --> 00:07:52,535
so we apply it with map.

129
00:07:52,535 --> 00:07:55,925
When loading a file with text line data set,

130
00:07:55,925 --> 00:08:00,250
one filename becomes a collection of text lines.

131
00:08:00,250 --> 00:08:04,285
So, that's a one to many transformations and it is applied with

132
00:08:04,285 --> 00:08:11,450
flat map to flatten all the resulting text lines into one data set.

133
00:08:11,450 --> 00:08:14,525
Now you know how to use data sets to generate

134
00:08:14,525 --> 00:08:19,060
proper input functions for your models and get them training on large,

135
00:08:19,060 --> 00:08:20,935
out of memory data sets.

136
00:08:20,935 --> 00:08:24,125
But data sets also offer a rich API

137
00:08:24,125 --> 00:08:29,040
for working on and transforming your data. Take advantage of it.