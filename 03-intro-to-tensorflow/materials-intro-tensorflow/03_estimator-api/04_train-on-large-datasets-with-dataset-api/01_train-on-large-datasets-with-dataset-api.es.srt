1
00:00:00,570 --> 00:00:03,225
En esta lección,
presentamos la API de Dataset

2
00:00:03,225 --> 00:00:07,445
que está integrada con TensorFlow
y diseñada para usar con los estimadores.

3
00:00:07,445 --> 00:00:10,250
Veamos cuándo
y por qué necesitará usarla.

4
00:00:10,250 --> 00:00:14,460
Ya vimos cómo escoger un modelo
y alimentarlo con datos de la memoria

5
00:00:14,460 --> 00:00:16,424
para el entrenamiento y las predicciones.

6
00:00:16,424 --> 00:00:18,120
Pero para modelos del mundo real

7
00:00:18,120 --> 00:00:21,645
nos falta resolver
algunos problemas prácticos.

8
00:00:21,645 --> 00:00:24,125
La API de Estimator
también puede ayudar con eso.

9
00:00:24,125 --> 00:00:26,145
Comencemos con el primero.

10
00:00:26,145 --> 00:00:27,660
El tamaño de los datos.

11
00:00:27,660 --> 00:00:30,335
En modelos reales,
los datos de entrenamiento

12
00:00:30,335 --> 00:00:32,055
rara vez entrarán en la memoria

13
00:00:32,055 --> 00:00:35,705
así que los cargará progresivamente
desde el disco durante el entrenamiento.

14
00:00:35,705 --> 00:00:39,409
¿Cómo escribir una función
de entrada para su estimador que haga eso?

15
00:00:39,409 --> 00:00:43,710
Con conjuntos de datos,
como en tf.data.Dataset.

16
00:00:44,050 --> 00:00:47,800
Los grandes conjuntos de datos
suelen fragmentarse en varios archivos

17
00:00:47,800 --> 00:00:49,770
que se pueden cargar progresivamente.

18
00:00:49,770 --> 00:00:53,300
Recuerde que se entrena
con "minilotes" de datos.

19
00:00:53,300 --> 00:00:57,010
No es necesario tener
todo el conjunto de datos en la memoria.

20
00:00:57,010 --> 00:01:01,460
Solo necesitamos un minilote
para cada paso de entrenamiento.

21
00:01:01,460 --> 00:01:03,310
Esta es la API de Dataset.

22
00:01:03,310 --> 00:01:06,525
Nos ayudará a crear funciones
de entrada para nuestro modelo

23
00:01:06,525 --> 00:01:08,805
que carguen datos progresivamente.

24
00:01:08,805 --> 00:01:11,400
Hay clases especializadas
de conjuntos de datos

25
00:01:11,400 --> 00:01:14,520
que pueden leer datos
incluidos en archivos de texto como CSV

26
00:01:14,520 --> 00:01:18,235
registros de TensorFlow
o archivos de registros de longitud fija.

27
00:01:18,235 --> 00:01:21,540
Para todo lo demás,
puede usar la clase GenericDataset

28
00:01:21,540 --> 00:01:24,165
y agregar su propio código
de decodificación.

29
00:01:24,735 --> 00:01:28,620
En este ejemplo, usamos TextLineDataset

30
00:01:28,620 --> 00:01:30,920
para cargar datos desde un archivo CSV.

31
00:01:30,920 --> 00:01:33,360
Veamos las diferentes partes.

32
00:01:33,680 --> 00:01:36,675
Esta parte del código le dice
a la clase del conjunto de datos

33
00:01:36,675 --> 00:01:44,509
cómo organizarlos para el entrenamiento:
lotes de 128 que se repiten por 15 ciclos

34
00:01:44,565 --> 00:01:49,640
y, por supuesto, se redistribuyen
con un búfer de 1,000 elementos.

35
00:01:50,210 --> 00:01:53,625
Aquí, instanciamos el conjunto
de datos de líneas de texto

36
00:01:53,625 --> 00:01:55,170
desde un nombre de archivo.

37
00:01:55,170 --> 00:01:58,890
Esto carga el archivo
y lo divide en líneas.

38
00:01:58,890 --> 00:02:02,430
El conjunto de datos resultante
es un conjunto de líneas de texto.

39
00:02:02,430 --> 00:02:06,330
Ahora, podemos usar la función map
para transformar las líneas.

40
00:02:06,330 --> 00:02:10,260
En este caso, queremos dividir
cada línea en elementos de datos.

41
00:02:10,260 --> 00:02:15,110
Map aplica una función a cada elemento
del conjunto de forma independiente.

42
00:02:15,810 --> 00:02:20,465
Aquí, usamos la función tf.decode_csv

43
00:02:20,465 --> 00:02:24,885
para extraer los valores separados
por comas de las líneas de texto

44
00:02:24,885 --> 00:02:30,400
y convertirlos en atributos
y etiquetas, como nuestro modelo espera.

45
00:02:30,830 --> 00:02:33,215
Después de map,
tenemos un conjunto de datos

46
00:02:33,215 --> 00:02:35,155
de capas de atributos y etiquetas.

47
00:02:35,155 --> 00:02:39,120
Finalmente, creamos
la función de entrada para nuestro modelo.

48
00:02:39,690 --> 00:02:43,065
Este fragmento
de código estándar hace eso.

49
00:02:43,065 --> 00:02:46,245
Quizá se pregunte
por qué se llama un iterator

50
00:02:46,255 --> 00:02:47,895
y por qué get_next

51
00:02:47,895 --> 00:02:50,415
y qué pasa realmente cuando entrenamos.

52
00:02:50,865 --> 00:02:54,760
Analicémoslo
en más detalle para entender mejor.

53
00:02:55,875 --> 00:02:59,655
TensorFlow funciona
con un principio de ejecución diferida.

54
00:02:59,655 --> 00:03:03,030
Los comandos tf.algo
que se escriben en Python

55
00:03:03,030 --> 00:03:04,950
no procesan datos realmente.

56
00:03:04,950 --> 00:03:08,635
Crean un gráfico
de operaciones en la memoria.

57
00:03:08,635 --> 00:03:12,455
Este gráfico se ejecutará
cuando entrenemos o hagamos predicciones.

58
00:03:12,455 --> 00:03:14,704
Cuando instanciamos un estimador

59
00:03:14,704 --> 00:03:17,710
como LinearRegressor, sucede lo mismo.

60
00:03:17,710 --> 00:03:20,890
Se crea un gráfico
de TensorFlow en la memoria

61
00:03:20,890 --> 00:03:22,610
el cual representa el modelo.

62
00:03:22,610 --> 00:03:26,520
El problema es
conectarlo a una fuente de datos.

63
00:03:26,520 --> 00:03:29,065
Para eso están las funciones de entrada.

64
00:03:29,065 --> 00:03:31,670
El compromiso de una función de entrada

65
00:03:31,670 --> 00:03:34,590
es mostrar un nodo de TensorFlow

66
00:03:34,590 --> 00:03:38,215
que represente los atributos
y las etiquetas que el modelo espera.

67
00:03:38,215 --> 00:03:41,760
Este nodo se conectará
a las entradas del modelo

68
00:03:41,760 --> 00:03:45,955
y su responsabilidad
es entregar un lote de datos nuevo

69
00:03:45,955 --> 00:03:47,845
cada vez que se ejecuta

70
00:03:47,845 --> 00:03:50,830
durante el entrenamiento o la inferencia.

71
00:03:50,830 --> 00:03:53,760
Para eso sirve la API de Dataset.

72
00:03:53,760 --> 00:03:58,305
Genera nodos de entrada
que entregan un lote de datos

73
00:03:58,305 --> 00:04:00,435
en cada paso de entrenamiento.

74
00:04:00,435 --> 00:04:04,100
Y también se aseguran de que los datos
se carguen progresivamente

75
00:04:04,100 --> 00:04:06,920
y nunca saturen la memoria.

76
00:04:06,920 --> 00:04:11,865
Cuando se llama a
dataset.makeiterator.getnext

77
00:04:11,865 --> 00:04:15,995
en realidad no se obtiene
el siguiente elemento del conjunto.

78
00:04:15,995 --> 00:04:18,290
Se obtiene un nodo de TensorFlow

79
00:04:18,290 --> 00:04:21,945
en cada ejecución
durante el entrenamiento

80
00:04:21,945 --> 00:04:24,265
que muestra
un lote de datos de entrenamiento.

81
00:04:24,685 --> 00:04:30,460
En resumen, las funciones de entrada
se llaman cuando se instancia un modelo.

82
00:04:30,460 --> 00:04:33,195
Muestran un par de nodos de TensorFlow

83
00:04:33,195 --> 00:04:35,435
que se adjuntarán
a las entradas del modelo

84
00:04:35,435 --> 00:04:40,005
y estos nodos son responsables
de inyectarle los datos al modelo

85
00:04:40,005 --> 00:04:42,455
durante el entrenamiento o la inferencia.

86
00:04:43,425 --> 00:04:48,410
Aclaremos un par de errores conceptuales
relacionados con las funciones de entrada.

87
00:04:49,000 --> 00:04:53,215
No se llama a una función de entrada
cada vez que el modelo necesita datos.

88
00:04:53,215 --> 00:04:57,370
Se llama una sola vez,
en el momento de la creación del modelo.

89
00:04:57,460 --> 00:05:01,180
Además, las funciones de entrada
en sí no muestran datos realmente

90
00:05:01,180 --> 00:05:04,490
aunque parezca
que fuera así cuando las escribimos.

91
00:05:04,490 --> 00:05:07,010
Muestran nodos de TensorFlow

92
00:05:07,010 --> 00:05:11,565
y son los nodos
los que muestran datos cuando se ejecutan.

93
00:05:11,565 --> 00:05:14,780
Pueden incluir
código arbitrariamente complejo

94
00:05:14,780 --> 00:05:17,580
en la función de entrada
para transformar sus datos

95
00:05:17,580 --> 00:05:21,430
pero recuerde
que se ejecutará una sola vez.

96
00:05:21,430 --> 00:05:24,480
Sin importar
las transformaciones que quiera aplicar

97
00:05:24,480 --> 00:05:27,365
ni si usa la API de Dataset o no

98
00:05:27,365 --> 00:05:31,295
asegúrese de expresarla
en comandos del tipo tf.algo

99
00:05:31,295 --> 00:05:33,155
que generan un gráfico de TensorFlow.

100
00:05:33,155 --> 00:05:38,155
Así se logra que las transformaciones
se apliquen a cada lote de datos

101
00:05:38,155 --> 00:05:40,985
a medida que se cargan en su modelo

102
00:05:40,985 --> 00:05:44,980
incluso si la función de entrada
solo se llama en el código una vez.

103
00:05:44,980 --> 00:05:47,010
Aquí está el código completo de nuevo.

104
00:05:47,010 --> 00:05:49,375
Revisémoslo una vez más.

105
00:05:49,375 --> 00:05:52,520
Comencemos desde abajo.

106
00:05:52,520 --> 00:05:56,320
model.train
inicia el bucle de entrenamiento.

107
00:05:56,320 --> 00:06:00,230
El modelo recibe datos
de sus nodos de entrada

108
00:06:00,230 --> 00:06:03,930
atributos y etiquetas,
según se defina en la función de entrada.

109
00:06:03,930 --> 00:06:08,870
Estos nodos iteran en el conjunto
y muestran un lote de datos

110
00:06:08,870 --> 00:06:13,010
cada vez que se ejecutan
en el bucle de entrenamiento.

111
00:06:13,010 --> 00:06:16,975
Esto explica por qué el nombre
de la API de Dataset que llaman

112
00:06:16,975 --> 00:06:21,795
para obtenerlos es
dataset.make_one_shot_iterator .get_next

113
00:06:21,795 --> 00:06:23,875
El conjunto de datos
redistribuye los datos

114
00:06:23,875 --> 00:06:25,985
repite durante 15 ciclos

115
00:06:25,985 --> 00:06:30,060
y crea minilotes de 128 elementos.

116
00:06:30,360 --> 00:06:32,850
El conjunto de datos
se produjo mediante la lectura

117
00:06:32,850 --> 00:06:34,535
de líneas de un archivo de texto

118
00:06:34,535 --> 00:06:38,465
y la decodificación de valores
separados por comas a partir de ellos.

119
00:06:38,465 --> 00:06:42,490
La operación map transforma
un conjunto de datos de líneas de texto

120
00:06:42,490 --> 00:06:45,665
en un conjunto de atributos y etiquetas.

121
00:06:46,125 --> 00:06:49,685
Para terminar, retomemos
el problema con el que comenzamos.

122
00:06:49,685 --> 00:06:54,560
La carga de grandes conjuntos de datos
de un conjunto de archivos fragmentados.

123
00:06:54,560 --> 00:06:57,700
Una línea adicional
de código es la solución.

124
00:06:57,700 --> 00:07:00,840
Primero, analizamos el disco
y cargamos un conjunto de datos

125
00:07:00,840 --> 00:07:05,125
de nombres de archivos
mediante las funciones Dataset.list_files.

126
00:07:06,225 --> 00:07:08,335
Usa una sintaxis similar a glob

127
00:07:08,335 --> 00:07:12,085
con asteriscos para buscar
nombres de archivos con un patrón común.

128
00:07:12,085 --> 00:07:15,450
Luego, usamos
TextLineDataset para cargar los archivos

129
00:07:15,450 --> 00:07:17,275
y convertir cada nombre de archivo

130
00:07:17,275 --> 00:07:19,555
en un conjunto
de datos de líneas de texto.

131
00:07:19,875 --> 00:07:23,835
Usamos flat_map para acoplar todo
en un único conjunto de datos.

132
00:07:23,835 --> 00:07:26,040
Y, luego, para cada línea de texto

133
00:07:26,040 --> 00:07:30,570
usamos map para aplicar
el algoritmo de análisis de CSV

134
00:07:30,570 --> 00:07:34,455
y obtener un conjunto de datos
de atributos y etiquetas.

135
00:07:34,455 --> 00:07:36,350
¿Por qué dos funciones de asignación?

136
00:07:36,350 --> 00:07:38,760
¿Map y flat_map?

137
00:07:38,760 --> 00:07:42,975
Una es simplemente
para transformaciones de uno a uno

138
00:07:42,975 --> 00:07:45,795
y la otra
para transformaciones de uno a varios.

139
00:07:45,795 --> 00:07:50,535
Analizar una línea de texto
es una transformación de uno a uno

140
00:07:50,535 --> 00:07:53,005
por lo que aplicamos map.

141
00:07:53,005 --> 00:07:56,510
Cuando cargamos un archivo con un conjunto
de datos de líneas de texto

142
00:07:56,520 --> 00:08:00,765
un nombre de archivo se convierte
en una colección de líneas de texto

143
00:08:00,765 --> 00:08:05,820
por lo que es una transformación
de uno a varios y se aplica con flat_map

144
00:08:05,820 --> 00:08:09,280
para acoplar
todas las líneas de texto resultantes

145
00:08:09,280 --> 00:08:11,130
en un solo conjunto de datos.

146
00:08:12,145 --> 00:08:14,290
Ahora ya sabe
cómo usar conjuntos de datos

147
00:08:14,290 --> 00:08:17,185
para generar funciones
de entrada adecuadas para sus modelos

148
00:08:17,185 --> 00:08:21,015
y que entrenen en grandes conjuntos
de datos fuera de la memoria.

149
00:08:21,295 --> 00:08:24,600
Pero Dataset
también ofrece una API potente

150
00:08:24,600 --> 00:08:27,039
para trabajar
con sus datos y transformarlos.

151
00:08:27,039 --> 00:08:28,509
Aprovéchela.