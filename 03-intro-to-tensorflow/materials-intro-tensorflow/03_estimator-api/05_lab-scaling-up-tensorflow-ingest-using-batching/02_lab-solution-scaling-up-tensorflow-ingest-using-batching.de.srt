1
00:00:00,260 --> 00:00:02,703
Gehen wir gemeinsam 
dieses zweite Code-Lab durch.

2
00:00:02,703 --> 00:00:08,665
Diesmal sehen wir, wie man mit der Dataset
API Eingabefunktionen erstellen kann.

3
00:00:08,665 --> 00:00:11,223
Suchen wir die Lab-Dateien.

4
00:00:11,223 --> 00:00:16,429
Wir rufen cloud.goole.com/console auf

5
00:00:16,429 --> 00:00:20,260
und melden uns mit unserem Lab-Konto an.

6
00:00:28,109 --> 00:00:30,090
Hier sind wir in der Console.

7
00:00:30,090 --> 00:00:32,280
Wir öffnen Cloud Shell

8
00:00:33,360 --> 00:00:37,990
und verbinden uns wieder mit
der bestehenden Data Lab-Instanz,

9
00:00:37,990 --> 00:00:43,348
indem wir
"datalab connect mylab" eingeben.

10
00:00:50,883 --> 00:00:53,883
Die Verbindung ist hergestellt.

11
00:00:53,883 --> 00:00:57,710
Jetzt klicken wir
auf diese Preview-Schaltfläche.

12
00:00:57,710 --> 00:01:01,498
Ändern Sie den Port zu 8081,
diesen verwendet Data Lab.

13
00:01:04,184 --> 00:01:09,761
Und hier ist die bekannte
Notebook-Umgebung.

14
00:01:09,761 --> 00:01:15,747
Wir erstellen ein neues Notebook,
um ein paar Git-Befehle einzugeben

15
00:01:15,747 --> 00:01:17,483
und die Codes abzurufen.

16
00:01:17,483 --> 00:01:20,645
Also %bash und

17
00:01:20,645 --> 00:01:26,244
wir klonen dieses Repository.

18
00:01:29,862 --> 00:01:35,755
Es läuft, und das Repository ist
hier als "training-data-analyst" zu sehen.

19
00:01:35,755 --> 00:01:38,130
Suchen wir die Lab-Dateien.

20
00:01:38,130 --> 00:01:44,228
Wir klicken auf "training-data-analyst",
dann auf "courses".

21
00:01:44,228 --> 00:01:47,986
Dann auf "machine_learning".

22
00:01:47,986 --> 00:01:54,050
Dann auf "deepdive"
und schließlich auf "tensorflow".

23
00:01:55,443 --> 00:02:00,578
Das zweite Lab heißt "c_dataset",
öffnen Sie das.

24
00:02:07,300 --> 00:02:12,153
Mit der Dataset API können Sie für den
Estimator Eingabefunktionen erstellen.

25
00:02:12,153 --> 00:02:17,409
Sie lädt Daten progressiv und sollte immer
mit großen Datasets verwendet werden.

26
00:02:19,665 --> 00:02:22,515
Sehen wir uns den Code hier an.

27
00:02:24,839 --> 00:02:29,944
Unser Dataset auf der Festplatte besteht
aus mehreren, aufgeteilten CSV-Dateien.

28
00:02:29,944 --> 00:02:35,025
Wir verwenden "Dataset.list_files",
um die Festplatte zu scannen und

29
00:02:35,025 --> 00:02:37,668
ein Dataset mit Dateinamen zu erhalten.

30
00:02:37,668 --> 00:02:42,625
Mit "TextLineDataset"
kann jede Datei gelesen

31
00:02:42,625 --> 00:02:45,467
und in eine Reihe von
Textzeilen umgewandelt werden.

32
00:02:45,467 --> 00:02:48,025
Dies ist eine 1-zu-n-Transformation.

33
00:02:48,025 --> 00:02:51,156
Ein Dateiname wird zu mehreren Textzeilen.

34
00:02:51,156 --> 00:02:54,070
Also wenden wir die Funktion "flat map".

35
00:02:54,070 --> 00:02:58,269
Wir haben jetzt ein
einziges Dataset mit Textzeilen,

36
00:02:58,269 --> 00:03:00,836
die aus all unseren Dateien
zusammengestellt wurden.

37
00:03:00,836 --> 00:03:03,006
Das ist nur eine Konzeptdarstellung.

38
00:03:03,006 --> 00:03:05,993
Die Dateien wurden
nicht alle in den Speicher geladen.

39
00:03:05,993 --> 00:03:07,630
Sie würden nicht hineinpassen.

40
00:03:07,630 --> 00:03:12,470
Zum Schluss verwenden
wir die Funktion "map",

41
00:03:12,470 --> 00:03:15,776
um eine 1-zu-1-Transformation
auf die Textzeilen anzuwenden.

42
00:03:15,776 --> 00:03:19,808
Jede Zeile wird als CSV,
also als kommagetrennte Werte,

43
00:03:19,808 --> 00:03:24,597
analysiert und in 
eine Featureliste umgewandelt.

44
00:03:24,597 --> 00:03:27,812
Hier erfolgt die Decodierung.

45
00:03:27,812 --> 00:03:33,192
Wir haben jetzt das Dataset
aus Features und Labels, das wir wollten.

46
00:03:36,039 --> 00:03:41,697
Wir mischen es mit einer
gegebenen Zufallsspeichergröße.

47
00:03:41,697 --> 00:03:46,754
Wir wiederholen das für
eine bestimmte Anzahl Epochen

48
00:03:46,754 --> 00:03:51,401
und teilen es in
Minibatches der Batchgröße auf.

49
00:03:51,401 --> 00:03:55,750
Schließlich rufen wir die
Funktion "get_next" auf,

50
00:03:55,750 --> 00:04:00,638
die die Features und Labels
als TensorFlow-Note zurückgibt.

51
00:04:00,638 --> 00:04:02,593
Das erwartet unser Modell.

52
00:04:02,593 --> 00:04:06,693
Jedes Mal, wenn das Modell während
des Trainings diese Notes ausführt,

53
00:04:06,693 --> 00:04:10,629
liefern sie das nächste
Batch von Features und Labels,

54
00:04:10,629 --> 00:04:15,303
wobei die Dateiladevorgänge schrittweise
nur bei Bedarf ausgelöst werden.

55
00:04:15,303 --> 00:04:20,678
Hier definieren wir
die Eingabefuntionen der Datasets

56
00:04:20,678 --> 00:04:29,170
für Training, Validierung und Test durch
Laden der entsprechenden CSV-Dateien,

57
00:04:29,170 --> 00:04:33,036
taxi-train.csv,
taxi-valid.csv und taxi-test.csv.

58
00:04:38,446 --> 00:04:41,882
Führen wir diese Zellen aus.

59
00:04:46,032 --> 00:04:51,786
Wir lassen unsere Featurespalten wie
sie sind, und können das Training starten.

60
00:04:56,892 --> 00:04:59,074
Das Training läuft.

61
00:04:59,074 --> 00:05:03,891
Dafür haben wir wie zuvor die Funktion
"model.train" in unserer Eingabefunktion

62
00:05:03,891 --> 00:05:07,750
aufgerufen, um das
Trainingsdataset zu erhalten.

63
00:05:10,367 --> 00:05:12,703
Wir haben ein trainiertes Modell.

64
00:05:12,703 --> 00:05:19,305
Und jetzt evaluieren wir es und
erhalten unsere Validierungsmesswerte.

65
00:05:22,448 --> 00:05:27,170
Hier sind sie.
Damit ist dieses Code-Lab beendet.