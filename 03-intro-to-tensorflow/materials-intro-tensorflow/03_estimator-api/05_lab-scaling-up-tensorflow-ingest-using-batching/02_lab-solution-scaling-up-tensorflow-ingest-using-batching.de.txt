Gehen wir gemeinsam 
dieses zweite Code-Lab durch. Diesmal sehen wir, wie man mit der Dataset
API Eingabefunktionen erstellen kann. Suchen wir die Lab-Dateien. Wir rufen cloud.goole.com/console auf und melden uns mit unserem Lab-Konto an. Hier sind wir in der Console. Wir öffnen Cloud Shell und verbinden uns wieder mit
der bestehenden Data Lab-Instanz, indem wir
"datalab connect mylab" eingeben. Die Verbindung ist hergestellt. Jetzt klicken wir
auf diese Preview-Schaltfläche. Ändern Sie den Port zu 8081,
diesen verwendet Data Lab. Und hier ist die bekannte
Notebook-Umgebung. Wir erstellen ein neues Notebook,
um ein paar Git-Befehle einzugeben und die Codes abzurufen. Also %bash und wir klonen dieses Repository. Es läuft, und das Repository ist
hier als "training-data-analyst" zu sehen. Suchen wir die Lab-Dateien. Wir klicken auf "training-data-analyst",
dann auf "courses". Dann auf "machine_learning". Dann auf "deepdive"
und schließlich auf "tensorflow". Das zweite Lab heißt "c_dataset",
öffnen Sie das. Mit der Dataset API können Sie für den
Estimator Eingabefunktionen erstellen. Sie lädt Daten progressiv und sollte immer
mit großen Datasets verwendet werden. Sehen wir uns den Code hier an. Unser Dataset auf der Festplatte besteht
aus mehreren, aufgeteilten CSV-Dateien. Wir verwenden "Dataset.list_files",
um die Festplatte zu scannen und ein Dataset mit Dateinamen zu erhalten. Mit "TextLineDataset"
kann jede Datei gelesen und in eine Reihe von
Textzeilen umgewandelt werden. Dies ist eine 1-zu-n-Transformation. Ein Dateiname wird zu mehreren Textzeilen. Also wenden wir die Funktion "flat map". Wir haben jetzt ein
einziges Dataset mit Textzeilen, die aus all unseren Dateien
zusammengestellt wurden. Das ist nur eine Konzeptdarstellung. Die Dateien wurden
nicht alle in den Speicher geladen. Sie würden nicht hineinpassen. Zum Schluss verwenden
wir die Funktion "map", um eine 1-zu-1-Transformation
auf die Textzeilen anzuwenden. Jede Zeile wird als CSV,
also als kommagetrennte Werte, analysiert und in 
eine Featureliste umgewandelt. Hier erfolgt die Decodierung. Wir haben jetzt das Dataset
aus Features und Labels, das wir wollten. Wir mischen es mit einer
gegebenen Zufallsspeichergröße. Wir wiederholen das für
eine bestimmte Anzahl Epochen und teilen es in
Minibatches der Batchgröße auf. Schließlich rufen wir die
Funktion "get_next" auf, die die Features und Labels
als TensorFlow-Note zurückgibt. Das erwartet unser Modell. Jedes Mal, wenn das Modell während
des Trainings diese Notes ausführt, liefern sie das nächste
Batch von Features und Labels, wobei die Dateiladevorgänge schrittweise
nur bei Bedarf ausgelöst werden. Hier definieren wir
die Eingabefuntionen der Datasets für Training, Validierung und Test durch
Laden der entsprechenden CSV-Dateien, taxi-train.csv,
taxi-valid.csv und taxi-test.csv. Führen wir diese Zellen aus. Wir lassen unsere Featurespalten wie
sie sind, und können das Training starten. Das Training läuft. Dafür haben wir wie zuvor die Funktion
"model.train" in unserer Eingabefunktion aufgerufen, um das
Trainingsdataset zu erhalten. Wir haben ein trainiertes Modell. Und jetzt evaluieren wir es und
erhalten unsere Validierungsmesswerte. Hier sind sie.
Damit ist dieses Code-Lab beendet.