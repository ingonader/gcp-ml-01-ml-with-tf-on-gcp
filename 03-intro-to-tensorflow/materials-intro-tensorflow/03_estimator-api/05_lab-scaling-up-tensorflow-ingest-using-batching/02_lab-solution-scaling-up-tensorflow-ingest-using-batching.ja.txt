2番目のCodelabに取りかかりましょう 今回はDataset APIを使って
input関数を作成します ラボのファイルの場所を見つけましょう cloud.google.com/consoleに移動し ラボのアカウントでログインします これがコンソールです Cloud Shellを開きます 既存のDatalabインスタンスに
再接続するために 「datalab connect mylab」と入力します 接続が確立されました この[ウェブでプレビュー]ボタンに移動します ポートを8081に変更します
Datalabはこれを使います いつものノートブックの
インターフェースが開きました 新しいノートブックを作成するために
gitコマンドを入力し コードをここに取得します %bash このリポジトリのクローンを作成します 実行中です。リポジトリの
training-data-analystが表示されました ラボ用のファイルを探しましょう training-data-analystをクリックして
coursesをクリックし 次はmachine_learning それからdeepdive
最後にtensorflowです 2番目のラボはc_datasetです
開いてみましょう Dataset APIを使って
Estimator用のinput関数を作成できます 順次読み込むので大きなデータセットの場合には
いつでも使ってください このコードを見てみましょう ディスク上ではデータセットが
多数の小さなCSVファイルになっています Dataset.list_files関数を使って
ディスクをスキャンし ファイル名からなるデータセットを取得します 次にTextLineDataset関数を使って
各ファイルを読み込み それを 複数テキスト行からなるセットに変換します これは1対多の変換です 1つのファイル名が
複数のテキスト行になります ですから flat_map関数を割り当てます これで1つのデータセットができました すべてのファイルの内容から得られた
複数テキスト行で構成されます これは概念的な表現にすぎません ファイルが全てメモリに
入ったわけではありません 収まらないからです 最後にmap関数を適用して 1対1変換をテキスト行に適用します 各行はCSV値のセットとして解析され 特徴のリストになります デコード自体はここで発生します これでfeaturesとlabelからなる
データセットができました 特定のバッファサイズでシャッフルします 特定のエポック数で繰り返し このバッチサイズのミニバッチに分割します 最後にget_next関数を呼び出します これはfeaturesとlabelを
TensorFlowノードとして返します モデルの想定どおりです モデルがこのノードを
トレーニングで実行するたびに featuresとlabelからなる
次のバッチが提供され ファイルの読み込み処理を
必要に応じて順次トリガーします ここでトレーニングの検証を定義し テストデータセット入力関数で
対応するCSVファイルを読み込みます taxi-train.csv、taxi-valid.csv、
taxi-text.csvです これらのセルを実行しましょう feature_columnはそのままにします
トレーニングの準備ができました トレーニングを実行しています 前と同じく
model.train関数を input関数で呼び出して トレーニング用データセットを取得します モデルのトレーニングが完了しました 最後に評価して評価指数を取得します できました
今回のCodelabはこれで終了です