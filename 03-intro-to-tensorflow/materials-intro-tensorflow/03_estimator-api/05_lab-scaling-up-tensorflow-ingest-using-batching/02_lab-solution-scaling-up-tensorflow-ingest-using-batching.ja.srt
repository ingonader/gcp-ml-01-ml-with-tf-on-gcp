1
00:00:00,000 --> 00:00:02,703
2番目のCodelabに取りかかりましょう

2
00:00:02,703 --> 00:00:08,615
今回はDataset APIを使って
input関数を作成します

3
00:00:08,615 --> 00:00:11,223
ラボのファイルの場所を見つけましょう

4
00:00:11,223 --> 00:00:16,429
cloud.google.com/consoleに移動し

5
00:00:16,429 --> 00:00:20,660
ラボのアカウントでログインします

6
00:00:27,969 --> 00:00:30,090
これがコンソールです

7
00:00:30,090 --> 00:00:32,740
Cloud Shellを開きます

8
00:00:33,360 --> 00:00:37,990
既存のDatalabインスタンスに
再接続するために

9
00:00:37,990 --> 00:00:43,348
「datalab connect mylab」と入力します

10
00:00:50,883 --> 00:00:53,883
接続が確立されました

11
00:00:53,883 --> 00:00:57,710
この[ウェブでプレビュー]ボタンに移動します

12
00:00:57,710 --> 00:01:01,498
ポートを8081に変更します
Datalabはこれを使います

13
00:01:04,184 --> 00:01:09,761
いつものノートブックの
インターフェースが開きました

14
00:01:09,761 --> 00:01:15,377
新しいノートブックを作成するために
gitコマンドを入力し

15
00:01:15,377 --> 00:01:17,483
コードをここに取得します

16
00:01:17,483 --> 00:01:20,645
%bash

17
00:01:20,645 --> 00:01:26,244
このリポジトリのクローンを作成します

18
00:01:29,862 --> 00:01:35,755
実行中です。リポジトリの
training-data-analystが表示されました

19
00:01:35,755 --> 00:01:38,130
ラボ用のファイルを探しましょう

20
00:01:38,130 --> 00:01:44,228
training-data-analystをクリックして
coursesをクリックし

21
00:01:44,228 --> 00:01:47,986
次はmachine_learning

22
00:01:47,986 --> 00:01:54,050
それからdeepdive
最後にtensorflowです

23
00:01:55,443 --> 00:02:00,578
2番目のラボはc_datasetです
開いてみましょう

24
00:02:07,300 --> 00:02:12,153
Dataset APIを使って
Estimator用のinput関数を作成できます

25
00:02:12,153 --> 00:02:17,889
順次読み込むので大きなデータセットの場合には
いつでも使ってください

26
00:02:19,665 --> 00:02:22,515
このコードを見てみましょう

27
00:02:24,839 --> 00:02:30,144
ディスク上ではデータセットが
多数の小さなCSVファイルになっています

28
00:02:30,144 --> 00:02:34,195
Dataset.list_files関数を使って
ディスクをスキャンし

29
00:02:34,195 --> 00:02:37,508
ファイル名からなるデータセットを取得します

30
00:02:37,508 --> 00:02:42,165
次にTextLineDataset関数を使って
各ファイルを読み込み それを

31
00:02:42,165 --> 00:02:45,467
複数テキスト行からなるセットに変換します

32
00:02:45,467 --> 00:02:48,025
これは1対多の変換です

33
00:02:48,025 --> 00:02:51,156
1つのファイル名が
複数のテキスト行になります

34
00:02:51,156 --> 00:02:54,070
ですから flat_map関数を割り当てます

35
00:02:54,070 --> 00:02:56,589
これで1つのデータセットができました

36
00:02:56,589 --> 00:03:00,836
すべてのファイルの内容から得られた
複数テキスト行で構成されます

37
00:03:00,836 --> 00:03:03,006
これは概念的な表現にすぎません

38
00:03:03,006 --> 00:03:05,993
ファイルが全てメモリに
入ったわけではありません

39
00:03:05,993 --> 00:03:07,630
収まらないからです

40
00:03:07,630 --> 00:03:11,170
最後にmap関数を適用して

41
00:03:11,170 --> 00:03:15,776
1対1変換をテキスト行に適用します

42
00:03:15,776 --> 00:03:21,238
各行はCSV値のセットとして解析され

43
00:03:21,238 --> 00:03:24,597
特徴のリストになります

44
00:03:24,597 --> 00:03:27,812
デコード自体はここで発生します

45
00:03:27,812 --> 00:03:33,192
これでfeaturesとlabelからなる
データセットができました

46
00:03:36,039 --> 00:03:41,697
特定のバッファサイズでシャッフルします

47
00:03:41,697 --> 00:03:46,754
特定のエポック数で繰り返し

48
00:03:46,754 --> 00:03:51,401
このバッチサイズのミニバッチに分割します

49
00:03:51,401 --> 00:03:55,750
最後にget_next関数を呼び出します

50
00:03:55,750 --> 00:04:00,638
これはfeaturesとlabelを
TensorFlowノードとして返します

51
00:04:00,638 --> 00:04:02,593
モデルの想定どおりです

52
00:04:02,593 --> 00:04:06,693
モデルがこのノードを
トレーニングで実行するたびに

53
00:04:06,693 --> 00:04:10,629
featuresとlabelからなる
次のバッチが提供され

54
00:04:10,629 --> 00:04:15,303
ファイルの読み込み処理を
必要に応じて順次トリガーします

55
00:04:15,303 --> 00:04:21,758
ここでトレーニングの検証を定義し

56
00:04:21,778 --> 00:04:28,860
テストデータセット入力関数で
対応するCSVファイルを読み込みます

57
00:04:28,860 --> 00:04:36,636
taxi-train.csv、taxi-valid.csv、
taxi-text.csvです

58
00:04:36,646 --> 00:04:39,652
これらのセルを実行しましょう

59
00:04:46,032 --> 00:04:51,786
feature_columnはそのままにします
トレーニングの準備ができました

60
00:04:56,892 --> 00:04:59,074
トレーニングを実行しています

61
00:04:59,074 --> 00:05:05,621
前と同じく
model.train関数を input関数で呼び出して

62
00:05:05,621 --> 00:05:08,370
トレーニング用データセットを取得します

63
00:05:10,367 --> 00:05:12,703
モデルのトレーニングが完了しました

64
00:05:12,703 --> 00:05:19,305
最後に評価して評価指数を取得します

65
00:05:22,448 --> 00:05:28,480
できました
今回のCodelabはこれで終了です