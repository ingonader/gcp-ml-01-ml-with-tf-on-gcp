3番目のCodelabに取りかかりましょう 今回はトレーニングと評価です いつもどおり
ラボのファイルの場所を見つけます cloud.google.com/consoleに移動し ラボのアカウントでログインします ログインしたら
このポータルでCloud Shellを開きます Datalabインスタンスに再接続するために 「datalab connect」と入力し
インスタンスの名前を入力します ここではmylabです 接続しています 完了したら
[ウェブでプレビュー]ボタンを使います ポートを8081に変更します
Datalabではこれを使うからです いつものノートブックの
インターフェースが開きました 新しいノートブックを作成して
bashコマンドを入力し コードを取り出します このリポジトリをgit cloneします それが完了すると
ここに training-data-analystが表示されます 3番目のCodelabのファイルは training-data-analyst、courses、
machine_learning deepdive、最後にtensorflowです このコードのラボは d_trainevalです
開いてみましょう ここではモデルの最後の仕上げをします では取りかかりましょう データの読み込みで
変更点はありません すでに完了しています
datasetを使用して 細分化されたCSVファイルから
CSVデータを読み込みます 特徴も変更しません
今はそのままにします これは新しい
serving_input（提供入力）関数です モデルのデプロイの準備を
完了するには これが必要です モデルがREST APIから予測を提供するとき データを JSONフィードとして受け取ります 幸いなことにAPIでは このCSVトレーニングデータと
まったく同じ形式のJSONフィードを 必ずしも使う必要はありません この serving_input関数が
必要なアノテーションを行います とはいえ ここで 
想定されるJSONフィードの形状を定義します 想定される名前を指定し 形状と 値のタイプをTensorFlowプレースホルダの
指定どおりに使います これらのプレースホルダは
JSONフィードからの値を受け取ります さらに この場合 追加の変換は必要ないと仮定すると 特徴は さきほど定義した
特徴プレースホルダとまったく同じになります モデルは この特徴の辞書を
そのまま理解できます train_and_evaluateを呼び出す準備が
ほとんどできました あと少しだけ構成が必要です モデルを選びます
ここでは LinearRegressorです TrainSpecを定義します ここで train_input関数が挿入されます exporterも定義します この LatestExporterです これは トレーニングの最後に デプロイ用モデルを
エクスポートするという意味です exporterは さきほど定義した
serving_input関数を知っている必要があります これです 次にEvalSpecを定義します ここに評価データの入力関数を挿入します また 評価とエクスポートはどちらも
チェックポイントの後で発生するので exporterをここで解析するのが適切です これで準備ができ 構成が完了しました TensorBoardを開始しましょう ここです TensorBoardが開始されました
クリックして開きます 今はダッシュボードが空ですね トレーニングをまだ始めていません では始めましょう このtrain_and_evaluate関数を実行します トレーニング中に生成されたマトリックスが
通常の出力ディレクトリに書き込まれます TensorBoardはその場所を知る必要があります ですから最初にここで指定しました トレーニングが開始したので
トレーニングログをここで確認でき TensorBoard側で
曲線がここにあります できました これが平均損失です 少し後でトレーニング用データセットで
計算された平均損失を確認し 評価用データセットでも確認します トレーニングと評価を
同時に行っているからです このモデルはまだトレーニング中です 終わるまで待ちましょう トレーニングが進むにつれて TensorBoardを自動的に更新表示できます その設定はここです データを30秒ごとにリロードします または この更新ボタンを押すと
データの表示が更新され トレーニング曲線がトレーニング中に
進捗するのを確認できます モデルのトレーニングが完了しました 最後に更新表示すると 最後のトレーニング曲線が表示されます これで終わりです
この曲線には注目すべき点がたくさんあり それは後で学習します 今は モデルのトレーニングが
あまり適切でないことだけを確認します 検証の損失は改善されていません でも それはわかっていました こうしてコードが動作し データが読み込まれ TensorBoardで動作を確認できるので データサイエンスを行う
準備ができました