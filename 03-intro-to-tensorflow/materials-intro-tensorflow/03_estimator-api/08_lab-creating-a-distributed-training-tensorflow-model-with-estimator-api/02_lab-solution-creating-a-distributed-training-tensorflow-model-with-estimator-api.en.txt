Let's go through our third cold lap together. This one about train and evaluate. First, as always, let's locate our lab files. So we go to clouds.google.com/console log in using your lab account. And once you're in, you can open cloud shell using this little portal and reconnect to your data lab instance by typing datalab connect and the name of your instance, mylab in my case. It's connecting. When it's done, you use the Web preview button. We have to change the port and go to 8081, because that is what datalab uses and we are in our familiar notebook interface. Let's create a new notebook to type in a couple of bash commands, so as to retrieve our code. So, git clone, this repository and as soon as this is done, we have the repository here as training data analyst. The files for this thread code lab or training data analyst courses machine learning, deep dive, and finally, TensorFlow. These code lab is the d_traineval. Let's open that. Here, we will be putting the final touches to our model. So let's go through it. There is nothing to change on the data loading front. We have already done that we used data sets to load CSV data from a set of charted CSV files. And, we are still not changing our features. They are good for now. This is new. The serving input function. We need it to make our model ready for deployment. When the model will be serving predictions from a REST API, it will be receiving data as a Json feed. Fortunately, the API does not force us to use a Json feed that looks exactly like our CSV training data. The serving input function is here to make the necessary annotations. Yet, here, you define the expected shape of your Json feed, with the names to expect and the shape, and type of values specified as TensorFlow placeholders. These placeholders will receive the values read from the Json feed. And in this case, let's say that's it we will not need any additional transformations, so we just say that our features are exactly the same as the feature placeholders we just defined. Our model can understand this dictionary of features as it is. We are almost ready to call, train, and evaluate. We just need a little bit of configuration. We pick a model, here a LinearRegressor. We define a training spec. This is where the train input function is plugged in here. We also define an exporter, here the LatestExporter which means that if we want to export the model ready for deployment at the end of the training. The exporter needs to know about the serving input function that we just defined, right here. Then we define our EvalSpec. This is where the eval data input function comes in and also, since both evaluations and exports only happen after a checkpoint, it makes sense to parse in our exporter here as well. And now, we are ready and configured. Let us start TensorBoard, right here. TensorBoard was started, we can click here to open it, and we see our dashboard empty for now. We have not started training yet. So, let's do that. Let's actually run this train and evaluate function. Matrix generated during training are written to our usual output directory, and TensorBoards needs to know where to find them. That's what we specified when we started it right here. And now that we have started the training, we see our training logs here and we should be saying on the tensor board site, our curves up here and here they are. Here is our average loss. And after a while, we start seeing our average loss computed on our training data set, and also on our evaluation data set. That's because we are training and evaluating at the same time. So, this model is still training. Let's wait until it finishes. And as the training progresses, TensoBoard can either refresh automatically. You have this under this setting here, reload data every 30 seconds, or you can hit the refresh button to refresh the data and see your training curves as they evolve during training. And the model is now trained. And if I refresh the last time here, I will see the final training curves. That's it. There is a lot to see in this training curves, you will learn that later. For the moment, we just see that our model is not training very well. The validation loss is not improving. But we already knew that. Now that the code works, data gets loaded, and we can see what is going in TensorBoard, we are ready to do some data science.