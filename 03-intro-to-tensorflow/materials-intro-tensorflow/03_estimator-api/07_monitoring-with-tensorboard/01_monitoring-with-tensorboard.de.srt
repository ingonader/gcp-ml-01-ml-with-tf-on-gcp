1
00:00:00,000 --> 00:00:04,805
Hier erfahren Sie, wie Sie das Training
mit TensorBoard überwachen können.

2
00:00:04,805 --> 00:00:06,655
Das haben wir bisher gelernt:

3
00:00:06,655 --> 00:00:08,715
Zwei Prüfungen
kennen wir, zwei fehlen noch.

4
00:00:08,715 --> 00:00:11,180
Wir nutzen schon "train_and_evaluate",

5
00:00:11,180 --> 00:00:15,300
sodass wir im Verlauf des Trainings
Validierungsmesswerte erhalten.

6
00:00:15,300 --> 00:00:19,680
Visualisieren wir sie
jetzt mit dem Tool TensorBoard.

7
00:00:19,680 --> 00:00:22,800
Das ist bei jedem Training sinnvoll.

8
00:00:22,800 --> 00:00:25,550
Sie finden viele nützliche Informationen,

9
00:00:25,550 --> 00:00:29,475
wenn Sie das Training und die Validierung
der Kurven in einem Graphen vergleichen.

10
00:00:29,475 --> 00:00:32,880
Ich nutze "train_and_evaluate" immer,

11
00:00:32,880 --> 00:00:35,525
nicht nur beim verteilten Training.

12
00:00:35,525 --> 00:00:39,200
TensorBoard ist ein Tool, mit dem Sie
die Training- und Validierungsmesswerte,

13
00:00:39,200 --> 00:00:42,290
die Ihr Modell auf die
Festplatte schreibt, visualisieren können.

14
00:00:42,290 --> 00:00:46,385
TensorBoard ist standardmäßig in
jeder TensorFlow-Installation enthalten.

15
00:00:46,385 --> 00:00:48,000
Es ist ein Befehlszeilentool,

16
00:00:48,000 --> 00:00:52,450
und verweist auf das Ausgabeverzeichnis
Ihrer Ausführungskonfiguration.

17
00:00:52,450 --> 00:00:58,525
Das TensorBoard-Dashboard wird
in der Local-Host-Spalte 606 angezeigt.

18
00:00:58,525 --> 00:01:03,300
Vorgefertigte Estimators enthalten
vordefinierte Standardmesswerte,

19
00:01:03,300 --> 00:01:05,595
sodass Sie nichts weiter
konfigurieren müssen.

20
00:01:05,595 --> 00:01:10,185
Zum Beispiel sehen Sie den Trainings-
und Validierungsverlust im selben Graphen.

21
00:01:10,185 --> 00:01:13,240
Das hilft dabei, eine
Überanpassung des Modells zu erkennen.

22
00:01:13,240 --> 00:01:15,599
Der Estimator des
dichten neuronalen Netzwerks

23
00:01:15,599 --> 00:01:20,115
zeichnet auch den Anteil von
Neuronen auf, die Nullen ausgeben.

24
00:01:20,115 --> 00:01:24,220
Das passiert, wenn Sie die
ReLU-Aktivierungsfunktion verwenden,

25
00:01:24,220 --> 00:01:26,175
aber Sie sollten sie im Auge behalten.

26
00:01:26,175 --> 00:01:28,710
Wenn alle Neuronen Nullen ausgeben,

27
00:01:28,710 --> 00:01:30,605
ist das neuronale Netzwerk tot.

28
00:01:30,605 --> 00:01:35,085
In TensorBoard können Sie
den TensorFlow-Graphen sehen.

29
00:01:35,085 --> 00:01:37,710
Das kann in der Fehlersuche nützlich sein

30
00:01:37,710 --> 00:01:41,105
oder wenn Sie sehen möchten,
welchen Graphen Ihr Code erzeugt hat.

31
00:01:41,105 --> 00:01:44,160
Wenn Sie einen
benutzerdefinierten Estimator erstellen,

32
00:01:44,160 --> 00:01:47,850
indem Sie Ihre eigenen
neuronalen Netzwerkschichten angeben,

33
00:01:47,850 --> 00:01:51,420
können Sie auch Befehle vom Typ
"tf.summary.irgendwas" verwenden,

34
00:01:51,420 --> 00:01:55,785
um Datentypen zu protokollieren
und in TensorBoard zu visualisieren.

35
00:01:55,785 --> 00:01:57,725
Das können Zahlen, Text,

36
00:01:57,725 --> 00:01:59,840
Bilder oder sogar Audiodateien sein.

37
00:01:59,840 --> 00:02:04,605
Mit der Estimator API braucht ein Modell
nur eine Zeile, um etwas zu schreiben.

38
00:02:04,605 --> 00:02:07,410
"tf.summary.scalar"
und den Namen des Graphen,

39
00:02:07,410 --> 00:02:10,380
auf dem Sie
das in TensorBoard sehen möchten,

40
00:02:10,380 --> 00:02:12,750
und den Tensor mit
den zu plottenden Werten.

41
00:02:12,750 --> 00:02:15,650
Wenn Sie nicht die
Estimator API verwenden,

42
00:02:15,650 --> 00:02:17,820
gibt es einige zusätzliche Schritte,

43
00:02:17,820 --> 00:02:20,140
die Sie in der Dokumentation
hier nachlesen können.

44
00:02:20,140 --> 00:02:24,015
Zum Beispiel ist dies ein Histogramm-Plot.

45
00:02:24,015 --> 00:02:28,640
Ich visualisiere gerne negative Dinge,
die mit Neuronenausgaben passieren können.

46
00:02:28,640 --> 00:02:29,990
Hier auf der linken Seite

47
00:02:29,990 --> 00:02:35,365
haben wir ein Histogramm aller Werte
aus einer Schicht des neuronalen Netzes,

48
00:02:35,365 --> 00:02:37,230
die von einem Sigmoid aktiviert wird.

49
00:02:37,230 --> 00:02:38,690
Dann sehen wir ein Problem.

50
00:02:38,690 --> 00:02:41,350
Es gibt ein Maximum bei null,
ein weiteres bei eins,

51
00:02:41,350 --> 00:02:45,720
und die meisten Neuronen sind gesättigt
und wahrscheinlich nicht sehr nützlich.

52
00:02:45,720 --> 00:02:49,995
Eine Regularisierungstechnik, 
die Batch-Normalisierung, behebt das.

53
00:02:49,995 --> 00:02:53,415
Hier ist die Ausgabe
derselben Schicht nach "batch norm",

54
00:02:53,415 --> 00:02:58,605
jetzt produzieren die Neuronen Werte
aus dem gesamten nützlichen Bereich.

55
00:02:58,605 --> 00:03:01,170
Ob dies zu besseren Ergebnissen führt,

56
00:03:01,170 --> 00:03:03,130
hängt vom Modell ab,

57
00:03:03,130 --> 00:03:06,705
aber zumindest sehe ich, dass
die Batch-Normalisierung funktioniert.

58
00:03:06,705 --> 00:03:12,070
Wenn Sie mit Bildern oder Ton arbeiten,
bietet TensorBoard spezielle Dashboards,

59
00:03:12,070 --> 00:03:14,995
auf denen Sie sehen
und hören können, was gerade passiert.

60
00:03:14,995 --> 00:03:17,930
Sie können die Funktionen "summary.image"

61
00:03:17,930 --> 00:03:21,270
und "summary.audio"
in Ihrem Code verwenden, um anzugeben,

62
00:03:21,270 --> 00:03:29,099
dass der von Ihnen protokollierte Tensor
ein Bild oder eine Audiodatei darstellt,

63
00:03:29,099 --> 00:03:33,945
dann wird er in TensorBoard auf dem
zugehörigen Dashboard angezeigt.

64
00:03:33,945 --> 00:03:37,400
Hier ist zum Beispiel eine Visualisierung,
die ich bei der Entwicklung

65
00:03:37,400 --> 00:03:40,190
eines Modells zur
Flugzeugerkennung verwendet habe.