1
00:00:00,000 --> 00:00:04,970
大きなデータセットと分散型トレーニングで
モデルをトレーニングしました

2
00:00:04,970 --> 00:00:10,500
TensorBoard曲線が終わりました
次の項目「デプロイ」は簡単です

3
00:00:10,500 --> 00:00:14,090
ML EngineのCloud Consoleで
何度かクリックすれば

4
00:00:14,090 --> 00:00:17,970
トレーニング済みモデルが
マネージドREST APIの裏で起動し

5
00:00:17,970 --> 00:00:20,720
JSONトラフィックを受け入れます

6
00:00:20,730 --> 00:00:23,030
ちよっと待って
JSONですか？

7
00:00:23,030 --> 00:00:25,980
モデルは JSONの読み方がわかりません

8
00:00:25,980 --> 00:00:29,630
トレーニング/テストデータの
input関数はありますが

9
00:00:29,630 --> 00:00:33,470
JSONデータがRESTエンドポイントに
来ても処理できません

10
00:00:33,470 --> 00:00:35,290
ですから1つ追加します

11
00:00:35,290 --> 00:00:40,260
すでにEvalSpecで説明した
exportersパラメータを思い出してください

12
00:00:40,260 --> 00:00:42,520
これで完全なモデルが定義されます

13
00:00:42,520 --> 00:00:45,900
トレーニング済みパラメータの
チェックポイントを使って

14
00:00:45,900 --> 00:00:48,110
デプロイの準備ができるだけでなく

15
00:00:48,110 --> 00:00:50,480
さらに 追加の入力関数を使用し

16
00:00:50,480 --> 00:00:56,730
REST APIが受け取るJSONと
モデルが想定する特徴をマップします

17
00:00:56,730 --> 00:01:00,050
この関数の名前はserving_inputです

18
00:01:00,050 --> 00:01:01,410
キーポイントは

19
00:01:01,410 --> 00:01:06,020
提供時とトレーニング時の入力が
しばしばまったく異なることです

20
00:01:06,020 --> 00:01:10,080
それを理解するために
もう一度詳しく確認しましょう

21
00:01:10,080 --> 00:01:13,050
TensorFlowでは
すべてのものがグラフです

22
00:01:13,050 --> 00:01:18,150
モデルをインスタンス化したときの
グラフがここにあります

23
00:01:18,150 --> 00:01:23,290
予測の方がやや単純ですが
基本的にトレーニング時と推測時は同じで

24
00:01:23,290 --> 00:01:26,838
どちらも特徴を読み込んで
予測を生成します

25
00:01:26,838 --> 00:01:30,018
データソースを入力に接続しましょう

26
00:01:30,018 --> 00:01:34,222
トレーニングでは
training_input関数を使います

27
00:01:34,222 --> 00:01:41,846
Dataset API を使ってCSVファイルから
順次読み込む入力ノードを作成し

28
00:01:41,846 --> 00:01:45,260
トレーニングデータの
バッチをモデルに送ります

29
00:01:45,260 --> 00:01:49,280
これに似たパターンを
デプロイ済みモデルでも使います

30
00:01:49,280 --> 00:01:52,340
serving_input（提供入力）関数では

31
00:01:52,340 --> 00:01:56,834
REST APIが受け取るJSONと
モデルが想定する機能の間に

32
00:01:56,834 --> 00:01:59,344
TensorFlow変換を追加できます

33
00:01:59,400 --> 00:02:03,950
JSONの解析は必要ありません
ML Engineが自動的に処理します

34
00:02:03,950 --> 00:02:07,690
その他の変換を
ここで記述する必要があります

35
00:02:08,360 --> 00:02:14,540
「RESTエンドポイントに入る すべての
データでserving_input関数が呼び出される」

36
00:02:14,540 --> 00:02:18,070
とよく誤解されますが
それは間違いです

37
00:02:18,070 --> 00:02:21,710
これはモデルのインスタンス化時に
1回だけ実行されます

38
00:02:21,710 --> 00:02:25,900
そこで生成されるTensorFlowグラフは

39
00:02:25,900 --> 00:02:31,080
一方で JSONパーサーに接続され
もう一方でモデルに接続されます

40
00:02:31,080 --> 00:02:34,440
JSON値から特徴に変換する方法を
自由に決められますが

41
00:02:34,440 --> 00:02:37,670
その際に必ずTensorFlowコマンドを
使ってください

42
00:02:37,670 --> 00:02:40,599
これにより変換のグラフが返されます

43
00:02:42,120 --> 00:02:44,920
グラフの断片を
いつまとめるのでしょうか？

44
00:02:44,920 --> 00:02:48,000
exporterでserving_input関数を指定して

45
00:02:48,000 --> 00:02:53,940
exporterをeval_specに追加すると
接続が発生します

46
00:02:53,940 --> 00:02:59,150
exporterはチェックポイント付きモデル
および変換情報を

47
00:02:59,150 --> 00:03:04,260
エクスポートモデルファイルの中に保存し
デプロイの準備ができます

48
00:03:04,260 --> 00:03:08,760
どのチェックポイントが保存されるかは
exporterの種類によります

49
00:03:08,780 --> 00:03:11,720
最もシンプルなのは このLatestExporterです

50
00:03:11,720 --> 00:03:15,830
これは利用できる最新のチェックポイントを
受け入れます

51
00:03:15,840 --> 00:03:20,820
ディスクにエクスポートされたモデルは
exportフォルダに入ります

52
00:03:20,820 --> 00:03:25,468
APIでこのexporterに
「pricing」という名前を付けたので

53
00:03:25,468 --> 00:03:28,518
pricingサブフォルダが作成されました

54
00:03:28,620 --> 00:03:33,990
この中の数字付きフォルダはそれぞれ
デプロイ準備ができたモデルです

55
00:03:33,990 --> 00:03:39,400
REST APIをテストするには JSONデータを
エンドポイントで送るだけです

56
00:03:39,400 --> 00:03:43,848
Google Cloud SDKの
gcloud ml-engine predictコマンドで

57
00:03:43,848 --> 00:03:48,900
JSONファイルのデータを使って
簡単にテストできます

58
00:03:48,900 --> 00:03:52,380
構文は1つのJSONフィールド「instances」です

59
00:03:52,380 --> 00:03:55,080
この中にJSONオブジェクトの一覧が

60
00:03:55,080 --> 00:03:59,950
serving_input関数の想定する形式で
含まれます

61
00:03:59,950 --> 00:04:03,660
ここにsq_footageとprop_typeがあります

62
00:04:03,660 --> 00:04:08,561
一覧のデータインスタンスは
自動的にバッチにまとまり

63
00:04:08,561 --> 00:04:16,007
serving_input関数が 面積の数値の一覧と
物件タイプ文字列の一覧を受け取ります

64
00:04:16,007 --> 00:04:20,223
これはデプロイしないでテストする
簡単な方法です

65
00:04:20,223 --> 00:04:24,071
gcloud ml-engine local predictコマンドでは

66
00:04:24,071 --> 00:04:28,330
ディスク上のエクスポートされた
モデルから予測を直接取得できます

67
00:04:28,330 --> 00:04:30,300
デプロイは不要です

68
00:04:30,300 --> 00:04:33,940
なお このコマンドでは
少し違う形式が想定されています

69
00:04:33,940 --> 00:04:38,600
JSONオブジェクトの一覧をファイルで提供し
1行に1つのオブジェクトです

70
00:04:38,820 --> 00:04:42,720
serving_input関数の一般的な使い方が
もう一つあります

71
00:04:42,720 --> 00:04:44,600
JPEG画像のデコードです

72
00:04:44,600 --> 00:04:46,890
画像を処理するモデルを扱う場合

73
00:04:46,890 --> 00:04:50,750
画像を常に圧縮して
ネットワーク経由で送るでしょう

74
00:04:50,750 --> 00:04:55,170
しかしモデルは
圧縮されていない画像を想定します

75
00:04:55,170 --> 00:04:58,380
serving_input関数で圧縮解除できます

76
00:04:58,380 --> 00:05:00,910
これがサンプルコードです

77
00:05:00,910 --> 00:05:06,070
JSONフィードから画像が
tf.string型として直接送られます

78
00:05:06,070 --> 00:05:11,160
これは TensorFlowではバイト文字列
つまりランダムなバイト一覧です

79
00:05:11,160 --> 00:05:13,580
JPEGはバイナリ形式ですが これを

80
00:05:13,580 --> 00:05:20,790
JSONで使えるテキスト文字列に変えるには
base64エンコードが必要です

81
00:05:20,790 --> 00:05:22,303
TensorFlowでは

82
00:05:22,303 --> 00:05:29,390
base64エンコードされたバイナリ文字列を
マークするカスタム JSON表記を採用します

83
00:05:29,390 --> 00:05:34,330
フィールド名の末尾に「_bytes」を付け
値をb64というJSONオブジェクトにして

84
00:05:34,330 --> 00:05:39,290
base64エンコードされた
文字列を値として指定します

85
00:05:40,490 --> 00:05:44,750
この表記によりbase64デコードが
自動的に始まります

86
00:05:44,750 --> 00:05:48,600
serving_input関数で
それを処理する必要はありません