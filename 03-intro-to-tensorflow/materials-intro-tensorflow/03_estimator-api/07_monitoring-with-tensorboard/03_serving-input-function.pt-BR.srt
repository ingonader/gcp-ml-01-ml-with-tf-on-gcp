1
00:00:00,470 --> 00:00:04,970
Treinamos o modelo em um grande conjunto
de dados usando treinamento distribuído,

2
00:00:04,970 --> 00:00:09,390
e nossas curvas TensorBoard estão
corretas. É hora de implantar.

3
00:00:09,390 --> 00:00:10,410
Isso é fácil.

4
00:00:10,410 --> 00:00:14,770
Alguns cliques no Console do Cloud ML
Engine e o modelo de treino estará ativo

5
00:00:14,770 --> 00:00:20,730
na API REST com escalonamento automático
gerenciado, pronto para o tráfego JSON.

6
00:00:20,730 --> 00:00:23,030
Espere. Você disse JSON?

7
00:00:23,030 --> 00:00:25,980
O modelo não sabe ler JSON.

8
00:00:25,980 --> 00:00:29,700
Temos funções de entrada para treinamento
e dados de teste, mas não

9
00:00:29,700 --> 00:00:33,510
para dados JSON ativos chegando ao ponto
de extremidade da REST.

10
00:00:33,510 --> 00:00:35,190
É hora de adicionar uma.

11
00:00:35,190 --> 00:00:40,760
Lembre do parâmetro do exportador
mencionado no eval_spec anteriormente.

12
00:00:40,760 --> 00:00:42,970
É isso que define um modelo completo.

13
00:00:42,970 --> 00:00:48,030
Pronto para implementação com um ponto de
verificação em bons parâmetros treinados

14
00:00:48,030 --> 00:00:51,890
e também com uma função de entrada
extra que será mapeada entre o JSON

15
00:00:51,890 --> 00:00:56,730
recebido pela API REST e os atributos
esperados pelo modelo.

16
00:00:56,730 --> 00:01:00,070
Este é chamado de função
serving_input_fn.

17
00:01:00,070 --> 00:01:01,410
Aqui está o ponto principal.

18
00:01:01,410 --> 00:01:06,020
Entradas de tempo de serviço e treinamento
geralmente são muito diferentes.

19
00:01:06,020 --> 00:01:10,080
Para entender o que está acontecendo,
vamos nos aprofundar mais nisso.

20
00:01:10,080 --> 00:01:13,050
No TensorFlow, tudo é um gráfico.

21
00:01:13,050 --> 00:01:18,150
E aqui está o gráfico de modelo produzido
quando instanciamos o modelo.

22
00:01:18,150 --> 00:01:22,590
É essencialmente o mesmo no tempo de
treino e inferência, mesmo que seja

23
00:01:22,590 --> 00:01:26,828
um pouco mais simples para a inferência,
ele absorve recursos e produz previsões.

24
00:01:26,828 --> 00:01:30,018
Vamos conectar uma fonte de dados
às entradas.

25
00:01:30,018 --> 00:01:34,222
No tempo de treinamento, isso é feito por
meio da função training_input_fn.

26
00:01:34,222 --> 00:01:39,516
Usamos os dados como uma API para criar um
node de entrada que lê progressivamente

27
00:01:39,516 --> 00:01:45,260
a partir de arquivos CSV e envia lotes de
dados de treinamento para o modelo.

28
00:01:45,260 --> 00:01:49,280
Usaremos um padrão semelhante
para o modelo implantado.

29
00:01:49,280 --> 00:01:53,910
A função serving_input_fn adiciona um
conjunto de transformações do TensorFlow

30
00:01:53,910 --> 00:01:58,529
entre o JSON que a API REST recebe e
os atributos esperados pelo modelo.

31
00:01:59,400 --> 00:02:03,950
Não precisamos analisar o JSON, que é
processado automaticamente pela ML Engine,

32
00:02:03,950 --> 00:02:07,010
mas quaisquer outras transformações
precisam ser gravadas lá.

33
00:02:08,360 --> 00:02:12,180
É um equívoco comum acreditar que a
função serving_input_fn

34
00:02:12,180 --> 00:02:16,030
será chamada em todos os dados
que o ponto de extremidade REST recebe.

35
00:02:16,030 --> 00:02:18,065
Não é assim que funciona.

36
00:02:18,065 --> 00:02:21,710
Ela é executada apenas uma vez, quando
o modelo é instanciado.

37
00:02:21,710 --> 00:02:26,170
E produz um gráfico do TensorFlow,
conectado em uma extremidade

38
00:02:26,170 --> 00:02:31,080
ao analisador JSON e, na outra
extremidade, ao modelo.

39
00:02:31,080 --> 00:02:34,500
Como você transforma dados de valores
JSON em recursos depende de você,

40
00:02:34,500 --> 00:02:37,670
mas lembre-se de fazer isso com
comandos do TensorFlow,

41
00:02:37,670 --> 00:02:40,599
para que um gráfico de transformações
seja retornado.

42
00:02:42,120 --> 00:02:44,920
Quando todos esses pedaços
de gráfico se juntam?

43
00:02:44,920 --> 00:02:49,400
A conexão acontece quando você especifica
a função serving_input_fn

44
00:02:49,400 --> 00:02:53,940
no exportador e o adiciona
ao eval_spec.

45
00:02:53,940 --> 00:02:58,140
Ele salvará uma versão com pontos de
verificação do modelo com as

46
00:02:58,140 --> 00:03:02,000
informações de transformação em um
arquivo de modelo exportado

47
00:03:02,000 --> 00:03:04,040
que está pronto para ser implantado.

48
00:03:04,260 --> 00:03:06,300
Qual ponto de verificação é salvo?

49
00:03:06,300 --> 00:03:08,780
Isso depende do tipo do exportador.

50
00:03:08,780 --> 00:03:12,270
O mais simples é o mais recente
exportador usado aqui,

51
00:03:12,270 --> 00:03:14,650
que usa o último ponto de
verificação disponível.

52
00:03:15,840 --> 00:03:20,810
Podemos ver o modelo exportado no disco
aqui na pasta export.

53
00:03:20,810 --> 00:03:27,417
Chamamos esse exportador de "pricing"
na API, e uma subpasta pricing foi criada.

54
00:03:28,620 --> 00:03:33,990
Nela, cada pasta numerada é um modelo
pronto para implementação.

55
00:03:33,990 --> 00:03:39,460
Para testar a API REST, basta enviar dados
JSON no ponto de extremidade.

56
00:03:39,460 --> 00:03:43,218
O Google Cloud SDK tem o comando de
previsão do Cloud ML Engine

57
00:03:43,218 --> 00:03:48,900
que permite testar facilmente os
dados em um arquivo JSON.

58
00:03:48,900 --> 00:03:53,480
A sintaxe para isso é um único campo JSON
chamado instances, que contém

59
00:03:53,480 --> 00:03:59,950
uma lista de objetos JSON com o formato
esperado pela função serving_input_fn.

60
00:03:59,950 --> 00:04:03,660
Aqui, área e
tipo de propriedade.

61
00:04:03,660 --> 00:04:08,561
As instâncias de dados na lista
serão agrupadas automaticamente

62
00:04:08,561 --> 00:04:13,697
e a função serving_input_fn receberá uma
lista de números de metragem quadrada e

63
00:04:13,697 --> 00:04:16,084
uma lista de strings de tipo
de propriedade.

64
00:04:16,084 --> 00:04:20,223
Há uma maneira ainda mais fácil de
testar sem implantar nada.

65
00:04:20,223 --> 00:04:24,071
O comando local de previsão do Google
Cloud ML Engine permite que você

66
00:04:24,071 --> 00:04:28,330
obtenha previsões diretamente de um
modelo exportado no disco.

67
00:04:28,330 --> 00:04:30,280
Não é preciso implantar.

68
00:04:30,300 --> 00:04:33,940
Observe o formato ligeiramente diferente
esperado por esse comando.

69
00:04:33,940 --> 00:04:37,530
Uma lista de objetos JSON em um arquivo,
um objeto por linha.

70
00:04:38,820 --> 00:04:42,590
Aqui está outro uso muito típico de uma
função serving_input _fn:

71
00:04:42,590 --> 00:04:44,360
decodificação de imagens JPEG.

72
00:04:44,360 --> 00:04:47,190
Se você trabalha com um modelo
de processamento de imagens,

73
00:04:47,190 --> 00:04:50,750
sempre as envia
pela rede compactadas.

74
00:04:50,750 --> 00:04:55,170
Mas o modelo espera que
elas estejam descompactadas.

75
00:04:55,170 --> 00:04:58,380
A função serving_input_fn pode
manipular a descompactação.

76
00:04:58,380 --> 00:05:00,910
E este é o código de
exemplo para isso.

77
00:05:00,910 --> 00:05:06,070
Você vê que a imagem está no
feed JSON como tipo tf.string, que na

78
00:05:06,070 --> 00:05:11,160
terminologia do TensorFlow é uma cadeia
de bytes, uma lista de bytes aleatórios.

79
00:05:11,160 --> 00:05:15,480
Sim, o JPEG é um formato binário e a
codificação da base 64

80
00:05:15,480 --> 00:05:20,790
é necessária para transformá-lo em uma
string de texto que funcionará no JSON.

81
00:05:20,790 --> 00:05:24,703
O TensorFlow adota uma convenção JSON
personalizada para marcar

82
00:05:24,703 --> 00:05:29,390
a string binária codificada
na base 64 como tal.

83
00:05:29,390 --> 00:05:34,330
O nome do campo termina com _bytes
e o valor é um objeto JSON

84
00:05:34,330 --> 00:05:39,290
chamado b64, com a string codificada na
base 64 como o valor.

85
00:05:40,490 --> 00:05:45,310
Com essa convenção, a decodificação da
base 64 ocorre automaticamente.

86
00:05:45,310 --> 00:05:48,580
Você não precisa lidar com isso na
função serving_input_fn.