Here, we will show how to monitor training by using TensorBoard. Let's revisit our progress. Two checks, two more to go. We are already using train and evaluate, so we are getting evaluation metrics as training progresses. Let us visualize them using a tool called TensorBoard. This is actually a best practice for any training. There are many useful things you can see when you compare training and evaluation of curves on a graph. I use train and evaluate for that all the time, not just when running distributed training. TensorBoard is a tool that lets you visualize the training and the biometrics that your model writes to disk. TensorBoard comes as standard with your TensorFlow installation. It's a common line tool, pointed to the output directory you have specified in your run config and TensorBoard dashboard appears at local host column 606. Pre-made estimators come with a set of predefined standard metrics, so there is nothing else for you to configure. For example, you will see your training and evaluation loss on the same graph. This is useful to see if your model is overfitting. The dense neural network estimator, also tracks the fraction of neurons that are outputting zeros. This does happen when you use the ReLU activation function, but you should keep an eye on it. If all your neurons are outputting zeros, your neural network is dead. TensorBoard is also where you can see your TensorFlow graph. This might be useful for debugging or if you want to see what graph your code has produced. If you're building a custom estimator, specifying your own neural network layers, you can also use tf dot summary dot something commands to log various types of data and visualize them in TensorBoard. They can be numbers, text, images, or even audio files. With the estimator API one line is really all it takes in a model to write something out. Tf dot summary dot scalar and then the name of the graph on which you want to see this in TensorBoard, and the Tensor with the values to plot. If you're not using the estimator API, there are a couple of additional steps, check them out in the documentation here. For example, this is a histogram plot. I find it useful to visualize bad things that can happen on your own outputs. Here on the left, we have a histogram through time of all the values coming out of a neural network layer activated by a sigmoid. Then we see a problem. There is a peak at zero, another at one and most of our neurons are saturated and probably not very useful. A regularization technique, called batch normalization can fix that. Here is the output of the same layer after batch norm, and now our neurons are producing values across the entire useful range. Whether this produces better results or not, will depend on the model, but at least I see that by batched normalization is working. When working with images or sounds, TensorBoard has specific dashboards where you can see and hear what is going on. You can use the summary dot image and summary dot audio functions in your code to specify that the Tensor you are logging represents an image or an audio file, and they will appear in their dedicated dashboard in TensorBoard. Here is for example, a visualization I was using when developing an airplane detection model.