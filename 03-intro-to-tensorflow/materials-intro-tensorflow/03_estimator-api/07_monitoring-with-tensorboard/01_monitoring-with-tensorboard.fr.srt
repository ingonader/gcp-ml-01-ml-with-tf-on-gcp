1
00:00:00,000 --> 00:00:04,495
Nous allons apprendre à surveiller
l'entraînement à l'aide de TensorBoard.

2
00:00:04,775 --> 00:00:06,405
Récapitulons notre avancement.

3
00:00:06,405 --> 00:00:07,770
Deux cases cochées !

4
00:00:07,770 --> 00:00:09,010
Encore deux points à voir.

5
00:00:09,010 --> 00:00:11,420
Nous savons déjà utiliser
"train_and_evaluate"

6
00:00:11,420 --> 00:00:15,190
pour obtenir des métriques d’évaluation
au fur et à mesure de l'entraînement.

7
00:00:15,360 --> 00:00:19,680
Visualisons-les
à l'aide d'un outil nommé TensorBoard.

8
00:00:19,680 --> 00:00:23,020
C'est en fait une méthode recommandée
pour n'importe quel entraînement.

9
00:00:23,020 --> 00:00:25,620
Vous pouvez voir beaucoup de choses utiles

10
00:00:25,620 --> 00:00:29,055
en comparant les courbes d'entraînement
et d'évaluation sur un graphique.

11
00:00:29,055 --> 00:00:32,960
J'utilise souvent
"train_and_evaluate" pour cela,

12
00:00:32,960 --> 00:00:35,525
et pas uniquement
pour l'entraînement distribué.

13
00:00:35,525 --> 00:00:39,060
TensorBoard vous permet
de visualiser les métriques d'entraînement

14
00:00:39,060 --> 00:00:42,180
et d'évaluation
que votre modèle écrit sur disque.

15
00:00:42,180 --> 00:00:46,195
TensorBoard est inclus
dans votre installation TensorFlow.

16
00:00:46,195 --> 00:00:48,000
C'est un outil de ligne de commande.

17
00:00:48,000 --> 00:00:50,620
Pointez TensorBoard
sur le répertoire de sortie spécifié

18
00:00:50,620 --> 00:00:52,380
dans votre configuration d'exécution,

19
00:00:52,380 --> 00:00:57,895
et le tableau de bord TensorBoard
s'affiche sur "localhost:6006".

20
00:00:58,685 --> 00:01:03,095
Les estimateurs prédéfinis incluent un
jeu de métriques standard prédéfinies.

21
00:01:03,095 --> 00:01:05,595
Vous n'avez rien d'autre à configurer.

22
00:01:05,595 --> 00:01:08,985
Vous pouvez voir par exemple
votre perte d'entraînement et d'évaluation

23
00:01:08,985 --> 00:01:10,170
sur le même graphique.

24
00:01:10,450 --> 00:01:13,429
C'est utile pour voir
si votre modèle est en surapprentissage.

25
00:01:13,429 --> 00:01:18,355
L'estimateur de réseau de neurones dense
suit également la quantité de neurones

26
00:01:18,355 --> 00:01:19,900
qui émettent des zéros.

27
00:01:20,040 --> 00:01:23,940
Cela se produit lorsque vous utilisez
la fonction d'activation ReLU.

28
00:01:23,940 --> 00:01:26,175
Nous vous conseillons
de garder un œil dessus.

29
00:01:26,525 --> 00:01:30,440
Si tous vos neurones émettent des zéros,
alors votre réseau de neurones est mort.

30
00:01:31,065 --> 00:01:35,085
TensorBoard vous permet également
de consulter votre graphique TensorFlow.

31
00:01:35,085 --> 00:01:37,290
Cela peut être utile pour le débogage,

32
00:01:37,290 --> 00:01:40,515
ou si vous voulez voir
quel graphique votre code a produit.

33
00:01:41,855 --> 00:01:44,000
Si vous construisez
un estimateur personnalisé

34
00:01:44,000 --> 00:01:47,705
en spécifiant vos propres couches
de réseau de neurones,

35
00:01:47,705 --> 00:01:51,335
vous pouvez aussi utiliser
les commandes de type "tf.summary"

36
00:01:51,335 --> 00:01:53,790
pour consigner divers types de données

37
00:01:53,790 --> 00:01:55,785
et les visualiser dans TensorBoard.

38
00:01:55,785 --> 00:01:59,455
Il peut s'agir de nombres, de textes,
d'images ou même de fichiers audio.

39
00:01:59,960 --> 00:02:04,605
Avec l'API Estimator, une ligne suffit
dans un modèle pour écrire quelque chose :

40
00:02:04,605 --> 00:02:09,110
"tf.summary.scalar", suivi du nom du
graphique que vous voulez voir

41
00:02:09,110 --> 00:02:12,750
dans TensorBoard,
puis du Tensor avec les valeurs à tracer.

42
00:02:12,750 --> 00:02:17,820
Si vous n'utilisez pas l'API Estimator,
d'autres étapes sont nécessaires,

43
00:02:17,820 --> 00:02:20,140
décrites dans la documentation ici.

44
00:02:21,440 --> 00:02:24,015
Voici par exemple un histogramme.

45
00:02:24,015 --> 00:02:26,580
Je le trouve utile
pour visualiser les défaillances

46
00:02:26,580 --> 00:02:28,640
qui peuvent se produire dans vos sorties.

47
00:02:28,840 --> 00:02:33,285
Sur la gauche, nous avons un histogramme
dans le temps de toutes les valeurs

48
00:02:33,285 --> 00:02:37,230
provenant d'une couche de réseau
de neurones activée par un sigmoïde.

49
00:02:37,230 --> 00:02:38,730
Nous pouvons voir un problème.

50
00:02:38,730 --> 00:02:41,070
Il y a un pic à 0, un autre à 1,

51
00:02:41,070 --> 00:02:45,720
et la plupart des neurones sont saturés,
et probablement pas très utiles.

52
00:02:45,890 --> 00:02:48,705
Une technique de régularisation,
la "normalisation de lots",

53
00:02:48,705 --> 00:02:50,285
permet de résoudre ce problème.

54
00:02:50,285 --> 00:02:53,295
Voici la sortie de la même couche
après la normalisation.

55
00:02:53,295 --> 00:02:58,605
Les neurones produisent maintenant
des valeurs sur toute la plage utile.

56
00:02:58,605 --> 00:03:02,230
L'efficacité de cette méthode
dépend du modèle,

57
00:03:02,230 --> 00:03:06,705
mais je vois au moins
que la normalisation de lots fonctionne.

58
00:03:07,165 --> 00:03:09,360
Si vous travaillez
avec des images ou des sons,

59
00:03:09,360 --> 00:03:12,955
TensorBoard possède des tableaux de bord
spécifiques qui permettent de voir

60
00:03:12,955 --> 00:03:14,740
et d'écouter tout ce qu'il se passe.

61
00:03:14,740 --> 00:03:19,700
Les fonctions "summary.image"
et "summary.audio" vous permettent

62
00:03:19,700 --> 00:03:27,249
de spécifier que le Tensor que vous
consignez représente un fichier image

63
00:03:27,249 --> 00:03:33,415
ou audio, et qu'il apparaîtra dans le
tableau de bord dédié dans TensorBoard.

64
00:03:34,785 --> 00:03:38,350
Voici par exemple une visualisation
que j'ai utilisée lors du développement

65
00:03:38,350 --> 00:03:40,000
d'un modèle de détection d'avion.