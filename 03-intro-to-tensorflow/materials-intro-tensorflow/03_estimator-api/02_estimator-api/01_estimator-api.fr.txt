Nous allons d'abord découvrir
les composants de l'API Estimator, et apprendre à gérer
d'autres tâches de ML, telles que l'entraînement distribué,
la surveillance et la diffusion. Les estimateurs sont
des API TensorFlow de haut niveau. Lors du module précédent,
vous avez découvert des API TensorFlow Core. Vous avez surtout utilisé TensorFlow
comme une bibliothèque de traitement numérique. En dessous de ce niveau, nous avons
une série d'API avec lesquelles nous interagissons rarement. Elles gèrent le matériel, les CPU,
les GPU, les TPU, ou des plates-formes alternatives
comme Android. Au-dessus des API TensorFlow Core,
nous avons des API pour toutes les briques typiques
nécessaires pour construire un modèle, différents types de couches
de réseau de neurones, différentes fonctions de perte, etc. Et pour encapsuler tout ça,
nous avons les estimateurs. Un modèle TensorFlow de niveau débutant
implique généralement plusieurs couches électriques neuronales
et une boucle d'entraînement. Vous vous demandez peut-être en quoi
vous auriez besoin d'aide. Vous êtes un développeur,
vous savez écrire une boucle ! Je suis d'accord, mais aujourd'hui, même
pour les petits modèles de prototypage, j'ai tendance à utiliser
des estimateurs. J'aime leur interchangeabilité,
et la possibilité de tester rapidement de nombreux modèles Estimator
standard prédéfinis. Plus les données et le temps
d'entraînement augmentent, plus vos besoins grandissent. Vous avez besoin de points de contrôle
pour suspendre et reprendre votre entraînement ? Les estimateurs en ont. Vos données ne rentrent plus en mémoire ? Les estimateurs sont conçus pour
fonctionner avec l'API Dataset, qui gère les ensembles
de données hors mémoire. Vous ne pouvez pas entraîner un grand
réseau sans voir ses performances. Les estimateurs donnent automatiquement
des métriques clés pendant l'entraînement, que vous pouvez visualiser
dans TensorBoard. Et pour l'entraînement distribué ? Le code d'exécution de cluster nécessaire
est intégré dans les estimateurs. Enfin, vous pouvez encapsuler votre modèle
pour le préparer au réglage des hyperparamètres de ML Engine,
et peut-être aussi pour l'envoyer en production derrière le service
de prédiction géré et en autoscaling de ML Engine. L'API Estimator est là pour ça. Alors, voulez-vous toujours écrire
votre boucle d'entraînement tout seul en répétant ce code récurrent
à chaque fois ? Je ne pense pas. Alors découvrons ensemble l'API Estimator. La classe de base "estimator" vous
permet d'encapsuler votre propre modèle, que vous pouvez créer à partir de couches
à l'aide de l'API TF Layers. Mais si votre projet est standard,
inutile d'aller jusque-là. TesnsorFlow dispose d'estimateurs
prédéfinis que vous pouvez essayer : des classificateurs de réseaux
de neurones linéaires ou denses pour classifier les données en catégories,
et des régresseurs similaires pour prédire des valeurs continues. Sans oublier DNNLinearCombinedClassifier,
le modèle large et profond selon l'étude de Google
qui l'a popularisé. Celui-ci est très important. Nous l'utilisons par exemple pour
alimenter le moteur de recommandations de Google Play. Mais il est très flexible, et il est
parfois décrit comme le cheval de trait du machine learning d'entreprise. Il est prêt à l'emploi et fonctionne pour
tous les types de données structurées. N'oubliez pas que
grâce à une API commune, les estimateurs prédéfinis
sont interchangeables. Vous pouvez les essayer et les tester
en toute simplicité.