Let's take an example. How about real estate? Can we predict the price of a property? We must first choose our features. That is the data we will be basing our predictions on. Why not try and build a model that predicts the price based on the size of a house or apartment? Our features will be, one, the square footage; and, two, the category house or apartment. Here's how we implement this. We can use the feature column API to define our features. First, a numeric column for the square footage, then a categorical column for the property type. Two possible categories in this simple model, house or apartment. We can now instantiate a linear regressor, one of the pre-made estimators for those features. A regressor is a model that outputs a number, in our case, the predicted sales price of the property. But why do we need feature columns? It's time to have a look under the hood. A linear regressor is a model that works on a vector of data. It computes a weighted sum of all input data elements and it can be trained to adjust the weights for your problem, here, predicting the sales price. But how can we pack our data into the single input vector that linear regressor expect? The answer is in various ways depending on what data we are packing, and so that is where the feature columns API comes in handy. It implements various standard ways of packing data into vector elements. Here, values in our numeric column are just numbers. They can get copied as they are into a single element of the input vector. On the other hand, our categorical column gets one-hot-encoded. We have two categories. So, house will be 1, 0 while an apartment will become 0, 1. A third category would be encoded as 0, 0, 1 and so on. Now, the linear regressor knows how to take the features we care about, pack them into its input vector, and apply whatever a linear regressor does. There are many more feature column types to choose from: columns for continuous values, you want to bucketized, word embeddings, column crosses, and so on. The transformations they apply are clearly described in the [inaudible] for documentation so that you always know what is going on. To train the model, we need to write an input function that will return the features named as in the feature columns. Since we are training, we also need the correct answers called labels. And now, we can call the train function of our estimator, which will train the model by repeating this data set 100 times. We will see how batching works later but for those of you who already know about the concept of batching, the code as written here trains on a single batch of data at each step and this batch contains the entire data set. Once trained, the model can be used for the predictions. We will need an input function that provides data for the prediction, here, a 1500-square feet house and an 1800-square feet apartment. The predict function in the estimator API returns a python generator which you can use to iterate through the predictions. Here is a summary of the estimator API so far. We used feature columns to get our data into a shape our model can understand. We instantiated a linear regressor based on these feature columns, we called train, to train the model for 100 steps. Training data is provided through a data input function, we called predict, to get predictions and the data for that was again provided through a data input function. We will get to those in more detail later in this course. To use a different pre-made estimator, just change the class name and supply the appropriate configuration parameters. For example, here, we could use a dense neural network, a regressor, with two hidden layers. The first one has three neurons, the second one only two, and we end on the single neuron that predicts the property price. Notice that the input vector is the same for both models. We can reuse the same feature columns. Here are some of the things you can adjust on a dense neural network: the number and size of hidden layers, the choice of activation function, regularization parameters like drop out or your favorite optimizer to drive the training. But most importantly, there are good defaults for almost all of them. For a DNN regressor, the only mandatory parameters are the hidden layers.