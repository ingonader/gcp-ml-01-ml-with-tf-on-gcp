{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2c. Loading large datasets progressively with the tf.data.Dataset </h1>\n",
    "\n",
    "In this notebook, we continue reading the same small dataset, but refactor our ML pipeline in two small, but significant, ways:\n",
    "<ol>\n",
    "<li> Refactor the input to read data from disk progressively.\n",
    "<li> Refactor the feature creation so that it is not one-to-one with inputs.\n",
    "</ol>\n",
    "<br/>\n",
    "The Pandas function in the previous notebook first read the whole data into memory -- on a large dataset, this won't be an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import datalab.bigquery as bq\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Refactor the input </h2>\n",
    "\n",
    "Read data created in Lab1a, but this time make it more general, so that we can later handle large datasets. We use the Dataset API for this. It ensures that, as data gets delivered to the model in mini-batches, it is loaded from disk only when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\n",
    "DEFAULTS = [[0.0], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]\n",
    "\n",
    "# TODO: Create an appropriate input function read_dataset\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "    #TODO Add CSV decoder function and dataset creation and methods\n",
    "    def decode_csv(row):\n",
    "        columns = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "        features = dict(zip(CSV_COLUMNS, columns))\n",
    "        label = features.pop('fare_amount')\n",
    "        return features, label\n",
    "      \n",
    "    ## read list of file names:\n",
    "    filenames_dataset = tf.data.Dataset.list_files(filename, shuffle = False)\n",
    "    ## read lines from this dataset:\n",
    "    textlines_dataset = filenames_dataset.flat_map(\n",
    "        tf.data.TextLineDataset\n",
    "    )\n",
    "    ## decode each line:\n",
    "    dataset = textlines_dataset.map(decode_csv)\n",
    "    \n",
    "    # Note:\n",
    "    # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "    # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None ## loop indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size = 10 * batch_size, seed = 2) ## and shuffle for training\n",
    "    else:\n",
    "        num_epochs = 1 ## only loop once through dataset\n",
    "    \n",
    "    ## repeat data set as needed (training) and form batches of data:\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    return dataset\n",
    "  \n",
    "def get_train_input_fn():\n",
    "  return read_dataset('./taxi-train.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid_input_fn():\n",
    "  return read_dataset('./taxi-valid.csv', mode = tf.estimator.ModeKeys.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Refactor the way features are created. </h2>\n",
    "\n",
    "For now, pass these through (same as previous lab).  However, refactoring this way will enable us to break the one-to-one relationship between inputs and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('pickuplon'),\n",
    "    tf.feature_column.numeric_column('pickuplat'),\n",
    "    tf.feature_column.numeric_column('dropofflat'),\n",
    "    tf.feature_column.numeric_column('dropofflon'),\n",
    "    tf.feature_column.numeric_column('passengers'),\n",
    "]\n",
    "\n",
    "def add_more_features(feats):\n",
    "  # Nothing to add (yet!)\n",
    "  return feats\n",
    "\n",
    "feature_cols = add_more_features(INPUT_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create and train the model </h2>\n",
    "\n",
    "Note that we train for num_steps * batch_size examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_train_distribute': None, '_session_config': None, '_save_summary_steps': 100, '_log_step_count_steps': 100, '_num_worker_replicas': 1, '_task_id': 0, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f40adfdbb70>, '_evaluation_master': '', '_tf_random_seed': None, '_is_chief': True, '_service': None, '_save_checkpoints_steps': None, '_global_id_in_cluster': 0, '_master': '', '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600, '_model_dir': 'taxi_trained'}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 96542.81\n",
      "INFO:tensorflow:global_step/sec: 41.5929\n",
      "INFO:tensorflow:step = 101, loss = 61699.812 (2.409 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 39530.062.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearRegressor at 0x7f40adfdbb00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "OUTDIR = 'taxi_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "model = tf.estimator.LinearRegressor(\n",
    "      feature_columns = feature_cols, model_dir = OUTDIR)\n",
    "model.train(input_fn = get_train_input_fn, steps = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Evaluate model </h3>\n",
    "\n",
    "As before, evaluate on the validation data.  We'll do the third refactoring (to move the evaluation into the training loop) in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-31-14:19:59\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-31-14:19:59\n",
      "INFO:tensorflow:Saving dict for global step 200: average_loss = 109.93547, global_step = 200, loss = 45760.64\n",
      "RMSE on dataset = 10.48501205444336\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(input_fn = get_valid_input_fn, steps = None)\n",
    "print('RMSE on dataset = {}'.format(np.sqrt(metrics['average_loss'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "Create a neural network that is capable of finding the volume of a cylinder given the radius of its base (r) and its height (h). Assume that the radius and height of the cylinder are both in the range 0.5 to 2.0. Unlike in the challenge exercise for b_estimator.ipynb, assume that your measurements of r, h and V are all rounded off to the nearest 0.1. Simulate the necessary training dataset. This time, you will need a lot more data to get a good predictor.\n",
    "\n",
    "Hint (highlight to see):\n",
    "<p style='color:white'>\n",
    "Create random values for r and h and compute V. Then, round off r, h and V (i.e., the volume is computed from the true value of r and h; it's only your measurement that is rounded off). Your dataset will consist of the round values of r, h and V. Do this for both the training and evaluation datasets.\n",
    "</p>\n",
    "\n",
    "Now modify the \"noise\" so that instead of just rounding off the value, there is up to a 10% error (uniformly distributed) in the measurement followed by rounding off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>r</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     h    r    v\n",
       "0  1.8  0.8  3.5\n",
       "1  1.0  1.7  9.1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datalab.bigquery as bq\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def gen_data(n):\n",
    "  r = np.random.uniform(.5, 2, n)\n",
    "  h = np.random.uniform(.5, 2, n)\n",
    "  v = r ** 2 * h * math.pi\n",
    "  dat = pd.DataFrame({\n",
    "    'r': np.round(r, 1),\n",
    "    'h': np.round(h, 1),\n",
    "    'v': np.round(v, 1)\n",
    "  })\n",
    "  return dat\n",
    "\n",
    "\n",
    "dat = gen_data(5000)\n",
    "dat_eval = gen_data(1000)\n",
    "dat_test = gen_data(1000)\n",
    "\n",
    "dat.head(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write data to file:\n",
    "dat.to_csv('dat-cyl-train.csv', header = False, index = False)\n",
    "dat_eval.to_csv('dat-cyl-eval.csv', header = False, index = False)\n",
    "dat_test.to_csv('dat-cyl-test.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 12266 Feb  1 07:45 dat-cyl-eval.csv\r\n",
      "-rw-r--r-- 1 root root 12232 Feb  1 07:45 dat-cyl-test.csv\r\n",
      "-rw-r--r-- 1 root root 61228 Feb  1 07:45 dat-cyl-train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l dat-cyl*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8,0.8,3.5\r\n",
      "1.0,1.7,9.1\r\n",
      "0.9,1.8,9.0\r\n",
      "1.6,0.7,2.3\r\n",
      "1.8,1.0,5.4\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 dat-cyl-train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['h', 'r', 'v']\n",
    "DEFAULTS = [[0.0], [0.0], [0.0]]\n",
    "LABEL_COLUMN = 'v'\n",
    "\n",
    "## create a function to read the dataset from disk:\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  ## Add CSV decoder function: \n",
    "  def decode_csv(row):\n",
    "    columns = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, columns))\n",
    "    label = features.pop(LABEL_COLUMN)\n",
    "    return features, label\n",
    "  \n",
    "  ## create dataset:\n",
    "  ## read list of file names:\n",
    "  filenames_dataset = tf.data.Dataset.list_files(filename, shuffle = False)\n",
    "  ## read lines from this dataset list:\n",
    "  textlines_dataset = filenames_dataset.flat_map(\n",
    "    tf.data.TextLineDataset   ## function that returns individual text lines of the file\n",
    "  )\n",
    "  ## then, decode each line:\n",
    "  dataset = textlines_dataset.map(decode_csv)\n",
    "\n",
    "  # Note:\n",
    "  # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "  # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    num_epochs = None ## loop indefinitely\n",
    "    dataset = dataset.shuffle(buffer_size = 10 * batch_size, seed = 2) ## and shuffle for training\n",
    "  else:\n",
    "    num_epochs = 1 ## only loop once through dataset\n",
    "    \n",
    "  ## repeat data set as needed (training) and form batches of data:\n",
    "  dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "  return dataset\n",
    "\n",
    "## define function that gets training input, using the read_dataset function above:\n",
    "\n",
    "def get_train_input_fn():\n",
    "  return read_dataset('./dat-cyl-train.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid_input_fn():\n",
    "  return read_dataset('./dat-cyl-eval.csv', mode = tf.estimator.ModeKeys.EVAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create features:\n",
    "INPUT_COLUMNS = [\n",
    "  tf.feature_column.numeric_column('h'),\n",
    "  tf.feature_column.numeric_column('r'),\n",
    "]\n",
    "\n",
    "def add_more_features(feats):\n",
    "  # Nothing to add (yet!)\n",
    "  return feats\n",
    "\n",
    "## currently, don't apply any feature transformations:\n",
    "feature_cols = add_more_features(INPUT_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_master': '', '_num_ps_replicas': 0, '_save_checkpoints_steps': None, '_service': None, '_tf_random_seed': None, '_evaluation_master': '', '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_model_dir': 'cyl_trained', '_train_distribute': None, '_task_id': 0, '_log_step_count_steps': 100, '_session_config': None, '_save_summary_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2f795c8908>, '_global_id_in_cluster': 0, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_task_type': 'worker'}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into cyl_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 39003.15, step = 1\n",
      "INFO:tensorflow:global_step/sec: 71.8915\n",
      "INFO:tensorflow:loss = 6514.0913, step = 101 (1.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.27\n",
      "INFO:tensorflow:loss = 1914.894, step = 201 (1.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.9446\n",
      "INFO:tensorflow:loss = 896.7904, step = 301 (1.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.7104\n",
      "INFO:tensorflow:loss = 823.28033, step = 401 (1.435 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into cyl_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 819.1782.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNRegressor at 0x7f2f796f42e8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (re-)set model output directory\n",
    "OUTDIR = 'cyl_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "## create and train the model:\n",
    "model = tf.estimator.DNNRegressor(\n",
    "  feature_columns = feature_cols, \n",
    "  hidden_units = [4, 8, 4], \n",
    "  model_dir = OUTDIR)\n",
    "model.train(input_fn = get_train_input_fn, steps = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-01-07:59:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from cyl_trained/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-01-07:59:54\n",
      "INFO:tensorflow:Saving dict for global step 500: average_loss = 1.607494, global_step = 500, loss = 803.747\n",
      "RMSE on dataset = 1.2678698301315308\n"
     ]
    }
   ],
   "source": [
    "## RMSE:\n",
    "metrics = model.evaluate(input_fn = get_valid_input_fn, steps = None)\n",
    "print('RMSE on dataset = {}'.format(np.sqrt(metrics['average_loss'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from cyl_trained/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_true</th>\n",
       "      <th>v_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.4</td>\n",
       "      <td>3.418332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.8</td>\n",
       "      <td>15.616695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.1</td>\n",
       "      <td>0.073160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.2</td>\n",
       "      <td>12.600562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3.418332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   v_true     v_pred\n",
       "0     3.4   3.418332\n",
       "1    16.8  15.616695\n",
       "2     2.1   0.073160\n",
       "3    12.2  12.600562\n",
       "4     3.5   3.418332"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make prediction iterator:\n",
    "pred_iter = model.predict(get_valid_input_fn)\n",
    "dat_pred = pd.DataFrame(columns = ['v_true', 'v_pred'])\n",
    "\n",
    "## predict a few values to get correlation:\n",
    "for i in range(1000):\n",
    "  dat_pred = dat_pred.append({\n",
    "    'v_true' : dat_eval['v'][i],\n",
    "    'v_pred' : next(pred_iter)['predictions'][0]\n",
    "  }, ignore_index = True)\n",
    "  #print(dat_eval['v'][i], next(pred_iter)['predictions'][0])\n",
    "  \n",
    "dat_pred.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v_true</th>\n",
       "      <th>v_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>v_true</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.97306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v_pred</th>\n",
       "      <td>0.97306</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         v_true   v_pred\n",
       "v_true  1.00000  0.97306\n",
       "v_pred  0.97306  1.00000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_pred.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
