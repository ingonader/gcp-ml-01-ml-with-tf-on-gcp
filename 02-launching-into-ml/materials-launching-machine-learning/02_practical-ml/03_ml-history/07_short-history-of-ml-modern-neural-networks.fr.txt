Les réseaux de neurones
réapparaissent dans la chronologie, mais désormais avec d'autres avantages,
comme les bonds, la puissance de calcul et le traitement
de nombreuses données. Les DNN sont devenus plus performants
que d'autres méthodes pour le test, comme la vision par ordinateur. En plus du développement
des améliorations matérielles, de nombreuses astuces et architectures
permettent d'améliorer la capacité d'entraînement des réseaux
de neurones profonds, comme les ReLu, les méthodes d'initialisation optimisées,
les CNN et le dropout. Nous avons abordé ces astuces
pour d'autres méthodes de ML. On a vu les fonctions d'activation
non linéaires, telles que les ReLu, qui sont souvent désormais
utilisées par défaut, dans la première vidéo
sur les réseaux de neurones. Adoptées pour améliorer la généralisation,
les couches de dropout fonctionnent comme des méthodes d'ensemble, abordées en parallèle des forêts
aléatoires et des arbres boostés. Les couches convolutives
ont permis de réduire la charge de calcul et de mémoire
grâce à leur connectivité incomplète, et à leur capacité à se concentrer
sur des aspects locaux, comme des images, plutôt que de comparer
des éléments non liés dans une image. Autrement dit, toutes les avancées
dans les autres méthodes de ML ont profité aux réseaux de neurones. Voyons un exemple
de réseau de neurones profond. L'évolution du machine learning
a abouti au deep learning, avec des DNN dotés
de centaines de couches, et de millions de paramètres,
mais apportant des résultats prodigieux. Voici le modèle GoogLeNet ou Inception, un modèle de classification d'images. Il a été entraîné lors du challenge de
reconnaissance visuelle à grande échelle d'ImageNet en 2014
avec des données de 2012. Il devait classer des images
dans mille classes, et disposait de 1,2 million d'images
pour l'entraînement. Il contient 22 couches profondes, 27 en comptant le pooling, dont nous parlerons plus tard, et 100 couches si vous le divisez
en blocs de construction indépendants. Il y a plus de 11 millions
de paramètres entraînés. Certaines couches sont entièrement
connectées et d'autres non, comme les couches convolutives
dont nous parlerons plus tard. Les couches de dropout
améliorent la généralisation en simulant un ensemble
de réseaux de neurones profonds. Comme avec les réseaux de neurones
et l'empilement, chaque case est une unité de
composants qui fait partie d'un groupe, comme celui sur lequel j'ai zoomé. L'expansion des blocs en un tout
supérieur à la somme de ses parties est l'un des éléments qui rend
le deep learning si efficace. L'abondance croissante de données, la puissance de calcul et davantage
de mémoire y contribuent aussi. Il existe désormais plusieurs versions bien plus grosses et précises. Le principal point
à retenir de cet historique est que la recherche en machine learning
réutilise des techniques d'anciens algorithmes et les combine pour créer des modèles très puissants,
mais surtout des expérimentations. Quel élément est important pour créer
des réseaux de neurones profonds ? La bonne réponse est
"Tous les éléments ci-dessus". Cette liste est non exhaustive, mais ces trois éléments
sont très importants à retenir. Vous devez d'abord veiller à
disposer de beaucoup de données. Des recherches tentent actuellement
de réduire les besoins en données du deep learning,
mais dans l'intervalle, il faut utiliser beaucoup de données. Cela est dû à la haute capacité
des nombreux paramètres à entraîner dans ces modèles
de grande envergure. Comme les modèles sont très complexes, ils doivent bien assimiler
la distribution des données. Ils ont donc besoin de nombreux signaux. Le but du machine learning
n'est pas d'entraîner de nombreux modèles élaborés
sans raison, mais pour qu'ils fassent
des prédictions très précises. Sans généralisation des données
pour faire des prédictions, votre modèle sera inutile. Il est donc important d'avoir
suffisamment de données, pour ne pas surapprendre un petit
ensemble de données très courant, mais plutôt un grand ensemble
de données plus rare. Vous disposez ainsi
d'ensembles assez grands de validation et d'évaluation
pour régler votre modèle. L'ajout de couches de dropout,
l'augmentation des données, l'ajout de bruit et autres vous permettent
d'améliorer la généralisation. Enfin, le machine learning est axé
sur l'expérimentation. Il existe de nombreux types d'algorithmes, d'hyperparamètres et de façons
de créer des ensembles de données. Il n'est pas primordial de connaître les bonnes options dès le début
pour tous les problèmes. Expérimentez et gardez une trace
de ce que vous avez déjà essayé et des performances mesurées
pour comparer vos modèles. Ainsi, vous vous amuserez
tout en créant des outils très puissants. Je vous expliquerai ensuite comment les réseaux de neurones s'appuient
sur les performances des anciens modèles. Voici les performances
de versions de modèles de réseaux de neurones
profonds au fil des ans. Comme le montre ce tableau, une avancée importante
s'est produite en 2014, ici surlignée en bleu, lorsque le modèle Inception de Google a réduit le taux d'erreur de 10 % à 6,7 %. Les performances des DNN continuent
de s'améliorer chaque année, car ils tirent des enseignements
des modèles précédents. En 2015, la version 3 du modèle Inception avait un taux d'erreur de 3,5 %. Pourquoi ces performances se sont-elles
tant améliorées sur une courte période ? Quand des chercheurs développent une
nouvelle technique ou méthode efficace, d'autres s'en servent souvent
ensuite comme point de départ. Cela permet de grandes avancées
en expérimentation et accélère le progrès. Il peut s'agir d'améliorer
des hyperparamètres, des couches, la généralisation ou des sous-composants,
comme les couches convolutives, etc. Expliquez comment
vous appliqueriez le ML à ce problème. Plusieurs réponses sont possibles. Vous possédez une station de ski et
voulez prédire l'affluence sur les pistes en fonction des quatre types de clients,
débutant, intermédiaire, avancé et expert, qui ont acheté un forfait, et des
chutes de neige précédentes. Notez votre réponse maintenant. Il peut s'agir d'un problème
de régression ou de classification, car je n'ai pas précisé
ce que j'entendais par affluence. S'agit-il du nombre de personnes
qui skient sur cette piste par heure ? Ou s'agit-il d'une catégorie telle que
affluence élevée, moyenne et faible ? Je commencerais par
une heuristique de base, comme le nombre moyen
de skieurs sur chaque piste, puis je passerais à un modèle basique
de régression linéaire ou logistique, en fonction de mon choix
entre la régression ou la classification. Selon les performances
et la quantité de données, j'utiliserais ensuite
des réseaux de neurones. S'il existe d'autres caractéristiques, je les testerais aussi
et suivrais les performances. Aux dernières nouvelles,
Google utilise en interne plus de 4 000 modèles de deep learning
pour optimiser ses systèmes. Tous ces modèles et leurs versions
améliorent leurs performances en s'appuyant sur les succès
et les échecs d'anciens modèles. Sibyl a été l'un des modèles
les plus utilisés au début. Il avait été créé pour recommander
des vidéos similaires sur YouTube. Comme ce moteur de recommandations
était très performant, il a été largement intégré dans
les annonces et d'autres services Google. Ce modèle était linéaire. Un autre modèle est devenu cette année le moteur de réglage de paramètres clé
pour les autres modèles et systèmes. Google Brain, la division
recherche en ML de Google, a créé une solution pour
exploiter la puissance de calcul de milliers de processeurs pour entraîner
de grands modèles, comme les DNN. De la conception et l'exécution
de ces modèles est né TensorFlow, une bibliothèque Open Source
pour le machine learning. Google a ensuite créé TFX, la plate-forme
de machine learning basée sur TensorFlow. Vous découvrirez comment créer et
déployer des modèles de ML de production avec TensorFlow et des outils comme
Cloud ML Engine, Dataflow et BigQuery. Pour résumer,
au cours des dernières décennies, l'adoption et les performances des réseaux
de neurones se sont développées. Grâce à l'ubiquité des données, ces modèles peuvent apprendre
de davantage d'exemples d'entraînement. L'augmentation des données et des exemples
associée à une infrastructure évolutive a engendré des modèles complexes et
distribués avec des milliers de couches. Une dernière chose avant de terminer. Bien que les réseaux de neurones soient
performants pour certaines applications, il existe de nombreux autres types
de modèles que vous pouvez tester. L'expérimentation est essentielle pour relever votre défi le plus
efficacement possible avec vos données.