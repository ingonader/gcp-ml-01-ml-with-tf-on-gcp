1
00:00:00,330 --> 00:00:02,495
Por que usar só uma camada de perceptron?

2
00:00:02,495 --> 00:00:06,000
Por que não enviar a saída de
uma camada como a entrada da próxima?

3
00:00:06,000 --> 00:00:10,885
Várias camadas de perceptron combinadas
parecem um modelo muito mais poderoso.

4
00:00:10,885 --> 00:00:14,550
No entanto, sem usar
funções de ativação não lineares,

5
00:00:14,550 --> 00:00:17,740
todas as camadas adicionais
podem voltar a ser comprimidas

6
00:00:17,740 --> 00:00:21,695
em apenas uma,
e não há benefício real.

7
00:00:21,695 --> 00:00:24,670
São necessárias 
funções de ativação não lineares.

8
00:00:24,670 --> 00:00:27,260
Portanto, a sigmoide 
ou tangente hiperbólica,

9
00:00:27,260 --> 00:00:28,720
ou "tan" na forma abreviada,

10
00:00:28,720 --> 00:00:32,200
são funções de ativação que
começaram a ser usadas em não linearidade.

11
00:00:32,200 --> 00:00:35,910
Éramos limitados 
apenas a elas porque precisávamos

12
00:00:35,910 --> 00:00:38,395
de uma função diferenciável, já que ela é

13
00:00:38,395 --> 00:00:41,765
explorada na propagação de retorno,
para ser os pesos do modelo.

14
00:00:41,765 --> 00:00:45,290
As funções de ativação modernas
não são necessariamente diferenciáveis,

15
00:00:45,290 --> 00:00:47,755
e as pessoas não sabiam
como trabalhar com elas.

16
00:00:48,495 --> 00:00:51,990
Essa limitação, com funções de
ativação diferenciáveis,

17
00:00:51,990 --> 00:00:54,310
dificultava o treinamento das redes.

18
00:00:54,310 --> 00:00:58,445
A eficácia dos modelos também
era limitada pela quantidade de dados,

19
00:00:58,445 --> 00:01:02,280
recursos computacionais disponíveis
e outras complicações no treinamento.

20
00:01:02,280 --> 00:01:06,325
Por exemplo, a otimização
costumava cair em pontos de sela.

21
00:01:06,325 --> 00:01:07,960
Ela não encontrava o mínimo global

22
00:01:07,960 --> 00:01:10,745
e achávamos que isso
acontecesse no gradiente descendente.

23
00:01:10,745 --> 00:01:16,100
No entanto, com a criação de
unidades lineares retificadas ou ReLUs,

24
00:01:16,100 --> 00:01:19,645
foi possível realizar treinamentos
até 10 vezes mais rápidos, com

25
00:01:19,645 --> 00:01:22,145
convergência na regressão
logística quase garantida.

26
00:01:23,515 --> 00:01:26,095
Ao criar o perceptron, como o cérebro,

27
00:01:26,095 --> 00:01:28,960
podemos conectar
várias delas para formar camadas

28
00:01:28,960 --> 00:01:31,545
e produzir redes neurais feedforward.

29
00:01:31,545 --> 00:01:35,340
Pouco mudou nos componentes
do perceptron de camada única,

30
00:01:35,340 --> 00:01:40,490
ainda há entradas, somas ponderadas,
funções de ativação e saídas.

31
00:01:41,145 --> 00:01:44,875
A diferença é que entradas nos neurônios
que não estão na camada de entrada

32
00:01:44,875 --> 00:01:48,925
não são as entradas originais, 
mas as saídas da camada anterior.

33
00:01:48,925 --> 00:01:52,690
Outra diferença é que os pesos que
conectam os neurônios entre camadas

34
00:01:52,690 --> 00:01:56,100
não são mais um vetor,
mas uma matriz

35
00:01:56,100 --> 00:01:59,730
por conta do caráter
de conexão total desses neurônios.

36
00:02:00,130 --> 00:02:03,610
Por exemplo, no diagrama, quatro
matrizes de peso na camada de entrada

37
00:02:03,610 --> 00:02:06,960
são divididas em duas,
e na camada oculta, duas divididas em uma.

38
00:02:07,640 --> 00:02:10,120
Vamos aprender depois que
as redes neurais nem sempre

39
00:02:10,120 --> 00:02:11,980
têm conectividade total, o que garante

40
00:02:11,980 --> 00:02:15,230
aplicativos e desempenho incríveis,
como no trabalho com imagens.

41
00:02:15,230 --> 00:02:19,185
Há também outras funções de ativação
além das de passos da unidade

42
00:02:19,185 --> 00:02:23,510
como sigmoide e tangente hiperbólica
ou funções de ativação tan.

43
00:02:23,510 --> 00:02:26,040
Consideramos cada neurônio
que não é entrada como

44
00:02:26,040 --> 00:02:29,240
um conjunto de três passos
comprimidos em uma unidade.

45
00:02:29,240 --> 00:02:31,670
O primeiro componente é a soma ponderada,

46
00:02:31,670 --> 00:02:34,190
o segundo é a função de ativação,

47
00:02:34,190 --> 00:02:37,520
e o terceiro é a saída dessa função.

48
00:02:37,520 --> 00:02:41,360
As redes neurais se tornam
complexas com todas as camadas,

49
00:02:41,360 --> 00:02:44,940
neurônios, funções de ativação
e formas de treiná-las.

50
00:02:44,940 --> 00:02:47,820
Durante o curso, usaremos
o TensorFlow Playground

51
00:02:47,820 --> 00:02:51,820
para ter uma ideia mais intuitiva de como
é o fluxo de informação pela rede neural.

52
00:02:51,820 --> 00:02:53,490
É também muito divertido,

53
00:02:53,490 --> 00:02:55,900
com a personalização
de mais hiperparâmetros,

54
00:02:55,900 --> 00:02:58,325
além de imagens da magnitude de espera

55
00:02:58,325 --> 00:03:01,035
e como a função de perda
evolui ao longo do tempo.

56
00:03:02,565 --> 00:03:04,715
Esta é a função de ativação linear,

57
00:03:04,715 --> 00:03:09,390
que é uma de identidade
porque a função de X retorna X.

58
00:03:09,390 --> 00:03:11,650
Esta era a função de ativação original.

59
00:03:11,650 --> 00:03:13,370
No entanto, como dito antes,

60
00:03:13,370 --> 00:03:15,900
mesmo em uma rede neural
com milhares de camadas,

61
00:03:15,900 --> 00:03:18,105
e todas com função de ativação linear,

62
00:03:18,105 --> 00:03:22,690
o resultado é apenas uma
combinação linear dos recursos da entrada.

63
00:03:22,690 --> 00:03:27,430
Isso é reduzido a cada recurso de entrada
multiplicado por uma constante.

64
00:03:27,430 --> 00:03:29,325
Soa familiar para você?

65
00:03:29,325 --> 00:03:31,290
É simplesmente uma regressão linear.

66
00:03:31,290 --> 00:03:34,780
Portanto, as funções de ativação
não lineares são necessárias para ter

67
00:03:34,780 --> 00:03:36,840
as funções complexas de cadeia para que

68
00:03:36,840 --> 00:03:40,325
redes neurais
aprendam distribuições de dados.

69
00:03:43,325 --> 00:03:47,190
Além da função de ativação linear,
em que f de X é igual a X,

70
00:03:47,190 --> 00:03:50,680
as principais funções de ativação
quando as redes neurais estavam

71
00:03:50,680 --> 00:03:54,920
na primeira era dourada
eram sigmoide e tan.

72
00:03:54,920 --> 00:03:59,525
A sigmoide é uma versão simplificada
da função de passo da unidade

73
00:03:59,525 --> 00:04:04,595
em que a assíntota é 0 no
infinito negativo e até um no positivo.

74
00:04:04,595 --> 00:04:07,380
Mas há valores intermediários
entre tudo isso.

75
00:04:10,470 --> 00:04:13,060
A tangente hiperbólica ou
tan na forma abreviada

76
00:04:13,060 --> 00:04:16,040
é outra função de ativação
muito usada neste momento,

77
00:04:16,040 --> 00:04:19,060
que é basicamente uma sigmoide
 em escala e transferida,

78
00:04:19,060 --> 00:04:21,404
agora com intervalo
negativo de um para um.

79
00:04:21,404 --> 00:04:24,270
Estas eram ótimas opções
por serem diferenciáveis

80
00:04:24,270 --> 00:04:27,365
em todos os lugares,
monotônicas e simples.

81
00:04:27,365 --> 00:04:31,280
No entanto, problemas
como saturação ocorriam por conta de

82
00:04:31,280 --> 00:04:35,120
valores altos ou baixos
de entrada nas funções,

83
00:04:35,120 --> 00:04:38,240
gerando estabilização assintótica.

84
00:04:38,240 --> 00:04:41,180
Como a curva é quase plana nesses pontos,

85
00:04:41,180 --> 00:04:43,825
as derivadas são quase zero.

86
00:04:43,825 --> 00:04:46,990
Portanto, como os
gradientes eram quase zero,

87
00:04:46,990 --> 00:04:50,840
o treinamento dos pesos
era muito lento ou até mesmo interrompido

88
00:04:50,840 --> 00:04:55,870
resultando em tamanhos de passo
muito pequenos no gradiente descendente.

89
00:04:55,870 --> 00:04:59,735
As funções de ativação linear eram
diferenciáveis, monotônicas e simples.

90
00:04:59,735 --> 00:05:01,235
No entanto, como dito antes,

91
00:05:01,235 --> 00:05:04,990
é possível reduzir a combinação linear
dessas funções pelos neurônios em uma.

92
00:05:04,990 --> 00:05:07,060
Isso não possibilita a criação da

93
00:05:07,060 --> 00:05:10,385
cadeia complexa de funções necessária
para descrever a linha de dados.

94
00:05:10,385 --> 00:05:12,640
A função de ativação linear
tinha aproximações,

95
00:05:12,640 --> 00:05:15,035
mas elas não eram
diferenciáveis em qualquer lugar.

96
00:05:15,035 --> 00:05:18,010
Só muito depois as pessoas
saberiam como lidar com elas.

97
00:05:19,550 --> 00:05:24,425
Muito famosa hoje é a unidade linear
retificada ou função de ativação ReLU.

98
00:05:24,425 --> 00:05:27,830
Não é linear, então é possível
conseguir o modelo complexo necessário,

99
00:05:27,830 --> 00:05:32,080
e ela não gera saturação na parte
não negativa do espaço de entrada.

100
00:05:32,080 --> 00:05:37,430
No entanto, a parte negativa do espaço de
entrada se transforma em zero de ativação,

101
00:05:37,430 --> 00:05:41,115
o que pode levar as camadas de ReLU 
a desaparecer ou não serem mais ativadas,

102
00:05:41,115 --> 00:05:44,180
causando também lentidão
ou interrupção no treinamento.

103
00:05:47,100 --> 00:05:49,065
Há maneiras
de resolver esse problema,

104
00:05:49,065 --> 00:05:52,260
uma delas é usar
outra função de ativação chamada de

105
00:05:52,260 --> 00:05:54,580
unidade linear exponencial ou ELU.

106
00:05:55,400 --> 00:05:59,140
É quase linear na parte
não negativa do espaço de entrada

107
00:05:59,140 --> 00:06:02,225
e é simples, monotônica e, principalmente,

108
00:06:02,225 --> 00:06:05,440
diferente de zero
na parte negativa do espaço de entrada.

109
00:06:05,440 --> 00:06:09,920
A principal desvantagem das ELUs é
que são mais caras na computação

110
00:06:09,920 --> 00:06:12,680
do que as ReLUs,
devido à exponencial calculada.

111
00:06:12,680 --> 00:06:15,695
Falaremos mais
sobre isso no próximo módulo.

112
00:06:16,355 --> 00:06:19,690
Para que minhas saídas
estejam na forma de probabilidade,

113
00:06:19,690 --> 00:06:22,690
qual função de ativação
é necessária na camada final?

114
00:06:26,340 --> 00:06:29,440
A resposta correta é
função de ativação "Sigmoide".

115
00:06:29,440 --> 00:06:33,090
Isso acontece porque o intervalo
da função sigmoide é entre zero e um,

116
00:06:33,090 --> 00:06:35,095
que também é o da probabilidade.

117
00:06:35,095 --> 00:06:36,630
Além do intervalo,

118
00:06:36,630 --> 00:06:39,680
a sigmoide é a função
de distribuição cumulativa

119
00:06:39,680 --> 00:06:42,600
da probabilidade logística
em que a função quantil

120
00:06:42,600 --> 00:06:46,275
é o inverso da lógica que cria
o modelo das probabilidades de registro.

121
00:06:46,275 --> 00:06:49,585
É por isso que pode ser usada
como probabilidade verdadeira.

122
00:06:49,585 --> 00:06:53,165
Falaremos mais sobre
esses motivos mais adiante no curso.

123
00:06:53,165 --> 00:06:57,545
"Tan" está incorreta porque, mesmo sendo
uma função comprimida como sigmoide,

124
00:06:57,545 --> 00:07:00,060
tem intervalo entre um negativo e um,

125
00:07:00,060 --> 00:07:02,335
que não é o mesmo
intervalo da probabilidade.

126
00:07:02,335 --> 00:07:04,500
Além disso, comprimir tan em

127
00:07:04,500 --> 00:07:07,595
uma sigmoide não
a transformará em uma probabilidade

128
00:07:07,595 --> 00:07:10,140
porque ela não tem 
as mesmas propriedades mencionadas

129
00:07:10,140 --> 00:07:13,345
que possibilitam que a saída seja
interpretada como probabilidade.

130
00:07:13,345 --> 00:07:15,800
Para fazer a conversão
correta em uma sigmoide,

131
00:07:15,800 --> 00:07:19,790
é necessário adicionar um e dividir por
dois, gerando o intervalo correto.

132
00:07:19,790 --> 00:07:22,555
E para conseguir o espalhamento certo,

133
00:07:22,555 --> 00:07:25,080
é necessário dividir
o argumento da tan por dois.

134
00:07:25,080 --> 00:07:27,365
Mas a tan já foi calculada,

135
00:07:27,365 --> 00:07:29,220
então você terá mais trabalho

136
00:07:29,220 --> 00:07:31,990
e poderia ter usado
uma sigmoide desde o início.

137
00:07:31,990 --> 00:07:36,565
"ReLU" é incorreta porque
o intervalo dela é entre zero e infinito,

138
00:07:36,565 --> 00:07:39,315
o que vai muito além
da representação de probabilidade.

139
00:07:39,315 --> 00:07:44,000
"ELU" é incorreta por ter intervalo
entre infinito negativo e positivo.