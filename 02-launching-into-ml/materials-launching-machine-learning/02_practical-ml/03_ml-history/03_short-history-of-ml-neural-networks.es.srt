1
00:00:00,000 --> 00:00:02,495
¿Por qué solo perceptrones
de una sola capa?

2
00:00:02,495 --> 00:00:06,000
¿Por qué no enviar la salida de una capa
como la entrada de la siguiente?

3
00:00:06,000 --> 00:00:09,205
La combinación de varias capas
de perceptrones suena como un modelo

4
00:00:09,205 --> 00:00:10,455
mucho más poderoso.

5
00:00:10,945 --> 00:00:14,550
No obstante,
sin funciones de activación no lineales

6
00:00:14,550 --> 00:00:20,010
las capas adicionales se pueden comprimir
en una sola capa lineal.

7
00:00:20,010 --> 00:00:21,555
No hay un beneficio real.

8
00:00:21,555 --> 00:00:24,670
Se necesitan
funciones de activación no lineales.

9
00:00:24,670 --> 00:00:27,450
Por lo tanto, se comenzaron
a usar funciones de activación

10
00:00:27,450 --> 00:00:31,830
sigmoidal, tangente hiperbólica
o tanh, por su no linealidad.

11
00:00:32,540 --> 00:00:35,290
En ese momento,
estábamos limitados a ellas

12
00:00:35,290 --> 00:00:38,910
porque necesitábamos una función
diferenciable, ya que eso se explota

13
00:00:38,910 --> 00:00:42,015
en la retropropagación
para actualizar los pesos del modelo.

14
00:00:42,025 --> 00:00:45,535
Las funciones de activación modernas
no son necesariamente diferenciables

15
00:00:45,535 --> 00:00:48,140
y las personas no sabían
cómo trabajar con ellas.

16
00:00:48,670 --> 00:00:52,135
Esta limitación sobre la diferenciabilidad
de las funciones de activación

17
00:00:52,135 --> 00:00:54,890
podría dificultar el entrenamiento
de las redes.

18
00:00:54,890 --> 00:00:58,455
La efectividad de estos modelos
también se limitó por la cantidad de datos

19
00:00:58,455 --> 00:01:02,280
los recursos informáticos disponibles
y otras dificultades del entrenamiento.

20
00:01:02,280 --> 00:01:05,115
Por ejemplo, la optimización
puede quedar atrapada

21
00:01:05,115 --> 00:01:08,110
en puntos de silla,
en vez de encontrar la mínima global

22
00:01:08,110 --> 00:01:10,775
que esperábamos,
durante el descenso de gradientes.

23
00:01:10,775 --> 00:01:12,910
No obstante,
cuando se desarrolló la solución

24
00:01:12,910 --> 00:01:16,440
mediante el uso
de unidades lineales rectificadas o ReLU

25
00:01:16,440 --> 00:01:19,730
se pudo acelerar el entrenamiento
de ocho a diez veces

26
00:01:19,730 --> 00:01:22,705
con convergencia casi garantizada
para la regresión logística.

27
00:01:23,470 --> 00:01:26,055
Si desarrollamos el perceptrón,
al igual que el cerebro

28
00:01:26,055 --> 00:01:29,150
podemos conectar muchos
de ellos para formar capas

29
00:01:29,150 --> 00:01:31,670
y crear redes neuronales prealimentadas.

30
00:01:31,670 --> 00:01:34,465
No ha habido muchos cambios
con respecto a los componentes

31
00:01:34,465 --> 00:01:38,235
del perceptrón de una sola capa:
hay entradas, sumas ponderadas,

32
00:01:38,235 --> 00:01:40,655
funciones de activación y salidas.

33
00:01:41,085 --> 00:01:43,650
Una diferencia es
que las entradas a las neuronas

34
00:01:43,650 --> 00:01:46,690
que no están en la capa de entrada,
no son entradas sin procesar

35
00:01:46,690 --> 00:01:49,040
sino las salidas de la capa anterior.

36
00:01:49,040 --> 00:01:51,840
Otra diferencia es que las vías
que conectan a las neuronas

37
00:01:51,840 --> 00:01:55,880
entre capas
ya no son vectores, sino una matriz

38
00:01:55,880 --> 00:01:59,910
debido a la conectividad completa
de todas las neuronas entre capas.

39
00:02:00,290 --> 00:02:02,950
Por ejemplo, en el diagrama,
la matriz de ponderaciones

40
00:02:02,950 --> 00:02:07,160
de la capa de entrada es de cuatro por dos
y la de la capa oculta es de dos por uno.

41
00:02:07,600 --> 00:02:09,970
Aprenderemos más adelante
que las redes neuronales

42
00:02:09,970 --> 00:02:13,185
no siempre tienen conectividad
completa, pero tienen aplicaciones

43
00:02:13,185 --> 00:02:15,900
y un rendimiento increíble;
por ejemplo, con las imágenes.

44
00:02:15,900 --> 00:02:19,500
También, hay funciones de activación
diferentes de las unidades,

45
00:02:19,500 --> 00:02:23,560
como las funciones de activación sigmoidal
y la tangente hiperbólica o tanh.

46
00:02:23,560 --> 00:02:26,310
Pueden considerar
a cada neurona sin entradas (non-input)

47
00:02:26,310 --> 00:02:29,210
como una colección de tres pasos
agrupados en una sola unidad.

48
00:02:29,570 --> 00:02:32,060
El primer componente
es una suma ponderada,

49
00:02:32,060 --> 00:02:37,100
el segundo es la función de activación
y el tercero es la salida de la función.

50
00:02:37,940 --> 00:02:40,330
Las redes neuronales
pueden ser bastante complejas

51
00:02:40,330 --> 00:02:43,440
con todas las capas,
las neuronas, las funciones de activación

52
00:02:43,440 --> 00:02:45,260
y los métodos para entrenarlas.

53
00:02:45,260 --> 00:02:47,670
En este curso,
usaremos TensorFlow Playground

54
00:02:47,670 --> 00:02:50,670
para tener una idea más intuitiva
de cómo la información fluye

55
00:02:50,670 --> 00:02:52,175
a través de una red neuronal.

56
00:02:52,175 --> 00:02:54,685
También es muy divertido;
les permite personalizar

57
00:02:54,685 --> 00:02:57,585
muchos más hiperparámetros,
así como visualizar las magnitudes

58
00:02:57,585 --> 00:03:00,040
de los pesos
y cómo la función de pérdida

59
00:03:00,040 --> 00:03:01,550
evoluciona en el tiempo.

60
00:03:02,750 --> 00:03:04,950
Esta es la función de activación lineal.

61
00:03:04,950 --> 00:03:09,660
Básicamente, es una función de identidad
porque la función de x simplemente es x.

62
00:03:09,660 --> 00:03:11,865
Esta era la función
de activación original.

63
00:03:11,865 --> 00:03:14,680
No obstante, como dije antes,
incluso con una red neuronal

64
00:03:14,680 --> 00:03:18,120
con miles de capas, en la que todas
usan una función de activación lineal

65
00:03:18,120 --> 00:03:21,245
al final, la salida
será una combinación lineal

66
00:03:21,245 --> 00:03:23,000
de los atributos de entrada.

67
00:03:23,000 --> 00:03:25,550
Esto se puede reducir
a los atributos de entrada

68
00:03:25,550 --> 00:03:27,630
cada uno multiplicado por una constante.

69
00:03:27,830 --> 00:03:29,275
¿Suena familiar?

70
00:03:29,505 --> 00:03:31,780
Es una regresión lineal simple.

71
00:03:31,780 --> 00:03:34,765
Por lo tanto, se necesitan
funciones de activación no lineales

72
00:03:34,765 --> 00:03:37,580
para obtener las funciones
complejas en cadena que permiten

73
00:03:37,580 --> 00:03:41,190
que las redes neuronales aprendan
tan bien las distribuciones de los datos.

74
00:03:43,250 --> 00:03:47,305
Además de la función de activación lineal,
en la que f de x es igual a x,

75
00:03:47,305 --> 00:03:50,970
las funciones de activación principales,
que se usaban durante la época de oro

76
00:03:50,970 --> 00:03:52,495
de las redes neuronales

77
00:03:52,495 --> 00:03:55,140
eran las funciones
de activación sigmoidal y tanh.

78
00:03:55,340 --> 00:03:58,330
La función de activación sigmoidal
es una versión continua

79
00:03:58,330 --> 00:04:01,570
de la función escalón unitario,
en la que la asíntota tiende a cero

80
00:04:01,570 --> 00:04:02,760
en el infinito negativo

81
00:04:02,760 --> 00:04:05,114
y la asíntota tiende a uno
en el infinito positivo

82
00:04:05,114 --> 00:04:07,600
pero hay valores en todo el intermedio.

83
00:04:10,540 --> 00:04:15,225
La tangente hiperbólica o tanh
es otra función de activación de uso común

84
00:04:15,225 --> 00:04:18,460
en ese punto,
que básicamente es una sigmoidal escalada

85
00:04:18,460 --> 00:04:21,950
y en intervalo,
ahora con un rango de menos uno a uno.

86
00:04:21,950 --> 00:04:25,160
Estas fueron excelentes opciones,
porque eran diferenciables,

87
00:04:25,160 --> 00:04:27,310
monótonas y continuas.

88
00:04:28,060 --> 00:04:32,175
No obstante, ocurren problemas
como saturación, debido a los valores

89
00:04:32,175 --> 00:04:35,340
de entrada altos o bajos en las funciones

90
00:04:35,340 --> 00:04:38,610
que terminarían
en la meseta asintótica de la función.

91
00:04:38,610 --> 00:04:41,410
Ya que la curva
es casi plana en estos puntos,

92
00:04:41,410 --> 00:04:43,835
las derivadas están muy cerca de cero.

93
00:04:43,835 --> 00:04:47,645
Por lo tanto, el entrenamiento
de los pesos sería muy lento

94
00:04:47,645 --> 00:04:49,760
o incluso se detendría,
ya que los gradientes

95
00:04:49,760 --> 00:04:53,330
estarían muy cerca de cero,
lo que resultaría en pasos muy pequeños

96
00:04:53,330 --> 00:04:55,905
de descenso del gradiente.

97
00:04:56,095 --> 00:04:57,890
Las funciones de activación lineales

98
00:04:57,890 --> 00:05:00,025
eran diferenciables,
monótonas y continuas.

99
00:05:00,025 --> 00:05:02,510
Pero, como mencioné antes,
una combinación lineal

100
00:05:02,510 --> 00:05:05,395
de funciones lineales
se pueden colapsar en una sola.

101
00:05:05,395 --> 00:05:08,980
Esto no nos permite crear la cadena
compleja de funciones que necesitamos

102
00:05:08,980 --> 00:05:10,710
para describir bien nuestros datos.

103
00:05:10,710 --> 00:05:13,200
Hubo aproximaciones
a la función de activación lineal

104
00:05:13,200 --> 00:05:15,265
pero no eran diferenciables
en todas partes.

105
00:05:15,265 --> 00:05:18,780
Fue mucho más adelante que las personas
supieron qué hacer con ellas.

106
00:05:19,390 --> 00:05:23,150
Hoy, la función de activación
de unidad lineal rectificada

107
00:05:23,150 --> 00:05:25,050
o ReLU es muy popular.

108
00:05:25,050 --> 00:05:28,370
Es no lineal, por lo que pueden obtener
el modelado complejo necesario

109
00:05:28,370 --> 00:05:32,415
y no tiene la saturación en la porción
no negativa del espacio de entrada.

110
00:05:32,415 --> 00:05:35,630
Sin embargo, debido a que la porción
negativa del espacio de entrada

111
00:05:35,630 --> 00:05:39,890
se traduce en cero activación,
las capas ReLU podrían terminar muriendo

112
00:05:39,890 --> 00:05:42,910
o no activándose,
lo que puede provocar que el entrenamiento

113
00:05:42,910 --> 00:05:45,065
sea lento o se detenga.

114
00:05:47,145 --> 00:05:49,340
Hay algunas soluciones para este problema,

115
00:05:49,340 --> 00:05:51,970
una de estas es usar
otra función de activación

116
00:05:51,970 --> 00:05:54,790
llamada unidad exponencial lineal o ELU.

117
00:05:55,720 --> 00:05:58,350
Es casi lineal en la porción no negativa

118
00:05:58,350 --> 00:06:02,125
del espacio de entrada;
es continua, monótona y, sobre todo,

119
00:06:02,125 --> 00:06:06,050
no es cero en la porción negativa
del espacio de entrada.

120
00:06:06,050 --> 00:06:09,940
La desventaja principal de las ELU
es que su computación es más costosa

121
00:06:09,940 --> 00:06:13,050
que las ReLU, debido a que
se tiene que calcular el exponencial.

122
00:06:13,050 --> 00:06:16,725
Experimentaremos con esto
mucho más en el siguiente módulo.

123
00:06:16,725 --> 00:06:19,935
Si quisiera que mis salidas
estén en la forma de probabilidades,

124
00:06:19,935 --> 00:06:23,265
¿qué función de activación
debería elegir en la capa final?

125
00:06:26,345 --> 00:06:29,505
La respuesta correcta es
A. La función de activación sigmoidal.

126
00:06:29,505 --> 00:06:31,910
Eso es porque el rango
de la función sigmoidal

127
00:06:31,910 --> 00:06:35,275
está entre cero y uno, que también
es el rango de la probabilidad.

128
00:06:35,275 --> 00:06:39,500
Además del rango, la función sigmoidal
es la función de distribución acumulada

129
00:06:39,500 --> 00:06:41,735
de la distribución logística
de la probabilidad

130
00:06:41,735 --> 00:06:44,910
cuya función cuantil
es la inversa del logit que modela

131
00:06:44,910 --> 00:06:46,695
el logit de la probabilidad.

132
00:06:46,695 --> 00:06:49,860
Por eso se puede usar
como una probabilidad verdadera.

133
00:06:49,860 --> 00:06:53,500
Hablaremos más sobre esas razones
más adelante.

134
00:06:53,500 --> 00:06:56,065
B. Tanh es incorrecto,
porque, aunque es una función

135
00:06:56,065 --> 00:07:00,300
que aplasta, similar a la sigmoidal,
su rango está entre menos uno y uno

136
00:07:00,300 --> 00:07:02,775
que no es el mismo rango
de la probabilidad.

137
00:07:02,775 --> 00:07:05,620
Además, aplastar tanh
en una sigmoidal, no la convertirá

138
00:07:05,620 --> 00:07:09,000
mágicamente en una probabilidad,
porque no tiene las mismas propiedades

139
00:07:09,000 --> 00:07:11,765
mencionadas
que permiten que la salida de la sigmoide

140
00:07:11,765 --> 00:07:13,825
se pueda interpretar
como una probabilidad.

141
00:07:13,825 --> 00:07:15,940
Para convertirla en una sigmoidal

142
00:07:15,940 --> 00:07:18,880
tendrían que sumar uno
y luego dividir por dos para obtener

143
00:07:18,880 --> 00:07:20,320
el rango correcto.

144
00:07:20,320 --> 00:07:22,780
A la vez,
para obtener la expansión correcta

145
00:07:22,780 --> 00:07:25,260
tendrían que dividir el argumento
de tanh entre dos.

146
00:07:25,260 --> 00:07:29,260
Pero ya calcularon tanh.
Estaríamos repitiendo el trabajo

147
00:07:29,260 --> 00:07:32,370
por lo que sería mejor
usar una sigmoidal desde el principio.

148
00:07:32,370 --> 00:07:36,360
C. ReLU es incorrecto,
porque su rango está entre cero e infinito

149
00:07:36,360 --> 00:07:39,450
lo que está muy lejos
de la representación de una probabilidad.

150
00:07:39,450 --> 00:07:42,086
D. ELU también es incorrecto,
porque su rango

151
00:07:42,086 --> 00:07:44,006
está entre infinito negativo e infinito.