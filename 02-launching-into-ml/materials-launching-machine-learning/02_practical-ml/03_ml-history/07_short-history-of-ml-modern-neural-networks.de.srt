1
00:00:00,000 --> 00:00:03,020
Neuronale Netze tauchen
wieder in der Zeitleiste auf,

2
00:00:03,020 --> 00:00:05,580
jetzt mit noch mehr
Vorteilen durch die Sprünge

3
00:00:05,580 --> 00:00:08,760
in der Rechenleistung
und viele, viele Daten.

4
00:00:08,760 --> 00:00:14,840
DNNs fingen an, andere getestete Methoden
wie Computervision klar zu übertreffen.

5
00:00:14,840 --> 00:00:17,680
Außer dem Boom verstärkter Hardware

6
00:00:17,680 --> 00:00:21,150
gibt es viele neue
Tricks und Architekturen,

7
00:00:21,150 --> 00:00:25,075
die die Lernfähigkeit neuronaler Deep-
Learning-Netze steigern, wie ReLUs, bessere

8
00:00:25,075 --> 00:00:30,925
Initialisierungsmethoden, CNNs oder
Convolutional Neural Networks und Dropout.

9
00:00:30,925 --> 00:00:34,650
Wir haben über einige dieser Tricks
aus anderen ML-Methoden gesprochen.

10
00:00:34,650 --> 00:00:38,090
Den Einsatz nicht linearer
Aktivierungsfunktionen wie ReLUs,

11
00:00:38,090 --> 00:00:40,355
die heute meistens
als Standard festgelegt sind,

12
00:00:40,355 --> 00:00:43,470
haben wir bei unserem ersten
Blick auf neuronale Netze besprochen.

13
00:00:43,470 --> 00:00:46,775
Man fing an, die Generalisierung
mit Dropout-Schichten zu verbessern,

14
00:00:46,775 --> 00:00:48,585
was wie Ensemblemethoden funktioniert,

15
00:00:48,585 --> 00:00:52,130
wie bei der Betrachtung von Random 
Forests und Boosting-Bäumen besprochen.

16
00:00:52,130 --> 00:00:54,760
Hinzu kamen Convolutional Layers, die

17
00:00:54,760 --> 00:00:58,825
den Rechen- und Speicheraufwand senkten,
da sie nicht vollständig verbunden

18
00:00:58,825 --> 00:01:01,990
und in der Lage sind,
sich auf lokale Aspekte zu konzentrieren,

19
00:01:01,990 --> 00:01:06,505
etwa Bilder, anstatt unzusammenhängende
Dinge in einem Bild zu vergleichen.

20
00:01:06,505 --> 00:01:10,850
Sprich, alle Errungenschaften
der anderen ML-Methoden

21
00:01:10,850 --> 00:01:13,005
wurden in neuronale Netze zurückgeführt.

22
00:01:13,005 --> 00:01:16,160
Betrachten wir ein Beispiel
für ein neuronales Deep-Learning-Netz.

23
00:01:16,160 --> 00:01:18,475
Die spannende Geschichte
des maschinellen Lernens

24
00:01:18,475 --> 00:01:22,320
kulminiert in Deep Learning mit
neuronalen Netzen, die Hunderte von

25
00:01:22,320 --> 00:01:26,400
Schichten und Millionen von Parametern
und erstaunliche Ergebnisse haben.

26
00:01:26,400 --> 00:01:29,065
Hier sehen wir GoogLeNet oder Inception,

27
00:01:29,065 --> 00:01:31,150
ein Bildklassifizierungsmodell.

28
00:01:31,150 --> 00:01:32,770
Es wurde für ImageNet

29
00:01:32,770 --> 00:01:38,030
Large Scale Visual Recognition Challenge
2014 mit Daten von 2012 trainiert,

30
00:01:38,030 --> 00:01:40,020
wobei es für das Training Bilder aus

31
00:01:40,020 --> 00:01:43,970
Tausenden Klassen mit 1,2 Millionen
Bildern klassifizieren musste.

32
00:01:43,970 --> 00:01:46,420
Es hat 22 Tiefenebenen,

33
00:01:46,420 --> 00:01:48,590
27 mit dem Pooling,

34
00:01:48,590 --> 00:01:50,630
worauf wir in
einem späteren Kurs eingehen,

35
00:01:50,630 --> 00:01:54,700
und hundert Schichten, wenn wir es in
seine unabhängigen Bausteine zerlegen.

36
00:01:54,700 --> 00:01:57,980
Es gibt über 11 Millionen Lernparameter.

37
00:01:57,980 --> 00:02:01,355
Es gibt vollständig verbundene
Schichten und solche, die es nicht sind,

38
00:02:01,355 --> 00:02:04,225
wie Convolutional Layers,
über die wir später sprechen.

39
00:02:04,225 --> 00:02:07,035
Es verwendet Dropout-Schichten
zur stärkeren Generalisierung

40
00:02:07,045 --> 00:02:10,290
durch Simulation eines Ensembles
von neuronalen Deep-Learning-Netzen.

41
00:02:10,290 --> 00:02:12,970
Wie wir bei neuronalen Netzen
und Stacking gesehen haben,

42
00:02:12,970 --> 00:02:16,695
ist jede Box eine Einheit von Komponenten,
die Teil einer Gruppe von Boxen ist,

43
00:02:16,695 --> 00:02:18,285
wie die, die ich vergrößert habe.

44
00:02:18,285 --> 00:02:21,660
Das Konzept der Bausteine, die
zusammen größer sind als die Summe ihrer

45
00:02:21,660 --> 00:02:25,720
Teile, ist einer der Aspekte, die das
Deep Learning so erfolgreich machen.

46
00:02:25,720 --> 00:02:28,520
Natürlich sind auch die
unaufhörlich wachsende Datenmengen

47
00:02:28,520 --> 00:02:31,805
und frische Rechenleistung
mit mehr Speicher eine Hilfe.

48
00:02:31,805 --> 00:02:34,830
Es existieren jetzt darüber
hinaus mehrere Versionen, die

49
00:02:34,830 --> 00:02:37,635
viel größer und sogar noch genauer sind.

50
00:02:37,635 --> 00:02:40,310
Was wir aus dieser langen Geschichte
vor allem mitnehmen,

51
00:02:40,310 --> 00:02:43,740
ist, dass die ML-Forschung
Stückchen der Techniken aus

52
00:02:43,740 --> 00:02:47,150
anderen Algorithmen der
Vergangenheit aufgreift und zu immer

53
00:02:47,150 --> 00:02:50,900
leistungsfähigeren Modellen kombiniert
und vor allem experimentiert.

54
00:02:50,900 --> 00:02:55,300
Was ist bei der Erstellung
neuronaler Deep-Learning-Netze wichtig?

55
00:02:55,300 --> 00:02:59,255
Die richtige Antwort ist: Alles genannte.

56
00:02:59,255 --> 00:03:01,445
Dies ist keine vollständige Liste,

57
00:03:01,445 --> 00:03:04,550
aber es ist sehr wichtig, 
an diese drei Dinge zu denken.

58
00:03:04,550 --> 00:03:07,790
Zuerst muss man viele Daten haben.

59
00:03:07,790 --> 00:03:10,450
Es wird viel Forschung
in dem Versuch betrieben,

60
00:03:10,450 --> 00:03:13,360
den Datenbedarf beim
Deep Learning zu verringern, aber vorerst

61
00:03:13,360 --> 00:03:15,530
müssen wir für große Mengen davon sorgen.

62
00:03:15,530 --> 00:03:18,780
Das liegt an der
hohen Kapazität durch die vielen

63
00:03:18,780 --> 00:03:22,080
Parameter, die in diesen massiven
Modellen gelernt werden müssen.

64
00:03:22,080 --> 00:03:24,360
Da das Modell so komplex ist,

65
00:03:24,360 --> 00:03:27,225
muss es die Datenverteilung
wirklich gut verinnerlichen.

66
00:03:27,225 --> 00:03:29,580
Deshalb braucht es eine große Signalmenge.

67
00:03:29,580 --> 00:03:32,680
Bedenken Sie, dass es nicht der Sinn
des maschinellen Lernens ist,

68
00:03:32,680 --> 00:03:35,630
aus Spaß eine Reihe
toller Modelle zu trainieren,

69
00:03:35,630 --> 00:03:39,355
sondern sie so zu trainieren, dass wir
sehr genaue Vorhersagen treffen können.

70
00:03:39,355 --> 00:03:42,650
Wenn wir die neuen Daten nicht für
Vorhersagen verallgemeinern können,

71
00:03:42,650 --> 00:03:44,055
wozu ist das Modell dann gut?

72
00:03:44,055 --> 00:03:48,030
Deshalb ist es so wichtig,
genügend Daten zu haben, damit

73
00:03:48,030 --> 00:03:52,125
keine Überanpassung an einem kleinen,
Millionen Male gesehenen Dataset

74
00:03:52,125 --> 00:03:55,405
erfolgt, anstelle eines riesigen,
viel weniger gesehenen Datasets.

75
00:03:55,405 --> 00:03:56,680
Dadurch haben Sie außerdem

76
00:03:56,680 --> 00:04:00,425
Validierungs- und Testsätze ausreichender
Größe für das Tuning Ihres Modells.

77
00:04:00,425 --> 00:04:03,860
Außerdem kann durch Hinzufügen von
Dropout-Layers, Datenaugmentation,

78
00:04:03,860 --> 00:04:08,435
zusätzliches Rauschen usw. eine noch
bessere Generalisierung erreicht werden.

79
00:04:08,435 --> 00:04:12,210
Schließlich ist das Experimentieren
das A und O beim maschinellen Lernen.

80
00:04:12,210 --> 00:04:14,760
Es gibt heute so viele
verschiedenartige Algorithmen,

81
00:04:14,760 --> 00:04:18,084
Hyperparameter und Methoden
zum Erstellen von ML-Datasets.

82
00:04:18,084 --> 00:04:20,570
Es ist wirklich für
fast alle Probleme unmöglich,

83
00:04:20,570 --> 00:04:24,005
a priori die optimalen
Entscheidungen zu kennen.

84
00:04:24,005 --> 00:04:27,330
Durch Experimentieren und
sorgfältige Dokumentation der Versuche

85
00:04:27,330 --> 00:04:30,535
und Leistung zum
modellübergreifenden Vergleich

86
00:04:30,535 --> 00:04:35,620
werden Sie nicht nur viel Spaß haben,
sondern auch richtig starke Tools bauen.

87
00:04:35,620 --> 00:04:38,060
Ich gehe jetzt noch etwas mehr darauf ein,

88
00:04:38,060 --> 00:04:41,805
wie neuronale Netze auf der Leistung
vergangener Modelle aufsetzen.

89
00:04:41,805 --> 00:04:43,640
Wir sehen hier die Leistung bestimmter

90
00:04:43,640 --> 00:04:47,240
Modellversionen von neuronalen
Deep-Learning-Netzen der letzten Jahre.

91
00:04:47,240 --> 00:04:49,080
Wie Sie in der Tabelle sehen,

92
00:04:49,080 --> 00:04:51,000
kam es 2014 zu einem bedeutenden Sprung,

93
00:04:51,000 --> 00:04:52,390
der blau hervorgehoben ist,

94
00:04:52,390 --> 00:04:54,310
als das Inception-Modell von Google die

95
00:04:54,310 --> 00:04:57,375
10-Prozent-Fehlerrate
mit 6,7 % durchbrach.

96
00:04:57,375 --> 00:05:00,350
Die Leistung von DNNs verbessert sich

97
00:05:00,350 --> 00:05:04,160
von Jahr zu Jahr und wir lernen aus
den Erkenntnissen aus vorherigen Modellen.

98
00:05:04,160 --> 00:05:06,480
2015 hat eine dritte Version des

99
00:05:06,480 --> 00:05:09,840
Inception-Modells eine
3,5-Prozent-Fehlerrate erreicht.

100
00:05:09,840 --> 00:05:14,045
Warum verbessern sich diese
Modelle in so kurzer Zeit so drastisch?

101
00:05:14,045 --> 00:05:18,465
Wenn Forschungsgruppen gut funktionierende
neue Techniken oder Methoden entwickeln,

102
00:05:18,465 --> 00:05:22,200
übernehmen andere Gruppen
diese Ideen und bauen darauf auf.

103
00:05:22,200 --> 00:05:28,260
Das bringt das Experimentieren vorwärts
und der Fortschritt wird beschleunigt.

104
00:05:28,260 --> 00:05:31,750
Das betrifft etwa bessere
Hyperparameter, mehr Schichten,

105
00:05:31,750 --> 00:05:36,565
bessere Teilkomponenten wie Convolutional
Layers, bessere Generalisierbarkeit usw.

106
00:05:36,565 --> 00:05:39,910
Erklären Sie, wie Sie ML auf
das Problem anwenden würden.

107
00:05:39,910 --> 00:05:43,365
Es kann mehrere richtige Antworten geben.

108
00:05:43,365 --> 00:05:47,810
Sie besitzen eine Skistation und
möchten den Verkehr auf den

109
00:05:47,810 --> 00:05:51,235
Pisten auf der Grundlage von vier
Kundentypen (Anfänger,

110
00:05:51,235 --> 00:05:53,670
Mittelstufe, Fortgeschrittene, Experten),

111
00:05:53,670 --> 00:05:58,075
die Tickets gekauft haben, und dem
vergangenen Schneefall vorhersagen.

112
00:05:58,075 --> 00:06:02,735
Nehmen Sie sich jetzt einen Moment,
um eine Antwort aufzuschreiben.

113
00:06:02,735 --> 00:06:07,265
Das könnte eine
Regression oder Klassifikation sein,

114
00:06:07,265 --> 00:06:11,240
da ich nicht genau angegeben
habe, was ich mit Verkehr meine.

115
00:06:11,240 --> 00:06:15,170
Meine ich die Anzahl der Leute,
die diese Piste pro Stunde benutzen?

116
00:06:15,170 --> 00:06:19,205
Oder möchte ich eher eine Kategorie,
etwa hoch, mittel und niedrig?

117
00:06:19,205 --> 00:06:21,830
Dafür würde ich mit einer
einfachen Heuristik beginnen,

118
00:06:21,830 --> 00:06:24,900
etwa die durchschnittliche
Anzahl Personen auf jeder Piste, und

119
00:06:24,900 --> 00:06:28,370
dann mit Grundmodellen linearer
oder logistischer Regression fortfahren,

120
00:06:28,370 --> 00:06:33,075
je nachdem, ob ich mich für Regression
oder Klassifikation entschieden habe.

121
00:06:33,075 --> 00:06:35,545
Je nach Leistung und Datenmenge

122
00:06:35,545 --> 00:06:38,195
würde ich dann wohl zu
neuronalen Netzen fortschreiten.

123
00:06:38,195 --> 00:06:40,240
Bei weiteren Merkmalen in den Daten

124
00:06:40,240 --> 00:06:44,225
würde ich diese auch
ausprobieren und die Leistung beobachten.

125
00:06:44,225 --> 00:06:48,790
Google-intern lagen zuletzt

126
00:06:48,790 --> 00:06:53,025
über 4.000 Deep ML-Produktionssysteme
vor, die Google-Systeme antreiben.

127
00:06:53,025 --> 00:06:56,470
Alle diese Modelle und Versionen
genießen den Leistungsvorteil durch

128
00:06:56,470 --> 00:07:00,290
den Aufbau auf den Erfolgen
und Misserfolgen vorheriger Modelle.

129
00:07:00,290 --> 00:07:03,585
Eines der anfangs
gebräuchlichsten war Sibyl,

130
00:07:03,585 --> 00:07:07,230
das ursprünglich für die Empfehlung
verwandter YouTube-Videos erstellt wurde.

131
00:07:07,230 --> 00:07:09,670
Dieser Empfehlungsdienst
funktioniert so gut,

132
00:07:09,670 --> 00:07:13,365
dass er bald weithin in Anzeigen und
andere Google-Komponenten integriert wurde.

133
00:07:13,365 --> 00:07:15,720
Es war ein lineares Modell.

134
00:07:15,720 --> 00:07:19,460
Vizier war ein weiteres Modell, aus dem

135
00:07:19,460 --> 00:07:23,980
der Parameterabstimmungsdienst
für andere Modelle und Systeme wurde.

136
00:07:23,980 --> 00:07:27,020
Google Brain, der
ML-Forschungszweig von Google,

137
00:07:27,020 --> 00:07:30,540
hat eine Methode gefunden, die
Rechenleistung von Tausenden CPUs

138
00:07:30,540 --> 00:07:34,590
zu nutzen, um große Modelle wie neuronale
Deep-Learning-Netze zu trainieren.

139
00:07:34,590 --> 00:07:36,940
Die Erfahrungen durch
das Erstellen und Ausführen

140
00:07:36,940 --> 00:07:39,730
dieser Modelle haben zur
Erzeugung von TensorFlow geführt,

141
00:07:39,730 --> 00:07:42,410
einer Open-Source-Bibliothek 
für maschinelles Lernen.

142
00:07:42,410 --> 00:07:47,210
Dann entwickelte Google TFX oder
die TensorFlow-basierte ML-Plattform.

143
00:07:47,210 --> 00:07:50,600
Wir werden Ihnen zeigen, wie Sie
ML-Produktionsmodelle mit TensorFlow

144
00:07:50,600 --> 00:07:55,040
und Tools wie Cloud ML Engine, Dataflow
und BigQuery bauen und bereitstellen.

145
00:07:55,040 --> 00:07:57,465
Um es zusammenzufassen:
In den letzten Jahrzehnten

146
00:07:57,465 --> 00:08:01,140
hat sich die Anwendung und Leistung
neuronaler Netze stark erhöht.

147
00:08:01,140 --> 00:08:02,700
Durch die Allgegenwart von Daten

148
00:08:02,700 --> 00:08:06,905
haben diese Modelle den Vorteil einer
immer größeren Anzahl von Lernbeispielen.

149
00:08:06,905 --> 00:08:10,410
Die Zunahme der Daten und Beispiele
wurde mit skalierbarer Infrastruktur

150
00:08:10,410 --> 00:08:15,900
gekoppelt und ergibt komplexe,
verteilte Modelle mit Tausenden Schichten.

151
00:08:15,900 --> 00:08:18,140
Ich gebe Ihnen den
Hinweis mit auf den Weg, dass

152
00:08:18,140 --> 00:08:22,025
die Leistung neuronaler Netze bei einigen
Anwendungen zwar hervorragend sein kann,

153
00:08:22,025 --> 00:08:25,990
aber sie nur eins von vielen Modellen sind,
mit denen Sie experimentieren können.

154
00:08:25,990 --> 00:08:28,070
Experimentieren ist der Schlüssel zur

155
00:08:28,070 --> 00:08:32,000
besten Leistung, um mit
Ihren Daten Ihr Problem zu lösen.