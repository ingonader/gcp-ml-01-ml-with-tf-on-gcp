Coming into the last few decades in the 2000s, machine learning research now had the computational power to combine and blend performance across many miles in what we call an ensemble method. You can imagine that if the errors are independent for a number of simple weak learners, combined, they would form a strong learner. DNN is going to approximate this by using dropout layers, which help regularize the model and prevent overfitting. This can be simulated by randomly turning off neurons in the network with some probability for each forward pass, which will essentially be creating a new network each time. Oftentimes, complex questions are better answered when aggregated from thousands of people's responses instead of those just by a sole individual. This is known as the wisdom of the crowd. The same applies to machine learning. When you aggregate the results of many predictors either classifiers or reggressors, the group will usually perform better than the best individual model. This group of predictors is an ensemble which when combined in this way, it leads to ensemble learning. The algorithm that performs this learning is an ensemble method. One of the most popular types of ensemble learning is the random forest. Instead of taking your entire training set and using that to build one decision tree, you could have a group of decision trees that each get a random subsample of the training data. Since they haven't seen the entire training set, they can't have memorized the entire thing. Once all the trees are trained and they're a subset of the data, you can now make the most important and valuable part of machine learning, predictions. To do so, you would pass your test sample through each tree in the forest, and then aggregate the results. If this is classification, there could be a majority vote across all trees which would then be the final output class. If it is regression, it could be an aggregate the values such as the mean, max, median, et cetera. To improve generalization, you can random sample the examples and/or the features. We call random sampling examples for replacement, bagging, short for bootstrap aggregating, and pasting when without replacement. Each individual predictor has higher bias being trained on the smaller subset rather than the full dataset, but the aggregation reduces both the bias and variance. This usually gives the ensemble a similar bias as a single predictor on the entire training set, but with a lower variance. A great method of validation for your generalization error, is to use your out of bagged data instead of having to have a separate set pulled from the dataset before training. It is reminiscent of k-fold validation using random holdouts. Random subspaces are made when we sample from the features, and if we random sample examples too is called random patches. Adaptive boosting or AdaBoost in gradient boosting are both examples of boosting, which is when we aggregate a number of weak learners to create a strong learner. Typically, this is done by training each learner sequentially which tries to correct any issues a learner had before it. For boosted trees, as more trees are added to the ensemble, the predictions usually improve. So, do we continue to add trees out of infinitum? Of course not. You can use your validation set to use early stopping, so that we don't start overfitting our training data, because we've added too many trees. Lastly, just as we saw with neural networks, we can perform stacking, where we can have meta-learners learn what to do with the pictures of the ensemble, which can in turn also be stacked into meta-learners and so on. We will see the subcomponent stacking and reuse in deep neural networks shortly. Which of the following is most likely false of random forests when comparing against individual decision trees? The correct answer is that it's most likely false, that random forests are easier to visually interpret. Similar to neural networks, the more layers in complexity you add to your model, the more difficult it will be to understand and explain. A random forest is usually more complex than an individual decision tree, so this makes it harder to visually interpret. The other three are most likely true. Random forests typically have better generalization through bagging and subspacing, also by using a voting system for classification or aggregation for regression, the forest can typically perform much better than an individual tree. Lastly, due to the random sampling of random forests, it keeps its bias similar to that of an individual tree, but also, has lower variance which once again, usually means, better generalization.