1
00:00:00,000 --> 00:00:03,020
Nuevamente en la línea de tiempo
están las redes neuronales

2
00:00:03,020 --> 00:00:05,040
ahora con más ventaja

3
00:00:05,040 --> 00:00:08,770
gracias a la potencia informática
y la gran cantidad de datos.

4
00:00:09,240 --> 00:00:12,910
Las DNN comenzaron a superar
significativamente a otros métodos

5
00:00:12,910 --> 00:00:15,180
en pruebas, como la visión artificial.

6
00:00:15,180 --> 00:00:20,730
Además del auge del hardware potenciado,
hay muchos trucos nuevos y arquitecturas

7
00:00:20,730 --> 00:00:24,090
que ayudan a mejorar el entrenamiento
de las redes neuronales profundas

8
00:00:24,090 --> 00:00:26,695
como las ReLU,
mejores métodos de inicialización,

9
00:00:26,695 --> 00:00:30,925
CNN o redes neuronales
convolucionales y retirados (dropout).

10
00:00:31,645 --> 00:00:34,890
Ya hablamos sobre algunos
de estos trucos de otros métodos de AA.

11
00:00:34,890 --> 00:00:38,090
El uso de funciones de activación
no lineales, como las ReLU,

12
00:00:38,090 --> 00:00:40,395
que hoy, por lo general,
son las predeterminadas.

13
00:00:40,395 --> 00:00:43,540
Hablamos de eso cuando
vimos las redes neuronales al principio.

14
00:00:43,540 --> 00:00:46,245
Las capas de retirados
se comenzaron a usar para ayudar

15
00:00:46,245 --> 00:00:49,535
con la generalización, que funciona mejor
con los métodos de ensambles

16
00:00:49,535 --> 00:00:52,540
que vimos con los bosques aleatorios
y los árboles potenciados.

17
00:00:52,540 --> 00:00:56,245
Las capas convolucionadas se agregaron
para reducir la carga de procesamiento

18
00:00:56,245 --> 00:00:59,170
y memoria,
debido a su conectividad incompleta

19
00:00:59,170 --> 00:01:02,275
al igual que su capacidad
de enfocarse en aspectos locales

20
00:01:02,275 --> 00:01:07,080
por ejemplo, imágenes, en vez de comparar
elementos no relacionados en una imagen.

21
00:01:07,080 --> 00:01:10,895
En otras palabras, los avances
que surgieron con otros métodos de AA

22
00:01:10,895 --> 00:01:13,150
se aprovecharon en las redes neuronales.

23
00:01:13,150 --> 00:01:15,525
Veamos un ejemplo
de una red neuronal profunda.

24
00:01:16,095 --> 00:01:18,780
Esta historia emocionante
del aprendizaje automático

25
00:01:18,780 --> 00:01:21,940
culminó en el aprendizaje profundo,
con redes neuronales

26
00:01:21,940 --> 00:01:24,795
que contienen cientos de capas
y millones de parámetros

27
00:01:24,795 --> 00:01:26,760
pero con resultados sorprendentes.

28
00:01:26,760 --> 00:01:29,420
Aquí vemos un GoogLeNet o Inception

29
00:01:29,420 --> 00:01:31,740
que es un modelo
de clasificación de imágenes.

30
00:01:31,740 --> 00:01:35,610
Se entrenó para el desafío
de reconocimiento visual de gran escala

31
00:01:35,610 --> 00:01:40,760
de ImageNet de 2014, con datos de 2012,
para clasificar imágenes de miles

32
00:01:40,760 --> 00:01:44,190
de clases, con 1.2 millones
de imágenes por entrenamiento.

33
00:01:44,190 --> 00:01:48,490
Tiene 22 capas profundas,
27 si se incluye el agrupamiento

34
00:01:48,490 --> 00:01:52,670
que veremos en un curso posterior,
y cientos de capas, si se las desglosa

35
00:01:52,670 --> 00:01:54,700
en sus componentes independientes.

36
00:01:54,700 --> 00:01:58,020
Tiene más de 11 millones
de parámetros entrenados.

37
00:01:58,570 --> 00:02:01,845
Hay capas completamente conectadas
y otras que no lo están,

38
00:02:01,845 --> 00:02:04,565
como las convolucionadas,
de las que hablaremos más tarde.

39
00:02:04,565 --> 00:02:07,165
Usó capas de retirados
para ayudar a generalizar más

40
00:02:07,165 --> 00:02:10,390
mediante la simulación de un ensamble
de redes neuronales profundas.

41
00:02:10,390 --> 00:02:13,380
Como en las redes neuronales
y la combinación de clasificadores

42
00:02:13,380 --> 00:02:16,555
cada caja es una unidad de componentes,
parte de un grupo de cajas

43
00:02:16,555 --> 00:02:18,335
como la que enfoqué.

44
00:02:18,335 --> 00:02:21,300
Esta idea de componentes
que se agregan para crear algo mayor

45
00:02:21,300 --> 00:02:24,550
que la suma de sus partes
es lo que ha posibilitado el gran éxito

46
00:02:24,550 --> 00:02:26,290
del aprendizaje profundo.

47
00:02:26,290 --> 00:02:29,185
Por supuesto, la abundancia de datos
en constante crecimiento

48
00:02:29,185 --> 00:02:32,440
el poder de procesamiento
y más memoria ayudan también.

49
00:02:32,440 --> 00:02:37,865
Hoy, hay varias versiones más recientes,
que son mucho más grandes y precisas.

50
00:02:38,275 --> 00:02:41,260
La lección de toda esta historia
es que la investigación del AA

51
00:02:41,260 --> 00:02:45,610
reutiliza elementos y técnicas
de otros algoritmos del pasado

52
00:02:45,610 --> 00:02:48,690
y los combina para crear
modelos muchos más poderosos

53
00:02:48,690 --> 00:02:50,850
y, sobre todo, para experimentar.

54
00:02:50,850 --> 00:02:54,540
¿Qué es importante cuando se crean
redes neuronales profundas?

55
00:02:56,770 --> 00:02:59,775
La respuesta correcta es
D. Todas las anteriores.

56
00:02:59,775 --> 00:03:03,005
Esta no es una lista completa,
pero estos tres elementos

57
00:03:03,005 --> 00:03:04,550
son muy importantes.

58
00:03:04,550 --> 00:03:07,790
Primero, hay que asegurarse
de tener: A. Muchos datos.

59
00:03:07,790 --> 00:03:11,140
Existe mucha investigación en curso
que intenta reducir las necesidades

60
00:03:11,140 --> 00:03:13,010
de datos para el aprendizaje profundo

61
00:03:13,010 --> 00:03:16,130
pero hasta que eso suceda,
debemos asegurarnos de tener muchos.

62
00:03:16,130 --> 00:03:19,360
Esto se debe a la alta capacidad
de la cantidad de parámetros

63
00:03:19,360 --> 00:03:22,080
que se deben entrenar
en estos modelos masivos.

64
00:03:22,510 --> 00:03:26,090
Ya que el modelo es tan complejo,
debe internalizar bien la distribución

65
00:03:26,090 --> 00:03:27,225
de los datos.

66
00:03:27,515 --> 00:03:29,710
Por lo tanto, necesita muchas señales.

67
00:03:29,710 --> 00:03:32,560
Recuerden, el objetivo central
del aprendizaje automático

68
00:03:32,560 --> 00:03:35,550
no es entrenar muchos modelos
elegantes solo porque sí.

69
00:03:35,550 --> 00:03:38,945
Es entrenarlos para que hagan
predicciones muy precisas.

70
00:03:39,305 --> 00:03:42,100
Si no pueden generalizar
nuevos datos para predecir,

71
00:03:42,100 --> 00:03:43,955
entonces, ¿de qué sirve ese modelo?

72
00:03:43,955 --> 00:03:47,670
Tener los suficientes datos es importante

73
00:03:47,670 --> 00:03:50,645
de modo que no se sobreajuste
a un conjunto de datos pequeño

74
00:03:50,645 --> 00:03:53,975
que ha visto un millón de veces,
en lugar de un conjunto gigante

75
00:03:53,975 --> 00:03:55,620
que ha visto mucho menos.

76
00:03:55,620 --> 00:03:58,335
Esto les permite tener conjuntos
de validación y pruebas

77
00:03:58,335 --> 00:04:00,790
lo suficientemente grandes
para ajustar sus modelos.

78
00:04:00,790 --> 00:04:02,795
Adicionalmente,
agregar capas de retirados,

79
00:04:02,795 --> 00:04:05,260
incrementar los datos,
agregar ruido, etcétera,

80
00:04:05,260 --> 00:04:08,970
es la forma de tener
aún mejor generalización.

81
00:04:08,970 --> 00:04:12,994
Por último, el aprendizaje automático
se trata sobre la experimentación.

82
00:04:12,994 --> 00:04:15,910
Hay tantos tipos diferentes
de algoritmos, hiperparámetros

83
00:04:15,910 --> 00:04:18,525
y formas de crear el conjunto
de datos de AA hoy en día.

84
00:04:18,525 --> 00:04:22,540
No hay manera de saber a priori
las opciones óptimas desde el principio

85
00:04:22,540 --> 00:04:24,185
para casi todos los problemas.

86
00:04:24,445 --> 00:04:28,420
Mediante la experimentación,
el registro de lo que ya intentaron

87
00:04:28,420 --> 00:04:30,860
y la medición del rendimiento
para comparar modelos

88
00:04:30,860 --> 00:04:34,035
no solo se divertirán,
sino que también crearán herramientas

89
00:04:34,035 --> 00:04:35,480
increíbles y poderosas.

90
00:04:36,190 --> 00:04:38,860
Ahora, hablaré sobre
cómo las redes neuronales

91
00:04:38,860 --> 00:04:41,760
continúan aprovechando
el rendimiento de los modelos pasados.

92
00:04:42,250 --> 00:04:45,090
Aquí, ven el rendimiento
de versiones de modelos específicas

93
00:04:45,090 --> 00:04:47,230
de redes neuronales
a lo largo de los años.

94
00:04:47,230 --> 00:04:50,950
Como pueden ver en el gráfico,
un salto significativo ocurrió el 2014,

95
00:04:50,950 --> 00:04:53,935
destacado en azul,
cuando el modelo Inception de Google

96
00:04:53,935 --> 00:04:56,910
superó el modelo de 10%
de tasa de error con un 6.7%.

97
00:04:57,690 --> 00:05:01,500
El rendimiento de las DNN
continúa mejorando cada año

98
00:05:01,500 --> 00:05:04,230
y con las lecciones aprendidas
de los modelos anteriores.

99
00:05:04,230 --> 00:05:07,590
En 2015, la tercera versión
del modelo Inception

100
00:05:07,590 --> 00:05:10,145
obtuvo una tasa de error del 3.5%.

101
00:05:10,145 --> 00:05:13,755
¿Qué permite estas grandes mejoras
en tan poco tiempo?

102
00:05:14,255 --> 00:05:16,440
A menudo,
cuando un grupo de investigadores

103
00:05:16,440 --> 00:05:19,260
desarrolla una nueva técnica
o método que funciona muy bien,

104
00:05:19,260 --> 00:05:22,490
otros grupos toman esas ideas
y desarrollan con base en ellas.

105
00:05:22,490 --> 00:05:26,575
Esto permite un gran salto
hacia adelante en la experimentación

106
00:05:26,575 --> 00:05:29,030
y el avance se acelera.

107
00:05:29,030 --> 00:05:33,405
Esto puede incluir hiperparámetros,
más capas, mejor generalizabilidad,

108
00:05:33,405 --> 00:05:36,640
mejores subcomponentes
como capas convolucionales, etcétera.

109
00:05:36,640 --> 00:05:40,475
Expliquen cómo aplicarían AA al problema.

110
00:05:40,475 --> 00:05:42,980
Puede haber más de una respuesta correcta.

111
00:05:44,340 --> 00:05:47,845
Son dueños de un centro de esquí
y quieren predecir los niveles de tráfico

112
00:05:47,845 --> 00:05:50,645
de las pistas de esquí
con base en cuatro tipos de clientes

113
00:05:50,645 --> 00:05:53,915
principiantes, intermedios,
avanzados y expertos

114
00:05:53,915 --> 00:05:58,020
que compraron boletos,
y la cantidad de nieve de años pasados.

115
00:05:59,930 --> 00:06:02,770
Tómense un momento
para escribir su respuesta.

116
00:06:04,160 --> 00:06:08,745
Podría ser regresión o clasificación,
ya que no especifiqué exactamente

117
00:06:08,745 --> 00:06:11,480
qué quiero decir con "niveles de tráfico".

118
00:06:11,480 --> 00:06:15,530
¿Me refiero a la cantidad de personas
que usan la pista por hora?

119
00:06:15,530 --> 00:06:19,610
¿O una descripción más categórica,
como alto, medio y bajo?

120
00:06:19,610 --> 00:06:22,425
Para esto,
comenzaría con una heurística base

121
00:06:22,425 --> 00:06:25,005
como la cantidad promedio
de personas en cada pendiente

122
00:06:25,005 --> 00:06:28,565
y, luego, modelos base
de regresión lineal o logística

123
00:06:28,565 --> 00:06:32,790
según lo que elija: regresión
o clasificación, respectivamente.

124
00:06:33,620 --> 00:06:35,715
Según el rendimiento
y la cantidad de datos,

125
00:06:35,715 --> 00:06:38,130
probablemente
avanzaría a las redes neuronales.

126
00:06:38,860 --> 00:06:41,845
Si hay otros atributos
en los datos, también los usaría

127
00:06:41,845 --> 00:06:44,220
y supervisaría el rendimiento.

128
00:06:45,590 --> 00:06:50,430
Según el último dato, en Google
hay más de 4,000 modelos de AA profundo

129
00:06:50,430 --> 00:06:53,225
internos que forman parte
de la tecnología de sus sistemas.

130
00:06:53,225 --> 00:06:56,400
Cada modelo y sus versiones
obtienen el beneficio del rendimiento

131
00:06:56,400 --> 00:06:59,910
basado en el éxito y los fracasos
de los modelos anteriores.

132
00:07:00,630 --> 00:07:03,965
Uno de los más usados
en el pasado fue Sibyl

133
00:07:03,965 --> 00:07:06,880
que se creó originalmente
para recomendar videos de YouTube.

134
00:07:06,880 --> 00:07:10,050
Este motor de recomendaciones
funcionó tan bien que se incorporó

135
00:07:10,050 --> 00:07:13,620
más tarde a los anuncios
y otros productos de Google.

136
00:07:13,620 --> 00:07:15,840
Era un modelo lineal.

137
00:07:16,350 --> 00:07:20,610
Este año, otro modelo
que se convirtió en motor de ajuste

138
00:07:20,610 --> 00:07:24,060
de parámetros predeterminado
para otros modelos y sistemas

139
00:07:24,060 --> 00:07:27,250
fue Google Brain, el brazo
de la investigación de AA de Google

140
00:07:27,250 --> 00:07:30,840
creado para aprovechar
el poder informático de miles de CPU

141
00:07:30,840 --> 00:07:34,280
para entrenar grandes modelos
como las redes neuronales profundas.

142
00:07:35,370 --> 00:07:39,030
La experiencia de crear y ejecutar
estos modelos es la que modeló la creación

143
00:07:39,030 --> 00:07:42,330
de TensorFlow, una biblioteca
de código abierto para el AA.

144
00:07:42,830 --> 00:07:47,270
Luego, Google creó TFX o la plataforma
de AA basada en TensorFlow.

145
00:07:47,270 --> 00:07:50,465
Les mostraremos cómo crear
y también implementar modelos de AA

146
00:07:50,465 --> 00:07:54,580
con TensorFlow y herramientas
como Cloud ML Engine, Dataflow y BigQuery.

147
00:07:55,380 --> 00:07:59,530
Para resumir, en las últimas décadas
se vio una proliferación en la adopción

148
00:07:59,530 --> 00:08:01,475
y el rendimiento de redes neuronales.

149
00:08:01,475 --> 00:08:05,050
Gracias a la universalidad de los datos,
estos modelos gozan del beneficio

150
00:08:05,050 --> 00:08:07,740
de cada vez más ejemplos
de entrenamiento para aprender.

151
00:08:07,740 --> 00:08:10,510
Los datos y ejemplos
en constante aumento se han combinado

152
00:08:10,510 --> 00:08:13,595
con la infraestructura escalable
para crear modelos más complejos

153
00:08:13,595 --> 00:08:15,760
y distribuidos con miles de capas.

154
00:08:16,400 --> 00:08:18,110
Los dejo con una observación.

155
00:08:18,110 --> 00:08:21,070
Aunque el rendimiento
con las redes neuronales puede ser mayor

156
00:08:21,070 --> 00:08:24,370
para algunas aplicaciones,
son apenas uno de los muchos tipos

157
00:08:24,370 --> 00:08:27,710
de modelos disponibles
para la experimentación, que es clave

158
00:08:27,710 --> 00:08:32,130
para obtener el mejor rendimiento
de los datos y así resolver sus desafíos.