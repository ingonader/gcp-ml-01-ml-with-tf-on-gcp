1
00:00:00,660 --> 00:00:02,960
À la fin des années 2000,

2
00:00:02,960 --> 00:00:06,470
la recherche en machine learning
disposait de la puissance de calcul

3
00:00:06,470 --> 00:00:11,040
pour combiner les performances
dans des méthodes d'ensemble.

4
00:00:11,630 --> 00:00:16,600
Si les erreurs sont indépendantes pour
plusieurs classifieurs faibles,

5
00:00:16,600 --> 00:00:19,710
elles forment un classifieur fort
une fois combinées.

6
00:00:20,010 --> 00:00:23,390
Le DNN effectue une approximation
à l'aide de couches d'abandon,

7
00:00:23,390 --> 00:00:26,200
qui régularisent le modèle
et empêchent le surapprentissage.

8
00:00:26,510 --> 00:00:30,030
Cela peut être simulé en désactivant
au hasard des neurones dans le réseau,

9
00:00:30,030 --> 00:00:32,695
avec une certaine probabilité
à chaque propagation avant,

10
00:00:32,695 --> 00:00:35,105
créant ainsi un nouveau
réseau à chaque fois.

11
00:00:36,175 --> 00:00:41,510
Souvent, l'avis de milliers de personnes
interrogées sur des questions complexes

12
00:00:41,510 --> 00:00:44,045
est plus juste que l'avis
d'une seule personne.

13
00:00:44,445 --> 00:00:46,960
Ce concept, c'est "la sagesse des foules".

14
00:00:47,200 --> 00:00:49,150
Idem pour le machine learning.

15
00:00:49,150 --> 00:00:53,560
Si les résultats proviennent de plusieurs
prédicteurs (classifieurs ou régresseurs),

16
00:00:53,560 --> 00:00:57,480
le groupe sera souvent plus performant
qu'un modèle individuel.

17
00:00:58,130 --> 00:01:01,450
Ce groupe de prédicteurs est un ensemble

18
00:01:01,450 --> 00:01:03,350
qui permet l'apprentissage d'ensemble.

19
00:01:03,650 --> 00:01:06,990
L'algorithme effectuant cet apprentissage
est une méthode d'ensemble.

20
00:01:07,320 --> 00:01:11,010
Les forêts aléatoires sont un type
d'apprentissage d'ensemble très utilisé.

21
00:01:11,750 --> 00:01:16,030
Au lieu d'un seul arbre de décision
avec votre ensemble d'apprentissage,

22
00:01:16,030 --> 00:01:18,290
vous pouvez créer
plusieurs arbres de décision,

23
00:01:18,290 --> 00:01:21,340
avec pour chaque un sous-échantillon
des données d'apprentissage.

24
00:01:21,710 --> 00:01:23,970
Les arbres n'ayant pas tout l'ensemble,

25
00:01:23,970 --> 00:01:26,150
ils n'ont pas mémorisé toutes les données.

26
00:01:26,740 --> 00:01:29,595
Une fois les arbres entraînés
et associés à un sous-ensemble,

27
00:01:29,595 --> 00:01:33,940
vous passez à l'étape la plus importante
du machine learning : les prédictions.

28
00:01:34,470 --> 00:01:37,710
Vous devez transférer vos données
test dans chaque arbre de la forêt,

29
00:01:37,710 --> 00:01:39,330
puis regrouper les résultats.

30
00:01:39,960 --> 00:01:41,450
Pour la classification,

31
00:01:41,450 --> 00:01:43,935
s'il existe un vote de majorité
dans tous les arbres,

32
00:01:43,935 --> 00:01:46,155
alors celui-ci sera
la classe de sortie finale.

33
00:01:46,315 --> 00:01:49,460
En régression, le résultat peut être
une compilation des valeurs,

34
00:01:49,460 --> 00:01:51,650
comme la moyenne,
le maximum, la médiane, etc.

35
00:01:51,960 --> 00:01:55,200
Vous pouvez améliorer la généralisation,
en échantillonnant au hasard

36
00:01:55,200 --> 00:01:57,360
les exemples et/ou les caractéristiques.

37
00:01:57,970 --> 00:02:01,320
L'échantillonnage aléatoire d'exemples
est un remplacement, ou bagging,

38
00:02:01,320 --> 00:02:02,905
abrégé de bootstrap aggregating,

39
00:02:02,905 --> 00:02:05,480
et un collage sans remplacement.

40
00:02:05,730 --> 00:02:09,360
Chaque prédicteur a un biais
plus élevé entraîné

41
00:02:09,360 --> 00:02:12,150
sur un sous-ensemble
plus petit de données.

42
00:02:12,200 --> 00:02:15,735
Toutefois, l'agrégation permet
de réduire le biais et la variance.

43
00:02:15,975 --> 00:02:18,780
L'ensemble dispose souvent
alors d'un biais similaire

44
00:02:18,780 --> 00:02:21,320
comme prédicteur
sur tout l'ensemble d'apprentissage,

45
00:02:21,320 --> 00:02:23,075
mais d'une variance inférieure.

46
00:02:23,285 --> 00:02:26,400
Une méthode de validation efficace
pour l'erreur de généralisation

47
00:02:26,400 --> 00:02:28,870
consiste à utiliser les données out-of-bag

48
00:02:28,870 --> 00:02:32,410
plutôt qu'un ensemble distinct tiré
des données avant l'apprentissage.

49
00:02:33,070 --> 00:02:36,690
Elle est semblable à la validation k-fold
avec une méthode holdout aléatoire.

50
00:02:37,390 --> 00:02:40,855
L'échantillonnage de caractéristiques
crée des sous-espaces aléatoires,

51
00:02:40,855 --> 00:02:44,640
et l'échantillonnage aléatoire d'exemples
crée des patchs aléatoires.

52
00:02:45,170 --> 00:02:50,085
AdaBoost, ou Adaptive boosting,
et Gradient boosting sont des algorithmes

53
00:02:50,085 --> 00:02:53,890
qui font de plusieurs classifieurs faibles
un classifieur fort.

54
00:02:54,200 --> 00:02:57,270
Les classifieurs sont entraînés
consécutivement,

55
00:02:57,270 --> 00:03:00,655
afin de corriger les éventuels problèmes
des classifieurs précédents.

56
00:03:01,015 --> 00:03:04,830
Avec les arbres boostés, comme davantage
d'arbres sont ajoutés à l'ensemble,

57
00:03:04,830 --> 00:03:06,735
les prédictions sont souvent améliorées.

58
00:03:06,935 --> 00:03:11,135
Doit-on ajouter des arbres à l'infini ?
Bien sûr que non.

59
00:03:11,515 --> 00:03:14,440
Utilisez votre ensemble
de validation pour un arrêt prématuré,

60
00:03:14,440 --> 00:03:17,010
pour ne pas surentraîner
les données d'apprentissage

61
00:03:17,010 --> 00:03:18,710
en ajoutant trop d'arbres.

62
00:03:19,500 --> 00:03:21,580
Enfin, comme avec les réseaux de neurones,

63
00:03:21,580 --> 00:03:22,865
l'empilement est possible.

64
00:03:22,865 --> 00:03:26,130
Des méta-classifieurs apprennent
quoi faire avec les images

65
00:03:26,130 --> 00:03:30,475
qui sont ensuite empilées
en méta-classifieurs, et ainsi de suite.

66
00:03:30,845 --> 00:03:35,305
Nous aborderons bientôt ce concept
dans les réseaux de neurones profonds.

67
00:03:35,675 --> 00:03:39,010
Parmi les propositions suivantes,
laquelle est souvent fausse

68
00:03:39,010 --> 00:03:42,560
pour les forêts aléatoires par rapport
aux arbres de décision individuels ?

69
00:03:45,070 --> 00:03:47,860
La bonne réponse est
qu'il est souvent faux de dire

70
00:03:47,860 --> 00:03:51,215
que l'interprétation visuelle
des forêts aléatoires est plus facile.

71
00:03:51,565 --> 00:03:53,210
Comme les réseaux de neurones,

72
00:03:53,210 --> 00:03:55,510
plus on ajoute de couches au modèle,

73
00:03:55,510 --> 00:03:58,230
plus il sera difficile
à comprendre et à expliquer.

74
00:03:58,630 --> 00:04:02,150
Une forêt aléatoire est souvent
plus complexe qu'un arbre de décision

75
00:04:02,150 --> 00:04:04,480
et donc plus difficile
à interpréter visuellement.

76
00:04:04,560 --> 00:04:06,770
Les trois autres propositions sont vraies.

77
00:04:06,880 --> 00:04:11,100
Une forêt a une meilleure généralisation
via le bagging et les sous-espaces.

78
00:04:11,400 --> 00:04:16,315
Grâce aux votes pour la classification
ou à l'agrégation pour la régression,

79
00:04:16,315 --> 00:04:19,515
la forêt peut être souvent
bien plus efficace qu'un arbre.

80
00:04:20,025 --> 00:04:23,265
Enfin, grâce à l'échantillonnage
aléatoire des forêts,

81
00:04:23,265 --> 00:04:26,180
le biais reste similaire
à celui d'un arbre individuel,

82
00:04:26,180 --> 00:04:29,370
mais la variance est inférieure,

83
00:04:29,370 --> 00:04:31,850
ce qui améliore souvent la généralisation.