1
00:00:00,000 --> 00:00:02,500
So, Linear Regression was pretty much it,

2
00:00:02,500 --> 00:00:04,485
as far as learning from data was concerned.

3
00:00:04,485 --> 00:00:07,585
Until the 1940's, a researcher, Frank Rosenblatt,

4
00:00:07,585 --> 00:00:10,780
comes up with a perceptron as a computational model of a neuron

5
00:00:10,780 --> 00:00:15,415
in the human brain and shows how it can learn simple functions.

6
00:00:15,415 --> 00:00:17,880
It is what we would call today,

7
00:00:17,880 --> 00:00:20,865
a Binary Linear Classifier where we are trying to find

8
00:00:20,865 --> 00:00:24,475
a single line that splits the data into two classes.

9
00:00:24,475 --> 00:00:30,140
A single layer of perceptrons would be the simplest possible feed forward neural network.

10
00:00:30,140 --> 00:00:35,660
Inputs which feed into a single layer perceptrons and a weighted sum will be performed.

11
00:00:35,660 --> 00:00:40,345
This sum would then pass through what we call today inactivation function,

12
00:00:40,345 --> 00:00:42,925
which is just a mathematical function you apply to

13
00:00:42,925 --> 00:00:45,830
each element that is now residing within that neuron.

14
00:00:45,830 --> 00:00:48,155
Remember though, at this point,

15
00:00:48,155 --> 00:00:50,820
this is still just a linear classifier.

16
00:00:50,820 --> 00:00:54,060
So the activation function which is linear in this case,

17
00:00:54,060 --> 00:00:56,250
just returns its inputs.

18
00:00:56,250 --> 00:00:59,340
Comparing the output of this to a threshold,

19
00:00:59,340 --> 00:01:02,650
would then determine which class each point belongs to.

20
00:01:02,650 --> 00:01:07,355
The errors would be aggregated and used to change the weights used in the sum,

21
00:01:07,355 --> 00:01:10,980
and the process would happen again and again until convergence.

22
00:01:10,980 --> 00:01:13,730
If you are trying to come up with a simple model of something that

23
00:01:13,730 --> 00:01:17,305
learns a desired output from a given input distribution,

24
00:01:17,305 --> 00:01:21,215
then you needn't look far since our brains do this all day long

25
00:01:21,215 --> 00:01:25,735
making sense out of the world around us and all the signals that our bodies receive.

26
00:01:25,735 --> 00:01:28,695
One of the fundamental units of the brain is the neuron.

27
00:01:28,695 --> 00:01:31,020
Neural networks are just groups of neurons

28
00:01:31,020 --> 00:01:33,995
connected together in different patterns or architectures.

29
00:01:33,995 --> 00:01:37,950
A biological neuron has several components specialized in passing

30
00:01:37,950 --> 00:01:42,070
along electrical signal which allows you and I to have thoughts,

31
00:01:42,070 --> 00:01:45,970
perform actions, and study the fascinating world of machine learning.

32
00:01:45,970 --> 00:01:48,560
Electrical signals from other neurons such as,

33
00:01:48,560 --> 00:01:50,500
sensory neurons in the retina of your eye,

34
00:01:50,500 --> 00:01:52,830
are propagated from neuron to neuron.

35
00:01:52,830 --> 00:01:55,850
The input signal is received at one end of the neuron,

36
00:01:55,850 --> 00:01:57,485
which is made up of dendrites.

37
00:01:57,485 --> 00:02:02,235
These dendrites might not just collect electrical signal from just one other neuron,

38
00:02:02,235 --> 00:02:03,865
but possibly from several,

39
00:02:03,865 --> 00:02:06,330
which all get summed together over windows in

40
00:02:06,330 --> 00:02:09,485
time that changes the electrical potential of the cell.

41
00:02:09,485 --> 00:02:14,445
A typical neuron has a resting electric potential of about negative 70 millivolts.

42
00:02:14,445 --> 00:02:17,930
As the input stimuli received at the dendrites increases,

43
00:02:17,930 --> 00:02:21,375
eventually it reaches a threshold around a negative 55 millivolts.

44
00:02:21,375 --> 00:02:24,595
In which case, a rapid depolarization of the axon occurs,

45
00:02:24,595 --> 00:02:28,785
with a bunch of voltage gates opening and allowing a sudden flow of ions.

46
00:02:28,785 --> 00:02:32,670
This causes the neuron to fire an action potential of electric current along

47
00:02:32,670 --> 00:02:37,815
the axon aided by the myelin sheath for better transmission to the axon terminals.

48
00:02:37,815 --> 00:02:40,820
Here, neurotransmitters are released at

49
00:02:40,820 --> 00:02:44,665
synapses that then travel across the synaptic cleft,

50
00:02:44,665 --> 00:02:47,090
to usually the dendrites of other neurons.

51
00:02:47,090 --> 00:02:49,650
Some of the neurotransmitters are excitatory,

52
00:02:49,650 --> 00:02:51,730
where they raise the potential the next cell,

53
00:02:51,730 --> 00:02:55,200
while some are inhibitory and lower the potential.

54
00:02:55,200 --> 00:03:00,815
The neuron repolarizes to an even lower potential than resting for a refractory period.

55
00:03:00,815 --> 00:03:03,660
And then the process continues in the next neuron, until maybe,

56
00:03:03,660 --> 00:03:08,515
it eventually reaches a motor neuron and moves your hand to shield the sun on your eyes.

57
00:03:08,515 --> 00:03:13,765
So, what does all this biology and neuroscience have to do with machine learning?

58
00:03:13,765 --> 00:03:15,865
Look familiar?

59
00:03:15,865 --> 00:03:18,710
This is a single layer perceptron.

60
00:03:18,710 --> 00:03:20,860
It too just like the neuron,

61
00:03:20,860 --> 00:03:25,745
has inputs which it then multiplies by weights and sums all together.

62
00:03:25,745 --> 00:03:27,870
The value here is now compared with

63
00:03:27,870 --> 00:03:31,765
a threshold and then transformed by an activation function.

64
00:03:31,765 --> 00:03:35,480
For instance, if the sum is greater than or equal to zero,

65
00:03:35,480 --> 00:03:39,205
then activate or press a value of one,

66
00:03:39,205 --> 00:03:42,960
otherwise, don't activate or press a value of zero.

67
00:03:42,960 --> 00:03:47,120
The inputs and weights act like the neurotransmitters in a neuron,

68
00:03:47,120 --> 00:03:50,165
where some can be positive and add to the sum,

69
00:03:50,165 --> 00:03:53,010
and some can be negative and subtract from the sum.

70
00:03:53,010 --> 00:03:57,015
The unit's step function acts as an all or none threshold.

71
00:03:57,015 --> 00:03:59,850
If the threshold is met, then pass the signal,

72
00:03:59,850 --> 00:04:02,080
otherwise, don't pass anything.

73
00:04:02,080 --> 00:04:05,850
Finally, there is an output and like biological neurons,

74
00:04:05,850 --> 00:04:10,910
this can actually pass as input to other neurons in a multi-layered perceptron,

75
00:04:10,910 --> 00:04:12,760
which we'll talk about next.

76
00:04:12,760 --> 00:04:15,970
This is all very cool however,

77
00:04:15,970 --> 00:04:19,805
it turns out that there are very simple functions that it can't learn.

78
00:04:19,805 --> 00:04:22,175
For example, the XOR function.

79
00:04:22,175 --> 00:04:25,470
Marvin Minsky, a famous computer scientist at MIT,

80
00:04:25,470 --> 00:04:29,980
pointed this out and then no one wanted to find AI for about 15 years.

81
00:04:29,980 --> 00:04:32,180
This was not the first time neural networks hit

82
00:04:32,180 --> 00:04:35,725
a brick wall and were essentially forgotten for a while.

83
00:04:35,725 --> 00:04:43,785
What component of a biological neuron is analogous to the input portion of a perceptron?

84
00:04:43,785 --> 00:04:48,065
The correct answer is the dendrites.

85
00:04:48,065 --> 00:04:50,595
They receive stimulus from other neurons,

86
00:04:50,595 --> 00:04:52,790
just like an artificial neural network does.

87
00:04:52,790 --> 00:04:57,725
The axon is incorrect since that is more analogous to the output of a perceptron.

88
00:04:57,725 --> 00:05:01,735
The nucleus is incorrect since that is where the cells genetic material is stored,

89
00:05:01,735 --> 00:05:03,755
and it controls the cells activities.

90
00:05:03,755 --> 00:05:07,695
The myelin sheath is incorrect since that helps transmission of the axon,

91
00:05:07,695 --> 00:05:11,000
which is once again, on the output portion of the perceptron.