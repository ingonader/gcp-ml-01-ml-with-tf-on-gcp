1
00:00:00,660 --> 00:00:02,960
Nas últimas décadas, já nos anos 2000,

2
00:00:02,960 --> 00:00:06,470
a pesquisa do ML adquiriu a capacidade
computacional para unir

3
00:00:06,470 --> 00:00:11,280
o desempenho em vários modelos do
que chamamos de método de combinação.

4
00:00:11,280 --> 00:00:16,600
Se os erros são independentes para uma
quantidade de aprendizes fracos simples,

5
00:00:16,600 --> 00:00:19,860
combinados, eles formam um aprendiz
forte.

6
00:00:19,860 --> 00:00:23,390
A DNN usa camadas de dropout para
aproximar esse fato,

7
00:00:23,390 --> 00:00:26,170
regularizando o modelo e evitando
o sobreajuste.

8
00:00:26,170 --> 00:00:29,250
Isso é simulado ao desligar neurônios
aleatoriamente

9
00:00:29,250 --> 00:00:32,185
na rede com alguma probabilidade em cada
transmissão direta,

10
00:00:32,185 --> 00:00:35,755
que basicamente criará uma nova rede
por vez.

11
00:00:35,755 --> 00:00:39,970
Com frequência, você responde melhor a
perguntas complexas quando são agregadas

12
00:00:39,970 --> 00:00:44,195
a partir de milhares de respostas, em vez
daquelas de apenas uma pessoa.

13
00:00:44,195 --> 00:00:47,090
Isso é conhecido como a sabedoria das
multidões.

14
00:00:47,090 --> 00:00:49,150
Ela é aplicada ao aprendizado de máquina.

15
00:00:49,150 --> 00:00:53,560
Agregando resultados de
preditores, classificadores, regressores,

16
00:00:53,560 --> 00:00:57,850
o grupo tem um desempenho melhor do que
o melhor modelo individual.

17
00:00:57,850 --> 00:01:01,720
Este grupo de preditores é uma combinação,
que quando feita deste jeito,

18
00:01:01,720 --> 00:01:03,370
gera aprendizado de combinação.

19
00:01:03,370 --> 00:01:07,150
O algoritmo que executa esse aprendizado
é um método de combinação.

20
00:01:07,150 --> 00:01:11,430
Um dos tipos mais famosos de aprendizado
de combinação é a floresta aleatória.

21
00:01:11,430 --> 00:01:16,130
Em vez de usar o conjunto de treinamento
para criar uma árvore de decisão,

22
00:01:16,130 --> 00:01:18,360
é possível ter um grupo delas em que

23
00:01:18,360 --> 00:01:21,400
cada recebe uma subamostra
aleatória dos dados de treinamento.

24
00:01:21,400 --> 00:01:23,970
Como não passaram por todo o conjunto de
treinamento,

25
00:01:23,970 --> 00:01:26,450
eles não terão memorizado tudo.

26
00:01:26,450 --> 00:01:29,565
Ao treinar árvores e transformá-las em
subconjuntos de dados,

27
00:01:29,565 --> 00:01:32,817
é possível executar a parte mais
importante do ML:

28
00:01:32,817 --> 00:01:34,350
previsões!

29
00:01:34,350 --> 00:01:37,820
Para isso, você passa a amostra de teste
por cada árvore na floresta

30
00:01:37,820 --> 00:01:39,720
e agrega os resultados.

31
00:01:39,720 --> 00:01:41,500
Se for classificação,

32
00:01:41,500 --> 00:01:43,175
poderá haver o voto de maioria

33
00:01:43,175 --> 00:01:46,215
em todas as árvores, sendo a classe de
saída final.

34
00:01:46,215 --> 00:01:49,900
Se for regressão, poderá ser a
agregação dos valores como média,

35
00:01:49,900 --> 00:01:51,990
máximo, mediana etc.

36
00:01:51,990 --> 00:01:58,140
A amostragem aleatória de exemplos e/ou
características melhora a generalização.

37
00:01:58,140 --> 00:02:01,260
Esses exemplos com substituição são
chamados de bagging,

38
00:02:01,260 --> 00:02:02,875
ou agregação via bootstrap,

39
00:02:02,875 --> 00:02:05,730
e pasting, quando não há substituição.

40
00:02:05,730 --> 00:02:08,990
Cada preditor individual tem alta
tendência

41
00:02:08,990 --> 00:02:12,150
e é treinado no menor subconjunto,
não no conjunto de dados total.

42
00:02:12,150 --> 00:02:15,975
Mas a agregação reduz a tendência e 
a variação.

43
00:02:15,975 --> 00:02:17,920
Isso proporciona à combinação

44
00:02:17,920 --> 00:02:21,320
a mesma tendência de um único preditor no
conjunto de treinamento,

45
00:02:21,320 --> 00:02:23,335
mas com variação menor.

46
00:02:23,335 --> 00:02:26,280
Um ótimo método de validação para o erro
de generalização

47
00:02:26,280 --> 00:02:29,180
é usar os dados out-of-bag,

48
00:02:29,180 --> 00:02:33,400
não um conjunto separado
extraído do conjunto antes do treinamento.

49
00:02:33,400 --> 00:02:37,100
É reminiscente da validação cruzada usando
holdouts aleatórios.

50
00:02:37,100 --> 00:02:40,645
São criados subespaços aleatórios ao fazer
a amostragem das características

51
00:02:40,645 --> 00:02:44,890
e, ao fazer a amostragem de exemplos
aleatórios, chamamos de patch aleatório.

52
00:02:44,890 --> 00:02:50,085
O aprimoramento adaptativo ou AdaBoost no
tipo gradiente são exemplos de boosting,

53
00:02:50,085 --> 00:02:54,100
que é quando agregamos uma quantidade de
aprendizes fracos para criar um forte.

54
00:02:54,100 --> 00:02:56,800
Geralmente, isso é feito ao treinar cada
aprendiz

55
00:02:56,800 --> 00:03:00,835
sequencialmente para corrigir qualquer
problema que ele já teve antes.

56
00:03:00,835 --> 00:03:04,870
Nas árvores aprimoradas, quanto mais
árvores adicionamos à combinação,

57
00:03:04,870 --> 00:03:06,795
maior é o aprimoramento das previsões.

58
00:03:06,795 --> 00:03:09,760
Então continuamos a adicionar árvores sem 
parar?

59
00:03:09,760 --> 00:03:11,375
Claro que não.

60
00:03:11,375 --> 00:03:14,440
Use o conjunto de validação para utilizar
interrupção antecipada.

61
00:03:14,440 --> 00:03:16,890
Isso evita o sobreajuste dos dados de
treinamento

62
00:03:16,890 --> 00:03:19,180
por conta de muitas árvores adicionadas.

63
00:03:19,180 --> 00:03:21,650
Por fim, como vimos nas redes neurais,

64
00:03:21,650 --> 00:03:22,865
é possível empilhar,

65
00:03:22,865 --> 00:03:26,130
treinando meta-aprendizes no que
fazer com imagens de combinação,

66
00:03:26,130 --> 00:03:30,935
que por sua vez podem ser empilhadas em
meta-aprendizes e assim por diante.

67
00:03:30,935 --> 00:03:35,675
Veremos em breve o empilhamento e reuso de
subcomponentes em redes neurais profundas.

68
00:03:35,675 --> 00:03:39,880
Qual das opções a seguir é falsa com
relação a florestas aleatórias,

69
00:03:39,880 --> 00:03:42,550
comparando com
árvores de decisão individual?

70
00:03:45,130 --> 00:03:48,260
A resposta correta para a questão é que

71
00:03:48,260 --> 00:03:51,435
florestas aleatórias são "Mais fáceis
de interpretar visualmente".

72
00:03:51,435 --> 00:03:53,040
Igualmente às redes neurais,

73
00:03:53,040 --> 00:03:55,510
quanto mais complexidade
você adiciona ao modelo,

74
00:03:55,510 --> 00:03:57,980
mais difícil é o entendimento e 
a explicação dele.

75
00:03:57,980 --> 00:04:02,270
A floresta aleatória é mais complexa do
que uma árvore de decisão individual,

76
00:04:02,270 --> 00:04:04,430
o que dificulta a interpretação visual.

77
00:04:04,430 --> 00:04:06,770
As outras três opções são verdadeiras.

78
00:04:06,770 --> 00:04:11,400
As florestas aleatórias têm melhor 
generalização via bagging e subespaços.

79
00:04:11,400 --> 00:04:16,315
E por usar um sistema de votação em
classificação ou agregação de regressão,

80
00:04:16,315 --> 00:04:19,765
a floresta tem desempenho muito melhor do
que a árvore individual.

81
00:04:19,765 --> 00:04:23,265
Por fim, devido à amostragem aleatória de
árvores desse tipo,

82
00:04:23,265 --> 00:04:26,180
a tendência é similar à da árvore
individual,

83
00:04:26,180 --> 00:04:29,370
mas também com menor variação que,
novamente,

84
00:04:29,370 --> 00:04:31,730
costuma levar à melhor generalização.