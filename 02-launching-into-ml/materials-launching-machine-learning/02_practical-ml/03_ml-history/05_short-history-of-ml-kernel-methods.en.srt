1
00:00:00,000 --> 00:00:01,740
Started in the 1990s,

2
00:00:01,740 --> 00:00:03,795
the field of kernel methods was formed.

3
00:00:03,795 --> 00:00:06,490
Corinna Cortes, Director of Google Research,

4
00:00:06,490 --> 00:00:08,185
was one of the pioneers.

5
00:00:08,185 --> 00:00:13,250
This field of study allows interesting classes of new nonlinear models,

6
00:00:13,250 --> 00:00:17,625
most prominently nonlinear SVMs or support vector machines,

7
00:00:17,625 --> 00:00:21,210
which are maximum margin classifiers that you may have heard of before.

8
00:00:21,210 --> 00:00:23,370
Fundamentally, core to an SVM is

9
00:00:23,370 --> 00:00:27,560
a nonlinear activation plus a sigmoid output for maximum margins.

10
00:00:27,560 --> 00:00:30,730
Earlier, we have seen how logistic regression is used to create

11
00:00:30,730 --> 00:00:35,965
a decision boundary to maximize the log likelihood of declassification probabilities.

12
00:00:35,965 --> 00:00:38,395
In the case of a linear decision boundary,

13
00:00:38,395 --> 00:00:40,920
logistic regression wants to have each point and

14
00:00:40,920 --> 00:00:43,340
the associated class as far from the hydroplane

15
00:00:43,340 --> 00:00:48,910
as possible and provides a probability which can be interpreted as prediction confidence.

16
00:00:48,910 --> 00:00:52,280
There are an infinite number of hyperplanes you can create between

17
00:00:52,280 --> 00:00:54,660
two linearly separable classes such as

18
00:00:54,660 --> 00:00:58,095
the two hyperplanes shown as the dotted lines in the two figures here.

19
00:00:58,095 --> 00:01:02,490
In SVMs, we include two parallel hyperplanes on either side of

20
00:01:02,490 --> 00:01:04,980
the decision boundary hyperplane where they

21
00:01:04,980 --> 00:01:08,040
intersect with the closest data point on each side of the hyperplane.

22
00:01:08,040 --> 00:01:10,480
These are the support vectors.

23
00:01:10,480 --> 00:01:14,070
The distance between the two separate vectors is the margin.

24
00:01:14,070 --> 00:01:18,530
On the left, we have a vertical hyperplane that indeed separates the two classes.

25
00:01:18,530 --> 00:01:22,125
However, the margin between the two support vectors is small.

26
00:01:22,125 --> 00:01:24,240
By choosing a different hyperplane,

27
00:01:24,240 --> 00:01:25,460
such as the one on the right,

28
00:01:25,460 --> 00:01:28,105
there is a much larger margin.

29
00:01:28,105 --> 00:01:32,360
The wider the margin, the more generalizable the decision boundary is,

30
00:01:32,360 --> 00:01:34,975
which should lead to better performance on you data.

31
00:01:34,975 --> 00:01:39,410
Therefore, SVM classifiers aim to maximize the margin between

32
00:01:39,410 --> 00:01:41,700
the two support vectors using a hinge

33
00:01:41,700 --> 00:01:46,055
last function compared to logistic regression minimization of cross-entropy.

34
00:01:46,055 --> 00:01:48,850
You might notice that I have only two classes

35
00:01:48,850 --> 00:01:51,350
which makes this a binary classification problem.

36
00:01:51,350 --> 00:01:53,620
One in classes label is given the value of

37
00:01:53,620 --> 00:01:57,580
one and the other classes label is given a value of negative one.

38
00:01:57,580 --> 00:01:59,850
If there are more than two classes,

39
00:01:59,850 --> 00:02:02,770
then a one vs all approach should be taken and then

40
00:02:02,770 --> 00:02:06,730
choose the best out of the promoted binary classifications.

41
00:02:06,730 --> 00:02:12,100
But, what happens if the data is not linearly separable into the two classes?

42
00:02:12,100 --> 00:02:16,480
The good news is that we can apply a kernel transformation which maps the data from

43
00:02:16,480 --> 00:02:19,100
our input vector space to a vector space that now

44
00:02:19,100 --> 00:02:22,540
has features that can be linearly separated as shown in the diagram.

45
00:02:22,540 --> 00:02:25,690
Just like before, the rise of deep neural networks,

46
00:02:25,690 --> 00:02:29,750
lots of time and work went into transforming the raw representation of

47
00:02:29,750 --> 00:02:34,380
data into a feature vector through a highly tuned user created feature map.

48
00:02:34,380 --> 00:02:36,520
However, with kernel methods,

49
00:02:36,520 --> 00:02:39,335
the only user-defined item is the kernel,

50
00:02:39,335 --> 00:02:44,285
just similarity function between pairs of points in the raw representation of the data.

51
00:02:44,285 --> 00:02:46,840
A kernel transformation is similar to

52
00:02:46,840 --> 00:02:49,300
how an activation function in neural networks maps

53
00:02:49,300 --> 00:02:52,200
the input to the function to transform space.

54
00:02:52,200 --> 00:02:55,350
The number of neurons in the layer controls the dimension.

55
00:02:55,350 --> 00:02:58,055
So, if you have two inputs and you have three neurons,

56
00:02:58,055 --> 00:03:01,755
you are mapping the input 2D space to a 3D space.

57
00:03:01,755 --> 00:03:06,040
There are many types of kernels with the most basic being the basic linear kernel,

58
00:03:06,040 --> 00:03:10,710
the polynomial kernel, and the Gaussian radial basis function kernel.

59
00:03:10,710 --> 00:03:13,350
When our binary classifier uses the kernel,

60
00:03:13,350 --> 00:03:16,175
it typically computes awaited some similarities.

61
00:03:16,175 --> 00:03:19,635
So, when should an SVM be used in several of discretion?

62
00:03:19,635 --> 00:03:24,870
Kernelized SVMs tend to provide sparser solutions and thus have better scalability.

63
00:03:24,870 --> 00:03:27,830
SVMs perform better when there is a high number of

64
00:03:27,830 --> 00:03:31,545
dimensions and when the predictors nearly certainly predict the response.

65
00:03:31,545 --> 00:03:37,365
We've seen how SVMs use kernels to map the inputs to a higher dimensional feature space.

66
00:03:37,365 --> 00:03:43,070
What thing in neural networks also can map to a higher dimensional vector space?

67
00:03:43,070 --> 00:03:45,380
The correct answer is,

68
00:03:45,380 --> 00:03:47,195
more neurons per layer.

69
00:03:47,195 --> 00:03:49,180
It is the number of neurons per layer that

70
00:03:49,180 --> 00:03:51,610
determine how many dimensions of vector space you are in.

71
00:03:51,610 --> 00:03:53,540
If I begin with three input features,

72
00:03:53,540 --> 00:03:55,975
I am in the R3 vector space.

73
00:03:55,975 --> 00:03:57,830
Even if I have a hundred layers,

74
00:03:57,830 --> 00:03:59,250
but with only three neurons each,

75
00:03:59,250 --> 00:04:04,300
I will still be in R3 vector space and I'm only changing the basis.

76
00:04:04,300 --> 00:04:08,495
For instance, when using a Gaussian RB of kernel with SVMs,

77
00:04:08,495 --> 00:04:11,425
the input space is mapped to infinite dimensions.

78
00:04:11,425 --> 00:04:13,660
The activation function changes the basis of

79
00:04:13,660 --> 00:04:16,820
the vector space but doesn't add or subtract dimensions.

80
00:04:16,820 --> 00:04:20,345
Think of them as simply a rotations and stretches and squeezes.

81
00:04:20,345 --> 00:04:21,750
They may be nonlinear,

82
00:04:21,750 --> 00:04:24,600
but you remain in the same vector space as before.

83
00:04:24,600 --> 00:04:28,550
The last function is your objective you are trying to minimize,

84
00:04:28,550 --> 00:04:32,910
is a scalar that uses its gradient to update the parameter weights of the model.

85
00:04:32,910 --> 00:04:37,265
This only changes how much you rotate and stretch and squeeze,

86
00:04:37,265 --> 00:04:38,570
not the number of dimensions.