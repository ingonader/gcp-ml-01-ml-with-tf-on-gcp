1
00:00:00,000 --> 00:00:03,590
Gehen wir auf eine kurze Reise durch die
Geschichte des maschinellen Lernens

2
00:00:03,590 --> 00:00:05,560
um zu sehen, wie sich daraus mit der Zeit

3
00:00:05,560 --> 00:00:09,035
neuronale Deep-Learning-Netze entwickelt
haben, die heute so beliebt sind.

4
00:00:09,035 --> 00:00:12,145
Obwohl neuronale Netze während der

5
00:00:12,145 --> 00:00:15,895
letzten Jahrzehnte 
mal "in" und mal "out" waren,

6
00:00:15,895 --> 00:00:19,535
lassen sich die für andere Algorithmen
entwickelten Tricks und Techniken auf

7
00:00:19,535 --> 00:00:21,935
neuronale Deep-Learning-Netze anwenden,

8
00:00:21,935 --> 00:00:23,810
wodurch sie sehr leistungsstark werden.

9
00:00:23,810 --> 00:00:28,030
Lineare Regression wurde zur
Vorhersage der Bewegung von Planeten

10
00:00:28,030 --> 00:00:31,280
und der Größe von Erbsenhülsen
basierend auf ihrem Aussehen entwickelt.

11
00:00:31,280 --> 00:00:34,470
Sir Francis Galton
war ein Pionier des Einsatzes von

12
00:00:34,470 --> 00:00:38,135
statistischen Methoden zum
Messen natürlicher Phänomene.

13
00:00:38,135 --> 00:00:42,595
Er betrachtete Daten zur
relativen Größe von Eltern und Kindern

14
00:00:42,595 --> 00:00:45,625
verschiedener Spezies,
einschließlich bestimmter Erbsen.

15
00:00:45,625 --> 00:00:50,155
Er beobachtete etwas, das nicht sofort ins
Auge fällt, etwas wirklich Merkwürdiges.

16
00:00:50,155 --> 00:00:55,985
Natürlich haben überdurchschnittlich große
Eltern eher übernormal große Kinder,

17
00:00:55,985 --> 00:01:00,950
aber wie viel größer ist das Kind als der
Durchschnitt der Kinder dieser Generation?

18
00:01:00,950 --> 00:01:05,295
Es zeigte sich, dass dieses Verhältnis für
die Nachkommen geringer ist

19
00:01:05,295 --> 00:01:07,565
als das entsprechende
Verhältnis für die Eltern.

20
00:01:07,565 --> 00:01:12,715
Wenn die Elterngröße
1,5 Standardabweichung vom Mittelwert ist,

21
00:01:12,715 --> 00:01:14,405
innerhalb der eigenen Generation,

22
00:01:14,405 --> 00:01:17,315
dann sagte er eine
Nachkommengröße von weniger

23
00:01:17,315 --> 00:01:21,140
als 1,5 Standardabweichungen vom
Mittelwert in deren Generation voraus.

24
00:01:21,140 --> 00:01:23,915
Wir sagen,
dass von Generation zu Generation

25
00:01:23,915 --> 00:01:25,935
die Dinge in der Natur rückläufig werden

26
00:01:25,935 --> 00:01:28,220
oder zum Mittelwert zurückkehren.

27
00:01:28,220 --> 00:01:31,250
Daher der Name lineare Regression.

28
00:01:31,250 --> 00:01:34,380
Diese Tabelle hier von 1877

29
00:01:34,380 --> 00:01:38,485
ist die erste lineare Regression
überhaupt. Ganz schön cool.

30
00:01:38,485 --> 00:01:42,650
Die Rechenleistung im 19. Jahrhundert
war doch recht begrenzt,

31
00:01:42,650 --> 00:01:44,910
sodass ihnen damals
nicht bewusst war, wie gut

32
00:01:44,910 --> 00:01:47,915
das auch noch mit großen
Datasets funktionieren würde.

33
00:01:47,915 --> 00:01:52,135
Es gab tatsächlich eine geschlossene Form
der Lösung für die lineare Regression,

34
00:01:52,135 --> 00:01:54,965
aber auch das Gradientenverfahren
kann verwendet werden,

35
00:01:54,965 --> 00:01:56,595
jedes mit Vor- und Nachteilen,

36
00:01:56,595 --> 00:01:58,095
je nach Ihrem Dataset.

37
00:01:58,095 --> 00:02:01,890
Nehmen wir die Funktionsweise
der linearen Regression unter die Lupe.

38
00:02:01,890 --> 00:02:03,790
Wir gehen etwas genauer ins Detail,

39
00:02:03,790 --> 00:02:06,985
um die Beweggründe in Bezug auf die
lineare Regression zu verstehen.

40
00:02:06,985 --> 00:02:10,895
Beginnen wir mit einer linearen Gleichung,
die vermutlich unser System beschreibt,

41
00:02:10,895 --> 00:02:15,700
indem wir Gewichtungen mit beobachteten
Merkmalvektoren multiplizieren

42
00:02:15,700 --> 00:02:17,140
und dann alles zusammenzählen.

43
00:02:17,140 --> 00:02:21,985
Wir können das in dieser Gleichung für
jedes Beispiel unseres Datasets zeigen,

44
00:02:21,985 --> 00:02:28,110
y=w0 × x0 + w1 × x1 + w2 × x2

45
00:02:28,110 --> 00:02:30,805
und so weiter für jedes
Merkmal in unserem Modell.

46
00:02:30,805 --> 00:02:35,355
Sprich: Wir wenden die Gleichung auf
jede Zeile in unserem Dataset an,

47
00:02:35,355 --> 00:02:37,440
wo die Gewichtungswerte
festgesetzt sind

48
00:02:37,440 --> 00:02:40,680
und die Merkmalwerte aus
jeder zugehörigen Spalte

49
00:02:40,680 --> 00:02:42,480
in unserem ML-Dataset stammen.

50
00:02:42,480 --> 00:02:45,905
Dies lässt sich gut in die
folgende Matrixgleichung packen:

51
00:02:45,905 --> 00:02:52,995
y = x × w. Diese
Hypothesengleichung ist sehr wichtig,

52
00:02:52,995 --> 00:02:54,620
nicht nur für lineare Regression,

53
00:02:54,620 --> 00:02:56,255
sondern auch für andere ML-Modelle,

54
00:02:56,255 --> 00:02:59,665
wie neuronale Deep-Learning-Netze,
auf die wir später noch eingehen.

55
00:02:59,665 --> 00:03:05,715
Wie erkennt man, ob die Gewichtungen
gute oder schlechte Schätzungen liefern?

56
00:03:05,715 --> 00:03:09,515
Die Antwort lautet: Wir müssen
eine Verlustfunktion erstellen,

57
00:03:09,515 --> 00:03:13,125
die an sich einfach eine objektive
Funktion ist, die wir optimieren möchten.

58
00:03:13,125 --> 00:03:16,970
Wie bereits erklärt, ist bei
Regressionsproblemen normalerweise

59
00:03:16,970 --> 00:03:19,490
die Verlustfunktion die
mittlere quadratische Abweichung,

60
00:03:19,490 --> 00:03:22,950
die in dieser Gleichung
als Matrix dargestellt ist.

61
00:03:22,950 --> 00:03:27,700
Ich lasse die Konstante weg, da sie
später in der Ableitung verschwindet.

62
00:03:27,700 --> 00:03:31,660
Zuerst suchen wir den Unterschied zwischen
dem Wert der tatsächlichen Zielantwort

63
00:03:31,660 --> 00:03:34,675
und dem Wert unseres
vorausgesagten Labels, y mit Hut × w,

64
00:03:34,675 --> 00:03:39,390
was einfach X × w ist.
Beachten Sie aber,

65
00:03:39,390 --> 00:03:42,815
dass es mein Ziel ist, den Verlust
so weit wie möglich zu verringern.

66
00:03:42,815 --> 00:03:44,720
Ich muss also
herausfinden, wie ich ihn

67
00:03:44,720 --> 00:03:46,730
bezüglich der
Gewichtungen minimieren kann.

68
00:03:46,730 --> 00:03:50,280
Dafür nehme ich die Ableitung
in Bezug auf die Gewichtungen,

69
00:03:50,280 --> 00:03:52,255
im eindimensionalen Fall

70
00:03:52,255 --> 00:03:56,335
oder allgemeiner die Steigung,
wenn ich mehrere Merkmale habe.

71
00:03:56,335 --> 00:03:59,710
Damit kann ich dann
das globale Minimum finden.

72
00:03:59,710 --> 00:04:03,380
Die Gleichung hier,
ich gehe nicht auf die Ableitung ein,

73
00:04:03,380 --> 00:04:07,270
liefert eine geschlossene analytische
Lösung für lineare Regression.

74
00:04:07,270 --> 00:04:12,015
Das heißt, wenn man
die x- und y-Werte in die Formel einsetzt,

75
00:04:12,015 --> 00:04:14,395
erhält man die Werte für die Gewichtungen.

76
00:04:14,395 --> 00:04:17,640
Das ist aber nicht sehr praktisch.

77
00:04:17,640 --> 00:04:19,654
Es gibt Probleme mit der Inverse.

78
00:04:19,654 --> 00:04:23,740
Wir gehen zuerst davon aus, dass die
Grand-Matrix, X transponiert X,

79
00:04:23,740 --> 00:04:26,015
nicht singulär ist, also dass

80
00:04:26,015 --> 00:04:29,890
alle Spalten unserer
Merkmalsmatrix X linear unabhängig sind.

81
00:04:29,890 --> 00:04:32,260
Aber in Datasets

82
00:04:32,260 --> 00:04:35,320
kommen häufig doppelte oder
nahezu doppelte Daten vor.

83
00:04:35,320 --> 00:04:38,270
Dieselben Kunden
kaufen dasselbe Produkt wieder,

84
00:04:38,270 --> 00:04:41,795
zwei Fotos zeigen denselben
Sonnenuntergang im Abstand von Sekunden.

85
00:04:41,795 --> 00:04:45,830
Selbst wenn die Grand-Matrix
technisch linear unabhängig ist,

86
00:04:45,830 --> 00:04:48,135
könnte sie trotzdem
schlecht konditioniert sein,

87
00:04:48,135 --> 00:04:50,790
wodurch sie
für die Berechnung singulär wird

88
00:04:50,790 --> 00:04:53,075
und uns weiterhin Probleme macht.

89
00:04:53,075 --> 00:04:58,610
Die Inverse hat außerdem
Zeitkomplexität O(n) hoch drei,

90
00:04:58,610 --> 00:05:00,630
bei Verwendung des naiven Algorithmus,

91
00:05:00,630 --> 00:05:04,285
wird aber mit komplexen
Algorithmen nicht viel besser.

92
00:05:04,285 --> 00:05:07,270
Und die haben an sich schon
einige numerische Probleme.

93
00:05:07,270 --> 00:05:10,900
Dasselbe gilt sogar für die Multiplikation
zur Erzeugung der Grand-Matrix.

94
00:05:10,900 --> 00:05:12,555
Stattdessen könnten wir zur Lösung

95
00:05:12,555 --> 00:05:15,270
der normalen Gleichungen eine Cholesky-

96
00:05:15,270 --> 00:05:16,615
oder QR-Zerlegung verwenden.

97
00:05:16,615 --> 00:05:23,030
Für O(n) hoch drei
oder sogar O(n) hoch 2,5,

98
00:05:23,030 --> 00:05:24,840
wenn n gleich 10.000 oder mehr ist,

99
00:05:24,840 --> 00:05:27,280
kann der Algorithmus sehr langsam sein.

100
00:05:27,280 --> 00:05:31,890
Ja, man kann mit der normalen Gleichung
genau auf die Gewichtungen auflösen,

101
00:05:31,890 --> 00:05:34,160
das ist aber sehr von den Daten abhängig,

102
00:05:34,160 --> 00:05:36,795
vom Modell, von den Matrixalgorithmen

103
00:05:36,795 --> 00:05:40,565
der linearen Algebra,
die Sie verwenden, usw. Zum Glück

104
00:05:40,565 --> 00:05:44,285
gibt es das Gradientenverfahren als
Optimierungsalgorithmus, das erstens

105
00:05:44,285 --> 00:05:48,330
weniger Kosten- und Zeitaufwand
für die Berechnung erfordert, zweitens

106
00:05:48,330 --> 00:05:50,775
abänderungsfähiger für
milde Generalisierung und

107
00:05:50,775 --> 00:05:54,400
drittens allgemein genug ist, um bei den
meisten Problemen zu funktionieren.

108
00:05:54,400 --> 00:05:56,665
Stattdessen haben wir
beim Gradientenverfahren

109
00:05:56,665 --> 00:05:58,290
unsere Verlustfunktion,

110
00:05:58,290 --> 00:06:00,685
oder allgemeiner,
unsere objektive Funktion,

111
00:06:00,685 --> 00:06:03,640
die von den Gewichtungen
unseres Modells parameterisiert ist.

112
00:06:03,640 --> 00:06:07,060
In diesem Raum gibt es Hügel und Täler,

113
00:06:07,060 --> 00:06:08,450
genau wie auf der Erde.

114
00:06:08,450 --> 00:06:11,310
In vielen Problemen des
maschinellen Lernens gibt es aber

115
00:06:11,310 --> 00:06:13,010
viel mehr Dimensionen

116
00:06:13,010 --> 00:06:15,800
als die dreidimensionale,
räumliche Welt, in der wir leben.

117
00:06:15,800 --> 00:06:18,240
Da dies das Verfahren
des steilsten Abstiegs ist,

118
00:06:18,240 --> 00:06:21,335
Minimierung in Richtung des negativen
Gradienten, nicht des positiven Gradienten,

119
00:06:21,335 --> 00:06:23,465
was eine Maximierung wäre,

120
00:06:23,465 --> 00:06:26,330
möchten wir die
Verlust-Hyperebene durchlaufen

121
00:06:26,330 --> 00:06:28,170
und das globale Minimum suchen.

122
00:06:28,170 --> 00:06:32,065
Sprich: Wir hoffen,
das niedrigste Tal zu finden,

123
00:06:32,065 --> 00:06:35,205
unabhängig von unserem
Startpunkt auf der Hyperebene.

124
00:06:35,205 --> 00:06:38,705
Dazu suchen wir den
Gradienten der Verlustfunktion

125
00:06:38,705 --> 00:06:41,325
und multiplizieren ihn
mit einem Hyperparameter,

126
00:06:41,325 --> 00:06:45,975
Lernrate, und ziehen diesen Wert
dann von den aktuellen Gewichtungen ab.

127
00:06:45,975 --> 00:06:49,300
Dies wird iteriert bis zur Konvergenz.

128
00:06:49,300 --> 00:06:52,875
Das Bestimmen der optimalen Lernrate
und die vielen Iterationen

129
00:06:52,875 --> 00:06:55,745
lassen Sie vielleicht stattdessen
die Normalgleichung wählen,

130
00:06:55,745 --> 00:06:57,800
sofern die Anzahl der Merkmale gering ist,

131
00:06:57,800 --> 00:06:59,690
keine Probleme mit Kollinearität bestehen usw.

132
00:06:59,690 --> 00:07:02,280
Oder einen Gradientenverfahren-
Optimierer hinzufügen,

133
00:07:02,280 --> 00:07:05,615
wie Momentum, oder eine
abklingende Lernrate verwenden.

134
00:07:05,615 --> 00:07:09,990
Im nächsten Modul werden wir noch
genau auf das Gradientenverfahren eingehen.

135
00:07:09,990 --> 00:07:14,995
Welcher Hyperparameter hilft, die
Schrittgröße des Gradientenverfahrens

136
00:07:14,995 --> 00:07:16,625
entlang der Hyperebene zu finden,

137
00:07:16,625 --> 00:07:20,400
um hoffentlich
die Konvergenz zu beschleunigen?

138
00:07:20,400 --> 00:07:23,945
Die richtige Antwort lautet: Lernrate.

139
00:07:23,945 --> 00:07:25,580
Die Lernrate zusammen mit

140
00:07:25,580 --> 00:07:29,170
anderen Hyperparametern, die Sie
in den künftigen Modulen kennenlernen,

141
00:07:29,170 --> 00:07:32,035
hilft bei der Wahl der Schrittgröße
beim Gradientenverfahren.

142
00:07:32,035 --> 00:07:37,020
Ist sie zu klein, dauert der
Abstieg zur Konvergenz sehr lang.

143
00:07:37,020 --> 00:07:38,480
Ist sie zu groß,

144
00:07:38,480 --> 00:07:41,130
kann das Gradientenverfahren
sogar divergieren

145
00:07:41,130 --> 00:07:43,560
und den Verlust mehr und mehr steigern.

146
00:07:43,560 --> 00:07:47,540
Die übrigen drei Antworten haben mit
Kollinearität und Konditionierung zu tun,

147
00:07:47,540 --> 00:07:49,775
was uns beim
Gradientenverfahren nicht kümmert.

148
00:07:49,775 --> 00:07:52,230
Anders wäre das bei Verwendung
der Normalgleichung.