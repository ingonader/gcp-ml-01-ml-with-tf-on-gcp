ここで 機械学習がどのように進化してきたか 簡単に振り返り
現在のディープラーニング ニューラルネットワークまでの
流れを見ましょう ニューラルネットワークは過去数十年間に 流行した時期も廃れた時期もありますが 他のアルゴリズム用に開発されたテクニックを ディープラーニング 
ニューラルネットワークに適用すると とても強力になります 線形回帰が発明された目的は 惑星の動きと エンドウ豆の「さや」の
中身のサイズを予測するためでした フランシス ゴルトンは自然現象の計測に 統計学の手法を取り入れた先駆者です スイートピーなど さまざまな植物の 親と子の相対サイズのデータを調べました すぐには分かりにくい
とても特異なことを発見しました 確かに 平均より大きい親は
平均より大きい子を生み出す傾向があります しかし その子は 同世代の平均的な子より
どれほど大きいでしょうか その比率は その親が平均より大きい比率を 下回ります たとえば親のサイズが 同世代の平均よりも
標準偏差の1.5倍大きいとします すると子のサイズは同世代の平均と比べて 標準偏差の1.5倍未満になると予測できます こうして何世代もかけて 自然界のものは回帰します つまり平均値に戻ります こうして 線形回帰という名前になりました このグラフは 1877年の世界で最初の線形回帰です
すごいですね 当時の計算能力はとても限定されていました 彼らは大きなデータセットがあれば 優れた予測ができると気づきませんでした 実際 閉形式で
線形回帰の問題を解いていましたが グリッド降下法も使用されました データセットに応じて それぞれ長所と短所があります では 線形回帰のしくみを見てみましょう 線形回帰の意図を 少し詳しく掘り下げます この一次方程式でシステムを記述するとします 計測された特徴要因xに
さまざまな重みwを掛けて すべてを合計します データセットのすべてのサンプルを
この方程式で表せます y = w0 x x0 + w1 x x1 + w2 x x2 ... 
と続いて モデルのすべての「特徴」を含めます 言い換えると データセットのすべての行に
この式を当てはめます 重みwは固定値です 特徴xの値は
該当する各列と 機械学習データセットから得られます これは下側の等式に一般化されます y = Xw です この仮説方程式はとても重要です
線形回帰だけでなく 他の機械学習モデル たとえば後で取り上げる ディープニューラルネットワークでも重要です しかし 重みの推測値が良いか悪いか
どうすればわかりますか 答えは 損失関数を作ることです これを目的関数として調整していきます すでに述べましたが 線形回帰では通常 平均二乗誤差が最後の関数です これを行列形式で表すとこの等式です 定数は あとで微分で消えるので省略しました まず 実際のラベル値yと 保護付きラベル値y^の差を計算します y^は 先程の「X 掛ける w」です ここでの目的は損失を
できるだけ減らすことです ですから最小化のために なんらかの方法で重みを調整すべきです そのために
一次元のケースでは重みを微分します より一般的に 複数の特徴があるケースでは勾配を求めます これを使って大域的な最小点を見つけます この等式では 微分ではなく 線形回帰の閉形式分析ソリューションです つまりxとyの値を式に代入すると 重みの値が得られます でも あまり実用的ではありません 逆行列の問題があります 最初にXをXに移行した総合行列は 非特異であると仮定しました つまり 特徴行列Xのすべての列が
線形的に独立しています でも 実世界のデータセットでは 重複データ または
ほぼ重複するデータがあります 同じお客さんが同じ製品を買います 日の出の写真を
数秒間隔で2枚撮ることがあります たとえ総合行列が線形的に独立でも 依然として悪条件かもしれず 計算上は特異であっても 引き続き問題が生じる可能性があります また逆行列には
単純なアルゴリズムを使った場合 ON三乗の計算時間のコストがあります 手が込んだアルゴリズムを使っても
あまり改善されません さらに独特の数値上の問題があります これは総合行列を作成するための
乗算でも同じです 代わりに いわゆる「コレスキー」 またはQR分解を使って 正規方程式を解くことができます ON三乗では またはON2.5乗の場合でさえ Nが10,000以上になると アルゴリズムが低速になることがあります 正規方程式を使って
重みを正確に解くことはできます でも データやモデルにかなり左右されます 線形代数では行列アルゴリズムがモデルです 幸いにも 最急降下最適化アルゴリズムがあります これは第一に 時間とメモリの点でコストが低く 第二に 軽度の一般化に適していて 第三に 汎用的で ほとんどの問題に使えます 代わりに 最急降下では損失関数 つまり目的関数があります これはモデルの重みでパラメータ化されます この空間には 地上のような山と谷があります しかし 多くの機械学習問題では 次元がもっと増えます 私達の世界はわずか3Dです これは最急降下です つまり 上昇での最大化ではなく 勾配での最小化ですから 最後の超平面を通り 大域的な最小点を探します 言い換えると
超平面のどこから出発するかにかかわらず 最も低い谷を見つけようとします そのためには損失関数の勾配を見つけ それにハイパーパラメータ
つまり学習率を掛けて その値を現在の重みから差し引きます 収束するまでこのプロセスを繰り返します 最適な学習率を選んで
繰り返しを何度も待つことになるので むしろ正規方程式を使えるかもしれません ただし 特徴の数が少なく 共線性の問題などがない場合です または運動量などの
最急降下オプティマイザを追加したり 減衰学習率を使う場合です Korean降下について
次のモジュールで詳しく見ていきます 最急降下の刻み幅を決めるのに役立つ
ハイパーパラメータは何ですか？ それとともにハイパーサーバーで 収束を早めることができるかもしれません 正解は「学習率」です 学習率と 今後のモジュールで学ぶ
他のハイパーパラメータを使えば 最急降下の刻み幅サイズを決定できます 設定が低すぎると 最急降下の収束に
長い時間がかかります 設定が高すぎると 最急降下が分岐して 損失がますます増える可能性があります 他の3つの選択肢は
共線性と条件付けに関係しています 最急降下では正規方程式のときのように これらを考慮しません