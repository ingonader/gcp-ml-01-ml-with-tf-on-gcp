À la fin des années 2000, la recherche en machine learning
disposait de la puissance de calcul pour combiner les performances
dans des méthodes d'ensemble. Si les erreurs sont indépendantes pour
plusieurs classifieurs faibles, elles forment un classifieur fort
une fois combinées. Le DNN effectue une approximation
à l'aide de couches d'abandon, qui régularisent le modèle
et empêchent le surapprentissage. Cela peut être simulé en désactivant
au hasard des neurones dans le réseau, avec une certaine probabilité
à chaque propagation avant, créant ainsi un nouveau
réseau à chaque fois. Souvent, l'avis de milliers de personnes
interrogées sur des questions complexes est plus juste que l'avis
d'une seule personne. Ce concept, c'est "la sagesse des foules". Idem pour le machine learning. Si les résultats proviennent de plusieurs
prédicteurs (classifieurs ou régresseurs), le groupe sera souvent plus performant
qu'un modèle individuel. Ce groupe de prédicteurs est un ensemble qui permet l'apprentissage d'ensemble. L'algorithme effectuant cet apprentissage
est une méthode d'ensemble. Les forêts aléatoires sont un type
d'apprentissage d'ensemble très utilisé. Au lieu d'un seul arbre de décision
avec votre ensemble d'apprentissage, vous pouvez créer
plusieurs arbres de décision, avec pour chaque un sous-échantillon
des données d'apprentissage. Les arbres n'ayant pas tout l'ensemble, ils n'ont pas mémorisé toutes les données. Une fois les arbres entraînés
et associés à un sous-ensemble, vous passez à l'étape la plus importante
du machine learning : les prédictions. Vous devez transférer vos données
test dans chaque arbre de la forêt, puis regrouper les résultats. Pour la classification, s'il existe un vote de majorité
dans tous les arbres, alors celui-ci sera
la classe de sortie finale. En régression, le résultat peut être
une compilation des valeurs, comme la moyenne,
le maximum, la médiane, etc. Vous pouvez améliorer la généralisation,
en échantillonnant au hasard les exemples et/ou les caractéristiques. L'échantillonnage aléatoire d'exemples
est un remplacement, ou bagging, abrégé de bootstrap aggregating, et un collage sans remplacement. Chaque prédicteur a un biais
plus élevé entraîné sur un sous-ensemble
plus petit de données. Toutefois, l'agrégation permet
de réduire le biais et la variance. L'ensemble dispose souvent
alors d'un biais similaire comme prédicteur
sur tout l'ensemble d'apprentissage, mais d'une variance inférieure. Une méthode de validation efficace
pour l'erreur de généralisation consiste à utiliser les données out-of-bag plutôt qu'un ensemble distinct tiré
des données avant l'apprentissage. Elle est semblable à la validation k-fold
avec une méthode holdout aléatoire. L'échantillonnage de caractéristiques
crée des sous-espaces aléatoires, et l'échantillonnage aléatoire d'exemples
crée des patchs aléatoires. AdaBoost, ou Adaptive boosting,
et Gradient boosting sont des algorithmes qui font de plusieurs classifieurs faibles
un classifieur fort. Les classifieurs sont entraînés
consécutivement, afin de corriger les éventuels problèmes
des classifieurs précédents. Avec les arbres boostés, comme davantage
d'arbres sont ajoutés à l'ensemble, les prédictions sont souvent améliorées. Doit-on ajouter des arbres à l'infini ?
Bien sûr que non. Utilisez votre ensemble
de validation pour un arrêt prématuré, pour ne pas surentraîner
les données d'apprentissage en ajoutant trop d'arbres. Enfin, comme avec les réseaux de neurones, l'empilement est possible. Des méta-classifieurs apprennent
quoi faire avec les images qui sont ensuite empilées
en méta-classifieurs, et ainsi de suite. Nous aborderons bientôt ce concept
dans les réseaux de neurones profonds. Parmi les propositions suivantes,
laquelle est souvent fausse pour les forêts aléatoires par rapport
aux arbres de décision individuels ? La bonne réponse est
qu'il est souvent faux de dire que l'interprétation visuelle
des forêts aléatoires est plus facile. Comme les réseaux de neurones, plus on ajoute de couches au modèle, plus il sera difficile
à comprendre et à expliquer. Une forêt aléatoire est souvent
plus complexe qu'un arbre de décision et donc plus difficile
à interpréter visuellement. Les trois autres propositions sont vraies. Une forêt a une meilleure généralisation
via le bagging et les sous-espaces. Grâce aux votes pour la classification
ou à l'agrégation pour la régression, la forêt peut être souvent
bien plus efficace qu'un arbre. Enfin, grâce à l'échantillonnage
aléatoire des forêts, le biais reste similaire
à celui d'un arbre individuel, mais la variance est inférieure, ce qui améliore souvent la généralisation.