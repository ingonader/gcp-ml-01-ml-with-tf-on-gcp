なぜパーセプトロンは1層のみなのでしょう？ 1つの層の出力を次の層に入力として送ったら 複数のパーセプトロン層を合わせて
もっと強力なモデルにできるのでは？ しかし もし非線形の活性化関数を使わないなら 層をいくら追加しても
元の単一の線形層と同じになり 実際に何の利点もありません 非線形の活性化関数が必要です こうしてシグモイドまたは
双曲正接（tanh）活性化関数が 非線形で使われるようになりました 当時は これだけに限定されていました なぜなら モデルの重みに
誤差逆伝搬するための 微分可能関数が必要だったからです 最近では 微分可能ではない
活性化関数もありますが 当時はその使い方がよく分かりませんでした この「活性化関数が微分可能で
なければならない」制約のために ネットワークのトレーニングが困難でした さらに このモデルの効果を
制約する要因として データ量や
コンピューティング リソースなどの 課題がありました たとえば 最急降下法で 大域的な最小値を見つける代わりに 鞍点で最適化してしまうことがあります しかし正規化線形関数（ReLU）の
使い方が開発されて以降 トレーニング速度が8～10倍になり ロジスティック回帰の収束がほぼ保証されます ちょうど脳のように パーセプトロンを互いに繋げて
複数の層にすると フィードフォワード
ニューラルネットワークになります 構成要素は 単一層パーセプトロンと
ほぼ同じです つまり 入力 重み付き合計 活性化関数 そして出力です 1つの違いは 入力層以外のニューロンへの入力が 生の入力ではなく前の層の出力であることです もう1つの違いですが
層と層の間でニューロンを繋ぐ方法が ベクトルではなく行列です なぜなら 繋ぎ方が複雑だからです たとえば 図では入力層の重み行列が4x2 隠し層の重み行列が2x1です 後で学びますが ニューラルネットワークには
画像などに応用して 優れた効果を発揮する
完全な接続性があるとは限りません さらに 活性化関数には
単位ステップ関数の他にも シグモイドや
双曲正接（tanh）などがあります 入力以外のそれぞれのニューロンは 3ステップからなる1つの単位だと
考えることができます 最初のステップは重み付き合計 次に活性化関数 さらに活性化関数の出力です 多くの層と ニューロン 活性化関数
トレーニング方法を含む 複雑なネットワークを形成できます このコースでは TensorFlow Playgroundを使って 情報の流れを直観的に見ていきます さらにハイパーパラメータを
カスタマイズしたり 重みを可視化したりしながら 損失関数の進歩を楽しく学びます この線形活性化関数は xに対して単にxを返すだけの恒等関数です これは初期の活性化関数です しかし すでに述べたとおり 線形活性化関数だけを使って何千もの層を含む ニューラルネットワークを作っても その最終出力は単に
入力を線形に組み合わせただけです これは 入力ごとに
それぞれ定数を掛けるのと同じです おわかりでしょうか？ 単なる線形回帰です ですからニューラルネットワークに データ分布をよく学習させるには 非線形の活性化関数を使って
連鎖関数にする必要があります f(x) = xである線形活性化関数に加えて ニューラルネットワークの最初の黄金期では シグモイドとtanh活性化関数が使われました シグモイド活性化関数は基本的に 単位ステップ関数の形をスムーズにした関数で 負の無限大に向かって0の漸近線と 正の無限大に向かって1の漸近線があり その間に中間値があります 双曲正接（tanh）活性化関数も よく使われた活性化関数です これは基本的にシグモイドを拡大して移動し -1から+1の範囲にしたものです すべての場所で微分可能で 単調で しかもスムーズな優れた関数でした しかし 関数の入力値が高い
または低いことが原因で 飽和の問題が生じて 関数の近似平坦域ができることがあります そのような地点ではカーブがほぼ平坦ですから 微分は極めてゼロに近づきます このため 重みのトレーニングが非常に遅く
止まることさえあります 勾配がゼロに近いので 最急降下での刻み幅サイズが
とても小さくなります 線形活性化関数は微分可能で
単調でスムーズでした すでに述べましたが 多くの線形関数を線形に繋げても
1つのニューロンと同じです これでは データを適切に記述するための 複雑な連鎖ができません 線形活性化関数の近似も行われましたが どこでも微分可能ではありませんでした 対処法が見つかったのはずっと後のことです 現在では正規化線形（ReLU）活性化関数が
よく使われています 非線形なので 必要に応じて
複雑なモデルを作成でき 入力空間の負以外の部分で飽和がありません ただし入力空間の負の部分は
ゼロ活性化に変換されるので ReLU層はまったく活性化しない場合があり このためトレーニングが遅延 停止する
可能性があります 解決策がいくつかあります 1つは 指数線形ユニット（ELU）という
別の活性化関数を使うことです これは 入力空間の負以外の部分でほぼ線形 しかも形がスムーズで単調です 最も重要なのは 入力空間の負の部分で非ゼロです ELUの主な欠点は 指数を計算するので ReLUよりも処理能力が必要なことです 次のモジュールでもっと実験します 出力を確率形式で得たい場合に 最後の層でどの活性化関数を選ぶべきですか？ 正解はシグモイド活性化関数です シグモイド関数の範囲が0～1だからです 確率の範囲もこれと同じです 範囲だけでなく シグモイド関数は
ロジスティック確率分布の累積分布関数です その分位点関数は 対数オッズをモデル化するロジックの逆です このため これを真の確率として使用できます これらの理由については
専門分野の後の部分でさらに扱います tanhは間違いです これは
シグモイドと同じスカッシング関数ですが 範囲が-1～+1なので 確率の範囲と異なります しかもtanhを単にシグモイドに
スカッシングしただけでは 確率になりません なぜならすでに述べたような シグモイド出力を確率と解釈する
特性がないためです シグモイドに正しく変換するには まず1を加えて2で割って
正しい範囲にする必要があります さらに正しい分布を得るには tanh引数を2で割る必要があります でもすでにtanhを計算しましたから 多くの作業をこのままにします 最初にシグモイドだけを使うのがよいでしょう ReLUも間違いです ゼロから無限大の範囲なので 確率を表すにはまったく不適切です ELUもマイナスの無限大から無限大なので
間違いです