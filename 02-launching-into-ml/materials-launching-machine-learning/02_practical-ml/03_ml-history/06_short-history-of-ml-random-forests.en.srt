1
00:00:00,000 --> 00:00:02,960
Coming into the last few decades in the 2000s,

2
00:00:02,960 --> 00:00:06,470
machine learning research now had the computational power to combine and

3
00:00:06,470 --> 00:00:11,280
blend performance across many miles in what we call an ensemble method.

4
00:00:11,280 --> 00:00:16,600
You can imagine that if the errors are independent for a number of simple weak learners,

5
00:00:16,600 --> 00:00:19,860
combined, they would form a strong learner.

6
00:00:19,860 --> 00:00:23,390
DNN is going to approximate this by using dropout layers,

7
00:00:23,390 --> 00:00:26,170
which help regularize the model and prevent overfitting.

8
00:00:26,170 --> 00:00:29,250
This can be simulated by randomly turning off neurons

9
00:00:29,250 --> 00:00:32,185
in the network with some probability for each forward pass,

10
00:00:32,185 --> 00:00:35,755
which will essentially be creating a new network each time.

11
00:00:35,755 --> 00:00:39,970
Oftentimes, complex questions are better answered when aggregated

12
00:00:39,970 --> 00:00:44,195
from thousands of people's responses instead of those just by a sole individual.

13
00:00:44,195 --> 00:00:47,090
This is known as the wisdom of the crowd.

14
00:00:47,090 --> 00:00:49,150
The same applies to machine learning.

15
00:00:49,150 --> 00:00:53,560
When you aggregate the results of many predictors either classifiers or reggressors,

16
00:00:53,560 --> 00:00:57,850
the group will usually perform better than the best individual model.

17
00:00:57,850 --> 00:01:01,720
This group of predictors is an ensemble which when combined in this way,

18
00:01:01,720 --> 00:01:03,370
it leads to ensemble learning.

19
00:01:03,370 --> 00:01:07,150
The algorithm that performs this learning is an ensemble method.

20
00:01:07,150 --> 00:01:11,430
One of the most popular types of ensemble learning is the random forest.

21
00:01:11,430 --> 00:01:16,130
Instead of taking your entire training set and using that to build one decision tree,

22
00:01:16,130 --> 00:01:18,360
you could have a group of decision trees

23
00:01:18,360 --> 00:01:21,400
that each get a random subsample of the training data.

24
00:01:21,400 --> 00:01:23,970
Since they haven't seen the entire training set,

25
00:01:23,970 --> 00:01:26,450
they can't have memorized the entire thing.

26
00:01:26,450 --> 00:01:29,565
Once all the trees are trained and they're a subset of the data,

27
00:01:29,565 --> 00:01:34,350
you can now make the most important and valuable part of machine learning, predictions.

28
00:01:34,350 --> 00:01:37,820
To do so, you would pass your test sample through each tree in the forest,

29
00:01:37,820 --> 00:01:39,720
and then aggregate the results.

30
00:01:39,720 --> 00:01:41,500
If this is classification,

31
00:01:41,500 --> 00:01:43,175
there could be a majority vote across

32
00:01:43,175 --> 00:01:46,215
all trees which would then be the final output class.

33
00:01:46,215 --> 00:01:49,900
If it is regression, it could be an aggregate the values such as the mean,

34
00:01:49,900 --> 00:01:51,990
max, median, et cetera.

35
00:01:51,990 --> 00:01:57,740
To improve generalization, you can random sample the examples and/or the features.

36
00:01:57,740 --> 00:02:01,350
We call random sampling examples for replacement, bagging,

37
00:02:01,350 --> 00:02:02,875
short for bootstrap aggregating,

38
00:02:02,875 --> 00:02:05,730
and pasting when without replacement.

39
00:02:05,730 --> 00:02:08,990
Each individual predictor has higher bias being

40
00:02:08,990 --> 00:02:12,150
trained on the smaller subset rather than the full dataset,

41
00:02:12,150 --> 00:02:15,975
but the aggregation reduces both the bias and variance.

42
00:02:15,975 --> 00:02:17,920
This usually gives the ensemble

43
00:02:17,920 --> 00:02:21,320
a similar bias as a single predictor on the entire training set,

44
00:02:21,320 --> 00:02:23,335
but with a lower variance.

45
00:02:23,335 --> 00:02:26,280
A great method of validation for your generalization error,

46
00:02:26,280 --> 00:02:29,180
is to use your out of bagged data instead

47
00:02:29,180 --> 00:02:32,760
of having to have a separate set pulled from the dataset before training.

48
00:02:32,760 --> 00:02:37,100
It is reminiscent of k-fold validation using random holdouts.

49
00:02:37,100 --> 00:02:40,645
Random subspaces are made when we sample from the features,

50
00:02:40,645 --> 00:02:44,890
and if we random sample examples too is called random patches.

51
00:02:44,890 --> 00:02:50,085
Adaptive boosting or AdaBoost in gradient boosting are both examples of boosting,

52
00:02:50,085 --> 00:02:54,100
which is when we aggregate a number of weak learners to create a strong learner.

53
00:02:54,100 --> 00:02:56,680
Typically, this is done by training each learner

54
00:02:56,680 --> 00:03:00,835
sequentially which tries to correct any issues a learner had before it.

55
00:03:00,835 --> 00:03:04,870
For boosted trees, as more trees are added to the ensemble,

56
00:03:04,870 --> 00:03:06,725
the predictions usually improve.

57
00:03:06,725 --> 00:03:11,375
So, do we continue to add trees out of infinitum? Of course not.

58
00:03:11,375 --> 00:03:14,440
You can use your validation set to use early stopping,

59
00:03:14,440 --> 00:03:16,890
so that we don't start overfitting our training data,

60
00:03:16,890 --> 00:03:19,180
because we've added too many trees.

61
00:03:19,180 --> 00:03:21,650
Lastly, just as we saw with neural networks,

62
00:03:21,650 --> 00:03:22,865
we can perform stacking,

63
00:03:22,865 --> 00:03:26,130
where we can have meta-learners learn what to do with the pictures of the ensemble,

64
00:03:26,130 --> 00:03:30,645
which can in turn also be stacked into meta-learners and so on.

65
00:03:30,645 --> 00:03:35,675
We will see the subcomponent stacking and reuse in deep neural networks shortly.

66
00:03:35,675 --> 00:03:39,010
Which of the following is most likely false of

67
00:03:39,010 --> 00:03:43,750
random forests when comparing against individual decision trees?

68
00:03:43,750 --> 00:03:48,260
The correct answer is that it's most likely false,

69
00:03:48,260 --> 00:03:51,435
that random forests are easier to visually interpret.

70
00:03:51,435 --> 00:03:53,040
Similar to neural networks,

71
00:03:53,040 --> 00:03:55,510
the more layers in complexity you add to your model,

72
00:03:55,510 --> 00:03:57,980
the more difficult it will be to understand and explain.

73
00:03:57,980 --> 00:04:02,270
A random forest is usually more complex than an individual decision tree,

74
00:04:02,270 --> 00:04:04,360
so this makes it harder to visually interpret.

75
00:04:04,360 --> 00:04:06,770
The other three are most likely true.

76
00:04:06,770 --> 00:04:11,400
Random forests typically have better generalization through bagging and subspacing,

77
00:04:11,400 --> 00:04:16,315
also by using a voting system for classification or aggregation for regression,

78
00:04:16,315 --> 00:04:19,765
the forest can typically perform much better than an individual tree.

79
00:04:19,765 --> 00:04:23,265
Lastly, due to the random sampling of random forests,

80
00:04:23,265 --> 00:04:26,180
it keeps its bias similar to that of an individual tree,

81
00:04:26,180 --> 00:04:29,370
but also, has lower variance which once again,

82
00:04:29,370 --> 00:04:32,000
usually means, better generalization.