1990年代に カーネル法が確立されました Google ResearchのCorinna Cortesなどが 先駆者です この研究は新しい非線形モデルの
分野を切り開きました 特にサポートベクターマシン（SVM）です これは最大マージン分類器としても
知られています SVMは基本的に 非線形活性化にシグモイド出力を加えて
最大マージンを求めます すでにロジスティック回帰を使って
決定境界を作成し 非分類確率の対数尤度を
最大化する方法を見ました ロジスティック回帰の線形の決定境界の例では それぞれの点とそのクラスを 超平面からできるだけ遠く離して 確率を出し それが予測信頼性となります 直線で分かれた2つのクラスの間に 無限の数の超平面を作成できます 
たとえば この2つの図で点線で示した超平面です SVMでは決定境界の超平面の両側に 2つの平行な超平面を作り 超平面のそれぞれの側の
最も近いデータ点と交差させます これらがサポートベクターです 2つのベクター間の距離がマージンです 左の図では垂直な超平面が
2つのクラスを分けています でも2つのサポートベクター間の
マージンは小さいです 右の図のような 別の超平面を選ぶと マージンがかなり大きくなります マージンが広いほど
決定境界を一般化しやすくなり データをより適切に解析できます このようにSVM分類器では
2つのサポートベクター間のマージンを ヒンジ損失関数を使って最大化します ロジスティック回帰の
交差エントロピー最小化とは対照的です ここではクラスが2つだけで バイナリ分類問題となっています 1つのクラスのラベルの値は1 もう1つのクラスのラベルの値は-1です クラスの数が2つより多い場合は 一対多の手法で 昇格バイナリ分類の中から
最適なものを選びます でも データを線形に
2クラスに分けられない場合は？ カーネル変換を使えます この図のように 入力ベクター空間のデータを
別のベクター空間にマップします 後者では特徴を線形に分けることができます ディープニューラルネットワークの初期と同様 調整されたユーザー作成の
特徴マップを使用して 元のデータ表現を特徴ベクターに変換します しかしカーネル手法では
ユーザー定義できるのはカーネルだけです これは単に生のデータ表現の
2点間の類似関数にすぎません カーネル変換は ニューラルネットワークの活性化関数が 入力を変換空間にマップするのに似ています 層のニューロン数で次元が決まります たとえば
2つの入力と3つのニューロンであれば 入力2D空間から3D空間へのマップになります さまざまなカーネルがあります
最も基本的な線形カーネル 多項式カーネル
ガウス放射基底関数カーネルなどです バイナリ分類器でカーネルを使うとき 通常は類似度を重み付きで合計します では どんな場合にSVMを使えますか？ カーネルSVMの解はよりスパースで
スケーラビリティに優れています 次元が多い場合や 予測変数による
応答の予測精度が非常に高い場合 SVMは優れた効果を発揮します このようにSVMでは カーネルで入力を高次元の
特徴空間にマップしますが ニューラルネットワークではどんな場合に 高次元ベクター空間にマップできますか 正解は「層当たりのニューロン数が
多い場合」です 層のニューロン数は ベクター空間の次元数を決定します 入力特徴が3つの場合は
R3ベクター空間です たとえ層の数が100個でも それぞれのニューロンが3個だけなら やはりR3ベクター空間です
基盤を変更しているだけです たとえばSVMでカーネルのガウスRBを使う場合 入力空間が無限次元にマップされます 活性化関数はベクター空間の基盤を変えますが 次元の数は変わりません 単なる回転や伸縮と考えることができます 非線形であるとしても 前と同じベクター空間です 損失関数は最小化の目的関数です 勾配を使ってモデルパラメータの
重みを調整するスカラーです これは回転や伸縮を変えるだけで 次元の数は変わりません