1
00:00:00,000 --> 00:00:02,960
In den letzten Jahrzehnten der 2000er

2
00:00:02,960 --> 00:00:06,470
verfügte die ML-Forschung über die
Rechenleistung zum Kombinieren und

3
00:00:06,470 --> 00:00:11,280
Vereinen von Ergebnissen aus vielen
Modellen in einer Ensemblemethode.

4
00:00:11,280 --> 00:00:16,600
Wenn die Fehler für eine Menge einfacher
schwacher Klassifikatoren unabhängig sind,

5
00:00:16,600 --> 00:00:19,860
würden sie zusammen einen
starken Klassifikator bilden.

6
00:00:19,860 --> 00:00:22,190
DNN nähert sich daran
durch Dropout-Schichten an,

7
00:00:22,190 --> 00:00:25,660
die zur Regularisierung des Modells
dienen und Überanpassung vermeiden.

8
00:00:25,660 --> 00:00:29,310
Dies lässt sich durch zufälliges 
Ausschalten von Neuronen

9
00:00:29,310 --> 00:00:32,975
im Netz simulieren, mit einer gewissen
Wahrscheinlichkeit je Vorwärtsschritt,

10
00:00:32,975 --> 00:00:36,375
wodurch im Grunde jedes Mal
ein neues Netz erstellt wird.

11
00:00:36,375 --> 00:00:39,970
Oft lassen sich komplexe Fragen besser
durch die zusammengefassten Antworten

12
00:00:39,970 --> 00:00:44,195
vieler tausend Personen
beantworten als von nur einer Person.

13
00:00:44,195 --> 00:00:47,090
Das nennt man die Intelligenz der Masse.

14
00:00:47,090 --> 00:00:49,150
Das gilt auch für maschinelles Lernen.

15
00:00:49,150 --> 00:00:53,560
Die Kombination der Ergebnisse vieler
Prädiktoren, entweder Klassifikatoren

16
00:00:53,560 --> 00:00:57,850
oder Regressoren, ist normalerweise
besser als das beste Einzelmodell.

17
00:00:57,850 --> 00:01:01,720
Diese Gruppe von Prädiktoren ist ein
Ensemble, das in derartiger Kombination

18
00:01:01,720 --> 00:01:03,370
zum Ensemble Learning führt.

19
00:01:03,370 --> 00:01:07,150
Der Algorithmus, der dieses Lernen
durchführt, ist eine Ensemblemethode.

20
00:01:07,150 --> 00:01:11,430
Eine der beliebtesten Arten des Ensemble
Learning ist der Random Forest.

21
00:01:11,430 --> 00:01:16,130
Anstatt einen Entscheidungsbaum aus
dem gesamten Trainingsdatensatz zu bauen,

22
00:01:16,130 --> 00:01:18,360
können wir eine Gruppe
von Entscheidungsbäumen

23
00:01:18,360 --> 00:01:21,400
mit je einer zufälligen
Teilgruppe der Trainingsdaten haben.

24
00:01:21,400 --> 00:01:23,970
Da sie nicht den gesamten
Datensatz gesehen haben,

25
00:01:23,970 --> 00:01:26,450
können sie auch nicht alles speichern.

26
00:01:26,450 --> 00:01:29,565
Sobald alle Bäume mit
Teilmengen der Daten trainiert sind,

27
00:01:29,565 --> 00:01:34,350
sind wir bereit für den wichtigsten und
wertvollsten Teil des ML: Vorhersagen.

28
00:01:34,350 --> 00:01:37,740
Dazu werden Testbeispiele durch jeden
Baum im Wald geschickt

29
00:01:37,740 --> 00:01:39,930
und die Ergebnisse werden dann
zusammengefasst.

30
00:01:39,930 --> 00:01:41,500
Bei Klassifikation

31
00:01:41,500 --> 00:01:43,265
kann es zu einer
Mehrheitsentscheidung

32
00:01:43,265 --> 00:01:46,415
aus allen Bäumen kommen, die zur
abschließenden Ausgabeklasse wird.

33
00:01:46,415 --> 00:01:49,900
Bei Regression kann es eine Summe der
Werte sein, wie etwa Mittelwert,

34
00:01:49,900 --> 00:01:51,990
Maximum, Median usw.

35
00:01:51,990 --> 00:01:57,740
Zur besseren Generalisierung wählen
wir zufällig Beispiele und/oder Merkmale.

36
00:01:57,740 --> 00:02:01,350
Wir nennen die Stichprobennahme
mit Zurücklegen "Bagging",

37
00:02:01,350 --> 00:02:02,875
kurz für Bootstrap aggregating,

38
00:02:02,875 --> 00:02:05,730
und ohne Zurücklegen "Pasting".

39
00:02:05,730 --> 00:02:08,990
Jeder einzelne Prädiktor hat
eine höhere Verzerrung, da

40
00:02:08,990 --> 00:02:12,150
er an einem Teilsatz anstatt dem
ganzen Dataset trainiert wurde,

41
00:02:12,150 --> 00:02:15,975
aber die Aggregation reduziert
sowohl Verzerrung als auch Varianz.

42
00:02:15,975 --> 00:02:17,870
Deshalb hat das Ensemble oft

43
00:02:17,870 --> 00:02:21,570
eine ähnliche Verzerrung wie ein einzelner
Prädiktor am ganzen Trainingssatz,

44
00:02:21,570 --> 00:02:23,335
aber eine geringere Varianz.

45
00:02:23,335 --> 00:02:26,280
Eine tolle Validierungsmethode
für den Generalisierungsfehler

46
00:02:26,280 --> 00:02:29,180
ist die Verwendung der
Out-of-Bag-Daten, anstatt

47
00:02:29,180 --> 00:02:32,760
einen separaten Satz vor dem Lernen
aus dem Dataset ziehen zu müssen.

48
00:02:32,760 --> 00:02:37,100
Es erinnert an k-fache
Validierung mit zufälligen Holdouts.

49
00:02:37,100 --> 00:02:40,645
Zufällige Teilräume entstehen, wenn
wir Stichproben der Merkmale nehmen

50
00:02:40,645 --> 00:02:44,890
und wenn wir auch zufällige Beispiele
ziehen, heißt das zufällige Patches.

51
00:02:44,890 --> 00:02:50,085
Adaptives Boosting oder AdaBoost im
Gradientenboosting, sind Beispiele für

52
00:02:50,085 --> 00:02:54,100
Boosting, wobei wir mehrere schwache
Klassifikatoren zu einem starken vereinen.

53
00:02:54,100 --> 00:02:56,680
Dazu werden alle
Klassifikatoren der Reihe nach

54
00:02:56,680 --> 00:03:00,835
trainiert, um etwaige Probleme des
vorherigen Klassifikators zu korrigieren.

55
00:03:00,835 --> 00:03:04,740
Für Boosting-Bäume gilt: Je mehr
Bäume ins Ensemble aufgenommen werden,

56
00:03:04,740 --> 00:03:06,725
desto besser ist meistens die Vorhersage.

57
00:03:06,725 --> 00:03:11,375
Fügen wir deshalb unendlich
viele Bäume hinzu? Natürlich nicht.

58
00:03:11,375 --> 00:03:14,440
Wir können unseren Validierungssatz
zum frühen Stoppen nutzen,

59
00:03:14,440 --> 00:03:16,890
damit wir keine Überanpassung
unserer Trainingsdaten

60
00:03:16,890 --> 00:03:19,180
aufgrund zu vieler Bäume auslösen.

61
00:03:19,180 --> 00:03:21,300
Zum Schluss, genau wie bei
neuronalen Netzen,

62
00:03:21,300 --> 00:03:22,705
können wir Stacking anwenden,

63
00:03:22,705 --> 00:03:26,740
wobei Metaklassifikatoren lernen, was mit
den Vorhersagen des Ensembles zu tun ist,

64
00:03:26,740 --> 00:03:30,645
die ihrerseits in Meta-Metaklassifikatoren
gepackt werden können usw.

65
00:03:30,645 --> 00:03:35,675
Das Teilkomponenten-Stacking und die
Wiederverwendung in DNN sehen wir gleich.

66
00:03:35,675 --> 00:03:39,010
Was trifft wahrscheinlich nicht auf

67
00:03:39,010 --> 00:03:43,750
Random Forests im Vergleich zu
einzelnen Entscheidungsbäumen zu?

68
00:03:43,750 --> 00:03:48,260
Die richtige Antwort ist: Es
trifft wahrscheinlich nicht zu,

69
00:03:48,260 --> 00:03:51,325
dass Random Forests optisch
einfacher zu interpretieren sind.

70
00:03:51,325 --> 00:03:52,640
Ähnlich wie neuronale Netze

71
00:03:52,640 --> 00:03:55,840
wird es mit steigender Anzahl
von Komplexitätsschichten des Modells

72
00:03:55,840 --> 00:03:57,980
schwieriger, es zu
verstehen und zu erklären.

73
00:03:57,980 --> 00:04:02,110
Ein Random Forest ist in der Regel
komplexer als nur ein Entscheidungsbaum

74
00:04:02,110 --> 00:04:04,360
und dadurch optisch
schwieriger interpretierbar.

75
00:04:04,360 --> 00:04:06,850
Die übrigen drei Aussagen
treffen wahrscheinlich zu.

76
00:04:06,850 --> 00:04:11,400
Random Forests sind besser generalisierbar
durch Bagging und Subspacing,

77
00:04:11,400 --> 00:04:16,205
und durch ein Abstimmsystem bei Klassifikation
bzw. durch Aggregation bei Regression

78
00:04:16,205 --> 00:04:19,765
bringt der Wald in der Regel eine viel
bessere Leistung als ein Einzelbaum.

79
00:04:19,765 --> 00:04:23,265
Durch die zufällige Nahme
von Stichproben bei Random Forests

80
00:04:23,265 --> 00:04:26,180
ist die Verzerrung geringer
als bei einem Einzelbaum,

81
00:04:26,180 --> 00:04:29,370
aber auch die Varianz,
was wie gesagt

82
00:04:29,370 --> 00:04:32,000
normalerweise eine bessere
Generalisierbarkeit bedeutet.