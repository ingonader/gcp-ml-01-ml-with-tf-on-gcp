1
00:00:00,000 --> 00:00:03,000
ここで 機械学習がどのように進化してきたか

2
00:00:03,000 --> 00:00:06,440
簡単に振り返り
現在のディープラーニング

3
00:00:06,440 --> 00:00:08,975
ニューラルネットワークまでの
流れを見ましょう

4
00:00:08,975 --> 00:00:11,955
ニューラルネットワークは過去数十年間に

5
00:00:11,955 --> 00:00:15,355
流行した時期も廃れた時期もありますが

6
00:00:15,355 --> 00:00:18,505
他のアルゴリズム用に開発されたテクニックを

7
00:00:18,505 --> 00:00:21,785
ディープラーニング 
ニューラルネットワークに適用すると

8
00:00:21,785 --> 00:00:23,670
とても強力になります

9
00:00:23,670 --> 00:00:27,160
線形回帰が発明された目的は 惑星の動きと

10
00:00:27,160 --> 00:00:31,110
エンドウ豆の「さや」の
中身のサイズを予測するためでした

11
00:00:31,110 --> 00:00:34,540
フランシス ゴルトンは自然現象の計測に

12
00:00:34,540 --> 00:00:38,135
統計学の手法を取り入れた先駆者です

13
00:00:38,135 --> 00:00:41,805
スイートピーなど さまざまな植物の

14
00:00:41,805 --> 00:00:45,625
親と子の相対サイズのデータを調べました

15
00:00:45,625 --> 00:00:50,155
すぐには分かりにくい
とても特異なことを発見しました

16
00:00:50,155 --> 00:00:55,365
確かに 平均より大きい親は
平均より大きい子を生み出す傾向があります

17
00:00:55,365 --> 00:01:00,950
しかし その子は 同世代の平均的な子より
どれほど大きいでしょうか

18
00:01:00,950 --> 00:01:05,295
その比率は その親が平均より大きい比率を

19
00:01:05,295 --> 00:01:07,445
下回ります

20
00:01:07,445 --> 00:01:09,545
たとえば親のサイズが

21
00:01:09,545 --> 00:01:14,445
同世代の平均よりも
標準偏差の1.5倍大きいとします

22
00:01:14,445 --> 00:01:17,225
すると子のサイズは同世代の平均と比べて

23
00:01:17,225 --> 00:01:21,220
標準偏差の1.5倍未満になると予測できます

24
00:01:21,220 --> 00:01:23,915
こうして何世代もかけて

25
00:01:23,915 --> 00:01:25,935
自然界のものは回帰します

26
00:01:25,935 --> 00:01:28,110
つまり平均値に戻ります

27
00:01:28,110 --> 00:01:31,250
こうして 線形回帰という名前になりました

28
00:01:31,760 --> 00:01:34,330
このグラフは

29
00:01:34,330 --> 00:01:38,235
1877年の世界で最初の線形回帰です
すごいですね

30
00:01:39,275 --> 00:01:42,530
当時の計算能力はとても限定されていました

31
00:01:42,530 --> 00:01:45,060
彼らは大きなデータセットがあれば

32
00:01:45,060 --> 00:01:47,915
優れた予測ができると気づきませんでした

33
00:01:47,915 --> 00:01:51,825
実際 閉形式で
線形回帰の問題を解いていましたが

34
00:01:51,825 --> 00:01:54,535
グリッド降下法も使用されました

35
00:01:54,535 --> 00:01:56,175
データセットに応じて

36
00:01:56,175 --> 00:01:58,095
それぞれ長所と短所があります

37
00:01:58,095 --> 00:02:01,840
では 線形回帰のしくみを見てみましょう

38
00:02:02,850 --> 00:02:04,520
線形回帰の意図を

39
00:02:04,520 --> 00:02:06,535
少し詳しく掘り下げます

40
00:02:06,555 --> 00:02:10,185
この一次方程式でシステムを記述するとします

41
00:02:10,185 --> 00:02:15,420
計測された特徴要因xに
さまざまな重みwを掛けて

42
00:02:15,420 --> 00:02:17,220
すべてを合計します

43
00:02:17,220 --> 00:02:22,095
データセットのすべてのサンプルを
この方程式で表せます

44
00:02:22,095 --> 00:02:28,110
y = w0 x x0 + w1 x x1 + w2 x x2 ... 
と続いて

45
00:02:28,110 --> 00:02:30,805
モデルのすべての「特徴」を含めます

46
00:02:30,805 --> 00:02:35,355
言い換えると データセットのすべての行に
この式を当てはめます

47
00:02:35,355 --> 00:02:37,440
重みwは固定値です

48
00:02:37,440 --> 00:02:39,760
特徴xの値は
該当する各列と

49
00:02:39,760 --> 00:02:42,480
機械学習データセットから得られます

50
00:02:42,480 --> 00:02:45,795
これは下側の等式に一般化されます

51
00:02:45,825 --> 00:02:49,125
y = Xw です

52
00:02:49,275 --> 00:02:53,380
この仮説方程式はとても重要です
線形回帰だけでなく

53
00:02:53,380 --> 00:02:56,085
他の機械学習モデル たとえば後で取り上げる

54
00:02:56,085 --> 00:02:59,745
ディープニューラルネットワークでも重要です

55
00:02:59,745 --> 00:03:05,625
しかし 重みの推測値が良いか悪いか
どうすればわかりますか

56
00:03:06,085 --> 00:03:09,465
答えは 損失関数を作ることです

57
00:03:09,465 --> 00:03:13,065
これを目的関数として調整していきます

58
00:03:13,355 --> 00:03:17,060
すでに述べましたが 線形回帰では通常

59
00:03:17,060 --> 00:03:19,490
平均二乗誤差が最後の関数です

60
00:03:19,490 --> 00:03:22,670
これを行列形式で表すとこの等式です

61
00:03:23,130 --> 00:03:27,700
定数は あとで微分で消えるので省略しました

62
00:03:27,700 --> 00:03:30,540
まず 実際のラベル値yと

63
00:03:30,540 --> 00:03:34,035
保護付きラベル値y^の差を計算します

64
00:03:34,035 --> 00:03:38,300
y^は 先程の「X 掛ける w」です

65
00:03:38,300 --> 00:03:42,225
ここでの目的は損失を
できるだけ減らすことです

66
00:03:42,225 --> 00:03:44,000
ですから最小化のために

67
00:03:44,000 --> 00:03:46,580
なんらかの方法で重みを調整すべきです

68
00:03:46,580 --> 00:03:50,430
そのために
一次元のケースでは重みを微分します

69
00:03:50,430 --> 00:03:52,055
より一般的に

70
00:03:52,055 --> 00:03:56,335
複数の特徴があるケースでは勾配を求めます

71
00:03:56,335 --> 00:03:59,810
これを使って大域的な最小点を見つけます

72
00:03:59,810 --> 00:04:03,050
この等式では 微分ではなく

73
00:04:03,050 --> 00:04:07,270
線形回帰の閉形式分析ソリューションです

74
00:04:07,270 --> 00:04:12,015
つまりxとyの値を式に代入すると

75
00:04:12,015 --> 00:04:14,395
重みの値が得られます

76
00:04:14,395 --> 00:04:17,640
でも あまり実用的ではありません

77
00:04:17,640 --> 00:04:19,724
逆行列の問題があります

78
00:04:19,724 --> 00:04:23,450
最初にXをXに移行した総合行列は

79
00:04:23,450 --> 00:04:25,795
非特異であると仮定しました つまり

80
00:04:25,795 --> 00:04:29,890
特徴行列Xのすべての列が
線形的に独立しています

81
00:04:29,890 --> 00:04:32,120
でも 実世界のデータセットでは

82
00:04:32,120 --> 00:04:35,320
重複データ または
ほぼ重複するデータがあります

83
00:04:35,320 --> 00:04:38,030
同じお客さんが同じ製品を買います

84
00:04:38,030 --> 00:04:41,795
日の出の写真を
数秒間隔で2枚撮ることがあります

85
00:04:41,795 --> 00:04:45,630
たとえ総合行列が線形的に独立でも

86
00:04:45,630 --> 00:04:48,135
依然として悪条件かもしれず

87
00:04:48,135 --> 00:04:50,340
計算上は特異であっても

88
00:04:50,340 --> 00:04:53,075
引き続き問題が生じる可能性があります

89
00:04:53,075 --> 00:04:57,260
また逆行列には
単純なアルゴリズムを使った場合

90
00:04:57,260 --> 00:05:00,450
ON三乗の計算時間のコストがあります

91
00:05:00,450 --> 00:05:04,285
手が込んだアルゴリズムを使っても
あまり改善されません

92
00:05:04,285 --> 00:05:07,270
さらに独特の数値上の問題があります

93
00:05:07,270 --> 00:05:10,900
これは総合行列を作成するための
乗算でも同じです

94
00:05:10,900 --> 00:05:12,925
代わりに いわゆる「コレスキー」

95
00:05:12,925 --> 00:05:14,980
またはQR分解を使って

96
00:05:14,980 --> 00:05:17,165
正規方程式を解くことができます

97
00:05:17,165 --> 00:05:21,710
ON三乗では またはON2.5乗の場合でさえ

98
00:05:21,710 --> 00:05:24,840
Nが10,000以上になると

99
00:05:24,840 --> 00:05:27,280
アルゴリズムが低速になることがあります

100
00:05:27,280 --> 00:05:31,890
正規方程式を使って
重みを正確に解くことはできます

101
00:05:31,890 --> 00:05:34,890
でも データやモデルにかなり左右されます

102
00:05:34,890 --> 00:05:39,335
線形代数では行列アルゴリズムがモデルです

103
00:05:39,335 --> 00:05:40,675
幸いにも

104
00:05:40,675 --> 00:05:43,905
最急降下最適化アルゴリズムがあります

105
00:05:43,905 --> 00:05:47,920
これは第一に 時間とメモリの点でコストが低く

106
00:05:47,920 --> 00:05:50,775
第二に 軽度の一般化に適していて

107
00:05:50,775 --> 00:05:54,250
第三に 汎用的で ほとんどの問題に使えます

108
00:05:54,400 --> 00:05:56,105
代わりに

109
00:05:56,105 --> 00:05:58,230
最急降下では損失関数

110
00:05:58,230 --> 00:06:00,685
つまり目的関数があります

111
00:06:00,685 --> 00:06:03,550
これはモデルの重みでパラメータ化されます

112
00:06:03,550 --> 00:06:05,500
この空間には

113
00:06:05,500 --> 00:06:08,450
地上のような山と谷があります

114
00:06:08,450 --> 00:06:11,310
しかし 多くの機械学習問題では

115
00:06:11,310 --> 00:06:13,120
次元がもっと増えます

116
00:06:13,120 --> 00:06:15,800
私達の世界はわずか3Dです

117
00:06:15,800 --> 00:06:18,240
これは最急降下です つまり

118
00:06:18,240 --> 00:06:21,005
上昇での最大化ではなく

119
00:06:21,005 --> 00:06:23,465
勾配での最小化ですから

120
00:06:23,465 --> 00:06:25,510
最後の超平面を通り

121
00:06:25,510 --> 00:06:28,170
大域的な最小点を探します

122
00:06:28,170 --> 00:06:32,405
言い換えると
超平面のどこから出発するかにかかわらず

123
00:06:32,405 --> 00:06:35,205
最も低い谷を見つけようとします

124
00:06:35,205 --> 00:06:38,485
そのためには損失関数の勾配を見つけ

125
00:06:38,485 --> 00:06:42,065
それにハイパーパラメータ
つまり学習率を掛けて

126
00:06:42,065 --> 00:06:45,975
その値を現在の重みから差し引きます

127
00:06:45,975 --> 00:06:49,300
収束するまでこのプロセスを繰り返します

128
00:06:49,300 --> 00:06:53,055
最適な学習率を選んで
繰り返しを何度も待つことになるので

129
00:06:53,055 --> 00:06:55,585
むしろ正規方程式を使えるかもしれません

130
00:06:55,585 --> 00:06:57,380
ただし 特徴の数が少なく

131
00:06:57,380 --> 00:06:59,410
共線性の問題などがない場合です

132
00:06:59,410 --> 00:07:03,290
または運動量などの
最急降下オプティマイザを追加したり

133
00:07:03,290 --> 00:07:05,615
減衰学習率を使う場合です

134
00:07:05,615 --> 00:07:09,990
Korean降下について
次のモジュールで詳しく見ていきます

135
00:07:09,990 --> 00:07:14,835
最急降下の刻み幅を決めるのに役立つ
ハイパーパラメータは何ですか？

136
00:07:14,835 --> 00:07:17,005
それとともにハイパーサーバーで

137
00:07:17,005 --> 00:07:20,400
収束を早めることができるかもしれません

138
00:07:21,290 --> 00:07:24,155
正解は「学習率」です

139
00:07:24,155 --> 00:07:25,580
学習率と

140
00:07:25,580 --> 00:07:29,330
今後のモジュールで学ぶ
他のハイパーパラメータを使えば

141
00:07:29,330 --> 00:07:32,035
最急降下の刻み幅サイズを決定できます

142
00:07:32,035 --> 00:07:37,020
設定が低すぎると 最急降下の収束に
長い時間がかかります

143
00:07:37,020 --> 00:07:38,770
設定が高すぎると

144
00:07:38,770 --> 00:07:40,890
最急降下が分岐して

145
00:07:40,890 --> 00:07:43,560
損失がますます増える可能性があります

146
00:07:43,560 --> 00:07:47,600
他の3つの選択肢は
共線性と条件付けに関係しています

147
00:07:47,600 --> 00:07:50,123
最急降下では正規方程式のときのように

148
00:07:50,123 --> 00:07:52,388
これらを考慮しません