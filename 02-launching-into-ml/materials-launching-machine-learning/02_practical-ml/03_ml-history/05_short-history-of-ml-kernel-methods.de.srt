1
00:00:00,000 --> 00:00:01,740
Zu Beginn der 1990er-Jahre

2
00:00:01,740 --> 00:00:03,795
entstand das Feld der Kernelmethoden.

3
00:00:03,795 --> 00:00:06,490
Corinna Cortes, Google-Forschungsleiterin,

4
00:00:06,490 --> 00:00:08,185
gehört zu den Pionieren.

5
00:00:08,185 --> 00:00:13,250
Dieses Forschungsgebiet ermöglicht neue
interessante nicht lineare Modellklassen,

6
00:00:13,250 --> 00:00:17,625
vor allem nicht lineare SVMs,
Support Vector Machines,

7
00:00:17,625 --> 00:00:21,210
wobei es sich um
Maximum-Margin-Klassifikatoren handelt.

8
00:00:21,210 --> 00:00:23,370
Der Kern einer SVM ist im Wesentlichen

9
00:00:23,370 --> 00:00:27,520
eine nicht lineare Aktivierung und ein
Sigmoid-Output für einen maximalen Rand.

10
00:00:27,520 --> 00:00:30,730
Die logistische Regression wird zur
Bildung von Entscheidungsgrenzen

11
00:00:30,730 --> 00:00:35,965
zur Maximierung der Log-Wahrscheinlichkeit
von Klassifikationsproblemen verwendet.

12
00:00:35,965 --> 00:00:38,395
Im Fall einer linearen Entscheidungsgrenze

13
00:00:38,395 --> 00:00:40,920
will die logistische
Regression jeden Punkt mit seiner

14
00:00:40,920 --> 00:00:43,380
Klasse maximal
von der Hyperebene entfernt haben und

15
00:00:43,380 --> 00:00:48,910
liefert eine Wahrscheinlichkeit, die als
Prognosesicherheit interpretierbar ist.

16
00:00:48,910 --> 00:00:52,280
Es können unendlich
viele Hyperebenen zwischen

17
00:00:52,280 --> 00:00:54,660
zwei linear trennbaren Klassen
erzeugt werden, etwa

18
00:00:54,660 --> 00:00:58,095
die zwei als gestrichelte Linien
dargestellten in diesen zwei Grafiken.

19
00:00:58,095 --> 00:01:02,490
Bei SVMs setzen wir zwei parallele
Hyperebenen auf jede Seite der

20
00:01:02,490 --> 00:01:04,980
Entscheidungsgrenze, wo sie sich mit

21
00:01:04,980 --> 00:01:08,040
dem nächsten Datenpunkt auf
jeder Seite der Hyperebene kreuzen.

22
00:01:08,040 --> 00:01:10,480
Das sind die Stützvektoren.

23
00:01:10,480 --> 00:01:14,070
Der Abstand zwischen den
beiden Stützvektoren ist die Marge.

24
00:01:14,070 --> 00:01:18,530
Links haben wir eine vertikale Hyperebene,
die tatsächlich die beiden Klassen trennt.

25
00:01:18,530 --> 00:01:22,125
Aber die Marge zwischen den
beiden Stützvektoren ist klein.

26
00:01:22,125 --> 00:01:24,240
Wenn wir eine andere Hyperebene wählen,

27
00:01:24,240 --> 00:01:25,460
etwa die rechts,

28
00:01:25,460 --> 00:01:28,105
ist die Marge viel breiter.

29
00:01:28,105 --> 00:01:32,210
Je breiter die Marge, desto mehr kann man
die Entscheidungsgrenze verallgemeinern,

30
00:01:32,210 --> 00:01:34,975
was zu einer besseren
Leistung führen sollte.

31
00:01:34,975 --> 00:01:39,410
Deshalb zielen SMV-Klassifikatoren
auf maximale Ränder zwischen den

32
00:01:39,410 --> 00:01:42,690
zwei Stützvektoren ab, und
verwenden dazu eine Hinge-Verlustfunktion,

33
00:01:42,690 --> 00:01:46,345
verglichen mit der Minimierung von
Kreuzentropie bei logistischer Regression.

34
00:01:46,345 --> 00:01:48,850
Wie Sie sehen, habe ich nur zwei Klassen,

35
00:01:48,850 --> 00:01:51,350
weshalb dies ein
binäres Klassifikationsproblem ist.

36
00:01:51,350 --> 00:01:53,620
Das Label der 
einen Klasse erhält den Wert 1

37
00:01:53,620 --> 00:01:57,580
und das Label der
anderen Klasse den Wert -1.

38
00:01:57,580 --> 00:01:59,850
Bei mehr als zwei Klassen

39
00:01:59,850 --> 00:02:02,770
sollte ein One-vs-all-Ansatz und dann die

40
00:02:02,770 --> 00:02:05,780
beste der permutierten binären
Klassifikationen gewählt werden.

41
00:02:05,780 --> 00:02:12,750
Was passiert, wenn die Daten nicht linear
in die zwei Klassen aufteilbar sind?

42
00:02:12,750 --> 00:02:16,380
Hier können wir eine Kernel-Transformation
anwenden, die unsere Daten aus dem

43
00:02:16,380 --> 00:02:19,100
Bereich des Input-Vektors
in einen Vektorbereich abbildet,

44
00:02:19,100 --> 00:02:22,540
der jetzt Merkmale hat, die wie zu sehen
linear aufgeteilt werden können.

45
00:02:22,540 --> 00:02:25,690
Genau wie vor dem Aufkommen
der tiefen neuronalen Netze

46
00:02:25,690 --> 00:02:29,750
kostete es viel Zeit und Arbeit,
die Rohdatendarstellung durch eine

47
00:02:29,750 --> 00:02:34,380
feine, benutzerdefinierte Merkmalskarte
als Merkmalsvektor abzubilden.

48
00:02:34,380 --> 00:02:36,520
Bei Kernelverfahren aber

49
00:02:36,520 --> 00:02:39,335
ist nur der Kernel benutzerdefiniert,

50
00:02:39,335 --> 00:02:44,285
durch eine Ähnlichkeitsfunktion zwischen
Punktpaaren in der Rohdatendarstellung.

51
00:02:44,285 --> 00:02:46,840
Eine Kernel-Transformation
ist vergleichbar damit,

52
00:02:46,840 --> 00:02:49,300
wie eine Aktivierungsfunktion
in neuronalen Netzen

53
00:02:49,300 --> 00:02:52,290
die Eingabe der Funktion abbildet,
um einen Raum zu überführen.

54
00:02:52,290 --> 00:02:55,350
Die Anzahl der Neuronen
in der Schicht bedingt die Dimension.

55
00:02:55,350 --> 00:02:58,055
Haben wir also zwei
Inputs und drei Neuronen,

56
00:02:58,055 --> 00:03:01,755
bilden wir den Input-2D-Raum
in einem 3D-Raum ab.

57
00:03:01,755 --> 00:03:06,040
Es gibt viele Arten von Kernels,
angefangen beim einfachen linearen Kernel,

58
00:03:06,040 --> 00:03:10,710
dem Polynom-Kernel und
dem Gaußschen RBF-Kernel.

59
00:03:10,710 --> 00:03:13,350
Wenn unser binärer
Klassifikator den Kernel verwendet,

60
00:03:13,350 --> 00:03:16,175
berechnet er eine 
gewichtete Summe der Ähnlichkeiten.

61
00:03:16,175 --> 00:03:19,965
Wann also sollte eine SVM anstelle von
logistischer Regression verwendet werden?

62
00:03:19,965 --> 00:03:24,870
Kernelisierte SVMs tendieren zu dünneren
Lösungen und sind besser skalierbar.

63
00:03:24,870 --> 00:03:27,830
Die SVM-Leistung ist besser,
wenn es viele Dimensionen gibt

64
00:03:27,830 --> 00:03:31,545
und die Prädiktoren die Antwort
nahezu sicher voraussagen.

65
00:03:31,545 --> 00:03:37,365
SVMs bilden Inputs mit Kernels in einem
höherdimensionalen Merkmalsraum ab.

66
00:03:37,365 --> 00:03:43,070
Welcher Aspekt neuronaler Netze überführt
auch in höherdimensionale Vektorräume?

67
00:03:43,070 --> 00:03:45,380
Die richtige Antwort ist:

68
00:03:45,380 --> 00:03:47,195
mehr Neuronen pro Schicht.

69
00:03:47,195 --> 00:03:49,180
Die Neuronenanzahl pro Schicht bestimmt,

70
00:03:49,180 --> 00:03:51,610
in welcher
Vektorraumdimension wir uns befinden.

71
00:03:51,610 --> 00:03:53,710
Wenn ich mit
drei Eingangsmerkmalen beginne,

72
00:03:53,710 --> 00:03:55,975
befinde ich mich im Vektorraum R3.

73
00:03:55,975 --> 00:03:57,830
Selbst bei Hunderten Schichten,

74
00:03:57,830 --> 00:03:59,250
aber nur je drei Neuronen,

75
00:03:59,250 --> 00:04:04,300
befinde ich mich im Vektorraum R3
und ändere nur die Basis.

76
00:04:04,300 --> 00:04:08,495
Bei Verwendung eines
Gaußschen RBF-Kernels mit SVMs

77
00:04:08,495 --> 00:04:11,425
wird der Input-Raum in
unendliche Dimensionen überführt.

78
00:04:11,425 --> 00:04:13,660
Die Aktivierungsfunktion
ändert die Basis des

79
00:04:13,660 --> 00:04:16,820
Vektorraums, fügt aber Dimensionen
weder hinzu noch verringert sie.

80
00:04:16,820 --> 00:04:20,345
Man muss es sich einfach wie ein
Drehen, Dehnen und Drücken vorstellen.

81
00:04:20,345 --> 00:04:21,750
Sie können nicht linear sein,

82
00:04:21,750 --> 00:04:24,600
aber wir bleiben in
demselben Vektorraum wie zuvor.

83
00:04:24,600 --> 00:04:28,550
Die Verlustfunktion ist das Objekt,
das wir zu minimieren versuchen,

84
00:04:28,550 --> 00:04:32,910
ein Skalar, mit dessen Gradient die
Parametergewichtungen aktualisiert werden.

85
00:04:32,910 --> 00:04:36,895
Dies ändert nur, wie viel
wir drehen, dehnen und drücken, aber

86
00:04:36,895 --> 00:04:38,570
nicht die Anzahl der Dimensionen.