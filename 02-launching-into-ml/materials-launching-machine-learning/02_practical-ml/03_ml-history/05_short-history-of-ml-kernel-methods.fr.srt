1
00:00:00,440 --> 00:00:03,790
Les méthodes du noyau
sont apparues dans les années 90.

2
00:00:04,145 --> 00:00:06,490
Corinna Cortes,
directrice de la recherche Google,

3
00:00:06,490 --> 00:00:07,935
a été l'une des pionnières.

4
00:00:08,335 --> 00:00:13,250
Ce domaine permet l'utilisation de classes
intéressantes de modèles non linéaires,

5
00:00:13,250 --> 00:00:17,625
principalement des SVM, ou machines
à vecteur de support, non linéaires,

6
00:00:17,625 --> 00:00:21,150
qui sont des classifieurs à vaste marge
que vous connaissez peut-être déjà.

7
00:00:21,660 --> 00:00:24,460
En substance, une SVM comprend
une activation non linéaire

8
00:00:24,460 --> 00:00:27,220
et une sortie sigmoïde
pour les vastes marges.

9
00:00:27,870 --> 00:00:32,140
Vous savez que la régression logistique
permet de créer une frontière de décision

10
00:00:32,140 --> 00:00:35,635
pour maximiser les probabilités
de déclassification.

11
00:00:36,025 --> 00:00:38,395
Dans le cas d'une frontière
de décision linéaire,

12
00:00:38,395 --> 00:00:41,970
la régression logistique doit avoir
les points et les classes associées

13
00:00:41,970 --> 00:00:44,285
aussi éloignés que possible
de l'hyperplan,

14
00:00:44,370 --> 00:00:48,250
et fournir une probabilité servant
d'indice de confiance de la prédiction.

15
00:00:49,450 --> 00:00:51,980
Vous pouvez créer de nombreux hyperplans

16
00:00:51,980 --> 00:00:54,660
entre deux classes
linéairement séparables,

17
00:00:54,660 --> 00:00:58,025
comme ceux présentés en pointillés
dans ces deux figures.

18
00:00:58,485 --> 00:01:01,760
Avec les SVM, deux hyperplans
parallèles sont ajoutés

19
00:01:01,760 --> 00:01:04,770
de chaque côté de l'hyperplan
de la frontière de décision,

20
00:01:04,770 --> 00:01:08,040
et croisent le point de données
le plus proche de chaque côté.

21
00:01:08,040 --> 00:01:10,230
Il s'agit des vecteurs de support.

22
00:01:10,670 --> 00:01:13,940
La distance entre les deux vecteurs
correspond à la marge.

23
00:01:14,360 --> 00:01:18,470
À gauche, l'hyperplan vertical
sépare les deux classes.

24
00:01:18,530 --> 00:01:21,955
Cependant, la marge entre
les deux vecteurs de support est faible.

25
00:01:22,425 --> 00:01:24,240
En optant pour un hyperplan différent,

26
00:01:24,240 --> 00:01:25,460
tel que celui de droite,

27
00:01:25,460 --> 00:01:27,955
vous obtenez une marge
beaucoup plus vaste.

28
00:01:28,105 --> 00:01:32,360
Plus la marge est vaste, plus la frontière
de décision est généralisable,

29
00:01:32,360 --> 00:01:34,745
ce qui vous permet de mieux
exploiter vos données.

30
00:01:35,395 --> 00:01:38,990
Par conséquent, les SVM cherchent
à maximiser la marge

31
00:01:38,990 --> 00:01:42,610
entre les deux vecteurs de support
à l'aide d'une fonction de marge maximale

32
00:01:42,610 --> 00:01:46,135
comparés à la minimisation par régression
logistique de l'entropie croisée.

33
00:01:46,465 --> 00:01:48,850
Notre exemple ne comporte que
deux classes,

34
00:01:48,850 --> 00:01:51,270
on a donc un problème
de classification binaire.

35
00:01:51,520 --> 00:01:53,620
Le label 1 est attribué
à l'une des classes,

36
00:01:53,620 --> 00:01:57,360
et le label -1 est attribué à l'autre.

37
00:01:58,010 --> 00:01:59,850
S'il y a plus de deux classes,

38
00:01:59,850 --> 00:02:02,670
utilisez une méthode "one-vs-all",
ou "une contre toutes",

39
00:02:02,670 --> 00:02:06,480
et choisissez la meilleure
des classifications binaires obtenues.

40
00:02:07,040 --> 00:02:11,840
Que faire si les données ne sont pas
linéairement séparables en deux classes ?

41
00:02:12,100 --> 00:02:16,480
Vous pouvez appliquer une transformation
de noyau pour transposer les données

42
00:02:16,480 --> 00:02:18,770
de votre espace vectoriel d'entrée
dans un espace

43
00:02:18,770 --> 00:02:22,300
où elles peuvent être séparées
linéairement, comme dans ce diagramme.

44
00:02:22,870 --> 00:02:25,930
Grâce au développement des réseaux
de neurones profonds et

45
00:02:25,930 --> 00:02:30,050
à beaucoup de travail, la représentation
brute des données est transformée

46
00:02:30,050 --> 00:02:34,190
en un espace vectoriel via une fonction
de projection créée par l'utilisateur.

47
00:02:34,680 --> 00:02:36,520
Mais, avec les méthodes de noyaux,

48
00:02:36,520 --> 00:02:39,335
l'utilisateur définit uniquement le noyau,

49
00:02:39,335 --> 00:02:43,565
la fonction de similarité entre les points
dans la représentation brute des données.

50
00:02:44,845 --> 00:02:46,810
Une transformation de noyau est semblable

51
00:02:46,810 --> 00:02:49,050
aux fonctions d'activation
d'un réseau neuronal

52
00:02:49,050 --> 00:02:52,000
qui associent l'entrée à la fonction
pour transformer l'espace.

53
00:02:52,530 --> 00:02:55,350
Le nombre de neurones
dans la couche contrôle la dimension.

54
00:02:55,450 --> 00:02:58,055
Si vous avez deux entrées
et trois neurones,

55
00:02:58,055 --> 00:03:01,285
vous transformez l'espace 2D d'entrée
en espace 3D.

56
00:03:01,985 --> 00:03:06,040
Il existe de nombreux types de noyaux,
les plus basiques étant le noyau linéaire,

57
00:03:06,040 --> 00:03:10,360
le noyau polynomial
et le noyau gaussien radial.

58
00:03:11,030 --> 00:03:13,490
Quand le classifieur
binaire utilise le noyau,

59
00:03:13,490 --> 00:03:16,065
il calcule typiquement
certaines similarités attendues.

60
00:03:16,415 --> 00:03:19,135
Quand utiliser une SVM
pour la discrimination ?

61
00:03:19,965 --> 00:03:24,650
Les SVM à noyau offrent une solution
parcimonieuse et donc plus d'évolutivité.

62
00:03:24,980 --> 00:03:28,240
Elles fonctionnent mieux
si le nombre de dimensions est élevé,

63
00:03:28,240 --> 00:03:31,425
et quand les prédicteurs prédisent
la réponse avec quasi-certitude.

64
00:03:32,105 --> 00:03:37,075
Les SVM à noyaux transposent l'entrée en
espace d'attributs à dimension supérieure.

65
00:03:37,695 --> 00:03:42,100
Dans les réseaux de neurones, qu'est-ce
qui permet aussi cette transposition ?

66
00:03:44,170 --> 00:03:45,380
La bonne réponse est :

67
00:03:45,380 --> 00:03:46,985
"Plus de neurones par couche".

68
00:03:47,465 --> 00:03:49,000
Le nombre de neurones par couche

69
00:03:49,000 --> 00:03:51,620
détermine le nombre
de dimensions de l'espace vectoriel.

70
00:03:51,750 --> 00:03:53,540
Si j'ai trois attributs d'entrée,

71
00:03:53,540 --> 00:03:55,845
j'ai un espace vectoriel R3.

72
00:03:56,025 --> 00:03:59,240
Même si j'ai des centaines de couches
avec trois neurones chacune,

73
00:03:59,250 --> 00:04:04,130
j'ai toujours un espace vectoriel R3,
et je ne change que la base.

74
00:04:04,450 --> 00:04:08,415
Par exemple, avec une SVM
à noyau gaussien RBF,

75
00:04:08,415 --> 00:04:11,315
l'espace d'entrée est transposé
en espace à dimension infinie.

76
00:04:11,595 --> 00:04:14,640
La fonction d'activation change
la base de l'espace vectoriel,

77
00:04:14,640 --> 00:04:16,820
mais n'ajoute ou ne supprime
aucune dimension.

78
00:04:17,100 --> 00:04:20,195
Ce sont des sortes de rotation,
d'étirement ou de compression.

79
00:04:20,345 --> 00:04:21,920
Les points ne sont pas linaires,

80
00:04:21,920 --> 00:04:24,180
mais l'espace vectoriel reste le même.

81
00:04:25,010 --> 00:04:28,550
La fonction de perte est l'objectif :
vous cherchez à minimiser.

82
00:04:28,700 --> 00:04:32,730
Ce scalaire met à jour avec son gradient
les pondérations de paramètres du modèle.

83
00:04:33,380 --> 00:04:37,205
Elle change juste le niveau de rotation,
d'étirement et de compression,

84
00:04:37,205 --> 00:04:38,570
pas le nombre de dimensions.