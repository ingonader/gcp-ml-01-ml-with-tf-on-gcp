A partir da década de 1990, foi formado o campo dos métodos de kernel. Corinna Cortes,
diretora do Google Research, foi uma das pioneiras. Esse campo de estudo introduz classes
interessantes de modelos não lineares, principalmente máquinas de vetores
de suporte, ou SVMs, não lineares, que são classificadores de margem
máxima que você talvez já conheça. Ativação não
linear e saída sigmoide para margens máximas são
essenciais para uma SVM. Anteriormente, vimos como
a regressão logística é usada para criar um limite de decisão
e maximizar a plausibilidade logarítmica das probabilidades de classificação. No caso do limite de decisão linear, a regressão logística requer cada ponto e as classes associadas o
mais longe possível do hiperplano e fornece uma probabilidade que pode ser
interpretada como confiança de previsão. Podemos criar
um número infinito de hiperplanos entre duas classes separáveis linearmente, como os hiperplanos representados
por linhas pontilhadas nas imagens. Nas SVMs, incluímos dois
hiperplanos paralelos em cada lado do hiperplano do limite de decisão, na interseção com o ponto de
dados mais próximo em cada lado. Esses são os vetores de suporte. A distância entre os dois
vetores de suporte é a margem. À esquerda, temos um hiperplano
vertical que separa as duas classes. Entretanto, a margem entre
os dois vetores de suporte é pequena. Se escolhermos um hiperplano diferente, como o da direita, teremos uma margem muito maior. Quanto maior a margem,
mais generalizável o limite de decisão, o que resultará em um
desempenho melhor com dados novos. Portanto, os classificadores
de SVM buscam maximizar a margem entre dois vetores de suporte,
usando uma função de perda de articulação, comparado à minimização da
regressão logística da entropia cruzada. Você deve ter notado
que há apenas duas classes, ou seja, é um
problema de classificação binária. Um dos rótulos das classes tem valor um e o outro rótulo tem valor de menos um. Se houver mais de duas classes, adote a abordagem de um x todos e escolha a melhor das classificações
binárias desativadas anteriormente. E quando os dados não podem ser
separados linearmente em duas classes? A boa notícia é que podemos
aplicar uma transformação de kernel, que mapeia dados do
espaço dos vetores de entrada para um espaço
de vetores com características que podem ser separadas linearmente,
como no diagrama. Assim como antes da ascensão
das redes neurais profundas, o usuário gastava muito tempo e trabalho
para transformar a representação bruta dos dados no vetor de característica, criando um mapa
que exigia muitos ajustes. Mas com os métodos de kernel, o único item definido
pelo usuário é o kernel, uma função de similaridade entre
pares de pontos na representação bruta. Uma transformação de kernel é semelhante a como uma função de ativação
nas redes mapeia a entrada para a função, para transformar o espaço. O número de neurônios
na camada controla a dimensão. Então, se tivermos
duas entradas e três neurônios, um espaço de entradas bidimensional
será mapeado para um tridimensional. Há muitos tipos de kernel,
os mais básicos são o linear básico, o polinomial e o
de função de base radial gaussiana. Quando o
classificador binário usa o kernel, ele normalmente calcula
a soma ponderada das similaridades. Então, quando devemos usar uma SVM? As SVMs com kernel tendem
a fornecer soluções mais esparsas e por isso têm escalabilidade melhor. As SVMs têm melhor desempenho
quando há um número alto de dimensões e os preditores preveem a resposta
quase com certeza absoluta. Vimos como as SVMs usam
kernels para mapear as entradas para um espaço com mais dimensões. Essas redes também mapeiam para
um espaço de vetores com mais dimensões? A resposta correta é
mais neurônios por camada. É o número de neurônios por camada que determina as
dimensões do espaço de vetores. Começando com 3
características de entrada, temos um espaço de vetores R³. Mesmo que haja uma centena de camadas, cada uma com três neurônios, ainda teremos um espaço de vetores R³
e apenas a base será diferente. Quando usamos um kernel de função
de base radial gaussiana com SVMs, o espaço de entradas é
mapeado para dimensões infinitas. A função de ativação altera
a base do espaço de vetores, mas não adiciona nem subtrai dimensões. Pense nisso simplesmente
como rotações, extensões e contrações. Elas podem ser não lineares, mas o espaço de vetores continua o mesmo. A função de perda é o que
estamos tentando minimizar. Um escalar usando gradiente para atualizar
os pesos dos parâmetros do modelo. Ela altera apenas o quanto
rotacionamos, estendemos e contraímos, não o número de dimensões.