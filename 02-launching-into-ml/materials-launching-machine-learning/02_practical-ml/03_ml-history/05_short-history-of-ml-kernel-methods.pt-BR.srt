1
00:00:00,000 --> 00:00:01,740
A partir da década de 1990,

2
00:00:01,740 --> 00:00:03,795
foi formado o campo dos métodos de kernel.

3
00:00:03,795 --> 00:00:06,490
Corinna Cortes,
diretora do Google Research,

4
00:00:06,490 --> 00:00:08,185
foi uma das pioneiras.

5
00:00:08,185 --> 00:00:13,250
Esse campo de estudo introduz classes
interessantes de modelos não lineares,

6
00:00:13,250 --> 00:00:17,625
principalmente máquinas de vetores
de suporte, ou SVMs, não lineares,

7
00:00:17,625 --> 00:00:21,210
que são classificadores de margem
máxima que você talvez já conheça.

8
00:00:21,210 --> 00:00:23,370
Ativação não
linear e saída sigmoide

9
00:00:23,370 --> 00:00:27,560
para margens máximas são
essenciais para uma SVM.

10
00:00:27,560 --> 00:00:30,620
Anteriormente, vimos como
a regressão logística é usada

11
00:00:30,620 --> 00:00:34,035
para criar um limite de decisão
e maximizar a plausibilidade logarítmica

12
00:00:34,035 --> 00:00:35,965
das probabilidades de classificação.

13
00:00:35,965 --> 00:00:38,395
No caso do limite de decisão linear,

14
00:00:38,395 --> 00:00:40,920
a regressão logística requer cada ponto

15
00:00:40,920 --> 00:00:43,900
e as classes associadas o
mais longe possível do hiperplano

16
00:00:43,910 --> 00:00:48,910
e fornece uma probabilidade que pode ser
interpretada como confiança de previsão.

17
00:00:48,910 --> 00:00:52,280
Podemos criar
um número infinito de hiperplanos

18
00:00:52,280 --> 00:00:54,660
entre duas classes separáveis linearmente,

19
00:00:54,660 --> 00:00:58,095
como os hiperplanos representados
por linhas pontilhadas nas imagens.

20
00:00:58,095 --> 00:01:02,490
Nas SVMs, incluímos dois
hiperplanos paralelos em cada lado

21
00:01:02,490 --> 00:01:04,980
do hiperplano do limite de decisão,

22
00:01:04,980 --> 00:01:08,040
na interseção com o ponto de
dados mais próximo em cada lado.

23
00:01:08,040 --> 00:01:10,480
Esses são os vetores de suporte.

24
00:01:10,480 --> 00:01:14,070
A distância entre os dois
vetores de suporte é a margem.

25
00:01:14,070 --> 00:01:18,530
À esquerda, temos um hiperplano
vertical que separa as duas classes.

26
00:01:18,530 --> 00:01:22,125
Entretanto, a margem entre
os dois vetores de suporte é pequena.

27
00:01:22,125 --> 00:01:24,240
Se escolhermos um hiperplano diferente,

28
00:01:24,240 --> 00:01:25,460
como o da direita,

29
00:01:25,460 --> 00:01:28,105
teremos uma margem muito maior.

30
00:01:28,105 --> 00:01:32,360
Quanto maior a margem,
mais generalizável o limite de decisão,

31
00:01:32,360 --> 00:01:34,975
o que resultará em um
desempenho melhor com dados novos.

32
00:01:34,975 --> 00:01:39,280
Portanto, os classificadores
de SVM buscam maximizar a margem

33
00:01:39,280 --> 00:01:42,790
entre dois vetores de suporte,
usando uma função de perda de articulação,

34
00:01:42,790 --> 00:01:46,055
comparado à minimização da
regressão logística da entropia cruzada.

35
00:01:46,055 --> 00:01:48,850
Você deve ter notado
que há apenas duas classes,

36
00:01:48,850 --> 00:01:51,350
ou seja, é um
problema de classificação binária.

37
00:01:51,350 --> 00:01:54,070
Um dos rótulos das classes tem valor um

38
00:01:54,070 --> 00:01:57,580
e o outro rótulo tem valor de menos um.

39
00:01:57,580 --> 00:01:59,850
Se houver mais de duas classes,

40
00:01:59,850 --> 00:02:02,770
adote a abordagem de um x todos

41
00:02:02,770 --> 00:02:06,730
e escolha a melhor das classificações
binárias desativadas anteriormente.

42
00:02:06,730 --> 00:02:12,100
E quando os dados não podem ser
separados linearmente em duas classes?

43
00:02:12,100 --> 00:02:15,470
A boa notícia é que podemos
aplicar uma transformação de kernel,

44
00:02:15,470 --> 00:02:17,810
que mapeia dados do
espaço dos vetores de entrada

45
00:02:17,810 --> 00:02:20,000
para um espaço
de vetores com características

46
00:02:20,000 --> 00:02:22,540
que podem ser separadas linearmente,
como no diagrama.

47
00:02:22,540 --> 00:02:25,690
Assim como antes da ascensão
das redes neurais profundas,

48
00:02:25,690 --> 00:02:29,750
o usuário gastava muito tempo e trabalho
para transformar a representação bruta

49
00:02:29,750 --> 00:02:31,620
dos dados no vetor de característica,

50
00:02:31,620 --> 00:02:34,380
criando um mapa
que exigia muitos ajustes.

51
00:02:34,380 --> 00:02:36,520
Mas com os métodos de kernel,

52
00:02:36,520 --> 00:02:39,335
o único item definido
pelo usuário é o kernel,

53
00:02:39,335 --> 00:02:44,285
uma função de similaridade entre
pares de pontos na representação bruta.

54
00:02:44,285 --> 00:02:46,840
Uma transformação de kernel é semelhante

55
00:02:46,840 --> 00:02:49,750
a como uma função de ativação
nas redes mapeia a entrada

56
00:02:49,750 --> 00:02:52,200
para a função, para transformar o espaço.

57
00:02:52,200 --> 00:02:55,350
O número de neurônios
na camada controla a dimensão.

58
00:02:55,350 --> 00:02:58,055
Então, se tivermos
duas entradas e três neurônios,

59
00:02:58,055 --> 00:03:01,755
um espaço de entradas bidimensional
será mapeado para um tridimensional.

60
00:03:01,755 --> 00:03:06,040
Há muitos tipos de kernel,
os mais básicos são o linear básico,

61
00:03:06,040 --> 00:03:10,710
o polinomial e o
de função de base radial gaussiana.

62
00:03:10,710 --> 00:03:13,350
Quando o
classificador binário usa o kernel,

63
00:03:13,350 --> 00:03:16,175
ele normalmente calcula
a soma ponderada das similaridades.

64
00:03:16,175 --> 00:03:19,635
Então, quando devemos usar uma SVM?

65
00:03:19,635 --> 00:03:22,880
As SVMs com kernel tendem
a fornecer soluções mais esparsas

66
00:03:22,880 --> 00:03:24,870
e por isso têm escalabilidade melhor.

67
00:03:24,870 --> 00:03:28,280
As SVMs têm melhor desempenho
quando há um número alto de dimensões

68
00:03:28,280 --> 00:03:31,545
e os preditores preveem a resposta
quase com certeza absoluta.

69
00:03:31,545 --> 00:03:35,275
Vimos como as SVMs usam
kernels para mapear as entradas

70
00:03:35,275 --> 00:03:37,365
para um espaço com mais dimensões.

71
00:03:37,365 --> 00:03:42,480
Essas redes também mapeiam para
um espaço de vetores com mais dimensões?

72
00:03:44,140 --> 00:03:47,190
A resposta correta é
mais neurônios por camada.

73
00:03:47,195 --> 00:03:49,180
É o número de neurônios por camada

74
00:03:49,180 --> 00:03:51,610
que determina as
dimensões do espaço de vetores.

75
00:03:51,610 --> 00:03:53,650
Começando com 3
características de entrada,

76
00:03:53,650 --> 00:03:55,975
temos um espaço de vetores R³.

77
00:03:55,975 --> 00:03:57,830
Mesmo que haja uma centena de camadas,

78
00:03:57,830 --> 00:03:59,250
cada uma com três neurônios,

79
00:03:59,250 --> 00:04:04,300
ainda teremos um espaço de vetores R³
e apenas a base será diferente.

80
00:04:04,300 --> 00:04:08,495
Quando usamos um kernel de função
de base radial gaussiana com SVMs,

81
00:04:08,495 --> 00:04:11,425
o espaço de entradas é
mapeado para dimensões infinitas.

82
00:04:11,425 --> 00:04:14,760
A função de ativação altera
a base do espaço de vetores,

83
00:04:14,760 --> 00:04:16,820
mas não adiciona nem subtrai dimensões.

84
00:04:16,820 --> 00:04:20,345
Pense nisso simplesmente
como rotações, extensões e contrações.

85
00:04:20,345 --> 00:04:21,750
Elas podem ser não lineares,

86
00:04:21,750 --> 00:04:24,600
mas o espaço de vetores continua o mesmo.

87
00:04:24,600 --> 00:04:28,550
A função de perda é o que
estamos tentando minimizar.

88
00:04:28,550 --> 00:04:32,910
Um escalar usando gradiente para atualizar
os pesos dos parâmetros do modelo.

89
00:04:32,910 --> 00:04:37,265
Ela altera apenas o quanto
rotacionamos, estendemos e contraímos,

90
00:04:37,265 --> 00:04:38,570
não o número de dimensões.