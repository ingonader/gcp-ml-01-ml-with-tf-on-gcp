1
00:00:00,000 --> 00:00:01,740
1990年代に

2
00:00:01,740 --> 00:00:03,795
カーネル法が確立されました

3
00:00:03,795 --> 00:00:06,880
Google ResearchのCorinna Cortesなどが

4
00:00:06,880 --> 00:00:08,245
先駆者です

5
00:00:08,245 --> 00:00:13,250
この研究は新しい非線形モデルの
分野を切り開きました

6
00:00:13,250 --> 00:00:17,455
特にサポートベクターマシン（SVM）です

7
00:00:17,455 --> 00:00:21,230
これは最大マージン分類器としても
知られています

8
00:00:21,230 --> 00:00:23,280
SVMは基本的に

9
00:00:23,280 --> 00:00:27,560
非線形活性化にシグモイド出力を加えて
最大マージンを求めます

10
00:00:27,560 --> 00:00:31,590
すでにロジスティック回帰を使って
決定境界を作成し

11
00:00:31,590 --> 00:00:35,965
非分類確率の対数尤度を
最大化する方法を見ました

12
00:00:35,965 --> 00:00:38,965
ロジスティック回帰の線形の決定境界の例では

13
00:00:38,965 --> 00:00:41,290
それぞれの点とそのクラスを

14
00:00:41,290 --> 00:00:43,780
超平面からできるだけ遠く離して

15
00:00:43,780 --> 00:00:48,400
確率を出し それが予測信頼性となります

16
00:00:49,510 --> 00:00:52,010
直線で分かれた2つのクラスの間に

17
00:00:52,010 --> 00:00:54,780
無限の数の超平面を作成できます 
たとえば

18
00:00:54,780 --> 00:00:58,095
この2つの図で点線で示した超平面です

19
00:00:58,095 --> 00:01:01,350
SVMでは決定境界の超平面の両側に

20
00:01:01,350 --> 00:01:03,710
2つの平行な超平面を作り

21
00:01:03,710 --> 00:01:08,040
超平面のそれぞれの側の
最も近いデータ点と交差させます

22
00:01:08,040 --> 00:01:10,580
これらがサポートベクターです

23
00:01:10,580 --> 00:01:14,070
2つのベクター間の距離がマージンです

24
00:01:14,070 --> 00:01:18,410
左の図では垂直な超平面が
2つのクラスを分けています

25
00:01:18,410 --> 00:01:22,235
でも2つのサポートベクター間の
マージンは小さいです

26
00:01:22,235 --> 00:01:23,870
右の図のような

27
00:01:23,870 --> 00:01:25,550
別の超平面を選ぶと

28
00:01:25,550 --> 00:01:28,275
マージンがかなり大きくなります

29
00:01:28,275 --> 00:01:32,360
マージンが広いほど
決定境界を一般化しやすくなり

30
00:01:32,360 --> 00:01:35,035
データをより適切に解析できます

31
00:01:35,585 --> 00:01:39,520
このようにSVM分類器では
2つのサポートベクター間のマージンを

32
00:01:39,520 --> 00:01:42,430
ヒンジ損失関数を使って最大化します

33
00:01:42,430 --> 00:01:46,155
ロジスティック回帰の
交差エントロピー最小化とは対照的です

34
00:01:46,155 --> 00:01:48,680
ここではクラスが2つだけで

35
00:01:48,680 --> 00:01:51,350
バイナリ分類問題となっています

36
00:01:51,350 --> 00:01:54,000
1つのクラスのラベルの値は1

37
00:01:54,000 --> 00:01:57,580
もう1つのクラスのラベルの値は-1です

38
00:01:57,580 --> 00:02:00,210
クラスの数が2つより多い場合は

39
00:02:00,210 --> 00:02:02,380
一対多の手法で

40
00:02:02,380 --> 00:02:06,680
昇格バイナリ分類の中から
最適なものを選びます

41
00:02:07,310 --> 00:02:11,590
でも データを線形に
2クラスに分けられない場合は？

42
00:02:12,270 --> 00:02:14,150
カーネル変換を使えます

43
00:02:14,150 --> 00:02:15,310
この図のように

44
00:02:15,310 --> 00:02:19,380
入力ベクター空間のデータを
別のベクター空間にマップします

45
00:02:19,380 --> 00:02:22,540
後者では特徴を線形に分けることができます

46
00:02:22,540 --> 00:02:25,690
ディープニューラルネットワークの初期と同様

47
00:02:25,690 --> 00:02:29,670
調整されたユーザー作成の
特徴マップを使用して

48
00:02:29,670 --> 00:02:34,380
元のデータ表現を特徴ベクターに変換します

49
00:02:34,380 --> 00:02:39,460
しかしカーネル手法では
ユーザー定義できるのはカーネルだけです

50
00:02:39,460 --> 00:02:44,225
これは単に生のデータ表現の
2点間の類似関数にすぎません

51
00:02:45,005 --> 00:02:46,490
カーネル変換は

52
00:02:46,490 --> 00:02:49,120
ニューラルネットワークの活性化関数が

53
00:02:49,120 --> 00:02:52,300
入力を変換空間にマップするのに似ています

54
00:02:52,300 --> 00:02:55,230
層のニューロン数で次元が決まります

55
00:02:55,230 --> 00:02:58,055
たとえば
2つの入力と3つのニューロンであれば

56
00:02:58,055 --> 00:03:01,755
入力2D空間から3D空間へのマップになります

57
00:03:01,755 --> 00:03:05,860
さまざまなカーネルがあります
最も基本的な線形カーネル

58
00:03:05,860 --> 00:03:10,710
多項式カーネル
ガウス放射基底関数カーネルなどです

59
00:03:10,710 --> 00:03:13,350
バイナリ分類器でカーネルを使うとき

60
00:03:13,350 --> 00:03:16,205
通常は類似度を重み付きで合計します

61
00:03:16,205 --> 00:03:19,635
では どんな場合にSVMを使えますか？

62
00:03:19,635 --> 00:03:24,870
カーネルSVMの解はよりスパースで
スケーラビリティに優れています

63
00:03:24,870 --> 00:03:29,280
次元が多い場合や 予測変数による
応答の予測精度が非常に高い場合

64
00:03:29,280 --> 00:03:31,655
SVMは優れた効果を発揮します

65
00:03:32,485 --> 00:03:34,005
このようにSVMでは

66
00:03:34,005 --> 00:03:37,365
カーネルで入力を高次元の
特徴空間にマップしますが

67
00:03:37,365 --> 00:03:40,300
ニューラルネットワークではどんな場合に

68
00:03:40,300 --> 00:03:43,120
高次元ベクター空間にマップできますか

69
00:03:43,670 --> 00:03:47,190
正解は「層当たりのニューロン数が
多い場合」です

70
00:03:47,195 --> 00:03:48,990
層のニューロン数は

71
00:03:48,990 --> 00:03:51,690
ベクター空間の次元数を決定します

72
00:03:51,690 --> 00:03:55,745
入力特徴が3つの場合は
R3ベクター空間です

73
00:03:55,745 --> 00:03:57,630
たとえ層の数が100個でも

74
00:03:57,630 --> 00:03:59,950
それぞれのニューロンが3個だけなら

75
00:03:59,950 --> 00:04:04,300
やはりR3ベクター空間です
基盤を変更しているだけです

76
00:04:04,300 --> 00:04:08,495
たとえばSVMでカーネルのガウスRBを使う場合

77
00:04:08,495 --> 00:04:11,425
入力空間が無限次元にマップされます

78
00:04:11,425 --> 00:04:14,750
活性化関数はベクター空間の基盤を変えますが

79
00:04:14,750 --> 00:04:16,820
次元の数は変わりません

80
00:04:16,820 --> 00:04:20,415
単なる回転や伸縮と考えることができます

81
00:04:20,415 --> 00:04:22,360
非線形であるとしても

82
00:04:22,360 --> 00:04:24,600
前と同じベクター空間です

83
00:04:25,090 --> 00:04:28,370
損失関数は最小化の目的関数です

84
00:04:28,370 --> 00:04:32,910
勾配を使ってモデルパラメータの
重みを調整するスカラーです

85
00:04:32,910 --> 00:04:36,045
これは回転や伸縮を変えるだけで

86
00:04:36,045 --> 00:04:38,570
次元の数は変わりません