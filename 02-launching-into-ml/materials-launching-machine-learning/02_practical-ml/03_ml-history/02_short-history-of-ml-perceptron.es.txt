Eso fue todo sobre la regresión lineal con referencia
al aprendizaje de los datos. Hasta la década de los 40 cuando el investigador Frank Rosenblatt propuso el perceptrón
como modelo computacional de una neurona del cerebro humano y mostró
cómo puede aprender funciones simples. Es lo que llamamos hoy
un clasificador lineal binario en el que intentamos encontrar una línea
única que divida los datos en dos clases. Una sola capa de perceptrones
sería la red neuronal prealimentada más simple posible. Con entradas que alimentarían
un perceptrón de una sola capa y en el que se realizaría
una suma ponderada. Luego, esta suma pasaría
por lo que se conoce hoy como una función de activación,
que es una función matemática que se aplica a cada elemento
que reside en esa neurona. Recuerden que, en este punto,
aún es un clasificador lineal. La función de activación,
que es lineal en este caso simplemente muestra sus entradas. Si compararamos esta salida con un umbral, determinaríamos
a qué clase pertenece cada punto. Los errores se agregarían
y se usarían para cambiar los pesos mediante la suma,
y el proceso se repetiría una y otra vez hasta la convergencia. Si desean crear un modelo simple
que aprende de una salida deseada a partir de una cierta
distribución de entradas no necesitan ir muy lejos,
ya que nuestros cerebros lo hacen todo el día
para entender el mundo que nos rodea y todas las señales
que nuestros cuerpos reciben. Una de las unidades básicas
del cerebro es la neurona. Las redes neuronales
son simplemente grupos de neuronas conectadas mediante diferentes
patrones o arquitecturas. Una neurona biológica
tiene varios componentes especializados en pasar señales eléctricas
que nos permiten tener pensamientos, realizar acciones
y estudiar el fascinante mundo del aprendizaje automático. Las señales eléctricas
de otras neuronas, como las sensoriales en nuestras retinas,
se propagan de neurona a neurona. La señal de entrada se recibe
en un extremo de la neurona que está compuesto por dendritas. Es posible que estas dendritas
no recolecten señales eléctricas solo de otra neurona sino de varias,
que se suman en períodos de tiempo y cambian el potencial eléctrico
de la célula. Una neurona típica
tiene un potencial eléctrico de reposo de aprox. -70 milivoltios. A medida que el estímulo de la señal
que reciben las dendritas aumenta, tarde o temprano, alcanza un umbral
de aprox. -55 milivoltios. En ese caso,
ocurre una rápida despolarización del axón y varias compuertas de voltaje se abren
y permiten un flujo repentino de iones. Esto causa que la neurona
active un potencial de acción de corriente eléctrica en el axón,
con la ayuda de la vaina de mielina, para una mejor transmisión
a las terminales de los axones. Aquí, los neurotransmisores
se liberan en la sinapsis y luego viajan
por la hendidura sináptica, por lo general,
a las dendritas de otras neuronas. Algunos
de los neurotransmisores son excitatorios pues aumentan el potencial
de la siguiente célula mientras otros son inhibidores
y reducen el potencial. La neurona se repolariza
a un potencial aún menor que el reposo
durante un periodo refractario. Y el proceso continúa
en la siguiente neurona hasta que alcance una neurona motora
que mueva sus manos para que cubran sus ojos del sol. ¿Qué tiene que ver
toda esta biología y neurociencia con el aprendizaje automático? ¿Les parece familiar? Este es un perceptrón de una sola capa. Al igual que una neurona,
tiene entradas que luego multiplica por los pesos y lo suma todo. Este valor se compara con un umbral y se
transforma con una función de activación. Por ejemplo,
si la suma es mayor que o igual a cero, entonces activar
o pasar un valor igual a uno; de otro modo, no activar
o pasar un valor igual a cero. Las entradas y los pesos
actúan como los neurotransmisores de una neurona.
Algunos pueden ser positivos y sumar,
y otros pueden ser negativos y restar. La función de paso de la unidad
actúa como umbral de todo o nada. Si se alcanza el umbral, pasar la señal;
de otro modo, no pasar nada. Finalmente, hay una salida
y, como las neuronas biológicas, puede pasar como entrada
a otras neuronas en un perceptrón de varias capas.
Hablaremos de esto a continuación. Todo esto es genial. Sin embargo,
resulta que hay funciones muy simples que no puede aprender. Por ejemplo, la función XOR. Marvin Minsky, un famoso científico
de la computación de MIT, lo señaló y nadie quiso financiar
la IA durante 15 años. No es la primera vez
que las redes neuronales se toparon con un muro
y fueron olvidadas por un tiempo. ¿Qué componente de una neurona biológica
es análogo a la parte de las entradas que recibe un perceptrón? La respuesta correcta es
C. Las dendritas. Reciben estímulos de otras neuronas,
igual que una red neuronal artificial. A. El axón es incorrecto,
porque tiene más analogía con la salida de un perceptrón. B. El núcleo es incorrecto,
porque allí es donde se almacena el material genético de las células
y se controlan sus actividades. D. La vaina de mielina es incorrecta,
porque ayuda a la transmisión del axón que, de nuevo, está en la parte
de la salida del perceptrón.