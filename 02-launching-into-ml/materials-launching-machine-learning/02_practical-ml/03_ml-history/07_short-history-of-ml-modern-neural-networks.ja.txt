再びニューラルネットワークの時代が来ました 今回は処理能力の向上と膨大なデータによって 飛躍的に進歩しています DNNはコンピュータビジョンなどのテストで 他の手法より優れた性能を
発揮するようになりました ハードウェアの進歩 さらに
新しい技法とアーキテクチャのおかげで DNNのトレーニング性が改善されました たとえばReLU初期化処理の改善 畳込みニューラルネットワーク（CNN）
ドロップアウトなどです すでに他のML手法のところで
このいくつかに触れました 現在ではデフォルトでよく設定されるReLUなど 非線形活性化関数についても ニューラルネットワークの説明で
すでに述べました 一般化に役立つ
ドロップアウト層が使われ始めました アンサンブル手法で ランダムフォレストやブーストされたツリーに
関連して述べたとおりです 畳み込み層が追加され
処理能力とメモリの負荷が減りました なぜなら 接続性が非完全で
画像など局所に焦点を絞れるからです 画像の中の無関係なものを
比較することがありません 言い換えると 他のML手法における進歩が ニューラルネットワークに組み込まれたのです DNNの例を見てみましょう 機械学習の最高潮として 何百もの層と何百万もの
パラメータを含むニューラルネットワークで ディープラーニングにより
驚異的な結果が得られます この図はGoogLeNet またはInception つまり画像分類モデルです 2012年のデータを使って2014年に行った ImageNetの大規模な画像認識で
トレーニングされたものです 120万の画像からなる1000クラスで 画像を分類するトレーニングを行いました 22個のディープ層があります 今後のコースで見ていく
プーリングを含めると27個です さらに個々のブロックに分けると
層の数は100個です トレーニングパラメータは1100万個を超えます 完全に繋がった層もありますが 繋がってない層もあります たとえば畳込み層ですが
これは後でお話しします 一般化のためにドロップアウト層を使い DNNアンサンブルをシミュレートしました ニューラルネットワークと
スタッキングのように 各ボックスを単位とするボックスグループです 先程の拡大図で見ましたね こうしてブロックを積み重ねて
単なる合計以上のものを得ることが ディープラーニングを成功させた要因です もちろん データがますます豊富になり 処理能力とメモリ量が
向上していることも要因です 現在では 規模と正確さの点で これより優れたバージョンもあります 機械学習の歴史を通して 他のアルゴリズムの技法を部分的に再利用し それを組み合わせて
ますます強力なモデルができます 最も重要な点ですが 実験が可能になります DNNの作成で重要な点は何ですか？ 正解は「上記すべて」です これは完全なリストではありませんが 最初の3つは特に重要です まず大量のデータが必要です ディープラーニングのデータ量を
減らす研究が盛んですが その成果が出るまでは 大量のデータを確保する必要があります なぜなら 巨大なモデルの中には トレーニングするパラメータが多くあり
これが効果を高めるためです モデルがこれほど複雑なので データ分布をよく内在化させる必要があります ですから多くのシグナルが必要です 機械学習の要点は優秀なさまざまなモデルを トレーニングすること自体ではなく トレーニングにより
精度の高い予測をすることです 優れた一般化で新しいデータを予測すべきです さもないとモデルは無用です ですから十分な量のデータはとても重要です 巨大なデータを何度か学習する代わりに 小さなデータセットを何百万回も調べると 過剰適合してしまいます データが多いと 十分な
検証とテスト用のデータも得られます さらにドロップアウト層を追加したり
データを拡張したり ノイズを追加したりすると
一般化がさらに改善します 最後に機械学習では実験が重要です 実にさまざまなアルゴリズムや
ハイパーパラメータ MLデータセットの作成方法があります すべての問題に適した選択肢を 特定する方法はありません モデルを比較するために実験を重ねて 計測された性能を調べ続けることは
単に楽しいだけでなく 驚くほど強力なツールができる
きっかけにもなります 次に ニューラルネットワークが 過去のモデルを活用していることを
もっと見ていきます この表は過去のDNNの さまざまなモデルの性能を示しています 2014年に 大きく向上したことが 青で示されています Google Inceptionモデルが 誤差率を6.7%まで下げました DNNの性能は毎年上がり続け 以前のモデルから教訓を学んでいます 2015年にInceptionバージョン3が 誤差率3.5%を達成しました 短い期間にモデルが劇的に
進歩したのはなぜでしょうか ある研究グループが優れた技法を
新しく開発すると 他のグループが
そのアイデアを基に開発を進めます こうして実験が大きく前進し進歩が加速します たとえばハイパーパラメータの改善
層の追加 一般化可能性の改善 畳み込み層などの
サブコンポーネントの改善などです MLをどのような問題に
適用できるか説明してください この正解は1つでは ありません 「あなたは冬のスキー場の経営者です ゲレンデの利用量レベルを予測します その際 利用券を買った
客のタイプ（初心者 中級 上級 プロ）と それまでの積雪量を基に予測します」 答えを書いてみてください この場合 回帰でも分類でも構いません 私は利用量レベルが何か
正確に言いませんでした 1時間あたりの
コース利用者数という意味でしょうか または高、中、低のような
カテゴリーでしょうか？ 最初に基礎ヒューリスティックを
述べるべきでしょう たとえば 各スロープの平均人数です その後で 回帰と分類のどちらを選ぶかに応じて それぞれ線形またはロジスティック回帰の
基礎モデルを考えます 性能とデータ量を考えて ニューラルネットワークにするでしょう データの中に他の特徴が存在するなら それも試して性能を観察します Googleでは現在 4000以上の
ディープMLモデルが稼働しており システムの強化に貢献しています それぞれのモデルとバージョンが 過去のモデルの成功と失敗に基づき
性能を向上します 最初によく使用していたのはSibylです これはYouTubeの関連ビデオを
おすすめするために作られました この推奨エンジンはとても優れていて 広告など他のGoogle機能にも
導入されるようになりました これは線形モデルです 今年は 別のモデルが普及し 他のモデルや
システムのパラメータ調整エンジンになりました Google ML研究部門であるGoogle Brainが 何千ものCPUの能力を活用して DNNのような大きなモデルの
トレーニング方法を考案しました このようなモデルを構築して稼働した経験に
基づきTensorFlowが作成されました ML用のオープンソースライブラリです 次にTFXまたはTensorFlowベースの
MLプラットフォームが作られました 後で見ていく
本稼働MLモデルの作成とデプロイでは TensorFlowとCloud ML Engine
DataflowさらにBigQueryを使います まとめると この数十年で ニューラルネットワークが広く導入され
性能が上がってきました データが広く得られるので モデルのトレーニングに使う例が
ますます豊富になってきました データと例が増えてきたことに加えて
数千の層からなる複雑な分散モデル向けの スケーラブルなインフラストラクチャが
実現しました 最後に1つ注意点をお伝えします ニューラルネットワークは
いくつかの分野で優れていますが これはさまざまな種類のモデルの
1つにすぎません 実験は重要です さまざまなモデルを試すことで 皆さんのデータで課題を解くことができます