1
00:00:00,000 --> 00:00:03,410
Let's now take a brief journey through the history of machine learning,

2
00:00:03,410 --> 00:00:05,740
to see how it has evolved over time into

3
00:00:05,740 --> 00:00:08,905
the deep learning neural networks that are so popular today.

4
00:00:08,905 --> 00:00:12,145
You'll notice, that despite neural networks coming

5
00:00:12,145 --> 00:00:15,895
in and out of style over the last several decades,

6
00:00:15,895 --> 00:00:19,395
that tricks and techniques developed for other algorithms,

7
00:00:19,395 --> 00:00:21,935
can be applied to deep learning neural networks,

8
00:00:21,935 --> 00:00:23,690
making them very powerful.

9
00:00:23,690 --> 00:00:28,030
Linear regression was invented for predicting the movement of planets,

10
00:00:28,030 --> 00:00:31,280
and the size of pea pods based on their appearance.

11
00:00:31,280 --> 00:00:34,470
Sir Francis Galton, pioneered the use of

12
00:00:34,470 --> 00:00:38,135
statistical methods to measurements of natural phenomena.

13
00:00:38,135 --> 00:00:42,595
He was looking at data on the relative sizes of the parents and children,

14
00:00:42,595 --> 00:00:45,625
in various species including sweet peas.

15
00:00:45,625 --> 00:00:50,155
He observed something that is not immediately obvious, something really strange.

16
00:00:50,155 --> 00:00:55,985
Sure, a larger than average parent tends to produce larger than average children,

17
00:00:55,985 --> 00:01:00,950
but how much larger is the child to the average of the other children in this generation?

18
00:01:00,950 --> 00:01:05,295
It turned out that this ratio for the child is less,

19
00:01:05,295 --> 00:01:07,445
than the corresponding ratio for the parent.

20
00:01:07,445 --> 00:01:12,715
For example, if the parent size is 1.5 standard deviations from the mean,

21
00:01:12,715 --> 00:01:14,405
within its own generation,

22
00:01:14,405 --> 00:01:17,315
then he would predict the child size will be less

23
00:01:17,315 --> 00:01:21,140
than the 1.5 Standard deviations from the mean within it's cohort.

24
00:01:21,140 --> 00:01:23,915
We say that generation by generation,

25
00:01:23,915 --> 00:01:25,935
things in nature regress,

26
00:01:25,935 --> 00:01:28,220
or go back to the mean,

27
00:01:28,220 --> 00:01:31,250
hence the name, linear regression.

28
00:01:31,250 --> 00:01:34,380
This chart here from 1877,

29
00:01:34,380 --> 00:01:38,485
is the first ever linear regression. Pretty cool.

30
00:01:38,485 --> 00:01:42,650
Compute power in 1800's, was somewhat limited,

31
00:01:42,650 --> 00:01:44,910
so they didn't even realize how well this would

32
00:01:44,910 --> 00:01:47,915
continue to work once we had large datasets.

33
00:01:47,915 --> 00:01:52,135
There was actually a closed form solution for solving linear regression,

34
00:01:52,135 --> 00:01:54,965
but gridding descent methods can also be used,

35
00:01:54,965 --> 00:01:56,595
each with their pros and cons,

36
00:01:56,595 --> 00:01:58,095
depending on your data set.

37
00:01:58,095 --> 00:02:01,890
Let's look under the hood on how a linear regression works.

38
00:02:01,890 --> 00:02:04,120
Let's go a little more into detail to

39
00:02:04,120 --> 00:02:06,535
understand the motivations around linear regression.

40
00:02:06,535 --> 00:02:11,145
We begin with a linear equation that we are hypothesizing describes our system,

41
00:02:11,145 --> 00:02:15,350
by multiplying various weights with our observed feature vectors,

42
00:02:15,350 --> 00:02:17,140
and then sum it all up.

43
00:02:17,140 --> 00:02:21,985
We can represent this in the equation above for each example in our data set,

44
00:02:21,985 --> 00:02:28,110
y= w0 times x0 + w1 times x 1 plus w2 times x2,

45
00:02:28,110 --> 00:02:30,805
and so on for each feature in our model.

46
00:02:30,805 --> 00:02:35,355
In other words, we apply this equation to every row in our data set,

47
00:02:35,355 --> 00:02:37,440
where the weight values are fixed,

48
00:02:37,440 --> 00:02:40,680
and the feature values are from each associated column,

49
00:02:40,680 --> 00:02:42,480
and our machine learning data set.

50
00:02:42,480 --> 00:02:45,905
This could be nicely packaged into the measures equation below,

51
00:02:45,905 --> 00:02:52,995
y equals X times w. This hypothesis equation is very important,

52
00:02:52,995 --> 00:02:54,350
not just for linear regression,

53
00:02:54,350 --> 00:02:56,255
but for other machine learning models,

54
00:02:56,255 --> 00:02:59,665
such as deep neural networks, which we will discuss later.

55
00:02:59,665 --> 00:03:05,715
But how can I determine if the weights I have chosen are making good or bad guesses?

56
00:03:05,715 --> 00:03:09,515
The answer, is we need to create a lost function,

57
00:03:09,515 --> 00:03:13,125
which essentially is just an objective function we want to optimize.

58
00:03:13,125 --> 00:03:16,970
As explained earlier, typically for regression problems,

59
00:03:16,970 --> 00:03:19,490
the last function is mean squared error,

60
00:03:19,490 --> 00:03:22,950
which in matrix form is shown in the equation here,

61
00:03:22,950 --> 00:03:27,700
I drop the constant since it will disappear later in the derivation.

62
00:03:27,700 --> 00:03:31,660
We are first finding the difference between the actual labels value,

63
00:03:31,660 --> 00:03:34,675
with our protected labels value, y hat,

64
00:03:34,675 --> 00:03:39,390
which is just, X times w. Remember though,

65
00:03:39,390 --> 00:03:42,815
that my goal is to reduce the loss as much as possible.

66
00:03:42,815 --> 00:03:44,980
So, I need to find a way to minimize it,

67
00:03:44,980 --> 00:03:46,580
with respect to the weights.

68
00:03:46,580 --> 00:03:50,280
To do this, I take the derivative with respect to the weights,

69
00:03:50,280 --> 00:03:52,255
in the one dimensional case,

70
00:03:52,255 --> 00:03:56,335
or more generally the gradient when I have multiple features.

71
00:03:56,335 --> 00:03:59,710
I can then use this to find the global minimum.

72
00:03:59,710 --> 00:04:03,380
The equation here, I won't get into the derivation,

73
00:04:03,380 --> 00:04:07,270
provides a closed form analytical solution for linear regression.

74
00:04:07,270 --> 00:04:12,015
Meaning that, if you plug in the x and y values into this formula,

75
00:04:12,015 --> 00:04:14,395
you will get the values for the weights.

76
00:04:14,395 --> 00:04:17,640
But, this is not very practical,

77
00:04:17,640 --> 00:04:19,655
there are issues with the inverse,

78
00:04:19,655 --> 00:04:23,740
we are first assuming that the grand matrix, X transpose X,

79
00:04:23,740 --> 00:04:26,015
is non singular, meaning that

80
00:04:26,015 --> 00:04:29,890
all the columns of our feature matrix X are linearly independent.

81
00:04:29,890 --> 00:04:32,260
But, in real world data sets,

82
00:04:32,260 --> 00:04:35,320
you do have duplicate or nearly duplicate data.

83
00:04:35,320 --> 00:04:38,270
The same customer buy the same product again,

84
00:04:38,270 --> 00:04:41,795
two photos the same sunrise taken seconds apart.

85
00:04:41,795 --> 00:04:45,830
Even if the grand matrix is technically linearly independent,

86
00:04:45,830 --> 00:04:48,135
it could still be ill conditioned,

87
00:04:48,135 --> 00:04:50,790
therefore making it computationally singular,

88
00:04:50,790 --> 00:04:53,075
and still causing problems for us.

89
00:04:53,075 --> 00:04:58,610
The inverse also has a time complexity of ON cubed,

90
00:04:58,610 --> 00:05:00,630
using the naive algorithm,

91
00:05:00,630 --> 00:05:04,285
but still, doesn't get much better, using fancy algorithms.

92
00:05:04,285 --> 00:05:07,270
And those come with some of their own numerical issues.

93
00:05:07,270 --> 00:05:10,900
The same goes for even the multiplication to create the grand matrix.

94
00:05:10,900 --> 00:05:12,555
We might instead solve,

95
00:05:12,555 --> 00:05:15,270
the normal equations using something called a Choleskie,

96
00:05:15,270 --> 00:05:17,165
or a QRD composition.

97
00:05:17,165 --> 00:05:21,710
For ON cubed, or even ON to the 2.5,

98
00:05:21,710 --> 00:05:24,840
when N equals 10,000 or more,

99
00:05:24,840 --> 00:05:27,280
the algorithm can be very slow.

100
00:05:27,280 --> 00:05:31,890
So yes, you can solve exactly for the weights using the normal equation,

101
00:05:31,890 --> 00:05:34,160
but it is very dependent on your data,

102
00:05:34,160 --> 00:05:36,795
your model, which in linear algebra,

103
00:05:36,795 --> 00:05:40,565
matrix algorithms you are using etc.. Thankfully,

104
00:05:40,565 --> 00:05:44,285
there is gradient descent optimization algorithm, which is one,

105
00:05:44,285 --> 00:05:48,330
less expensive computationally in both time and or memory, two,

106
00:05:48,330 --> 00:05:50,775
more amenable to mild generalization,

107
00:05:50,775 --> 00:05:54,400
and three, generic enough to work on most problems.

108
00:05:54,400 --> 00:05:56,665
Instead, in gradient descent,

109
00:05:56,665 --> 00:05:58,290
we have our loss function,

110
00:05:58,290 --> 00:06:00,685
or more generally, our objective function,

111
00:06:00,685 --> 00:06:03,400
that is parameterized by the weights of our model.

112
00:06:03,400 --> 00:06:07,060
Within this space, there are hills and valleys,

113
00:06:07,060 --> 00:06:08,450
just like the Earth has.

114
00:06:08,450 --> 00:06:11,310
However, in many machine learning problems there,

115
00:06:11,310 --> 00:06:13,230
will be many more dimensions,

116
00:06:13,230 --> 00:06:15,800
in a 3D spatial world that we live in.

117
00:06:15,800 --> 00:06:18,240
Since this is gradient descent,

118
00:06:18,240 --> 00:06:21,295
minimization along the gradient, and not ascent,

119
00:06:21,295 --> 00:06:23,465
which instead will be, maximization,

120
00:06:23,465 --> 00:06:26,330
we want to traverse the last hyper surface,

121
00:06:26,330 --> 00:06:28,170
searching for the global minimum.

122
00:06:28,170 --> 00:06:32,065
In other words, we hope to find the lowest valley,

123
00:06:32,065 --> 00:06:35,205
regardless of where we start on the hyper surface.

124
00:06:35,205 --> 00:06:38,705
This can be done by finding the gradient of the loss function,

125
00:06:38,705 --> 00:06:41,325
and multiplying that with a hyper parameter,

126
00:06:41,325 --> 00:06:45,975
learning rate, and then subtracting that value from the current weights.

127
00:06:45,975 --> 00:06:49,300
This process iterates until convergence.

128
00:06:49,300 --> 00:06:52,875
Choosing the optimal learning rate and waiting for many iterations,

129
00:06:52,875 --> 00:06:55,745
could make you choose to use the normal equation instead,

130
00:06:55,745 --> 00:06:57,800
assuming the number of features is small,

131
00:06:57,800 --> 00:06:59,690
no colliniearity issues etc.

132
00:06:59,690 --> 00:07:02,280
Or, added gradient descent optimizer,

133
00:07:02,280 --> 00:07:05,615
such as momenta, or using a decaying learning rate.

134
00:07:05,615 --> 00:07:09,990
We'll talk much more about the details of Korean descent in the next module.

135
00:07:09,990 --> 00:07:15,245
What is a hyper parameter that helps determine gradient dissents step size,

136
00:07:15,245 --> 00:07:16,625
along the hyper servers,

137
00:07:16,625 --> 00:07:20,400
to hopefully speed up convergence?

138
00:07:20,400 --> 00:07:23,945
The correct answer, is learning rate.

139
00:07:23,945 --> 00:07:25,580
The learning rate along with

140
00:07:25,580 --> 00:07:29,330
some other hyper parameters that you will learn in the future modules,

141
00:07:29,330 --> 00:07:32,035
helps to size the step size in gradient descent.

142
00:07:32,035 --> 00:07:37,020
If set too low, then gradient descent takes a very long time to converge.

143
00:07:37,020 --> 00:07:38,480
If set too high,

144
00:07:38,480 --> 00:07:41,130
the gradient descent might even diverge,

145
00:07:41,130 --> 00:07:43,560
and increase the loss more and more.

146
00:07:43,560 --> 00:07:47,600
The other three answers all have to do with collinearity and conditioning,

147
00:07:47,600 --> 00:07:49,775
which we don't have to worry about with gradient descent,

148
00:07:49,775 --> 00:07:52,000
like we would using the normal equation.