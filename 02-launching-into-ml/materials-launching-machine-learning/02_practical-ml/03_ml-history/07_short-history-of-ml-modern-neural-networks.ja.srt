1
00:00:00,000 --> 00:00:03,240
再びニューラルネットワークの時代が来ました

2
00:00:03,240 --> 00:00:06,670
今回は処理能力の向上と膨大なデータによって

3
00:00:06,670 --> 00:00:08,760
飛躍的に進歩しています

4
00:00:08,760 --> 00:00:11,540
DNNはコンピュータビジョンなどのテストで

5
00:00:11,540 --> 00:00:15,030
他の手法より優れた性能を
発揮するようになりました

6
00:00:15,030 --> 00:00:20,380
ハードウェアの進歩 さらに
新しい技法とアーキテクチャのおかげで

7
00:00:20,380 --> 00:00:23,785
DNNのトレーニング性が改善されました

8
00:00:23,785 --> 00:00:26,515
たとえばReLU初期化処理の改善

9
00:00:26,515 --> 00:00:31,035
畳込みニューラルネットワーク（CNN）
ドロップアウトなどです

10
00:00:31,035 --> 00:00:34,670
すでに他のML手法のところで
このいくつかに触れました

11
00:00:34,670 --> 00:00:38,250
現在ではデフォルトでよく設定されるReLUなど

12
00:00:38,250 --> 00:00:40,155
非線形活性化関数についても

13
00:00:40,155 --> 00:00:43,370
ニューラルネットワークの説明で
すでに述べました

14
00:00:43,370 --> 00:00:46,855
一般化に役立つ
ドロップアウト層が使われ始めました

15
00:00:46,855 --> 00:00:48,345
アンサンブル手法で

16
00:00:48,345 --> 00:00:52,350
ランダムフォレストやブーストされたツリーに
関連して述べたとおりです

17
00:00:52,350 --> 00:00:57,040
畳み込み層が追加され
処理能力とメモリの負荷が減りました

18
00:00:57,040 --> 00:01:02,575
なぜなら 接続性が非完全で
画像など局所に焦点を絞れるからです

19
00:01:02,575 --> 00:01:06,705
画像の中の無関係なものを
比較することがありません

20
00:01:06,705 --> 00:01:09,970
言い換えると 他のML手法における進歩が

21
00:01:09,970 --> 00:01:13,005
ニューラルネットワークに組み込まれたのです

22
00:01:13,005 --> 00:01:15,430
DNNの例を見てみましょう

23
00:01:15,800 --> 00:01:18,475
機械学習の最高潮として

24
00:01:18,475 --> 00:01:22,640
何百もの層と何百万もの
パラメータを含むニューラルネットワークで

25
00:01:22,640 --> 00:01:26,400
ディープラーニングにより
驚異的な結果が得られます

26
00:01:26,400 --> 00:01:29,485
この図はGoogLeNet またはInception

27
00:01:29,485 --> 00:01:31,290
つまり画像分類モデルです

28
00:01:31,290 --> 00:01:33,630
2012年のデータを使って2014年に行った

29
00:01:33,630 --> 00:01:38,030
ImageNetの大規模な画像認識で
トレーニングされたものです

30
00:01:38,030 --> 00:01:40,870
120万の画像からなる1000クラスで

31
00:01:40,870 --> 00:01:44,160
画像を分類するトレーニングを行いました

32
00:01:44,160 --> 00:01:46,380
22個のディープ層があります

33
00:01:46,380 --> 00:01:50,470
今後のコースで見ていく
プーリングを含めると27個です

34
00:01:50,480 --> 00:01:54,650
さらに個々のブロックに分けると
層の数は100個です

35
00:01:54,650 --> 00:01:57,810
トレーニングパラメータは1100万個を超えます

36
00:01:57,810 --> 00:01:59,930
完全に繋がった層もありますが

37
00:01:59,930 --> 00:02:01,750
繋がってない層もあります

38
00:02:01,750 --> 00:02:04,475
たとえば畳込み層ですが
これは後でお話しします

39
00:02:04,475 --> 00:02:06,905
一般化のためにドロップアウト層を使い

40
00:02:06,905 --> 00:02:09,780
DNNアンサンブルをシミュレートしました

41
00:02:09,780 --> 00:02:12,540
ニューラルネットワークと
スタッキングのように

42
00:02:12,540 --> 00:02:15,845
各ボックスを単位とするボックスグループです

43
00:02:15,845 --> 00:02:17,835
先程の拡大図で見ましたね

44
00:02:17,835 --> 00:02:22,540
こうしてブロックを積み重ねて
単なる合計以上のものを得ることが

45
00:02:22,540 --> 00:02:25,720
ディープラーニングを成功させた要因です

46
00:02:25,720 --> 00:02:28,340
もちろん データがますます豊富になり

47
00:02:28,340 --> 00:02:31,805
処理能力とメモリ量が
向上していることも要因です

48
00:02:31,805 --> 00:02:34,650
現在では 規模と正確さの点で

49
00:02:34,650 --> 00:02:37,865
これより優れたバージョンもあります

50
00:02:38,205 --> 00:02:40,310
機械学習の歴史を通して

51
00:02:40,310 --> 00:02:43,580
他のアルゴリズムの技法を部分的に再利用し

52
00:02:43,580 --> 00:02:47,150
それを組み合わせて
ますます強力なモデルができます

53
00:02:47,150 --> 00:02:50,830
最も重要な点ですが 実験が可能になります

54
00:02:51,430 --> 00:02:55,020
DNNの作成で重要な点は何ですか？

55
00:02:56,750 --> 00:02:59,295
正解は「上記すべて」です

56
00:02:59,295 --> 00:03:01,795
これは完全なリストではありませんが

57
00:03:01,795 --> 00:03:04,550
最初の3つは特に重要です

58
00:03:04,550 --> 00:03:07,570
まず大量のデータが必要です

59
00:03:07,570 --> 00:03:11,350
ディープラーニングのデータ量を
減らす研究が盛んですが

60
00:03:11,350 --> 00:03:13,140
その成果が出るまでは

61
00:03:13,140 --> 00:03:15,680
大量のデータを確保する必要があります

62
00:03:15,680 --> 00:03:18,220
なぜなら 巨大なモデルの中には

63
00:03:18,220 --> 00:03:22,080
トレーニングするパラメータが多くあり
これが効果を高めるためです

64
00:03:22,080 --> 00:03:24,140
モデルがこれほど複雑なので

65
00:03:24,140 --> 00:03:27,225
データ分布をよく内在化させる必要があります

66
00:03:27,225 --> 00:03:29,710
ですから多くのシグナルが必要です

67
00:03:29,710 --> 00:03:32,680
機械学習の要点は優秀なさまざまなモデルを

68
00:03:32,680 --> 00:03:35,320
トレーニングすること自体ではなく

69
00:03:35,320 --> 00:03:38,485
トレーニングにより
精度の高い予測をすることです

70
00:03:38,485 --> 00:03:42,230
優れた一般化で新しいデータを予測すべきです

71
00:03:42,230 --> 00:03:44,265
さもないとモデルは無用です

72
00:03:44,265 --> 00:03:47,790
ですから十分な量のデータはとても重要です

73
00:03:47,790 --> 00:03:50,645
巨大なデータを何度か学習する代わりに

74
00:03:50,645 --> 00:03:53,445
小さなデータセットを何百万回も調べると

75
00:03:53,445 --> 00:03:55,395
過剰適合してしまいます

76
00:03:55,395 --> 00:04:00,100
データが多いと 十分な
検証とテスト用のデータも得られます

77
00:04:00,100 --> 00:04:04,140
さらにドロップアウト層を追加したり
データを拡張したり

78
00:04:04,140 --> 00:04:08,395
ノイズを追加したりすると
一般化がさらに改善します

79
00:04:08,905 --> 00:04:12,530
最後に機械学習では実験が重要です

80
00:04:12,530 --> 00:04:15,390
実にさまざまなアルゴリズムや
ハイパーパラメータ

81
00:04:15,390 --> 00:04:18,084
MLデータセットの作成方法があります

82
00:04:18,084 --> 00:04:20,730
すべての問題に適した選択肢を

83
00:04:20,730 --> 00:04:24,045
特定する方法はありません

84
00:04:24,045 --> 00:04:27,150
モデルを比較するために実験を重ねて

85
00:04:27,150 --> 00:04:31,505
計測された性能を調べ続けることは
単に楽しいだけでなく

86
00:04:31,505 --> 00:04:35,340
驚くほど強力なツールができる
きっかけにもなります

87
00:04:36,170 --> 00:04:38,160
次に ニューラルネットワークが

88
00:04:38,160 --> 00:04:41,905
過去のモデルを活用していることを
もっと見ていきます

89
00:04:41,905 --> 00:04:44,050
この表は過去のDNNの

90
00:04:44,050 --> 00:04:47,050
さまざまなモデルの性能を示しています

91
00:04:47,050 --> 00:04:48,590
2014年に

92
00:04:48,590 --> 00:04:50,720
大きく向上したことが

93
00:04:50,720 --> 00:04:52,390
青で示されています

94
00:04:52,390 --> 00:04:54,690
Google Inceptionモデルが

95
00:04:54,690 --> 00:04:57,085
誤差率を6.7%まで下げました

96
00:04:57,935 --> 00:05:00,740
DNNの性能は毎年上がり続け

97
00:05:00,740 --> 00:05:03,970
以前のモデルから教訓を学んでいます

98
00:05:03,970 --> 00:05:06,800
2015年にInceptionバージョン3が

99
00:05:06,800 --> 00:05:09,810
誤差率3.5%を達成しました

100
00:05:10,410 --> 00:05:14,145
短い期間にモデルが劇的に
進歩したのはなぜでしょうか

101
00:05:14,595 --> 00:05:18,645
ある研究グループが優れた技法を
新しく開発すると

102
00:05:18,645 --> 00:05:22,200
他のグループが
そのアイデアを基に開発を進めます

103
00:05:22,700 --> 00:05:28,160
こうして実験が大きく前進し進歩が加速します

104
00:05:28,160 --> 00:05:32,850
たとえばハイパーパラメータの改善
層の追加 一般化可能性の改善

105
00:05:32,850 --> 00:05:36,565
畳み込み層などの
サブコンポーネントの改善などです

106
00:05:36,565 --> 00:05:40,150
MLをどのような問題に
適用できるか説明してください

107
00:05:40,150 --> 00:05:43,365
この正解は1つでは ありません

108
00:05:44,195 --> 00:05:47,390
「あなたは冬のスキー場の経営者です

109
00:05:47,390 --> 00:05:50,265
ゲレンデの利用量レベルを予測します

110
00:05:50,265 --> 00:05:54,810
その際 利用券を買った
客のタイプ（初心者 中級 上級 プロ）と

111
00:05:54,810 --> 00:05:58,315
それまでの積雪量を基に予測します」

112
00:05:59,805 --> 00:06:01,755
答えを書いてみてください

113
00:06:03,965 --> 00:06:06,865
この場合 回帰でも分類でも構いません

114
00:06:06,865 --> 00:06:11,240
私は利用量レベルが何か
正確に言いませんでした

115
00:06:11,240 --> 00:06:14,960
1時間あたりの
コース利用者数という意味でしょうか

116
00:06:14,960 --> 00:06:19,185
または高、中、低のような
カテゴリーでしょうか？

117
00:06:19,185 --> 00:06:22,510
最初に基礎ヒューリスティックを
述べるべきでしょう

118
00:06:22,510 --> 00:06:24,900
たとえば 各スロープの平均人数です

119
00:06:24,900 --> 00:06:28,510
その後で 回帰と分類のどちらを選ぶかに応じて

120
00:06:28,510 --> 00:06:33,225
それぞれ線形またはロジスティック回帰の
基礎モデルを考えます

121
00:06:33,225 --> 00:06:35,395
性能とデータ量を考えて

122
00:06:35,395 --> 00:06:38,195
ニューラルネットワークにするでしょう

123
00:06:38,195 --> 00:06:40,860
データの中に他の特徴が存在するなら

124
00:06:40,860 --> 00:06:43,715
それも試して性能を観察します

125
00:06:45,365 --> 00:06:50,400
Googleでは現在 4000以上の
ディープMLモデルが稼働しており

126
00:06:50,400 --> 00:06:53,085
システムの強化に貢献しています

127
00:06:53,085 --> 00:06:55,690
それぞれのモデルとバージョンが

128
00:06:55,690 --> 00:07:00,290
過去のモデルの成功と失敗に基づき
性能を向上します

129
00:07:00,290 --> 00:07:02,885
最初によく使用していたのはSibylです

130
00:07:02,885 --> 00:07:06,760
これはYouTubeの関連ビデオを
おすすめするために作られました

131
00:07:06,760 --> 00:07:09,560
この推奨エンジンはとても優れていて

132
00:07:09,560 --> 00:07:13,555
広告など他のGoogle機能にも
導入されるようになりました

133
00:07:13,555 --> 00:07:15,720
これは線形モデルです

134
00:07:15,720 --> 00:07:23,960
今年は 別のモデルが普及し 他のモデルや
システムのパラメータ調整エンジンになりました

135
00:07:23,980 --> 00:07:27,020
Google ML研究部門であるGoogle Brainが

136
00:07:27,020 --> 00:07:30,050
何千ものCPUの能力を活用して

137
00:07:30,050 --> 00:07:34,590
DNNのような大きなモデルの
トレーニング方法を考案しました

138
00:07:34,990 --> 00:07:39,800
このようなモデルを構築して稼働した経験に
基づきTensorFlowが作成されました

139
00:07:39,840 --> 00:07:42,410
ML用のオープンソースライブラリです

140
00:07:42,770 --> 00:07:47,310
次にTFXまたはTensorFlowベースの
MLプラットフォームが作られました

141
00:07:47,310 --> 00:07:50,670
後で見ていく
本稼働MLモデルの作成とデプロイでは

142
00:07:50,670 --> 00:07:55,040
TensorFlowとCloud ML Engine
DataflowさらにBigQueryを使います

143
00:07:55,040 --> 00:07:57,265
まとめると この数十年で

144
00:07:57,265 --> 00:08:01,190
ニューラルネットワークが広く導入され
性能が上がってきました

145
00:08:01,190 --> 00:08:02,830
データが広く得られるので

146
00:08:02,830 --> 00:08:06,905
モデルのトレーニングに使う例が
ますます豊富になってきました

147
00:08:06,905 --> 00:08:12,190
データと例が増えてきたことに加えて
数千の層からなる複雑な分散モデル向けの

148
00:08:12,190 --> 00:08:15,900
スケーラブルなインフラストラクチャが
実現しました

149
00:08:16,230 --> 00:08:18,590
最後に1つ注意点をお伝えします

150
00:08:18,590 --> 00:08:22,415
ニューラルネットワークは
いくつかの分野で優れていますが

151
00:08:22,415 --> 00:08:25,620
これはさまざまな種類のモデルの
1つにすぎません

152
00:08:25,620 --> 00:08:27,530
実験は重要です

153
00:08:27,530 --> 00:08:29,730
さまざまなモデルを試すことで

154
00:08:29,730 --> 00:08:32,630
皆さんのデータで課題を解くことができます