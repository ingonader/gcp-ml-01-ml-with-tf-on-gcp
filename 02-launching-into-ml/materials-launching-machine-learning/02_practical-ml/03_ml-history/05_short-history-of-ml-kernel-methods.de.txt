Zu Beginn der 1990er-Jahre entstand das Feld der Kernelmethoden. Corinna Cortes, Google-Forschungsleiterin, gehört zu den Pionieren. Dieses Forschungsgebiet ermöglicht neue
interessante nicht lineare Modellklassen, vor allem nicht lineare SVMs,
Support Vector Machines, wobei es sich um
Maximum-Margin-Klassifikatoren handelt. Der Kern einer SVM ist im Wesentlichen eine nicht lineare Aktivierung und ein
Sigmoid-Output für einen maximalen Rand. Die logistische Regression wird zur
Bildung von Entscheidungsgrenzen zur Maximierung der Log-Wahrscheinlichkeit
von Klassifikationsproblemen verwendet. Im Fall einer linearen Entscheidungsgrenze will die logistische
Regression jeden Punkt mit seiner Klasse maximal
von der Hyperebene entfernt haben und liefert eine Wahrscheinlichkeit, die als
Prognosesicherheit interpretierbar ist. Es können unendlich
viele Hyperebenen zwischen zwei linear trennbaren Klassen
erzeugt werden, etwa die zwei als gestrichelte Linien
dargestellten in diesen zwei Grafiken. Bei SVMs setzen wir zwei parallele
Hyperebenen auf jede Seite der Entscheidungsgrenze, wo sie sich mit dem nächsten Datenpunkt auf
jeder Seite der Hyperebene kreuzen. Das sind die Stützvektoren. Der Abstand zwischen den
beiden Stützvektoren ist die Marge. Links haben wir eine vertikale Hyperebene,
die tatsächlich die beiden Klassen trennt. Aber die Marge zwischen den
beiden Stützvektoren ist klein. Wenn wir eine andere Hyperebene wählen, etwa die rechts, ist die Marge viel breiter. Je breiter die Marge, desto mehr kann man
die Entscheidungsgrenze verallgemeinern, was zu einer besseren
Leistung führen sollte. Deshalb zielen SMV-Klassifikatoren
auf maximale Ränder zwischen den zwei Stützvektoren ab, und
verwenden dazu eine Hinge-Verlustfunktion, verglichen mit der Minimierung von
Kreuzentropie bei logistischer Regression. Wie Sie sehen, habe ich nur zwei Klassen, weshalb dies ein
binäres Klassifikationsproblem ist. Das Label der 
einen Klasse erhält den Wert 1 und das Label der
anderen Klasse den Wert -1. Bei mehr als zwei Klassen sollte ein One-vs-all-Ansatz und dann die beste der permutierten binären
Klassifikationen gewählt werden. Was passiert, wenn die Daten nicht linear
in die zwei Klassen aufteilbar sind? Hier können wir eine Kernel-Transformation
anwenden, die unsere Daten aus dem Bereich des Input-Vektors
in einen Vektorbereich abbildet, der jetzt Merkmale hat, die wie zu sehen
linear aufgeteilt werden können. Genau wie vor dem Aufkommen
der tiefen neuronalen Netze kostete es viel Zeit und Arbeit,
die Rohdatendarstellung durch eine feine, benutzerdefinierte Merkmalskarte
als Merkmalsvektor abzubilden. Bei Kernelverfahren aber ist nur der Kernel benutzerdefiniert, durch eine Ähnlichkeitsfunktion zwischen
Punktpaaren in der Rohdatendarstellung. Eine Kernel-Transformation
ist vergleichbar damit, wie eine Aktivierungsfunktion
in neuronalen Netzen die Eingabe der Funktion abbildet,
um einen Raum zu überführen. Die Anzahl der Neuronen
in der Schicht bedingt die Dimension. Haben wir also zwei
Inputs und drei Neuronen, bilden wir den Input-2D-Raum
in einem 3D-Raum ab. Es gibt viele Arten von Kernels,
angefangen beim einfachen linearen Kernel, dem Polynom-Kernel und
dem Gaußschen RBF-Kernel. Wenn unser binärer
Klassifikator den Kernel verwendet, berechnet er eine 
gewichtete Summe der Ähnlichkeiten. Wann also sollte eine SVM anstelle von
logistischer Regression verwendet werden? Kernelisierte SVMs tendieren zu dünneren
Lösungen und sind besser skalierbar. Die SVM-Leistung ist besser,
wenn es viele Dimensionen gibt und die Prädiktoren die Antwort
nahezu sicher voraussagen. SVMs bilden Inputs mit Kernels in einem
höherdimensionalen Merkmalsraum ab. Welcher Aspekt neuronaler Netze überführt
auch in höherdimensionale Vektorräume? Die richtige Antwort ist: mehr Neuronen pro Schicht. Die Neuronenanzahl pro Schicht bestimmt, in welcher
Vektorraumdimension wir uns befinden. Wenn ich mit
drei Eingangsmerkmalen beginne, befinde ich mich im Vektorraum R3. Selbst bei Hunderten Schichten, aber nur je drei Neuronen, befinde ich mich im Vektorraum R3
und ändere nur die Basis. Bei Verwendung eines
Gaußschen RBF-Kernels mit SVMs wird der Input-Raum in
unendliche Dimensionen überführt. Die Aktivierungsfunktion
ändert die Basis des Vektorraums, fügt aber Dimensionen
weder hinzu noch verringert sie. Man muss es sich einfach wie ein
Drehen, Dehnen und Drücken vorstellen. Sie können nicht linear sein, aber wir bleiben in
demselben Vektorraum wie zuvor. Die Verlustfunktion ist das Objekt,
das wir zu minimieren versuchen, ein Skalar, mit dessen Gradient die
Parametergewichtungen aktualisiert werden. Dies ändert nur, wie viel
wir drehen, dehnen und drücken, aber nicht die Anzahl der Dimensionen.