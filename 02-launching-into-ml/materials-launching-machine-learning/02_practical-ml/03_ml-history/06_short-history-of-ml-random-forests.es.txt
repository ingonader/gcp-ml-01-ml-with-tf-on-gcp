En las últimas décadas del siglo XX la investigación del AA
finalmente tuvo el poder informático para incluir y combinar el rendimiento en muchos modelos, mediante
lo que llamamos el método del ensamble. Pueden imaginarse que, si los errores
son independientes para una cantidad de clasificadores simples débiles,
combinados constituirán uno robusto. DNN se acercará a esto
mediante capas de retirados que ayudan a regularizar el modelo
y prevenir el sobreajuste. Esto se puede simular
desactivando neuronas aleatoriamente en la red con cierta probabilidad
para cada propagación hacia adelante lo que básicamente
creará una nueva red cada vez. Muchas veces, las preguntas complejas
se responden mejor mediante la agregación
de las respuestas de miles de personas en vez de la de un solo individuo. Esto se conoce
como la "sabiduría de los grupos". Lo mismo se aplica al AA. Cuando se agregan los resultados
de muchos predictores, ya sean clasificadores o regresores,
el grupo tendrá mejor rendimiento que el mejor modelo individual. Este grupo de predictores es un ensamble
que, cuando se combina así, conduce al aprendizaje por ensamblado. El algoritmo que realiza este aprendizaje
es un método de ensamble. Uno de los aprendizajes
por ensamblado más populares es el bosque aleatorio. En vez de usar todo el conjunto
de datos de entrenamiento para crear un árbol de decisión,
pueden tener un grupo de árboles y cada uno
obtiene una submuestra aleatoria de los datos de entrenamiento. Como no vieron todo el conjunto
de entrenamiento, no pueden haberlo memorizado. Una vez que todos los árboles
estén entrenados y sean un subconjunto de los datos,
podrán hacer lo más importante y valioso del AA: las predicciones. Para hacerlo,
propagarán la muestra de la prueba a cada árbol en el bosque
y luego agregarán los resultados. Si se trata de clasificación,
podría existir un voto mayoritario en todos los árboles,
que luego sería la clase de salida final. Si es regresión,
podría ser un agregado de los valores como la media, máxima, mediana, etcétera. Para mejorar la generalización,
pueden realizar una muestra aleatoria de los ejemplos o los atributos. A este muestreo aleatorio
de ejemplos con reemplazo se le llama
agregación de bootstrap (bagging) y se le llama "pasting"
cuando es sin reemplazo. Cada predictor individual
tiene un sesgo mayor, ya que se entrenó en el subconjunto
más pequeño en lugar de todo el conjunto, pero la agregación reduce
tanto el sesgo como la varianza. Eso proporciona al ensamble
un sesgo similar al de un predictor único en todo el conjunto de entrenamiento,
pero con menor varianza. Un excelente método de validación
para el error de generalización es usar los datos
de la agregación de bootstrap en lugar de un conjunto separado
obtenido del conjunto de datos previo al entrenamiento. Es algo similar a la validación de k
con exclusiones aleatorias. Los subespacios aleatorios
ocurren cuando se obtiene la muestra de los atributos.
Si se obtienen muestras aleatorias de los ejemplos,
se llaman parcelas aleatorias (patches). La potenciación adaptativa o AdaBoost
y la potenciación del gradiente son ejemplos de potenciación,
que es cuando se agregan clasificadores débiles para crear uno robusto. Por lo general, se hace
mediante el entrenamiento secuencial de cada clasificador
para corregir los problemas que tuvo. En los árboles potenciados,
a medida que se agregan más árboles al ensamble,
las predicciones suelen mejorar. Entonces, ¿continuamos agregando árboles
hasta el infinito? Claro que no. Pueden usar su conjunto de validación
para la interrupción anticipada y no sobreajustar los datos
de entrenamiento debido a la presencia
de demasiados árboles. Por último,
igual que con las redes neuronales podemos combinar clasificadores
(stacking), donde los metaclasificadores
aprenden qué hacer con las predicciones del ensamble,
que a su vez se pueden combinar en metaclasificadores, etcétera. Veremos la combinación de subcomponentes
y la reutilizaremos en las redes neuronales profundas dentro de poco. ¿Cuál de las siguientes es probablemente
falsa sobre los bosques aleatorios cuando se los compara con los árboles
de decisión individuales? La respuesta correcta es
que es probablemente falso que los árboles aleatorios son
D. Más fáciles de interpretar visualmente. Igual que las redes neuronales,
mientras más capas de complejidad se agreguen al modelo,
más difícil será entender y explicar. Un bosque aleatorio es más complejo
que un árbol de decisión individual lo que lo hace
más difícil de interpretar visualmente. Las otras tres respuestas
son probablemente verdaderas. Los bosques aleatorios
tienen mejor generalización gracias a la agregación de bootstrap
y los subespacios o la agregación para la regresión y mediante un sistema
de votación para la clasificación el bosque puede tener
mejor rendimiento que un árbol individual. Finalmente, debido al muestreo aleatorio
de los bosques aleatorios mantiene un sesgo similar
al de un árbol individual pero también tiene menor varianza que, por lo general,
significa mejor generalización.