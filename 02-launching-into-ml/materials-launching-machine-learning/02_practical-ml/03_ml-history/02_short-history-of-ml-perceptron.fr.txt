Voilà ce qu'il en était
de la régression linéaire pour l'apprentissage des données. Jusqu'à ce que, dans les années 1940, Frank Rosenblatt, un chercheur,
suggère un perceptron comme modèle computationnel
d'un neurone du cerveau humain, et montre comment il peut
apprendre des fonctions simples. C'est que nous appellerions aujourd'hui
un classifieur linéaire binaire, qui permet de trouver une droite unique
qui sépare les données en deux classes. Une couche unique de perceptrons était
le réseau de neurones à propagation avant le plus simple. Les entrées se propageaient
dans une couche unique de perceptrons, et une somme pondérée était effectuée. Cette somme passait ensuite
par une fonction d'activation, qui est une fonction mathématique
qu'on applique à chaque élément qui réside dans ce neurone. Mais n'oubliez pas qu'à ce stade,
ce n'est qu'un classifieur linéaire. La fonction d'activation,
qui, dans ce cas, est linéaire, renvoie simplement ses entrées. La comparaison de la sortie
à un seuil déterminait ensuite à quelle classe chaque point appartenait.
Les erreurs étaient agrégées et utilisées pour modifier les pondérations
utilisées dans la somme, et le processus se répétait
jusqu'à la convergence. Si vous essayez de créer un modèle simple de quelque chose qui apprend une sortie
à partir d'une distribution d'entrées, vous n'avez pas besoin de chercher loin,
car nos cerveaux font cela tous les jours, trouver le sens de notre environnement
et des signaux reçus par notre corps. L'une des unités fondamentales
du cerveau est le neurone. Les réseaux de neurones
sont des groupes de neurones connectés selon différents
schémas ou architectures. Un neurone biologique
a différents composants spécialisés dans la communication
d'un signal électrique qui nous permet de penser,
de réaliser des actions et d'étudier le monde fascinant
du machine learning. Les signaux électriques d'autres neurones, comme les neurones sensoriels
dans la rétine de votre œil, passent de neurone en neurone. Le signal d'entrée est reçu
par une extrémité du neurone qui est constituée de dendrites. Ces dendrites peuvent
collecter des signaux électriques auprès de plusieurs neurones,
qui sont tous additionnés au fil du temps, ce qui change le potentiel
électrique de la cellule. Un neurone typique présente
un potentiel électrique de repos d'environ -70 mV. Comme les stimuli d'entrée reçus
par les dendrites augmentent, il finit par atteindre
un seuil d'environ -55 mV. Une dépolarisation rapide
de l'axone se produit alors, des canaux ioniques
sensibles à la tension s'ouvrent, et une entrée soudaine d'ions se produit. Le neurone déclenche alors un potentiel
d'action de courant électrique le long de l'axone,
à l'aide de la gaine de myéline pour une meilleure transmission
aux terminaisons axonales. Des neurotransmetteurs
sont libérés au niveau des synapses, puis voyagent
à travers la fente synaptique jusqu'aux dendrites d'autres neurones. Certains neurotransmetteurs
sont excitateurs, augmentant
le potentiel de la cellule suivante, et certains sont inhibiteurs
et diminuent le potentiel. Le neurone se repolarise à un potentiel
encore plus bas que celui du repos, pendant la période réfractaire. Le processus continue
dans le neurone suivant jusqu'à ce qu'il atteigne un motoneurone et bouge votre main
pour protéger vos yeux du soleil. Quel est donc le lien entre la biologie,
les neurosciences et le machine learning ? Cela vous parle ? Il s'agit d'un perceptron monocouche. Lui aussi, comme le neurone,
présente des entrées qu'il multiplie par des pondérations
et additionne. La valeur est alors comparée à un seuil, puis transformée
par une fonction d'activation. Par exemple, si la somme
est supérieure ou égale à zéro, activez, ou appuyez sur la valeur 1, sinon, n'activez pas,
ou appuyez sur la valeur 0. Les entrées et pondérations agissent
comme les neurotransmetteurs d'un neurone. Certains peuvent être positifs
et ajoutés à la somme, et d'autres peuvent être négatifs
et soustraits de la somme. La fonction de Heaviside agit
comme un seuil tout-ou-rien. Si le seuil est atteint,
transmettez le signal, autrement, ne transmettez rien. Enfin, il y a une sortie,
et comme pour les neurones biologiques, elle peut être transmise comme entrée à d'autres neurones
dans un perceptron multicouche, dont nous parlerons par la suite. Tout cela est très intéressant, mais il y a certaines fonctions
très simples qu'il ne peut pas apprendre. Par exemple, la fonction XOR. Marvin Minsky,
un informaticien célèbre du MIT, a mis le doigt là-dessus,
et plus personne n'a voulu faire de la recherche en IA
pendant une quinzaine d'années. Les réseaux de neurones ont été
presque oubliés pendant un moment. Quel composant
d'un neurone biologique est similaire à la couche d'entrée d'un perceptron ? Ce sont les dendrites. Elles reçoivent
un stimulus d'autres neurones, comme un réseau de neurones artificiel. Ce n'est pas l'axone, car il ressemble
plus à la sortie d'un perceptron. Ce n'est pas le noyau, car c'est là où est stocké
le matériel génétique des cellules, et il contrôle les activités des cellules. Ce n'est pas la gaine de myéline,
car elle aide à transmettre l'axone, qui se trouve aussi
sur la couche de sortie du perceptron.