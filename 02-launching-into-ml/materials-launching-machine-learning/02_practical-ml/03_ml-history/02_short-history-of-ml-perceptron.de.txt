Lineare Regression war also der Renner, was das Lernen aus Daten betraf. Bis in den 1940er-Jahren
der Forscher Frank Rosenblatt das Perzeptron vorstellt, ein
Rechenmodell eines Neurons im menschlichen Gehirn, und zeigt,
wie es einfache Funktionen lernen kann. Das war, was wir heute als einen binären linearen Klassifikator
bezeichnen, mit dem wir eine einzelne Gerade suchen, die
die Daten in zwei Klassen aufteilt. Ein einlagiges Perzeptron wäre das
einfachste mögliche Feedforward-Netz. Inputs in ein einlagiges Perzeptron, und
eine gewichtete Summe wird errechnet. Diese Summe durchläuft dann, was wir
heute eine Aktivierungsfunktion nennen, was einfach eine
mathematische Funktion ist, die man auf jedes jetzt in diesem Neuron
vorhandene Element anwendet. Beachten Sie aber, dass dies bisher nur ein linearer Klassifikator ist. Die Aktivierungsfunktion, die in diesem
Fall linear ist, gibt also einfach die Inputs zurück. Durch Vergleich der Ausgabe
davon mit einem Schwellenwert wird dann ermittelt, welcher Klasse
die einzelnen Punkte angehören. Die aggregierten Fehler werden verwendet,
um die Gewichte in der Summe zu verändern, und das würde sich bis zur
Konvergenz wiederholen. Wenn man ein einfaches
Modell für etwas möchte, das eine gewünschte Ausgabe aus
einer gegebenen Eingangsverteilung lernt, muss man nicht lang suchen, denn unser
Gehirn tut das den ganzen Tag lang: die Welt um uns herum und alle Signale,
die unser Körper empfängt, einordnen. Eine der Grundeinheiten
des Gehirns ist das Neuron. Neuronale Netze sind
einfach Gruppen von Neuronen, miteinander verbunden in
verschiedenen Mustern oder Architekturen. Ein biologisches Neuron hat verschiedene
Komponenten, die darauf spezialisiert sind, elektrische Signale weiterzuleiten,
die es uns ermöglichen, Gedanken zu haben, zu handeln und die faszinierende Welt
des maschinellen Lernens zu untersuchen. Elektrische Signale von anderen Neuronen, wie sensorischen
in der Netzhaut des Auges, werden von Neuron zu Neuron weitergegeben. Das Eingangssignal wird
an einem Ende des Neurons empfangen, das aus Dendriten besteht. Die Dendriten nehmen möglicherweise
nicht nur von einem Neuron Signale auf, sondern vielleicht von mehreren, die über bestimmte
Zeiträume hinweg summiert werden und das elektrische
Potenzial der Zelle ändern. Ein typisches Neuron
hat ein Ruhepotenzial von etwa -70 mV. Wenn sich die Eingangsreize
an den Dendriten verstärken, wird schließlich ein
Schwellenwert von etwa -55 mV erreicht. In diesem Fall erfolgt eine
schnelle Depolarisierung des Axons, wobei sich eine Menge Ionenkanäle öffnen
und einen plötzlichen Ionenstrom erlauben. Dadurch wird ein Aktionspotenzial
elektrischen Stroms entlang des Axons ausgelöst, wobei die Markscheide die
Fortleitung an die Axonterminale fördert. Hier werden Neurotransmitter an Synapsen ausgeschüttet, die dann über
den Synapsenspalt wandern, normalerweise zu den
Dendriten anderer Neuronen. Einige der Neurotransmitter sind erregend und erhöhen das
Potenzial der Nachbarzelle, andere sind hemmend
und senken das Potenzial. Das Neuron repolarisiert auf ein Potenzial
unter dem Ruhepotenzial: die Refraktärzeit. Dann setzt sich der Vorgang
mit dem nächsten Neuron fort, erreicht dann ein motorisches Neuron und
führt die Hand schützend vor die Augen. Was aber haben diese Biologie und
Neurowissenschaft mit ML zu tun? Kommt Ihnen das bekannt vor? Das ist ein einlagiges Perzeptron. Auch dieses, genau wie das Neuron, hat Inputs, die es dann mit Gewichtungen
multipliziert und alles summiert. Der Wert hier wird
jetzt mit einem Schwellenwert verglichen und dann durch
eine Aktivierungsfunktion transformiert. Zum Beispiel: Wenn die Summe
größer oder gleich null ist, dann aktivieren oder
geben Sie den Wert 1 weiter, anderenfalls aktivieren Sie nicht
oder geben Sie den Wert 0 weiter. Die Inputs und Gewichte verhalten sich
wie die Neurotransmitter in einem Neuron, wobei einige positiv sind
und die Summe erhöhen und andere negativ
und die Summe verringern. Die Einheitssprungfunktion fungiert
als eine Alles-oder-Nichts-Schwelle. Wenn der Schwellenwert
erreicht wird: Signal weiterleiten. Ansonsten: Nichts weiterleiten. Zum Schluss gibt es ein Output und wie
bei biologischen Neuronen kann es als Input an andere Neuronen in einem
mehrlagigen Perzeptron übergehen. Darüber reden wir gleich. Das ist alles sehr toll, aber tatsächlich gibt es sehr einfache
Funktionen, die es nicht lernen kann. Zum Beispiel die XOR-Funktion. Marvin Minsky,
ein berühmter Forscher am MIT, zeigte dies auf und dann war etwa 15 Jahre
lang niemand mehr an KI interessiert. Das war nicht das erste Mal,
dass neuronale Netze auf ein Hindernis stießen und eine Zeit lang
mehr oder weniger vergessen wurden. Welche Komponente des biologischen Neurons
entspricht dem Input-Teil des Perzeptrons? Die Antwort lautet: die Dendriten. Sie empfangen Reize von anderen Neuronen, genau wie ein künstliches neuronales Netz. Das Axon ist falsch, da es eher dem
Output eines Perzeptrons entspricht. Der Zellkern ist falsch, denn dort ist das
genetische Material der Zelle gespeichert und werden die Aktivitäten der Zelle gesteuert. Die Markscheide ist falsch, da diese die
Fortleitung über das Axon unterstützt, das, wie gesagt, zum
Output-Teil des Perzeptrons gehört.