ID3やC 4.5などの ツリーアルゴリズムが 1980～90年代に発明されました これらは特定の線形回帰の問題に適していて 人間が簡単に解釈できます ツリー作成時に最適分割を見つけることは
NP困難の問題ですから 貪欲アルゴリズムを使って できるだけ最適に近いツリーを作ろうとします これにより基本的に再緩層の 区分線形決定面ができます しかし DNN（ディープニューラル
ネットワーク）では 実際の各層が結びついて
ハイパー平面決定面を形成します これで さらに強力になります DNNが決定ツリーよりも
優れているのはなぜでしょうか まず決定ツリーについてお話ししましょう 決定ツリーはきわめて直観的な
機械学習アルゴリズムです 分類と回帰の両方にこれを使用できます あるデータセットのデータを さまざまな容器に分ける方法を決めるとします 最初にすべきことは データセットを使った質問を考えることです 例を挙げましょう これはタイタニック号の大事故の
生存者と死亡者を予測する有名な問題です あらゆる職業 階層 さまざまな背景 境遇の人が乗船していました これらの特徴のいずれかで
データをはっきり分割して 高い確率で生存者を
予測できるかどうか試みます 最初に思いつく特徴は乗客の性別でしょう そこで「性別は男性ですか」
という質問に基づき データの中で男性を1つのバケットに入れて 残りを別のバケットに入れます データの64%が男性バケットに 36%が別のバケットに入りました 男性バケットで続きを考えましょう 次の質問は それぞれの乗船客の等級です すると全乗客の14%が 最も低い等級で乗船した男性でした 乗客の50%が それより上の2つの等級の男性でした 続いて 同じ区分がツリーの
女性分岐にも適用されます 決定ツリー作成アルゴリズムを1つ戻ると 性別の値は2つしかないので 性別が2つに分岐します でも 低い等級の乗客を左の枝に 高い等級を右の枝に分けるよう どうやって決定したのでしょうか たとえば 単純な分類と回帰ツリー
つまりCARTアルゴリズムでは 最も純粋なサブセットに分割できるよう 1つの特徴としきい値のペアを選びます 分類ツリーでは
列指標としてジニ不純度を使いますが エントロピーもあります 適切に分割されたら 別の特徴としきい値のペアを探して それもサブセットに分けます このプロセスを繰り返すとやがて ツリーの特定の最大深度に達するか またはこれ以上分割で
不純度を下げられなくなります 再帰ツリーの場合は分割指標として
平均二乗誤差をよく使います データを2つのサブセットに分ける方法は
前にもありましたね それぞれの分割は単なる
バイナリ線形分類器です ある特徴を特定のしきい値で
切り分ける超平面を探すのです しきい値は超平面の反対側に
行ってしまうクラスメンバーの 数を最小化します ツリーでこのような超平面を
再帰的に作ることは ニューラルネットワークの
線形分類器ノードの層に似ています とても面白いですね さて 決定ツリーの作り方がわかったので このツリーをもう少し作っていきます たぶん年齢しきい値で このデータがうまく分かれるでしょう そこで「17歳半より上ですか」
という質問をします 男性分岐の下で最も低い等級分岐を見ると 乗客の13%が18歳以上 それより若い人はわずか1%です 各ノードの等級を見ると 今のところ男性分岐のここだけが
生存に分類されています さらに深くして 別の特徴を選んでツリーを拡張していくと やがて すべてのノードに 生存または死亡の一方だけが
入るようになるでしょう しかし ここでの問題は私がデータを記憶して ツリーを完璧に適合させていることです 実際には 新しいデータ向けに
一般化したいのです トレーニングセットを記憶したモデルは 現実ではうまく機能しないでしょう 正規化する方法として 各リーフノードの最小サンプル数や リーフノード最大数 特徴の最大数を設定できます またツリー全体を作った後で 不要なノードを切り取ることもできます ツリーを有効活用するには 複数を合わせてフォレストにできます これについてまもなくお話しします 決定分類ツリーの それぞれの決定つまりノードは
何で構成されますか？ 正解は「1つの特徴の線形分類器」です ツリーの各ノードでは データを2つのサブセットに分ける
特徴としきい値のペアを選び それを再帰的に続けます 最大深度を2以上に設定すると 多くの特徴がやがて分割されますが 一度に1深度につき1つの特徴です 各ノードが一度に1つの特徴を分けるので すべての特徴を対象とする
線形分類器は間違いです 平均二乗誤差ミニマイザーと ユークリッド距離ミニマイザーはよく似ていて どちらも分類ではなく回帰で使われます