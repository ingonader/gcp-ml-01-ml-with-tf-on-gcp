Nuevamente en la línea de tiempo
están las redes neuronales ahora con más ventaja gracias a la potencia informática
y la gran cantidad de datos. Las DNN comenzaron a superar
significativamente a otros métodos en pruebas, como la visión artificial. Además del auge del hardware potenciado,
hay muchos trucos nuevos y arquitecturas que ayudan a mejorar el entrenamiento
de las redes neuronales profundas como las ReLU,
mejores métodos de inicialización, CNN o redes neuronales
convolucionales y retirados (dropout). Ya hablamos sobre algunos
de estos trucos de otros métodos de AA. El uso de funciones de activación
no lineales, como las ReLU, que hoy, por lo general,
son las predeterminadas. Hablamos de eso cuando
vimos las redes neuronales al principio. Las capas de retirados
se comenzaron a usar para ayudar con la generalización, que funciona mejor
con los métodos de ensambles que vimos con los bosques aleatorios
y los árboles potenciados. Las capas convolucionadas se agregaron
para reducir la carga de procesamiento y memoria,
debido a su conectividad incompleta al igual que su capacidad
de enfocarse en aspectos locales por ejemplo, imágenes, en vez de comparar
elementos no relacionados en una imagen. En otras palabras, los avances
que surgieron con otros métodos de AA se aprovecharon en las redes neuronales. Veamos un ejemplo
de una red neuronal profunda. Esta historia emocionante
del aprendizaje automático culminó en el aprendizaje profundo,
con redes neuronales que contienen cientos de capas
y millones de parámetros pero con resultados sorprendentes. Aquí vemos un GoogLeNet o Inception que es un modelo
de clasificación de imágenes. Se entrenó para el desafío
de reconocimiento visual de gran escala de ImageNet de 2014, con datos de 2012,
para clasificar imágenes de miles de clases, con 1.2 millones
de imágenes por entrenamiento. Tiene 22 capas profundas,
27 si se incluye el agrupamiento que veremos en un curso posterior,
y cientos de capas, si se las desglosa en sus componentes independientes. Tiene más de 11 millones
de parámetros entrenados. Hay capas completamente conectadas
y otras que no lo están, como las convolucionadas,
de las que hablaremos más tarde. Usó capas de retirados
para ayudar a generalizar más mediante la simulación de un ensamble
de redes neuronales profundas. Como en las redes neuronales
y la combinación de clasificadores cada caja es una unidad de componentes,
parte de un grupo de cajas como la que enfoqué. Esta idea de componentes
que se agregan para crear algo mayor que la suma de sus partes
es lo que ha posibilitado el gran éxito del aprendizaje profundo. Por supuesto, la abundancia de datos
en constante crecimiento el poder de procesamiento
y más memoria ayudan también. Hoy, hay varias versiones más recientes,
que son mucho más grandes y precisas. La lección de toda esta historia
es que la investigación del AA reutiliza elementos y técnicas
de otros algoritmos del pasado y los combina para crear
modelos muchos más poderosos y, sobre todo, para experimentar. ¿Qué es importante cuando se crean
redes neuronales profundas? La respuesta correcta es
D. Todas las anteriores. Esta no es una lista completa,
pero estos tres elementos son muy importantes. Primero, hay que asegurarse
de tener: A. Muchos datos. Existe mucha investigación en curso
que intenta reducir las necesidades de datos para el aprendizaje profundo pero hasta que eso suceda,
debemos asegurarnos de tener muchos. Esto se debe a la alta capacidad
de la cantidad de parámetros que se deben entrenar
en estos modelos masivos. Ya que el modelo es tan complejo,
debe internalizar bien la distribución de los datos. Por lo tanto, necesita muchas señales. Recuerden, el objetivo central
del aprendizaje automático no es entrenar muchos modelos
elegantes solo porque sí. Es entrenarlos para que hagan
predicciones muy precisas. Si no pueden generalizar
nuevos datos para predecir, entonces, ¿de qué sirve ese modelo? Tener los suficientes datos es importante de modo que no se sobreajuste
a un conjunto de datos pequeño que ha visto un millón de veces,
en lugar de un conjunto gigante que ha visto mucho menos. Esto les permite tener conjuntos
de validación y pruebas lo suficientemente grandes
para ajustar sus modelos. Adicionalmente,
agregar capas de retirados, incrementar los datos,
agregar ruido, etcétera, es la forma de tener
aún mejor generalización. Por último, el aprendizaje automático
se trata sobre la experimentación. Hay tantos tipos diferentes
de algoritmos, hiperparámetros y formas de crear el conjunto
de datos de AA hoy en día. No hay manera de saber a priori
las opciones óptimas desde el principio para casi todos los problemas. Mediante la experimentación,
el registro de lo que ya intentaron y la medición del rendimiento
para comparar modelos no solo se divertirán,
sino que también crearán herramientas increíbles y poderosas. Ahora, hablaré sobre
cómo las redes neuronales continúan aprovechando
el rendimiento de los modelos pasados. Aquí, ven el rendimiento
de versiones de modelos específicas de redes neuronales
a lo largo de los años. Como pueden ver en el gráfico,
un salto significativo ocurrió el 2014, destacado en azul,
cuando el modelo Inception de Google superó el modelo de 10%
de tasa de error con un 6.7%. El rendimiento de las DNN
continúa mejorando cada año y con las lecciones aprendidas
de los modelos anteriores. En 2015, la tercera versión
del modelo Inception obtuvo una tasa de error del 3.5%. ¿Qué permite estas grandes mejoras
en tan poco tiempo? A menudo,
cuando un grupo de investigadores desarrolla una nueva técnica
o método que funciona muy bien, otros grupos toman esas ideas
y desarrollan con base en ellas. Esto permite un gran salto
hacia adelante en la experimentación y el avance se acelera. Esto puede incluir hiperparámetros,
más capas, mejor generalizabilidad, mejores subcomponentes
como capas convolucionales, etcétera. Expliquen cómo aplicarían AA al problema. Puede haber más de una respuesta correcta. Son dueños de un centro de esquí
y quieren predecir los niveles de tráfico de las pistas de esquí
con base en cuatro tipos de clientes principiantes, intermedios,
avanzados y expertos que compraron boletos,
y la cantidad de nieve de años pasados. Tómense un momento
para escribir su respuesta. Podría ser regresión o clasificación,
ya que no especifiqué exactamente qué quiero decir con "niveles de tráfico". ¿Me refiero a la cantidad de personas
que usan la pista por hora? ¿O una descripción más categórica,
como alto, medio y bajo? Para esto,
comenzaría con una heurística base como la cantidad promedio
de personas en cada pendiente y, luego, modelos base
de regresión lineal o logística según lo que elija: regresión
o clasificación, respectivamente. Según el rendimiento
y la cantidad de datos, probablemente
avanzaría a las redes neuronales. Si hay otros atributos
en los datos, también los usaría y supervisaría el rendimiento. Según el último dato, en Google
hay más de 4,000 modelos de AA profundo internos que forman parte
de la tecnología de sus sistemas. Cada modelo y sus versiones
obtienen el beneficio del rendimiento basado en el éxito y los fracasos
de los modelos anteriores. Uno de los más usados
en el pasado fue Sibyl que se creó originalmente
para recomendar videos de YouTube. Este motor de recomendaciones
funcionó tan bien que se incorporó más tarde a los anuncios
y otros productos de Google. Era un modelo lineal. Este año, otro modelo
que se convirtió en motor de ajuste de parámetros predeterminado
para otros modelos y sistemas fue Google Brain, el brazo
de la investigación de AA de Google creado para aprovechar
el poder informático de miles de CPU para entrenar grandes modelos
como las redes neuronales profundas. La experiencia de crear y ejecutar
estos modelos es la que modeló la creación de TensorFlow, una biblioteca
de código abierto para el AA. Luego, Google creó TFX o la plataforma
de AA basada en TensorFlow. Les mostraremos cómo crear
y también implementar modelos de AA con TensorFlow y herramientas
como Cloud ML Engine, Dataflow y BigQuery. Para resumir, en las últimas décadas
se vio una proliferación en la adopción y el rendimiento de redes neuronales. Gracias a la universalidad de los datos,
estos modelos gozan del beneficio de cada vez más ejemplos
de entrenamiento para aprender. Los datos y ejemplos
en constante aumento se han combinado con la infraestructura escalable
para crear modelos más complejos y distribuidos con miles de capas. Los dejo con una observación. Aunque el rendimiento
con las redes neuronales puede ser mayor para algunas aplicaciones,
son apenas uno de los muchos tipos de modelos disponibles
para la experimentación, que es clave para obtener el mejor rendimiento
de los datos y así resolver sus desafíos.