2000年代になり処理能力が向上すると 機械学習では多数のモデルの性能を
組み合わせるようになりました これをアンサンブル法といいます 個々の学習器は弱くても
それを多数組み合わせることで 誤差が個々に依存せず強力なMLになります DNNではドロップアウト層を使って
これを近似化し モデルを正規化して過剰適合を避けます たとえば前方パスの確率を使って いくつかのニューロンを
ランダムにオフにすると 実質的に毎回異なるネットワークができます 複雑な問題を解くには多くの場合 1人の回答よりも何千人もの
回答を集めるべきです いわゆる群衆の叡智です 機械学習も同じです 分類でも回帰でも
多くの予測変数の結果を集計すると 最適な1つのモデルより
優れていることが多いです このように組み合わせた予測変数グループは アンサンブル学習になります このような学習アルゴリズムを
アンサンブル法といいます アンサンブル学習の中で
ランダムフォレストが特に有名です トレーニングセット全体から
1つの決定ツリーを作る代わりに 複数の決定ツリーを集めて それぞれにランダムなサブサンプルを与えます トレーニングセット全体は見えないので 全体を記憶することができません サブセットで
すべてのツリーをトレーニングした後 機械学習の最も重要な部分
つまり予測を実行します フォレストの各ツリーにテスト標本を送って その結果を集計します 分類であれば すべてのツリーによる多数決が 最終的な出力クラスになります 回帰であれば平均値 最大値 中間値などを 集計することができます より良い一般化のために
例や特徴をランダムサンプル化できます ランダムサンプリングの例には 置換 バギング（ブートストラップ集計） 置換なし貼り付けがあります データセット全体ではなく小さなサブセットで 個々の予測変数がトレーニングされ
偏りが大きくなりますが 集計することで偏りと分散が減ります アンサンブルの偏りは通常 トレーニングセット全体での1つの予測変数
の場合とほぼ同じですが 分散はより小さくなります 一般化誤差を検証する優れた方法が out-of-bagデータです これを使えば あらかじめデータセットから
個別のセットを抜き出す必要がありません ランダムホールドアウトを使った
K分割検証を連想させます 特徴から標本を得て しかも
例をランダムサンプリングすると ランダムサブ空間ができ
ランダムパッチと呼ばれます 弱い学習器をたくさん集めて
強い学習器を作るときには 適応的ブースティングや
勾配ブースティングなどを使います 通常はそれぞれの学習器を
順番にトレーニングし 1つ前の学習器の問題点を修正しようとします ブーストされたツリーでは
アンサンブルにツリーが加わるごとに 通常は予測が改善します でも無制限にツリーを
追加し続けるわけではありません 早期終了のために検証セットを使えば ツリーが多くなりすぎて トレーニングデータに
過剰適合するのを避けられます 最後に ニューラルネットワークのときと同様 スタッキングできます つまりアンサンブルの扱いを
メタ学習器に学習させ さらに その上に次々と
メタ学習器を積み重ねていきます DNNでのサブコンポーネントの積み重ねと
再使用については後で学びます この中で 個々の決定ツリーと比べて ランダムフォレストに
最も当てはまらないのはどれですか？ 正解は 「視覚的に解釈しやすい」です ニューラルネットワークと同様 モデルに層を追加すればするほど 複雑で理解しにくくなります ランダムフォレストは通常
個々の決定ツリーより複雑ですから 視覚的に解釈しにくいです 他の3つは正しいです ランダムフォレストの一般化は
バギングとサブ空間で改善します また分類や回帰の集計で
投票システムを使うことで フォレストの性能が個々のツリーより
かなり良くなります 最後にランダムフォレストの
ランダムサンプリングにより 個々のツリーとほぼ同じ偏りになるだけでなく 分散が小さくなります これも より良い一般化に役立ちます