1
00:00:00,000 --> 00:00:02,495
なぜパーセプトロンは1層のみなのでしょう？

2
00:00:02,495 --> 00:00:06,000
1つの層の出力を次の層に入力として送ったら

3
00:00:06,000 --> 00:00:10,885
複数のパーセプトロン層を合わせて
もっと強力なモデルにできるのでは？

4
00:00:10,885 --> 00:00:14,550
しかし もし非線形の活性化関数を使わないなら

5
00:00:14,550 --> 00:00:18,740
層をいくら追加しても
元の単一の線形層と同じになり

6
00:00:18,740 --> 00:00:21,695
実際に何の利点もありません

7
00:00:21,695 --> 00:00:24,670
非線形の活性化関数が必要です

8
00:00:24,670 --> 00:00:29,240
こうしてシグモイドまたは
双曲正接（tanh）活性化関数が

9
00:00:29,240 --> 00:00:32,260
非線形で使われるようになりました

10
00:00:32,260 --> 00:00:35,220
当時は これだけに限定されていました

11
00:00:35,220 --> 00:00:38,935
なぜなら モデルの重みに
誤差逆伝搬するための

12
00:00:38,935 --> 00:00:41,765
微分可能関数が必要だったからです

13
00:00:41,765 --> 00:00:45,370
最近では 微分可能ではない
活性化関数もありますが

14
00:00:45,370 --> 00:00:48,275
当時はその使い方がよく分かりませんでした

15
00:00:48,275 --> 00:00:51,920
この「活性化関数が微分可能で
なければならない」制約のために

16
00:00:51,920 --> 00:00:54,500
ネットワークのトレーニングが困難でした

17
00:00:54,500 --> 00:00:57,585
さらに このモデルの効果を
制約する要因として

18
00:00:57,585 --> 00:01:00,780
データ量や
コンピューティング リソースなどの

19
00:01:00,780 --> 00:01:02,280
課題がありました

20
00:01:02,280 --> 00:01:04,755
たとえば 最急降下法で

21
00:01:04,755 --> 00:01:07,570
大域的な最小値を見つける代わりに

22
00:01:07,570 --> 00:01:10,705
鞍点で最適化してしまうことがあります

23
00:01:10,705 --> 00:01:16,290
しかし正規化線形関数（ReLU）の
使い方が開発されて以降

24
00:01:16,290 --> 00:01:19,365
トレーニング速度が8～10倍になり

25
00:01:19,365 --> 00:01:22,565
ロジスティック回帰の収束がほぼ保証されます

26
00:01:23,355 --> 00:01:25,105
ちょうど脳のように

27
00:01:25,105 --> 00:01:28,390
パーセプトロンを互いに繋げて
複数の層にすると

28
00:01:28,390 --> 00:01:31,545
フィードフォワード
ニューラルネットワークになります

29
00:01:31,545 --> 00:01:35,400
構成要素は 単一層パーセプトロンと
ほぼ同じです

30
00:01:35,400 --> 00:01:37,770
つまり 入力 重み付き合計

31
00:01:37,770 --> 00:01:40,805
活性化関数 そして出力です

32
00:01:40,805 --> 00:01:42,365
1つの違いは

33
00:01:42,365 --> 00:01:45,235
入力層以外のニューロンへの入力が

34
00:01:45,235 --> 00:01:48,965
生の入力ではなく前の層の出力であることです

35
00:01:48,965 --> 00:01:53,540
もう1つの違いですが
層と層の間でニューロンを繋ぐ方法が

36
00:01:53,540 --> 00:01:56,140
ベクトルではなく行列です

37
00:01:56,140 --> 00:01:59,480
なぜなら 繋ぎ方が複雑だからです

38
00:02:00,500 --> 00:02:03,850
たとえば 図では入力層の重み行列が4x2

39
00:02:03,850 --> 00:02:07,060
隠し層の重み行列が2x1です

40
00:02:07,060 --> 00:02:08,560
後で学びますが

41
00:02:08,560 --> 00:02:11,980
ニューラルネットワークには
画像などに応用して

42
00:02:11,980 --> 00:02:15,340
優れた効果を発揮する
完全な接続性があるとは限りません

43
00:02:15,340 --> 00:02:19,295
さらに 活性化関数には
単位ステップ関数の他にも

44
00:02:19,295 --> 00:02:23,420
シグモイドや
双曲正接（tanh）などがあります

45
00:02:23,420 --> 00:02:25,960
入力以外のそれぞれのニューロンは

46
00:02:25,960 --> 00:02:29,400
3ステップからなる1つの単位だと
考えることができます

47
00:02:29,400 --> 00:02:31,860
最初のステップは重み付き合計

48
00:02:31,860 --> 00:02:34,020
次に活性化関数

49
00:02:34,020 --> 00:02:37,100
さらに活性化関数の出力です

50
00:02:38,020 --> 00:02:42,170
多くの層と ニューロン 活性化関数
トレーニング方法を含む

51
00:02:42,170 --> 00:02:44,940
複雑なネットワークを形成できます

52
00:02:44,940 --> 00:02:46,345
このコースでは

53
00:02:46,345 --> 00:02:49,127
TensorFlow Playgroundを使って

54
00:02:49,127 --> 00:02:51,837
情報の流れを直観的に見ていきます

55
00:02:51,837 --> 00:02:54,840
さらにハイパーパラメータを
カスタマイズしたり

56
00:02:54,840 --> 00:02:58,080
重みを可視化したりしながら

57
00:02:58,080 --> 00:03:01,735
損失関数の進歩を楽しく学びます

58
00:03:02,785 --> 00:03:04,965
この線形活性化関数は

59
00:03:04,965 --> 00:03:09,390
xに対して単にxを返すだけの恒等関数です

60
00:03:09,390 --> 00:03:11,480
これは初期の活性化関数です

61
00:03:11,480 --> 00:03:13,270
しかし すでに述べたとおり

62
00:03:13,270 --> 00:03:16,240
線形活性化関数だけを使って何千もの層を含む

63
00:03:16,240 --> 00:03:18,275
ニューラルネットワークを作っても

64
00:03:18,275 --> 00:03:22,690
その最終出力は単に
入力を線形に組み合わせただけです

65
00:03:22,690 --> 00:03:27,430
これは 入力ごとに
それぞれ定数を掛けるのと同じです

66
00:03:27,430 --> 00:03:29,325
おわかりでしょうか？

67
00:03:29,325 --> 00:03:31,260
単なる線形回帰です

68
00:03:31,260 --> 00:03:33,580
ですからニューラルネットワークに

69
00:03:33,580 --> 00:03:35,910
データ分布をよく学習させるには

70
00:03:35,910 --> 00:03:41,485
非線形の活性化関数を使って
連鎖関数にする必要があります

71
00:03:43,275 --> 00:03:47,175
f(x) = xである線形活性化関数に加えて

72
00:03:47,175 --> 00:03:50,680
ニューラルネットワークの最初の黄金期では

73
00:03:50,680 --> 00:03:54,920
シグモイドとtanh活性化関数が使われました

74
00:03:54,920 --> 00:03:57,325
シグモイド活性化関数は基本的に

75
00:03:57,325 --> 00:04:00,170
単位ステップ関数の形をスムーズにした関数で

76
00:04:00,170 --> 00:04:03,005
負の無限大に向かって0の漸近線と

77
00:04:03,005 --> 00:04:05,910
正の無限大に向かって1の漸近線があり

78
00:04:05,910 --> 00:04:08,630
その間に中間値があります

79
00:04:10,170 --> 00:04:13,170
双曲正接（tanh）活性化関数も

80
00:04:13,170 --> 00:04:15,970
よく使われた活性化関数です

81
00:04:15,970 --> 00:04:18,720
これは基本的にシグモイドを拡大して移動し

82
00:04:18,720 --> 00:04:21,404
-1から+1の範囲にしたものです

83
00:04:21,404 --> 00:04:24,120
すべての場所で微分可能で 単調で

84
00:04:24,120 --> 00:04:27,405
しかもスムーズな優れた関数でした

85
00:04:28,115 --> 00:04:32,030
しかし 関数の入力値が高い
または低いことが原因で

86
00:04:32,030 --> 00:04:34,420
飽和の問題が生じて

87
00:04:34,420 --> 00:04:38,240
関数の近似平坦域ができることがあります

88
00:04:38,240 --> 00:04:41,290
そのような地点ではカーブがほぼ平坦ですから

89
00:04:41,290 --> 00:04:43,825
微分は極めてゼロに近づきます

90
00:04:43,825 --> 00:04:48,330
このため 重みのトレーニングが非常に遅く
止まることさえあります

91
00:04:48,330 --> 00:04:50,660
勾配がゼロに近いので

92
00:04:50,660 --> 00:04:55,950
最急降下での刻み幅サイズが
とても小さくなります

93
00:04:55,950 --> 00:04:59,735
線形活性化関数は微分可能で
単調でスムーズでした

94
00:04:59,735 --> 00:05:01,355
すでに述べましたが

95
00:05:01,355 --> 00:05:05,360
多くの線形関数を線形に繋げても
1つのニューロンと同じです

96
00:05:05,360 --> 00:05:07,880
これでは データを適切に記述するための

97
00:05:07,880 --> 00:05:10,235
複雑な連鎖ができません

98
00:05:10,235 --> 00:05:12,660
線形活性化関数の近似も行われましたが

99
00:05:12,660 --> 00:05:15,115
どこでも微分可能ではありませんでした

100
00:05:15,115 --> 00:05:18,490
対処法が見つかったのはずっと後のことです

101
00:05:19,450 --> 00:05:24,225
現在では正規化線形（ReLU）活性化関数が
よく使われています

102
00:05:24,225 --> 00:05:28,010
非線形なので 必要に応じて
複雑なモデルを作成でき

103
00:05:28,010 --> 00:05:32,040
入力空間の負以外の部分で飽和がありません

104
00:05:32,520 --> 00:05:37,430
ただし入力空間の負の部分は
ゼロ活性化に変換されるので

105
00:05:37,430 --> 00:05:41,115
ReLU層はまったく活性化しない場合があり

106
00:05:41,115 --> 00:05:45,660
このためトレーニングが遅延 停止する
可能性があります

107
00:05:46,980 --> 00:05:49,085
解決策がいくつかあります

108
00:05:49,085 --> 00:05:50,280
1つは

109
00:05:50,280 --> 00:05:54,700
指数線形ユニット（ELU）という
別の活性化関数を使うことです

110
00:05:55,530 --> 00:05:58,740
これは 入力空間の負以外の部分でほぼ線形

111
00:05:58,740 --> 00:06:02,325
しかも形がスムーズで単調です 最も重要なのは

112
00:06:02,325 --> 00:06:05,440
入力空間の負の部分で非ゼロです

113
00:06:05,440 --> 00:06:08,610
ELUの主な欠点は 指数を計算するので

114
00:06:08,610 --> 00:06:12,470
ReLUよりも処理能力が必要なことです

115
00:06:13,160 --> 00:06:16,315
次のモジュールでもっと実験します

116
00:06:16,315 --> 00:06:19,690
出力を確率形式で得たい場合に

117
00:06:19,690 --> 00:06:24,260
最後の層でどの活性化関数を選ぶべきですか？

118
00:06:26,370 --> 00:06:29,440
正解はシグモイド活性化関数です

119
00:06:29,440 --> 00:06:33,000
シグモイド関数の範囲が0～1だからです

120
00:06:33,000 --> 00:06:35,165
確率の範囲もこれと同じです

121
00:06:35,165 --> 00:06:36,800
範囲だけでなく

122
00:06:36,800 --> 00:06:40,950
シグモイド関数は
ロジスティック確率分布の累積分布関数です

123
00:06:40,950 --> 00:06:42,740
その分位点関数は

124
00:06:42,740 --> 00:06:46,275
対数オッズをモデル化するロジックの逆です

125
00:06:46,275 --> 00:06:49,585
このため これを真の確率として使用できます

126
00:06:49,585 --> 00:06:53,435
これらの理由については
専門分野の後の部分でさらに扱います

127
00:06:53,435 --> 00:06:57,795
tanhは間違いです これは
シグモイドと同じスカッシング関数ですが

128
00:06:57,795 --> 00:07:00,030
範囲が-1～+1なので

129
00:07:00,030 --> 00:07:02,285
確率の範囲と異なります

130
00:07:02,285 --> 00:07:06,040
しかもtanhを単にシグモイドに
スカッシングしただけでは

131
00:07:06,040 --> 00:07:07,665
確率になりません

132
00:07:07,665 --> 00:07:09,900
なぜならすでに述べたような

133
00:07:09,900 --> 00:07:13,235
シグモイド出力を確率と解釈する
特性がないためです

134
00:07:13,235 --> 00:07:15,610
シグモイドに正しく変換するには

135
00:07:15,610 --> 00:07:19,790
まず1を加えて2で割って
正しい範囲にする必要があります

136
00:07:19,790 --> 00:07:22,245
さらに正しい分布を得るには

137
00:07:22,245 --> 00:07:25,130
tanh引数を2で割る必要があります

138
00:07:25,130 --> 00:07:27,515
でもすでにtanhを計算しましたから

139
00:07:27,515 --> 00:07:29,300
多くの作業をこのままにします

140
00:07:29,300 --> 00:07:32,180
最初にシグモイドだけを使うのがよいでしょう

141
00:07:32,180 --> 00:07:36,565
ReLUも間違いです ゼロから無限大の範囲なので

142
00:07:36,565 --> 00:07:39,315
確率を表すにはまったく不適切です

143
00:07:39,315 --> 00:07:44,340
ELUもマイナスの無限大から無限大なので
間違いです