Voltando para a linha do tempo,
falamos agora das redes neurais, com ainda mais melhorias com
os saltos na capacidade computacional, além de uma enorme quantidade de dados. As DNNs começaram a superar outros
métodos de teste como visão computacional. Além do grande
avanço do hardware aprimorado, há muitas
novas técnicas e arquiteturas que otimizaram o treinamento de
redes neurais profundas como ReLUs, métodos melhores de inicialização, CNNs
(redes neurais convolucionais) e dropout. Falamos sobre algumas
dessas técnicas em outros métodos de ML. Sobre o uso de funções
de ativação não lineares como ReLUs, que agora costumam
ser definidas como padrão, falamos durante
a primeira abordagem de redes neurais. Camadas de dropout começaram
a ser usadas para ajudar a generalização, que é como métodos de combinação, abordados quando falamos de
florestas aleatórias e árvores de decisão. As camadas convolucionais
foram adicionadas para reduzir a carga computacional e de memória
devido à conectividade não total dela, além de se concentrar em aspectos locais. Por exemplo, em imagens, em vez de 
comparar fatores não relacionados nelas. Em outras palavras, todos os avanços
que surgiram em outros métodos de ML foram transformados em redes neurais. Vamos ver um exemplo
de rede neural profunda. Este incrível histórico
do aprendizado de máquina culminou na aprendizagem profunda
com redes neurais que contêm centenas de camadas e milhões de parâmetros,
mas com resultados excelentes. Este é o GoogLeNet ou Inception, um modelo de classificação de imagem. Ele foi treinado para o ImageNet 
Large Visual Recognition Challenge em 2014, usando dados de 2012,
para classificar imagens em milhares de classes com
1,2 milhão de imagens de treinamento. Ele tem 22 camadas profundas, 27, se incluir o pool, que abordaremos mais adiante no curso, além de centenas de camadas, se dividi-lo
em blocos de construção independentes. Há mais de 11 milhões
de parâmetros treinados. Algumas camadas são totalmente conectadas,
outras não, como as convolucionais,
que abordaremos mais tarde. São usadas camadas de dropout
para aumentar a generalização, simulando uma combinação
de redes neurais profundas. Como vimos nas
redes neurais e empilhamento, cada caixa é uma unidade de
componentes em um grupo maior, como aquele que mostramos ampliado. Essa ideia de blocos de construção
que formam algo maior do que a soma das partes é um dos motivos
do sucesso da aprendizagem profunda. Claro, uma quantidade
de dados cada vez maior, uma capacidade de computação robusta e
mais memória também ajudam. Há agora diversas versões
que vão além disso, são muito maiores
e têm precisão ainda melhor. O principal ponto de
todo este histórico é que a pesquisa do aprendizado de máquina
reutiliza partes de técnicas de outros algoritmos antigos
e as combina para criar modelos muito avançados
e, principalmente, realizar testes. O que é importante ao
criar redes neurais profundas? Resposta correta: "Todas as opções acima". Esta não é uma lista detalhada, mas é muito importante
considerar estas três opções. Primeiro, você precisa ter muitos dados. Muitas pesquisas estão
sendo feitas para tentar reduzir os dados para aprendizagem profunda. Até lá, precisamos ter muitos deles. Isso acontece por conta
da alta capacidade do número de parâmetros que precisam
ser treinados nestes modelos enormes. Como o modelo é muito complexo, ele realmente precisa
internalizar a distribuição de dados. Portanto, são necessários muitos sinais. Lembre-se: o propósito do
aprendizado de máquina não é simplesmente treinar
um grupo de modelos sofisticados. É treiná-los para que
façam previsões bastante precisas. Se não é possível generalizar
novos dados para fazer previsões, então qual é o sentido desse modelo? Portanto, repetindo, ter
dados suficientes é importante para que não se sobreajustem a
um conjunto visto milhões de vezes, em vez de a um enorme, visto muito menos. Isso também possibilita ter conjuntos de teste e validação grandes
para ajustar modelos. Além disso, ao adicionar camadas
de dropout, executar aumento de dados, incluir ruído etc., você aprimora
ainda mais a generalização. Por fim, o aprendizado de máquina
é experimentação. Há vários tipos diferentes de algoritmo, hiperparâmetros e formas de
criar conjuntos de dados atualmente. Não há uma forma prioritária de saber, desde o início, quais as melhores
opções para quase todos os problemas. Com a experimentação e o acompanhamento
cuidadoso do que já foi realizado e avaliações de desempenho
para comparar modelos, você se divertirá muito e
criará algumas ferramentas poderosas. Em seguida, vamos falar
um pouco mais sobre como redes neurais continuam a se basear
no desempenho de modelos antigos. Este é o desempenho de versões específicas de modelo de
redes neurais profundas em vários anos. Como é possível ver no gráfico, um grande salto ocorreu em 2014, destacado em azul, em que o modelo "Inception" do Google evoluiu de 10% a 6,7% de taxa de erro. O desempenho das DNNs
continua melhorando a cada ano, usando a experiência
aprendida de modelos anteriores. Em 2015, uma terceira
versão do modelo "Inception" alcançou 3,5% de taxa de erro. O que levou estes modelos a
se aprimorar tanto em um período pequeno? Muitas vezes, quando pesquisadores criam
uma nova técnica ou método muito bons, outros usam essas ideias
como base de criação. Isso gera um grande salto na
experimentação para acelerar o progresso. Isso inclui melhores hiperparâmetros,
mais camadas, melhor generalização, subcomponentes 
aprimorados como camada convolucional etc. Explique como você
aplicaria o ML ao problema. Pode haver mais de uma resposta correta. Você é o dono de uma estação de
esqui e quer prever níveis de tráfego nas pistas com base em
quatro tipos de cliente, iniciante, intermediário, avançado
e especialista, que compraram bilhetes, e com base na quantidade passada de neve. Escreva agora sua resposta. Aplicam-se regressão ou classificação, já que o significado de níveis de tráfego
não foi exatamente especificado. Queremos dizer o número
de pessoas que usam a pista por hora? Ou algo mais categórico
como alto, médio e baixo? Para isso, começamos
com uma heurística de base como o número médio
de pessoas em cada pista e depois modelos de base
de regressão logística ou linear, dependendo do processo
de classificação ou regressão. Conforme o desempenho,
a quantidade de dados, talvez seja necessário usar redes neurais. Se houver outros recursos nos dados, é bom testá-los e monitorar o desempenho. No Google, segundo a última contagem, há mais de 4.000 modelos de ML profundo
de produção capacitando sistemas. Cada modelo e versão
melhora o desempenho ao se basear no sucesso e falhas de modelos antigos. Um dos mais usados
antigamente era o Sibyl, criado inicalmente para recomendar
vídeos relacionados do YouTube. Esse mecanismo funcionava tão bem que depois foi muito incorporado a
anúncios e outras partes do Google. O modelo era linear. Neste ano, outro modelo acabou se tornando o verdadeiro mecanismo de ajuste de
parâmetro de outros modelos e sistemas. O Google Brain é
a divisão de pesquisa de ML que criou um jeito de aproveitar
a capacidade computacional de milhares de CPUs para treinar grandes modelos
como redes neurais profundas. A experiência ao criar
e executar os modelos foi o que moldou
a criação do TensorFlow, uma biblioteca de código aberto de ML. O Google então criou o TFX ou
a plataforma de ML baseada no TensorFlow. Mostraremos a você como criar
 e implantar modelos de ML de produção com o TensorFlow e o
Cloud ML Engine, Dataflow e BigQuery. Recapitulando, ocorreu nas últimas décadas um aumento na adoção 
e no desempenho de redes neurais. Com a onipresença dos dados, estes modelos têm a vantagem de aprender
com cada vez mais exemplos de treinamento. O aumento nos dados e exemplos se aliou à infraestrutura escalonável para modelos
complexos distribuídos de várias camadas. Uma observação que
deixaremos com você é que, mesmo que o desempenho de redes neurais
seja ótimo em alguns aplicativos, elas são apenas algumas dos vários tipos
de modelo disponíveis para testar. A experimentação é o segredo para garantir o melhor desempenho usando
seus dados para superar seus desafios.