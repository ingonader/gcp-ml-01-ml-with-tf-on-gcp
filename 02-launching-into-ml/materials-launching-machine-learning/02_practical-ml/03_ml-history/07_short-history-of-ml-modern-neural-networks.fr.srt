1
00:00:00,210 --> 00:00:02,980
Les réseaux de neurones
réapparaissent dans la chronologie,

2
00:00:02,980 --> 00:00:05,610
mais désormais avec d'autres avantages,
comme les bonds,

3
00:00:05,610 --> 00:00:08,560
la puissance de calcul et le traitement
de nombreuses données.

4
00:00:08,760 --> 00:00:13,090
Les DNN sont devenus plus performants
que d'autres méthodes pour le test,

5
00:00:13,090 --> 00:00:14,730
comme la vision par ordinateur.

6
00:00:15,070 --> 00:00:17,680
En plus du développement
des améliorations matérielles,

7
00:00:17,680 --> 00:00:21,610
de nombreuses astuces et architectures
permettent d'améliorer la capacité

8
00:00:21,610 --> 00:00:24,615
d'entraînement des réseaux
de neurones profonds, comme les ReLu,

9
00:00:24,615 --> 00:00:30,195
les méthodes d'initialisation optimisées,
les CNN et le dropout.

10
00:00:30,935 --> 00:00:34,600
Nous avons abordé ces astuces
pour d'autres méthodes de ML.

11
00:00:34,650 --> 00:00:38,120
On a vu les fonctions d'activation
non linéaires, telles que les ReLu,

12
00:00:38,120 --> 00:00:40,435
qui sont souvent désormais
utilisées par défaut,

13
00:00:40,435 --> 00:00:42,920
dans la première vidéo
sur les réseaux de neurones.

14
00:00:43,470 --> 00:00:47,235
Adoptées pour améliorer la généralisation,
les couches de dropout fonctionnent

15
00:00:47,235 --> 00:00:48,695
comme des méthodes d'ensemble,

16
00:00:48,695 --> 00:00:51,810
abordées en parallèle des forêts
aléatoires et des arbres boostés.

17
00:00:52,370 --> 00:00:54,760
Les couches convolutives
ont permis de réduire

18
00:00:54,760 --> 00:00:58,825
la charge de calcul et de mémoire
grâce à leur connectivité incomplète,

19
00:00:58,825 --> 00:01:01,990
et à leur capacité à se concentrer
sur des aspects locaux, comme

20
00:01:01,990 --> 00:01:05,905
des images, plutôt que de comparer
des éléments non liés dans une image.

21
00:01:06,875 --> 00:01:10,780
Autrement dit, toutes les avancées
dans les autres méthodes de ML

22
00:01:10,780 --> 00:01:12,895
ont profité aux réseaux de neurones.

23
00:01:13,085 --> 00:01:15,490
Voyons un exemple
de réseau de neurones profond.

24
00:01:15,910 --> 00:01:20,485
L'évolution du machine learning
a abouti au deep learning,

25
00:01:20,485 --> 00:01:22,830
avec des DNN dotés
de centaines de couches,

26
00:01:22,830 --> 00:01:26,170
et de millions de paramètres,
mais apportant des résultats prodigieux.

27
00:01:26,400 --> 00:01:29,065
Voici le modèle GoogLeNet ou Inception,

28
00:01:29,065 --> 00:01:30,900
un modèle de classification d'images.

29
00:01:31,310 --> 00:01:35,050
Il a été entraîné lors du challenge de
reconnaissance visuelle à grande échelle

30
00:01:35,050 --> 00:01:37,930
d'ImageNet en 2014
avec des données de 2012.

31
00:01:38,030 --> 00:01:41,250
Il devait classer des images
dans mille classes,

32
00:01:41,250 --> 00:01:43,920
et disposait de 1,2 million d'images
pour l'entraînement.

33
00:01:44,190 --> 00:01:46,420
Il contient 22 couches profondes,

34
00:01:46,420 --> 00:01:48,590
27 en comptant le pooling,

35
00:01:48,590 --> 00:01:50,480
dont nous parlerons plus tard,

36
00:01:50,480 --> 00:01:54,340
et 100 couches si vous le divisez
en blocs de construction indépendants.

37
00:01:54,800 --> 00:01:57,850
Il y a plus de 11 millions
de paramètres entraînés.

38
00:01:58,440 --> 00:02:01,355
Certaines couches sont entièrement
connectées et d'autres non,

39
00:02:01,355 --> 00:02:04,225
comme les couches convolutives
dont nous parlerons plus tard.

40
00:02:04,565 --> 00:02:07,225
Les couches de dropout
améliorent la généralisation

41
00:02:07,225 --> 00:02:09,840
en simulant un ensemble
de réseaux de neurones profonds.

42
00:02:10,000 --> 00:02:12,410
Comme avec les réseaux de neurones
et l'empilement,

43
00:02:12,410 --> 00:02:15,845
chaque case est une unité de
composants qui fait partie d'un groupe,

44
00:02:15,845 --> 00:02:17,735
comme celui sur lequel j'ai zoomé.

45
00:02:17,905 --> 00:02:22,100
L'expansion des blocs en un tout
supérieur à la somme de ses parties

46
00:02:22,100 --> 00:02:25,390
est l'un des éléments qui rend
le deep learning si efficace.

47
00:02:25,970 --> 00:02:28,300
L'abondance croissante de données,

48
00:02:28,300 --> 00:02:31,485
la puissance de calcul et davantage
de mémoire y contribuent aussi.

49
00:02:32,115 --> 00:02:34,830
Il existe désormais plusieurs versions

50
00:02:34,830 --> 00:02:37,575
bien plus grosses et précises.

51
00:02:38,085 --> 00:02:40,310
Le principal point
à retenir de cet historique

52
00:02:40,310 --> 00:02:43,740
est que la recherche en machine learning
réutilise des techniques

53
00:02:43,740 --> 00:02:47,150
d'anciens algorithmes et les combine

54
00:02:47,150 --> 00:02:50,675
pour créer des modèles très puissants,
mais surtout des expérimentations.

55
00:02:51,040 --> 00:02:54,430
Quel élément est important pour créer
des réseaux de neurones profonds ?

56
00:02:56,660 --> 00:02:59,195
La bonne réponse est
"Tous les éléments ci-dessus".

57
00:02:59,255 --> 00:03:01,445
Cette liste est non exhaustive,

58
00:03:01,445 --> 00:03:04,460
mais ces trois éléments
sont très importants à retenir.

59
00:03:04,760 --> 00:03:07,790
Vous devez d'abord veiller à
disposer de beaucoup de données.

60
00:03:07,930 --> 00:03:11,300
Des recherches tentent actuellement
de réduire les besoins en données

61
00:03:11,300 --> 00:03:13,360
du deep learning,
mais dans l'intervalle,

62
00:03:13,360 --> 00:03:15,360
il faut utiliser beaucoup de données.

63
00:03:15,760 --> 00:03:19,240
Cela est dû à la haute capacité
des nombreux paramètres

64
00:03:19,240 --> 00:03:21,680
à entraîner dans ces modèles
de grande envergure.

65
00:03:22,350 --> 00:03:24,360
Comme les modèles sont très complexes,

66
00:03:24,360 --> 00:03:27,165
ils doivent bien assimiler
la distribution des données.

67
00:03:27,455 --> 00:03:29,630
Ils ont donc besoin de nombreux signaux.

68
00:03:29,910 --> 00:03:32,930
Le but du machine learning
n'est pas d'entraîner

69
00:03:32,930 --> 00:03:35,440
de nombreux modèles élaborés
sans raison,

70
00:03:35,440 --> 00:03:38,525
mais pour qu'ils fassent
des prédictions très précises.

71
00:03:39,025 --> 00:03:41,810
Sans généralisation des données
pour faire des prédictions,

72
00:03:41,810 --> 00:03:43,715
votre modèle sera inutile.

73
00:03:44,465 --> 00:03:47,660
Il est donc important d'avoir
suffisamment de données,

74
00:03:47,660 --> 00:03:52,125
pour ne pas surapprendre un petit
ensemble de données très courant,

75
00:03:52,125 --> 00:03:54,795
mais plutôt un grand ensemble
de données plus rare.

76
00:03:55,275 --> 00:03:57,320
Vous disposez ainsi
d'ensembles assez grands

77
00:03:57,320 --> 00:03:59,895
de validation et d'évaluation
pour régler votre modèle.

78
00:04:00,105 --> 00:04:03,860
L'ajout de couches de dropout,
l'augmentation des données,

79
00:04:03,860 --> 00:04:08,095
l'ajout de bruit et autres vous permettent
d'améliorer la généralisation.

80
00:04:08,705 --> 00:04:12,080
Enfin, le machine learning est axé
sur l'expérimentation.

81
00:04:12,650 --> 00:04:14,760
Il existe de nombreux types d'algorithmes,

82
00:04:14,760 --> 00:04:18,024
d'hyperparamètres et de façons
de créer des ensembles de données.

83
00:04:18,294 --> 00:04:20,570
Il n'est pas primordial de connaître

84
00:04:20,570 --> 00:04:23,615
les bonnes options dès le début
pour tous les problèmes.

85
00:04:24,245 --> 00:04:28,110
Expérimentez et gardez une trace
de ce que vous avez déjà essayé

86
00:04:28,110 --> 00:04:30,685
et des performances mesurées
pour comparer vos modèles.

87
00:04:30,715 --> 00:04:35,170
Ainsi, vous vous amuserez
tout en créant des outils très puissants.

88
00:04:35,900 --> 00:04:37,860
Je vous expliquerai ensuite comment

89
00:04:37,860 --> 00:04:41,465
les réseaux de neurones s'appuient
sur les performances des anciens modèles.

90
00:04:42,245 --> 00:04:44,470
Voici les performances
de versions de modèles

91
00:04:44,470 --> 00:04:46,740
de réseaux de neurones
profonds au fil des ans.

92
00:04:47,160 --> 00:04:48,590
Comme le montre ce tableau,

93
00:04:48,590 --> 00:04:50,970
une avancée importante
s'est produite en 2014,

94
00:04:50,970 --> 00:04:52,390
ici surlignée en bleu,

95
00:04:52,390 --> 00:04:54,390
lorsque le modèle Inception de Google

96
00:04:54,390 --> 00:04:57,215
a réduit le taux d'erreur de 10 % à 6,7 %.

97
00:04:57,765 --> 00:05:01,330
Les performances des DNN continuent
de s'améliorer chaque année,

98
00:05:01,330 --> 00:05:03,950
car ils tirent des enseignements
des modèles précédents.

99
00:05:04,290 --> 00:05:07,390
En 2015, la version 3 du modèle Inception

100
00:05:07,390 --> 00:05:09,700
avait un taux d'erreur de 3,5 %.

101
00:05:10,010 --> 00:05:13,775
Pourquoi ces performances se sont-elles
tant améliorées sur une courte période ?

102
00:05:14,415 --> 00:05:18,465
Quand des chercheurs développent une
nouvelle technique ou méthode efficace,

103
00:05:18,465 --> 00:05:21,950
d'autres s'en servent souvent
ensuite comme point de départ.

104
00:05:22,600 --> 00:05:27,830
Cela permet de grandes avancées
en expérimentation et accélère le progrès.

105
00:05:28,610 --> 00:05:31,750
Il peut s'agir d'améliorer
des hyperparamètres, des couches,

106
00:05:31,750 --> 00:05:36,415
la généralisation ou des sous-composants,
comme les couches convolutives, etc.

107
00:05:36,925 --> 00:05:39,710
Expliquez comment
vous appliqueriez le ML à ce problème.

108
00:05:40,350 --> 00:05:42,445
Plusieurs réponses sont possibles.

109
00:05:44,285 --> 00:05:48,510
Vous possédez une station de ski et
voulez prédire l'affluence sur les pistes

110
00:05:48,510 --> 00:05:53,465
en fonction des quatre types de clients,
débutant, intermédiaire, avancé et expert,

111
00:05:53,465 --> 00:05:57,045
qui ont acheté un forfait, et des
chutes de neige précédentes.

112
00:06:00,025 --> 00:06:01,735
Notez votre réponse maintenant.

113
00:06:04,095 --> 00:06:07,265
Il peut s'agir d'un problème
de régression ou de classification,

114
00:06:07,265 --> 00:06:10,900
car je n'ai pas précisé
ce que j'entendais par affluence.

115
00:06:11,480 --> 00:06:14,950
S'agit-il du nombre de personnes
qui skient sur cette piste par heure ?

116
00:06:15,480 --> 00:06:19,045
Ou s'agit-il d'une catégorie telle que
affluence élevée, moyenne et faible ?

117
00:06:19,515 --> 00:06:21,830
Je commencerais par
une heuristique de base,

118
00:06:21,830 --> 00:06:24,680
comme le nombre moyen
de skieurs sur chaque piste,

119
00:06:24,680 --> 00:06:28,370
puis je passerais à un modèle basique
de régression linéaire ou logistique,

120
00:06:28,370 --> 00:06:32,775
en fonction de mon choix
entre la régression ou la classification.

121
00:06:33,355 --> 00:06:35,645
Selon les performances
et la quantité de données,

122
00:06:35,645 --> 00:06:37,785
j'utiliserais ensuite
des réseaux de neurones.

123
00:06:38,425 --> 00:06:40,240
S'il existe d'autres caractéristiques,

124
00:06:40,240 --> 00:06:42,755
je les testerais aussi
et suivrais les performances.

125
00:06:45,445 --> 00:06:48,790
Aux dernières nouvelles,
Google utilise en interne

126
00:06:48,790 --> 00:06:52,775
plus de 4 000 modèles de deep learning
pour optimiser ses systèmes.

127
00:06:53,145 --> 00:06:56,230
Tous ces modèles et leurs versions
améliorent leurs performances

128
00:06:56,230 --> 00:06:59,860
en s'appuyant sur les succès
et les échecs d'anciens modèles.

129
00:07:00,650 --> 00:07:03,565
Sibyl a été l'un des modèles
les plus utilisés au début.

130
00:07:03,565 --> 00:07:06,810
Il avait été créé pour recommander
des vidéos similaires sur YouTube.

131
00:07:06,900 --> 00:07:09,630
Comme ce moteur de recommandations
était très performant,

132
00:07:09,630 --> 00:07:13,155
il a été largement intégré dans
les annonces et d'autres services Google.

133
00:07:13,585 --> 00:07:15,540
Ce modèle était linéaire.

134
00:07:16,040 --> 00:07:19,460
Un autre modèle est devenu cette année

135
00:07:19,460 --> 00:07:23,570
le moteur de réglage de paramètres clé
pour les autres modèles et systèmes.

136
00:07:24,130 --> 00:07:27,020
Google Brain, la division
recherche en ML de Google,

137
00:07:27,020 --> 00:07:29,810
a créé une solution pour
exploiter la puissance de calcul

138
00:07:29,810 --> 00:07:34,060
de milliers de processeurs pour entraîner
de grands modèles, comme les DNN.

139
00:07:35,130 --> 00:07:39,700
De la conception et l'exécution
de ces modèles est né TensorFlow,

140
00:07:39,700 --> 00:07:42,230
une bibliothèque Open Source
pour le machine learning.

141
00:07:42,680 --> 00:07:47,060
Google a ensuite créé TFX, la plate-forme
de machine learning basée sur TensorFlow.

142
00:07:47,170 --> 00:07:50,650
Vous découvrirez comment créer et
déployer des modèles de ML de production

143
00:07:50,650 --> 00:07:54,710
avec TensorFlow et des outils comme
Cloud ML Engine, Dataflow et BigQuery.

144
00:07:55,040 --> 00:07:57,465
Pour résumer,
au cours des dernières décennies,

145
00:07:57,465 --> 00:08:00,990
l'adoption et les performances des réseaux
de neurones se sont développées.

146
00:08:01,330 --> 00:08:02,810
Grâce à l'ubiquité des données,

147
00:08:02,810 --> 00:08:06,635
ces modèles peuvent apprendre
de davantage d'exemples d'entraînement.

148
00:08:07,375 --> 00:08:11,550
L'augmentation des données et des exemples
associée à une infrastructure évolutive

149
00:08:11,550 --> 00:08:15,550
a engendré des modèles complexes et
distribués avec des milliers de couches.

150
00:08:16,150 --> 00:08:18,240
Une dernière chose avant de terminer.

151
00:08:18,240 --> 00:08:22,015
Bien que les réseaux de neurones soient
performants pour certaines applications,

152
00:08:22,015 --> 00:08:25,540
il existe de nombreux autres types
de modèles que vous pouvez tester.

153
00:08:25,790 --> 00:08:27,790
L'expérimentation est essentielle

154
00:08:27,790 --> 00:08:31,360
pour relever votre défi le plus
efficacement possible avec vos données.