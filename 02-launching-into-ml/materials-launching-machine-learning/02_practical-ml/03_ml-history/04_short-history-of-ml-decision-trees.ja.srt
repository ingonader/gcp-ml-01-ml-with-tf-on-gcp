1
00:00:00,000 --> 00:00:03,685
ID3やC 4.5などの ツリーアルゴリズムが

2
00:00:03,685 --> 00:00:06,130
1980～90年代に発明されました

3
00:00:06,130 --> 00:00:09,075
これらは特定の線形回帰の問題に適していて

4
00:00:09,075 --> 00:00:11,450
人間が簡単に解釈できます

5
00:00:11,450 --> 00:00:15,695
ツリー作成時に最適分割を見つけることは
NP困難の問題ですから

6
00:00:15,695 --> 00:00:18,260
貪欲アルゴリズムを使って

7
00:00:18,260 --> 00:00:21,700
できるだけ最適に近いツリーを作ろうとします

8
00:00:22,530 --> 00:00:25,040
これにより基本的に再緩層の

9
00:00:25,040 --> 00:00:27,495
区分線形決定面ができます

10
00:00:27,495 --> 00:00:30,745
しかし DNN（ディープニューラル
ネットワーク）では

11
00:00:30,745 --> 00:00:34,525
実際の各層が結びついて
ハイパー平面決定面を形成します

12
00:00:34,525 --> 00:00:36,875
これで さらに強力になります

13
00:00:36,875 --> 00:00:40,640
DNNが決定ツリーよりも
優れているのはなぜでしょうか

14
00:00:40,640 --> 00:00:43,405
まず決定ツリーについてお話ししましょう

15
00:00:43,405 --> 00:00:47,890
決定ツリーはきわめて直観的な
機械学習アルゴリズムです

16
00:00:47,890 --> 00:00:51,785
分類と回帰の両方にこれを使用できます

17
00:00:51,785 --> 00:00:54,060
あるデータセットのデータを

18
00:00:54,060 --> 00:00:57,155
さまざまな容器に分ける方法を決めるとします

19
00:00:57,155 --> 00:00:58,800
最初にすべきことは

20
00:00:58,800 --> 00:01:02,170
データセットを使った質問を考えることです

21
00:01:02,170 --> 00:01:04,370
例を挙げましょう

22
00:01:05,570 --> 00:01:10,870
これはタイタニック号の大事故の
生存者と死亡者を予測する有名な問題です

23
00:01:10,870 --> 00:01:13,165
あらゆる職業 階層

24
00:01:13,165 --> 00:01:16,465
さまざまな背景 境遇の人が乗船していました

25
00:01:16,465 --> 00:01:20,400
これらの特徴のいずれかで
データをはっきり分割して

26
00:01:20,400 --> 00:01:25,510
高い確率で生存者を
予測できるかどうか試みます

27
00:01:26,330 --> 00:01:30,235
最初に思いつく特徴は乗客の性別でしょう

28
00:01:30,235 --> 00:01:33,815
そこで「性別は男性ですか」
という質問に基づき

29
00:01:33,815 --> 00:01:37,300
データの中で男性を1つのバケットに入れて

30
00:01:37,300 --> 00:01:39,310
残りを別のバケットに入れます

31
00:01:39,310 --> 00:01:41,700
データの64%が男性バケットに

32
00:01:41,700 --> 00:01:44,315
36%が別のバケットに入りました

33
00:01:44,315 --> 00:01:47,275
男性バケットで続きを考えましょう

34
00:01:48,215 --> 00:01:52,315
次の質問は それぞれの乗船客の等級です

35
00:01:52,315 --> 00:01:55,030
すると全乗客の14%が

36
00:01:55,030 --> 00:01:58,510
最も低い等級で乗船した男性でした

37
00:01:58,510 --> 00:02:00,220
乗客の50%が

38
00:02:00,220 --> 00:02:03,290
それより上の2つの等級の男性でした

39
00:02:03,830 --> 00:02:07,730
続いて 同じ区分がツリーの
女性分岐にも適用されます

40
00:02:07,730 --> 00:02:11,285
決定ツリー作成アルゴリズムを1つ戻ると

41
00:02:11,285 --> 00:02:14,155
性別の値は2つしかないので

42
00:02:14,155 --> 00:02:16,875
性別が2つに分岐します

43
00:02:16,875 --> 00:02:19,780
でも 低い等級の乗客を左の枝に

44
00:02:19,780 --> 00:02:22,230
高い等級を右の枝に分けるよう

45
00:02:22,230 --> 00:02:24,925
どうやって決定したのでしょうか

46
00:02:25,505 --> 00:02:30,880
たとえば 単純な分類と回帰ツリー
つまりCARTアルゴリズムでは

47
00:02:30,880 --> 00:02:33,930
最も純粋なサブセットに分割できるよう

48
00:02:33,930 --> 00:02:37,390
1つの特徴としきい値のペアを選びます

49
00:02:38,140 --> 00:02:41,980
分類ツリーでは
列指標としてジニ不純度を使いますが

50
00:02:41,980 --> 00:02:43,835
エントロピーもあります

51
00:02:43,835 --> 00:02:45,720
適切に分割されたら

52
00:02:45,720 --> 00:02:48,495
別の特徴としきい値のペアを探して

53
00:02:48,495 --> 00:02:50,735
それもサブセットに分けます

54
00:02:50,735 --> 00:02:53,440
このプロセスを繰り返すとやがて

55
00:02:53,440 --> 00:02:56,315
ツリーの特定の最大深度に達するか

56
00:02:56,315 --> 00:03:00,450
またはこれ以上分割で
不純度を下げられなくなります

57
00:03:00,450 --> 00:03:04,565
再帰ツリーの場合は分割指標として
平均二乗誤差をよく使います

58
00:03:04,565 --> 00:03:09,155
データを2つのサブセットに分ける方法は
前にもありましたね

59
00:03:09,155 --> 00:03:12,970
それぞれの分割は単なる
バイナリ線形分類器です

60
00:03:12,970 --> 00:03:17,285
ある特徴を特定のしきい値で
切り分ける超平面を探すのです

61
00:03:17,285 --> 00:03:21,490
しきい値は超平面の反対側に
行ってしまうクラスメンバーの

62
00:03:21,490 --> 00:03:23,815
数を最小化します

63
00:03:23,815 --> 00:03:26,830
ツリーでこのような超平面を
再帰的に作ることは

64
00:03:26,830 --> 00:03:30,845
ニューラルネットワークの
線形分類器ノードの層に似ています

65
00:03:30,845 --> 00:03:32,685
とても面白いですね

66
00:03:32,685 --> 00:03:35,270
さて 決定ツリーの作り方がわかったので

67
00:03:35,270 --> 00:03:37,910
このツリーをもう少し作っていきます

68
00:03:38,820 --> 00:03:40,435
たぶん年齢しきい値で

69
00:03:40,435 --> 00:03:43,570
このデータがうまく分かれるでしょう

70
00:03:43,570 --> 00:03:47,675
そこで「17歳半より上ですか」
という質問をします

71
00:03:47,675 --> 00:03:50,940
男性分岐の下で最も低い等級分岐を見ると

72
00:03:50,940 --> 00:03:53,985
乗客の13%が18歳以上

73
00:03:53,985 --> 00:03:56,795
それより若い人はわずか1%です

74
00:03:57,355 --> 00:03:59,680
各ノードの等級を見ると

75
00:03:59,680 --> 00:04:04,495
今のところ男性分岐のここだけが
生存に分類されています

76
00:04:04,495 --> 00:04:06,120
さらに深くして

77
00:04:06,120 --> 00:04:09,240
別の特徴を選んでツリーを拡張していくと

78
00:04:09,240 --> 00:04:11,210
やがて すべてのノードに

79
00:04:11,210 --> 00:04:15,220
生存または死亡の一方だけが
入るようになるでしょう

80
00:04:15,220 --> 00:04:18,460
しかし ここでの問題は私がデータを記憶して

81
00:04:18,460 --> 00:04:22,079
ツリーを完璧に適合させていることです

82
00:04:22,079 --> 00:04:25,815
実際には 新しいデータ向けに
一般化したいのです

83
00:04:25,815 --> 00:04:28,170
トレーニングセットを記憶したモデルは

84
00:04:28,170 --> 00:04:31,195
現実ではうまく機能しないでしょう

85
00:04:31,195 --> 00:04:33,370
正規化する方法として

86
00:04:33,370 --> 00:04:36,010
各リーフノードの最小サンプル数や

87
00:04:36,010 --> 00:04:37,805
リーフノード最大数

88
00:04:37,805 --> 00:04:39,925
特徴の最大数を設定できます

89
00:04:39,925 --> 00:04:41,820
またツリー全体を作った後で

90
00:04:41,820 --> 00:04:44,495
不要なノードを切り取ることもできます

91
00:04:44,495 --> 00:04:46,190
ツリーを有効活用するには

92
00:04:46,190 --> 00:04:48,410
複数を合わせてフォレストにできます

93
00:04:48,410 --> 00:04:50,770
これについてまもなくお話しします

94
00:04:51,710 --> 00:04:53,660
決定分類ツリーの

95
00:04:53,660 --> 00:04:57,395
それぞれの決定つまりノードは
何で構成されますか？

96
00:04:59,265 --> 00:05:02,910
正解は「1つの特徴の線形分類器」です

97
00:05:02,910 --> 00:05:05,130
ツリーの各ノードでは

98
00:05:05,130 --> 00:05:10,410
データを2つのサブセットに分ける
特徴としきい値のペアを選び

99
00:05:10,410 --> 00:05:12,365
それを再帰的に続けます

100
00:05:12,365 --> 00:05:14,550
最大深度を2以上に設定すると

101
00:05:14,550 --> 00:05:17,070
多くの特徴がやがて分割されますが

102
00:05:17,070 --> 00:05:20,040
一度に1深度につき1つの特徴です

103
00:05:20,040 --> 00:05:22,915
各ノードが一度に1つの特徴を分けるので

104
00:05:22,915 --> 00:05:26,535
すべての特徴を対象とする
線形分類器は間違いです

105
00:05:26,535 --> 00:05:28,650
平均二乗誤差ミニマイザーと

106
00:05:28,650 --> 00:05:31,630
ユークリッド距離ミニマイザーはよく似ていて

107
00:05:31,630 --> 00:05:34,210
どちらも分類ではなく回帰で使われます