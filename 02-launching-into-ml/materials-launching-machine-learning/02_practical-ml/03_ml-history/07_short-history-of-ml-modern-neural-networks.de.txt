Neuronale Netze tauchen
wieder in der Zeitleiste auf, jetzt mit noch mehr
Vorteilen durch die Sprünge in der Rechenleistung
und viele, viele Daten. DNNs fingen an, andere getestete Methoden
wie Computervision klar zu übertreffen. Außer dem Boom verstärkter Hardware gibt es viele neue
Tricks und Architekturen, die die Lernfähigkeit neuronaler Deep-
Learning-Netze steigern, wie ReLUs, bessere Initialisierungsmethoden, CNNs oder
Convolutional Neural Networks und Dropout. Wir haben über einige dieser Tricks
aus anderen ML-Methoden gesprochen. Den Einsatz nicht linearer
Aktivierungsfunktionen wie ReLUs, die heute meistens
als Standard festgelegt sind, haben wir bei unserem ersten
Blick auf neuronale Netze besprochen. Man fing an, die Generalisierung
mit Dropout-Schichten zu verbessern, was wie Ensemblemethoden funktioniert, wie bei der Betrachtung von Random 
Forests und Boosting-Bäumen besprochen. Hinzu kamen Convolutional Layers, die den Rechen- und Speicheraufwand senkten,
da sie nicht vollständig verbunden und in der Lage sind,
sich auf lokale Aspekte zu konzentrieren, etwa Bilder, anstatt unzusammenhängende
Dinge in einem Bild zu vergleichen. Sprich, alle Errungenschaften
der anderen ML-Methoden wurden in neuronale Netze zurückgeführt. Betrachten wir ein Beispiel
für ein neuronales Deep-Learning-Netz. Die spannende Geschichte
des maschinellen Lernens kulminiert in Deep Learning mit
neuronalen Netzen, die Hunderte von Schichten und Millionen von Parametern
und erstaunliche Ergebnisse haben. Hier sehen wir GoogLeNet oder Inception, ein Bildklassifizierungsmodell. Es wurde für ImageNet Large Scale Visual Recognition Challenge
2014 mit Daten von 2012 trainiert, wobei es für das Training Bilder aus Tausenden Klassen mit 1,2 Millionen
Bildern klassifizieren musste. Es hat 22 Tiefenebenen, 27 mit dem Pooling, worauf wir in
einem späteren Kurs eingehen, und hundert Schichten, wenn wir es in
seine unabhängigen Bausteine zerlegen. Es gibt über 11 Millionen Lernparameter. Es gibt vollständig verbundene
Schichten und solche, die es nicht sind, wie Convolutional Layers,
über die wir später sprechen. Es verwendet Dropout-Schichten
zur stärkeren Generalisierung durch Simulation eines Ensembles
von neuronalen Deep-Learning-Netzen. Wie wir bei neuronalen Netzen
und Stacking gesehen haben, ist jede Box eine Einheit von Komponenten,
die Teil einer Gruppe von Boxen ist, wie die, die ich vergrößert habe. Das Konzept der Bausteine, die
zusammen größer sind als die Summe ihrer Teile, ist einer der Aspekte, die das
Deep Learning so erfolgreich machen. Natürlich sind auch die
unaufhörlich wachsende Datenmengen und frische Rechenleistung
mit mehr Speicher eine Hilfe. Es existieren jetzt darüber
hinaus mehrere Versionen, die viel größer und sogar noch genauer sind. Was wir aus dieser langen Geschichte
vor allem mitnehmen, ist, dass die ML-Forschung
Stückchen der Techniken aus anderen Algorithmen der
Vergangenheit aufgreift und zu immer leistungsfähigeren Modellen kombiniert
und vor allem experimentiert. Was ist bei der Erstellung
neuronaler Deep-Learning-Netze wichtig? Die richtige Antwort ist: Alles genannte. Dies ist keine vollständige Liste, aber es ist sehr wichtig, 
an diese drei Dinge zu denken. Zuerst muss man viele Daten haben. Es wird viel Forschung
in dem Versuch betrieben, den Datenbedarf beim
Deep Learning zu verringern, aber vorerst müssen wir für große Mengen davon sorgen. Das liegt an der
hohen Kapazität durch die vielen Parameter, die in diesen massiven
Modellen gelernt werden müssen. Da das Modell so komplex ist, muss es die Datenverteilung
wirklich gut verinnerlichen. Deshalb braucht es eine große Signalmenge. Bedenken Sie, dass es nicht der Sinn
des maschinellen Lernens ist, aus Spaß eine Reihe
toller Modelle zu trainieren, sondern sie so zu trainieren, dass wir
sehr genaue Vorhersagen treffen können. Wenn wir die neuen Daten nicht für
Vorhersagen verallgemeinern können, wozu ist das Modell dann gut? Deshalb ist es so wichtig,
genügend Daten zu haben, damit keine Überanpassung an einem kleinen,
Millionen Male gesehenen Dataset erfolgt, anstelle eines riesigen,
viel weniger gesehenen Datasets. Dadurch haben Sie außerdem Validierungs- und Testsätze ausreichender
Größe für das Tuning Ihres Modells. Außerdem kann durch Hinzufügen von
Dropout-Layers, Datenaugmentation, zusätzliches Rauschen usw. eine noch
bessere Generalisierung erreicht werden. Schließlich ist das Experimentieren
das A und O beim maschinellen Lernen. Es gibt heute so viele
verschiedenartige Algorithmen, Hyperparameter und Methoden
zum Erstellen von ML-Datasets. Es ist wirklich für
fast alle Probleme unmöglich, a priori die optimalen
Entscheidungen zu kennen. Durch Experimentieren und
sorgfältige Dokumentation der Versuche und Leistung zum
modellübergreifenden Vergleich werden Sie nicht nur viel Spaß haben,
sondern auch richtig starke Tools bauen. Ich gehe jetzt noch etwas mehr darauf ein, wie neuronale Netze auf der Leistung
vergangener Modelle aufsetzen. Wir sehen hier die Leistung bestimmter Modellversionen von neuronalen
Deep-Learning-Netzen der letzten Jahre. Wie Sie in der Tabelle sehen, kam es 2014 zu einem bedeutenden Sprung, der blau hervorgehoben ist, als das Inception-Modell von Google die 10-Prozent-Fehlerrate
mit 6,7 % durchbrach. Die Leistung von DNNs verbessert sich von Jahr zu Jahr und wir lernen aus
den Erkenntnissen aus vorherigen Modellen. 2015 hat eine dritte Version des Inception-Modells eine
3,5-Prozent-Fehlerrate erreicht. Warum verbessern sich diese
Modelle in so kurzer Zeit so drastisch? Wenn Forschungsgruppen gut funktionierende
neue Techniken oder Methoden entwickeln, übernehmen andere Gruppen
diese Ideen und bauen darauf auf. Das bringt das Experimentieren vorwärts
und der Fortschritt wird beschleunigt. Das betrifft etwa bessere
Hyperparameter, mehr Schichten, bessere Teilkomponenten wie Convolutional
Layers, bessere Generalisierbarkeit usw. Erklären Sie, wie Sie ML auf
das Problem anwenden würden. Es kann mehrere richtige Antworten geben. Sie besitzen eine Skistation und
möchten den Verkehr auf den Pisten auf der Grundlage von vier
Kundentypen (Anfänger, Mittelstufe, Fortgeschrittene, Experten), die Tickets gekauft haben, und dem
vergangenen Schneefall vorhersagen. Nehmen Sie sich jetzt einen Moment,
um eine Antwort aufzuschreiben. Das könnte eine
Regression oder Klassifikation sein, da ich nicht genau angegeben
habe, was ich mit Verkehr meine. Meine ich die Anzahl der Leute,
die diese Piste pro Stunde benutzen? Oder möchte ich eher eine Kategorie,
etwa hoch, mittel und niedrig? Dafür würde ich mit einer
einfachen Heuristik beginnen, etwa die durchschnittliche
Anzahl Personen auf jeder Piste, und dann mit Grundmodellen linearer
oder logistischer Regression fortfahren, je nachdem, ob ich mich für Regression
oder Klassifikation entschieden habe. Je nach Leistung und Datenmenge würde ich dann wohl zu
neuronalen Netzen fortschreiten. Bei weiteren Merkmalen in den Daten würde ich diese auch
ausprobieren und die Leistung beobachten. Google-intern lagen zuletzt über 4.000 Deep ML-Produktionssysteme
vor, die Google-Systeme antreiben. Alle diese Modelle und Versionen
genießen den Leistungsvorteil durch den Aufbau auf den Erfolgen
und Misserfolgen vorheriger Modelle. Eines der anfangs
gebräuchlichsten war Sibyl, das ursprünglich für die Empfehlung
verwandter YouTube-Videos erstellt wurde. Dieser Empfehlungsdienst
funktioniert so gut, dass er bald weithin in Anzeigen und
andere Google-Komponenten integriert wurde. Es war ein lineares Modell. Vizier war ein weiteres Modell, aus dem der Parameterabstimmungsdienst
für andere Modelle und Systeme wurde. Google Brain, der
ML-Forschungszweig von Google, hat eine Methode gefunden, die
Rechenleistung von Tausenden CPUs zu nutzen, um große Modelle wie neuronale
Deep-Learning-Netze zu trainieren. Die Erfahrungen durch
das Erstellen und Ausführen dieser Modelle haben zur
Erzeugung von TensorFlow geführt, einer Open-Source-Bibliothek 
für maschinelles Lernen. Dann entwickelte Google TFX oder
die TensorFlow-basierte ML-Plattform. Wir werden Ihnen zeigen, wie Sie
ML-Produktionsmodelle mit TensorFlow und Tools wie Cloud ML Engine, Dataflow
und BigQuery bauen und bereitstellen. Um es zusammenzufassen:
In den letzten Jahrzehnten hat sich die Anwendung und Leistung
neuronaler Netze stark erhöht. Durch die Allgegenwart von Daten haben diese Modelle den Vorteil einer
immer größeren Anzahl von Lernbeispielen. Die Zunahme der Daten und Beispiele
wurde mit skalierbarer Infrastruktur gekoppelt und ergibt komplexe,
verteilte Modelle mit Tausenden Schichten. Ich gebe Ihnen den
Hinweis mit auf den Weg, dass die Leistung neuronaler Netze bei einigen
Anwendungen zwar hervorragend sein kann, aber sie nur eins von vielen Modellen sind,
mit denen Sie experimentieren können. Experimentieren ist der Schlüssel zur besten Leistung, um mit
Ihren Daten Ihr Problem zu lösen.