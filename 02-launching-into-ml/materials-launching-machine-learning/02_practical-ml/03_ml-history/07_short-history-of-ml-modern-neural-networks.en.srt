1
00:00:00,000 --> 00:00:03,020
Back again in the timeline are neural networks,

2
00:00:03,020 --> 00:00:05,580
now with even more of an advantage through leaps

3
00:00:05,580 --> 00:00:08,760
and computational power and lots and lots of data.

4
00:00:08,760 --> 00:00:14,840
DNNs began to substantially outperform other methods on test such as computer vision.

5
00:00:14,840 --> 00:00:17,680
In addition to the boom from boosted hardware,

6
00:00:17,680 --> 00:00:21,150
there are many new tricks and architectures that help

7
00:00:21,150 --> 00:00:24,695
to improve trainability of deep neural networks like ReLUs,

8
00:00:24,695 --> 00:00:30,925
better initialization methods, CNNs or convolutional neural networks and dropout.

9
00:00:30,925 --> 00:00:34,650
We talked about some of these tricks from other ML methods.

10
00:00:34,650 --> 00:00:38,090
The use of nonlinear activation functions such as ReLUs,

11
00:00:38,090 --> 00:00:40,325
which usually are set as the default now,

12
00:00:40,325 --> 00:00:43,270
we talked about during the first look at neural networks.

13
00:00:43,270 --> 00:00:46,945
Dropout layers began being used to help with generalization,

14
00:00:46,945 --> 00:00:48,585
which works like ensemble methods,

15
00:00:48,585 --> 00:00:52,010
which we explored when talking about random forests and boosted trees.

16
00:00:52,010 --> 00:00:54,760
Convolutional layers were added that reduced

17
00:00:54,760 --> 00:00:58,825
the computational and memory load due to their non-complete connectedness,

18
00:00:58,825 --> 00:01:01,990
as well as being able to focus on local aspects,

19
00:01:01,990 --> 00:01:06,505
for instance, images, rather than comparing unrelated things in an image.

20
00:01:06,505 --> 00:01:10,850
In other words, all the advances that came about in other ML methods,

21
00:01:10,850 --> 00:01:13,005
got folded back into neural networks.

22
00:01:13,005 --> 00:01:15,600
Let's look at an example of the deep neural network.

23
00:01:15,600 --> 00:01:18,475
This exciting history of machine learning,

24
00:01:18,475 --> 00:01:22,320
has culminated into deep learning with neural networks containing hundreds of

25
00:01:22,320 --> 00:01:26,400
layers and millions of parameters but with amazing results.

26
00:01:26,400 --> 00:01:29,065
Shown here is a GoogLeNet or Inception,

27
00:01:29,065 --> 00:01:31,150
which is an image classification model.

28
00:01:31,150 --> 00:01:32,770
It was trained for the image

29
00:01:32,770 --> 00:01:38,030
net large visual recognition challenge in 2014 using data from 2012,

30
00:01:38,030 --> 00:01:40,020
where it has to classify images across

31
00:01:40,020 --> 00:01:43,970
a thousand classes with 1.2 million images for training.

32
00:01:43,970 --> 00:01:46,420
It has 22 deep layers,

33
00:01:46,420 --> 00:01:48,590
27 if you include pooling,

34
00:01:48,590 --> 00:01:50,480
which we will discuss in the later course,

35
00:01:50,480 --> 00:01:54,700
and a hundred layers if you break it down into its independent building blocks.

36
00:01:54,700 --> 00:01:58,150
There are over 11 million trained parameters.

37
00:01:58,150 --> 00:02:01,355
There are completely connected layers and some that aren't,

38
00:02:01,355 --> 00:02:04,225
such as convolutional layers which we will talk about later.

39
00:02:04,225 --> 00:02:07,255
It used dropout layers to help generalize more,

40
00:02:07,255 --> 00:02:10,000
simulating an ensemble of deep neural networks.

41
00:02:10,000 --> 00:02:12,410
Just like we saw with neural networks and stacking,

42
00:02:12,410 --> 00:02:15,845
each box is a unit of components which is part of a group of boxes,

43
00:02:15,845 --> 00:02:17,735
such as the one I've zoomed in on.

44
00:02:17,735 --> 00:02:21,420
This idea of building blocks adding up to something greater than the sum of

45
00:02:21,420 --> 00:02:25,720
its parts is one of the things that has made deep learning so successful.

46
00:02:25,720 --> 00:02:28,420
Of course, in ever-growing abundance of data and

47
00:02:28,420 --> 00:02:31,805
fresh compute power and more memory helps too.

48
00:02:31,805 --> 00:02:34,830
There are now several versions beyond this that

49
00:02:34,830 --> 00:02:37,865
are much bigger and have even greater accuracy.

50
00:02:37,865 --> 00:02:40,310
The main takeaway from all of this history is that

51
00:02:40,310 --> 00:02:43,740
machine learning research reuses bits and pieces of techniques from

52
00:02:43,740 --> 00:02:47,150
other algorithms from the past to combine together to

53
00:02:47,150 --> 00:02:50,900
make ever powerful models and most importantly experiment.

54
00:02:50,900 --> 00:02:55,300
What is important when creating deep neural networks?

55
00:02:55,300 --> 00:02:59,255
The correct answer is all the above.

56
00:02:59,255 --> 00:03:01,445
This is not an exhaustive list,

57
00:03:01,445 --> 00:03:04,550
but these three things are very important to keep in mind.

58
00:03:04,550 --> 00:03:07,790
First, you need to make sure you have lots of data.

59
00:03:07,790 --> 00:03:10,680
There is a lot of research taking place trying to reduce

60
00:03:10,680 --> 00:03:13,360
the data needs of deep learning but until then,

61
00:03:13,360 --> 00:03:15,530
we need to make sure we have a lot of it.

62
00:03:15,530 --> 00:03:18,780
This is due to the high capacity from the number of

63
00:03:18,780 --> 00:03:22,080
parameters that need to be trained in these massive models.

64
00:03:22,080 --> 00:03:24,360
Since the model is so complex,

65
00:03:24,360 --> 00:03:27,225
it really needs to internalize the data distribution well.

66
00:03:27,225 --> 00:03:29,710
Therefore, it needs a lot of signal.

67
00:03:29,710 --> 00:03:32,680
Remember, the entire point of machine learning is not to

68
00:03:32,680 --> 00:03:35,550
train a whole bunch of fancy models just because.

69
00:03:35,550 --> 00:03:38,945
It's to train them so that they can make very accurate predictions.

70
00:03:38,945 --> 00:03:41,810
If you can't generalize a new data to predict from,

71
00:03:41,810 --> 00:03:43,895
then what good of a model is that?

72
00:03:43,895 --> 00:03:48,030
Therefore, once again, having enough data is important so that it

73
00:03:48,030 --> 00:03:52,125
doesn't overfit to a small dataset that had just seen a million times,

74
00:03:52,125 --> 00:03:55,165
instead of a gigantic dataset just seen much less.

75
00:03:55,165 --> 00:03:56,680
This also allows you to have

76
00:03:56,680 --> 00:03:59,915
a large enough validation and test sets to tune your model with.

77
00:03:59,915 --> 00:04:03,860
Additionally, adding dropout layers, performing data augmentation,

78
00:04:03,860 --> 00:04:08,435
adding noise, etcetera, is the way that you can have even better generalization.

79
00:04:08,435 --> 00:04:12,410
Lastly, machine learning is all about experimentation.

80
00:04:12,410 --> 00:04:14,760
There are so many different types of algorithms,

81
00:04:14,760 --> 00:04:18,085
hyperparameters and ways to create your machine learning datasets these days.

82
00:04:18,085 --> 00:04:20,570
There really is no way a priority to know

83
00:04:20,570 --> 00:04:24,005
the optimal choices from the start for almost all problems.

84
00:04:24,005 --> 00:04:27,330
By experimenting and keeping careful track what you've

85
00:04:27,330 --> 00:04:30,535
tried already and performance measured to compare models across,

86
00:04:30,535 --> 00:04:35,620
you not only will have a lot of fun but also will create some amazingly powerful tools.

87
00:04:35,620 --> 00:04:38,060
Next, I'll talk through a bit more on how

88
00:04:38,060 --> 00:04:41,805
neural networks continue to build on performance of past models.

89
00:04:41,805 --> 00:04:43,630
Here, you see the performance of

90
00:04:43,630 --> 00:04:47,050
specific model versions of deep neural networks over the years.

91
00:04:47,050 --> 00:04:48,590
As you can see in the chart,

92
00:04:48,590 --> 00:04:50,970
a significant jump came in 2014,

93
00:04:50,970 --> 00:04:52,390
which is highlighted in blue,

94
00:04:52,390 --> 00:04:54,390
where Google's Inception model broke through

95
00:04:54,390 --> 00:04:57,375
that 10 percent error rate with a 6.7 percent.

96
00:04:57,375 --> 00:05:00,350
The performance of DNNs continues to improve with

97
00:05:00,350 --> 00:05:04,160
each passing year and learned from the lessons gained from prior models.

98
00:05:04,160 --> 00:05:06,480
In 2015, a version three of

99
00:05:06,480 --> 00:05:09,840
the inception model scored a three and a half percent error rate.

100
00:05:09,840 --> 00:05:14,045
So, what makes these miles improved so drastically over a short span of time?

101
00:05:14,045 --> 00:05:18,465
Often, when a research group develops a new technique or method that works very well,

102
00:05:18,465 --> 00:05:22,200
other groups then take those ideas and build off of them.

103
00:05:22,200 --> 00:05:28,260
This provides a significant jump forward in experimentation so that progress speeds up.

104
00:05:28,260 --> 00:05:31,750
This can involve better hyperparameters, more layers,

105
00:05:31,750 --> 00:05:36,565
better generalizability, better sub-components like convolutional layers, etcetera.

106
00:05:36,565 --> 00:05:39,910
Explain how you would apply ML to the problem.

107
00:05:39,910 --> 00:05:43,365
There could be more than one right answer.

108
00:05:43,365 --> 00:05:47,810
You own a winter ski resort and want to project the traffic levels

109
00:05:47,810 --> 00:05:51,235
of ski runs based on four types of customers: beginner,

110
00:05:51,235 --> 00:05:53,670
intermediate, advanced and expert,

111
00:05:53,670 --> 00:05:58,075
that have bought tickets and the amount of previous snowfall.

112
00:05:58,075 --> 00:06:02,735
Take a moment to write an answer now.

113
00:06:02,735 --> 00:06:07,265
This could be regression or classification,

114
00:06:07,265 --> 00:06:11,240
since I didn't specify what exactly I mean by traffic levels.

115
00:06:11,240 --> 00:06:15,170
Do I mean the number of people who use that ski run per hour?

116
00:06:15,170 --> 00:06:19,285
Or do I want something more categorical such as high, medium and low?

117
00:06:19,285 --> 00:06:21,830
For this, I would begin with a base heuristics,

118
00:06:21,830 --> 00:06:24,900
such as the average number of people on each slope and then

119
00:06:24,900 --> 00:06:28,370
move on to base models of linear or logistic regression,

120
00:06:28,370 --> 00:06:33,075
depending on if I decided to go to regression or classification route, respectively.

121
00:06:33,075 --> 00:06:35,545
Depending on performance and amount of data,

122
00:06:35,545 --> 00:06:38,195
I would then probably move onto neural networks.

123
00:06:38,195 --> 00:06:40,240
If there are other features in the data,

124
00:06:40,240 --> 00:06:44,225
I would also try those and monitor performance.

125
00:06:44,225 --> 00:06:48,790
Internally at Google, there are, at last count,

126
00:06:48,790 --> 00:06:53,025
over 4,000 production deep ML models powering their systems.

127
00:06:53,025 --> 00:06:56,470
Each of these models and versions gets the performance benefit of building

128
00:06:56,470 --> 00:07:00,290
on the successes and failures of past models.

129
00:07:00,290 --> 00:07:03,585
One of the most widely used early on was Sibyl,

130
00:07:03,585 --> 00:07:06,760
which was originally created to recommend related YouTube videos.

131
00:07:06,760 --> 00:07:09,670
This recommendation engine works so well,

132
00:07:09,670 --> 00:07:13,365
it was later incorporated widely into ads and other parts of Google.

133
00:07:13,365 --> 00:07:15,720
It was a linear model.

134
00:07:15,720 --> 00:07:19,460
This year was another model which ended up becoming

135
00:07:19,460 --> 00:07:23,980
the de facto parameter tuning engine for other models and systems.

136
00:07:23,980 --> 00:07:27,020
Google Brain, the ML research arm within Google,

137
00:07:27,020 --> 00:07:30,540
created a way to harness the computational power of thousands of

138
00:07:30,540 --> 00:07:34,590
CPUs to train large models like deep neural networks.

139
00:07:34,590 --> 00:07:36,940
The experience building and running

140
00:07:36,940 --> 00:07:39,730
these models is what shaped the creation of TensorFlow,

141
00:07:39,730 --> 00:07:42,410
an open source library for machine learning.

142
00:07:42,410 --> 00:07:47,210
Google then created TFX or the TensorFlow-based machine learning platform.

143
00:07:47,210 --> 00:07:50,600
And we'll show you how to build and deploy production ML models using

144
00:07:50,600 --> 00:07:55,040
TensorFlow and tools like Cloud ML Engine, Dataflow and BigQuery.

145
00:07:55,040 --> 00:07:57,465
To recap, the last few decades have seen

146
00:07:57,465 --> 00:08:01,190
a proliferation in the adoption and performance of neural networks.

147
00:08:01,190 --> 00:08:02,700
With the ubiquity of data,

148
00:08:02,700 --> 00:08:06,905
these models have the benefit of more and more training examples to learn from.

149
00:08:06,905 --> 00:08:10,410
The increase in data and examples has been coupled with

150
00:08:10,410 --> 00:08:15,900
scalable infrastructure for even complex and distributed models with thousands of layers.

151
00:08:15,900 --> 00:08:18,620
One note that we'll leave you with is that although

152
00:08:18,620 --> 00:08:21,975
performance with neural networks may be great for some applications,

153
00:08:21,975 --> 00:08:25,790
they are just one of many types of models available for you to experiment with.

154
00:08:25,790 --> 00:08:28,070
Experimentation is key to getting

155
00:08:28,070 --> 00:08:32,000
the best performance using your data to solve your challenge.