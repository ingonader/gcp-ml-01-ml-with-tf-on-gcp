1
00:00:00,340 --> 00:00:01,740
En la década de los 90

2
00:00:01,740 --> 00:00:03,795
se creó el campo
de los métodos de kernel.

3
00:00:03,795 --> 00:00:06,490
Corinna Cortes,
directora de Google Research,

4
00:00:06,490 --> 00:00:08,185
fue una de las pioneras.

5
00:00:08,185 --> 00:00:12,060
Este campo de estudio usa clases
interesantes de nuevos modelos

6
00:00:12,060 --> 00:00:16,185
no lineales,
principalmente SVM no lineales,

7
00:00:16,185 --> 00:00:21,630
o máquinas de vectores de soporte,
que son clasificadores de margen máximo.

8
00:00:21,630 --> 00:00:24,800
Básicamente, el principio
de una SVM es una activación no lineal

9
00:00:24,800 --> 00:00:27,820
más una salida tipo sigmoide
para márgenes máximos.

10
00:00:27,820 --> 00:00:30,190
Hace poco, vimos
cómo la regresión logística

11
00:00:30,190 --> 00:00:33,445
se usa para crear un límite de decisión
para maximizar el logaritmo

12
00:00:33,445 --> 00:00:36,455
de la verosimilitud
de las probabilidades de clasificación.

13
00:00:36,455 --> 00:00:39,745
En el caso de un límite de decisión
lineal, en la regresión logística

14
00:00:39,745 --> 00:00:42,925
se quiere que cada punto
y su clase asociada estén tan lejos

15
00:00:42,925 --> 00:00:45,995
del hiperplano como sea posible
y proporciona una probabilidad

16
00:00:45,995 --> 00:00:49,055
que se puede interpretar
como confianza de la predicción.

17
00:00:49,675 --> 00:00:52,510
Hay una cantidad infinita
de hiperplanos que se pueden crear

18
00:00:52,510 --> 00:00:56,345
entre dos clases linealmente separables,
como los dos hiperplanos que se muestran

19
00:00:56,345 --> 00:00:58,585
con las líneas punteadas
en estas dos figuras.

20
00:00:58,585 --> 00:01:01,745
En las SVM,
incluimos dos hiperplanos paralelos

21
00:01:01,745 --> 00:01:04,845
en cualquiera de los lados
del hiperplano del límite de decisión

22
00:01:04,845 --> 00:01:08,765
en el que se cruzan con el punto de datos
más cercano en cada lado del hiperplano.

23
00:01:08,765 --> 00:01:10,785
Estos son los vectores de soporte.

24
00:01:10,785 --> 00:01:14,110
La distancia entre dos vectores
de soporte es el margen.

25
00:01:14,445 --> 00:01:18,560
A la izquierda, tenemos un hiperplano
vertical que separa las dos clases.

26
00:01:18,560 --> 00:01:21,240
No obstante, el margen
entre los dos vectores de soporte

27
00:01:21,240 --> 00:01:22,615
es pequeño.

28
00:01:22,615 --> 00:01:25,660
Si elegimos un hiperplano diferente,
como el de la derecha,

29
00:01:25,660 --> 00:01:28,415
hay un margen mucho más grande.

30
00:01:28,695 --> 00:01:32,530
Mientras más amplio el margen,
más generalizable es el límite de decisión

31
00:01:32,530 --> 00:01:35,550
lo que debería conducir
a un mejor rendimiento de los datos.

32
00:01:35,550 --> 00:01:38,905
Por lo tanto, los clasificadores SVM
buscan maximizar el margen

33
00:01:38,905 --> 00:01:42,010
entre los dos vectores de soporte
mediante una función de pérdida

34
00:01:42,010 --> 00:01:44,120
de bisagra
comparada con la minimización

35
00:01:44,120 --> 00:01:46,580
de la regresión logística
de la entropía cruzada.

36
00:01:46,580 --> 00:01:49,660
Notarán que solo tengo dos clases,
es decir que es un problema

37
00:01:49,660 --> 00:01:51,750
de clasificación binaria.

38
00:01:51,750 --> 00:01:54,620
A una de las etiquetas de las clases
se le da el valor de uno

39
00:01:54,620 --> 00:01:57,690
y a la etiqueta de la otra clase
se le da el valor de menos uno.

40
00:01:58,020 --> 00:02:01,600
Si hay más de dos clases,
debería adoptarse el enfoque

41
00:02:01,600 --> 00:02:04,950
de uno frente a todos
y elegir la mejor de las clasificaciones

42
00:02:04,950 --> 00:02:07,140
binarias permutadas.

43
00:02:07,140 --> 00:02:10,780
Pero, ¿qué pasa si los datos
no se pueden separar linealmente

44
00:02:10,780 --> 00:02:12,100
en las dos clases?

45
00:02:12,390 --> 00:02:15,470
La buena noticia es que podemos
aplicar una transformación kernel

46
00:02:15,470 --> 00:02:18,170
que asigna los datos
del espacio vectorial de entrada

47
00:02:18,170 --> 00:02:21,550
a un espacio que ahora tiene atributos
que se pueden separar linealmente

48
00:02:21,560 --> 00:02:23,185
como se muestra en el diagrama.

49
00:02:23,185 --> 00:02:26,305
Como antes, durante la aparición
de las redes neuronales profundas

50
00:02:26,305 --> 00:02:29,590
se dedicó mucho tiempo a transformar
la representación sin procesar

51
00:02:29,590 --> 00:02:32,580
de los datos en un vector
de atributo mediante un mapa

52
00:02:32,580 --> 00:02:34,940
de atributos muy ajustado,
creado por el usuario.

53
00:02:34,940 --> 00:02:38,770
No obstante, con los métodos kernel,
el único elemento definido por el usuario

54
00:02:38,770 --> 00:02:41,645
es el kernel,
solo función de similitud entre pares

55
00:02:41,645 --> 00:02:44,485
de puntos en la representación
sin procesar de los datos.

56
00:02:44,925 --> 00:02:46,660
Una transformación kernel es similar

57
00:02:46,660 --> 00:02:49,310
a cómo una función de activación
en las redes neuronales

58
00:02:49,310 --> 00:02:52,100
asigna la entrada a la función
para transformar el espacio.

59
00:02:52,790 --> 00:02:55,615
La cantidad de neuronas
en la capa controla la dimensión.

60
00:02:55,615 --> 00:02:59,075
Si tienen dos entradas y tres neuronas,
están asignando el espacio

61
00:02:59,075 --> 00:03:02,100
de la entrada 2D al espacio 3D.

62
00:03:02,100 --> 00:03:05,810
Hay muchos tipos de kernels
y los más básicos son el lineal,

63
00:03:06,430 --> 00:03:10,845
el kernel polinomial
y el de función de base radial gaussiana.

64
00:03:11,205 --> 00:03:13,495
Cuando nuestro clasificador binario
usa el kernel

65
00:03:13,495 --> 00:03:16,400
por lo general, calcula una suma
ponderada de similitudes.

66
00:03:16,400 --> 00:03:19,450
¿Cuándo se debe usar una SVM
en vez de la regresión logística?

67
00:03:20,270 --> 00:03:23,125
Las SVM con kernel tienden a ofrecer
soluciones más dispersas

68
00:03:23,125 --> 00:03:25,400
y, por ende, tienen mejor escalabilidad.

69
00:03:25,400 --> 00:03:28,330
Las SVM tienen mejor rendimiento
cuando hay una gran cantidad

70
00:03:28,330 --> 00:03:31,720
de dimensiones y los predictores
casi con certeza predicen la respuesta.

71
00:03:32,165 --> 00:03:35,450
Vimos cómo las SVM usan kernels
para asignar las entradas a un espacio

72
00:03:35,450 --> 00:03:37,400
dimensional de atributos más alto.

73
00:03:37,400 --> 00:03:41,200
¿De qué otra forma también
se puede asignar a un espacio vectorial

74
00:03:41,200 --> 00:03:43,515
de dimensión más alta
en las redes neuronales?

75
00:03:44,325 --> 00:03:47,255
La respuesta correcta es:
C. Más neuronas por capa.

76
00:03:47,435 --> 00:03:50,540
La cantidad de neuronas por capa
determina en cuántas dimensiones

77
00:03:50,540 --> 00:03:52,290
de espacio vectorial se encuentran.

78
00:03:52,290 --> 00:03:54,225
Si comienzo
con tres atributos de entrada

79
00:03:54,225 --> 00:03:56,120
estoy en el espacio vectorial R3.

80
00:03:56,120 --> 00:03:59,500
Aunque tenga cientos de capas
pero solo tres neuronas en cada una,

81
00:03:59,500 --> 00:04:04,760
seguiré en el espacio vectorial R3
y solo estoy cambiando la base.

82
00:04:04,760 --> 00:04:08,640
Por ejemplo, si uso
un kernel de BR gaussiana con las SVM,

83
00:04:08,640 --> 00:04:11,715
el espacio de entrada se asigna
a infinitas dimensiones.

84
00:04:11,715 --> 00:04:14,830
A. La función de activación
cambia la base del espacio vectorial

85
00:04:14,830 --> 00:04:17,140
pero no agrega ni sustrae dimensiones.

86
00:04:17,140 --> 00:04:20,530
Considérenlas como rotaciones,
estiramientos y compresiones.

87
00:04:20,530 --> 00:04:22,220
Es posible que no sean lineales

88
00:04:22,220 --> 00:04:25,290
pero se mantienen en el mismo
espacio vectorial que antes.

89
00:04:25,290 --> 00:04:28,750
D. La función de pérdida
es el objetivo que intentan minimizar.

90
00:04:28,750 --> 00:04:31,840
Es un escalar que usa su gradiente
para actualizar los pesos

91
00:04:31,840 --> 00:04:33,620
de los parámetros del modelo.

92
00:04:33,620 --> 00:04:37,085
Eso solo cambia cuánto se rota,
estira y comprime

93
00:04:37,085 --> 00:04:38,575
no la cantidad de dimensiones.