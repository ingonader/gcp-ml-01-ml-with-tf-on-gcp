1
00:00:00,420 --> 00:00:02,780
En las últimas décadas del siglo XX

2
00:00:02,780 --> 00:00:05,650
la investigación del AA
finalmente tuvo el poder informático

3
00:00:05,650 --> 00:00:08,200
para incluir y combinar el rendimiento

4
00:00:08,200 --> 00:00:11,400
en muchos modelos, mediante
lo que llamamos el método del ensamble.

5
00:00:11,720 --> 00:00:15,200
Pueden imaginarse que, si los errores
son independientes para una cantidad

6
00:00:15,200 --> 00:00:19,540
de clasificadores simples débiles,
combinados constituirán uno robusto.

7
00:00:20,550 --> 00:00:23,570
DNN se acercará a esto
mediante capas de retirados

8
00:00:23,570 --> 00:00:26,620
que ayudan a regularizar el modelo
y prevenir el sobreajuste.

9
00:00:26,620 --> 00:00:29,475
Esto se puede simular
desactivando neuronas aleatoriamente

10
00:00:29,475 --> 00:00:32,765
en la red con cierta probabilidad
para cada propagación hacia adelante

11
00:00:32,765 --> 00:00:36,080
lo que básicamente
creará una nueva red cada vez.

12
00:00:36,410 --> 00:00:39,435
Muchas veces, las preguntas complejas
se responden mejor

13
00:00:39,435 --> 00:00:42,360
mediante la agregación
de las respuestas de miles de personas

14
00:00:42,360 --> 00:00:44,150
en vez de la de un solo individuo.

15
00:00:44,520 --> 00:00:47,230
Esto se conoce
como la "sabiduría de los grupos".

16
00:00:47,470 --> 00:00:49,330
Lo mismo se aplica al AA.

17
00:00:49,330 --> 00:00:51,940
Cuando se agregan los resultados
de muchos predictores,

18
00:00:51,940 --> 00:00:56,140
ya sean clasificadores o regresores,
el grupo tendrá mejor rendimiento

19
00:00:56,140 --> 00:00:57,910
que el mejor modelo individual.

20
00:00:58,300 --> 00:01:01,890
Este grupo de predictores es un ensamble
que, cuando se combina así,

21
00:01:01,890 --> 00:01:04,120
conduce al aprendizaje por ensamblado.

22
00:01:04,120 --> 00:01:07,580
El algoritmo que realiza este aprendizaje
es un método de ensamble.

23
00:01:07,580 --> 00:01:10,080
Uno de los aprendizajes
por ensamblado más populares

24
00:01:10,080 --> 00:01:11,890
es el bosque aleatorio.

25
00:01:11,890 --> 00:01:14,650
En vez de usar todo el conjunto
de datos de entrenamiento

26
00:01:14,650 --> 00:01:18,570
para crear un árbol de decisión,
pueden tener un grupo de árboles

27
00:01:18,570 --> 00:01:20,625
y cada uno
obtiene una submuestra aleatoria

28
00:01:20,625 --> 00:01:22,190
de los datos de entrenamiento.

29
00:01:22,190 --> 00:01:24,530
Como no vieron todo el conjunto
de entrenamiento,

30
00:01:24,530 --> 00:01:26,770
no pueden haberlo memorizado.

31
00:01:26,770 --> 00:01:28,990
Una vez que todos los árboles
estén entrenados

32
00:01:28,990 --> 00:01:32,175
y sean un subconjunto de los datos,
podrán hacer lo más importante

33
00:01:32,175 --> 00:01:34,365
y valioso del AA: las predicciones.

34
00:01:34,825 --> 00:01:37,210
Para hacerlo,
propagarán la muestra de la prueba

35
00:01:37,210 --> 00:01:40,140
a cada árbol en el bosque
y luego agregarán los resultados.

36
00:01:40,140 --> 00:01:43,185
Si se trata de clasificación,
podría existir un voto mayoritario

37
00:01:43,185 --> 00:01:46,360
en todos los árboles,
que luego sería la clase de salida final.

38
00:01:46,360 --> 00:01:49,340
Si es regresión,
podría ser un agregado de los valores

39
00:01:49,340 --> 00:01:52,135
como la media, máxima, mediana, etcétera.

40
00:01:52,415 --> 00:01:55,660
Para mejorar la generalización,
pueden realizar una muestra aleatoria

41
00:01:55,660 --> 00:01:58,310
de los ejemplos o los atributos.

42
00:01:58,310 --> 00:02:00,760
A este muestreo aleatorio
de ejemplos con reemplazo

43
00:02:00,760 --> 00:02:03,295
se le llama
agregación de bootstrap (bagging)

44
00:02:03,295 --> 00:02:05,710
y se le llama "pasting"
cuando es sin reemplazo.

45
00:02:06,010 --> 00:02:08,870
Cada predictor individual
tiene un sesgo mayor,

46
00:02:08,870 --> 00:02:12,595
ya que se entrenó en el subconjunto
más pequeño en lugar de todo el conjunto,

47
00:02:12,595 --> 00:02:15,540
pero la agregación reduce
tanto el sesgo como la varianza.

48
00:02:16,170 --> 00:02:20,110
Eso proporciona al ensamble
un sesgo similar al de un predictor único

49
00:02:20,110 --> 00:02:23,210
en todo el conjunto de entrenamiento,
pero con menor varianza.

50
00:02:23,580 --> 00:02:26,630
Un excelente método de validación
para el error de generalización

51
00:02:26,630 --> 00:02:28,905
es usar los datos
de la agregación de bootstrap

52
00:02:28,905 --> 00:02:32,010
en lugar de un conjunto separado
obtenido del conjunto de datos

53
00:02:32,010 --> 00:02:33,425
previo al entrenamiento.

54
00:02:33,425 --> 00:02:37,530
Es algo similar a la validación de k
con exclusiones aleatorias.

55
00:02:37,530 --> 00:02:40,590
Los subespacios aleatorios
ocurren cuando se obtiene la muestra

56
00:02:40,590 --> 00:02:43,125
de los atributos.
Si se obtienen muestras aleatorias

57
00:02:43,125 --> 00:02:45,860
de los ejemplos,
se llaman parcelas aleatorias (patches).

58
00:02:45,860 --> 00:02:49,170
La potenciación adaptativa o AdaBoost
y la potenciación del gradiente

59
00:02:49,170 --> 00:02:52,475
son ejemplos de potenciación,
que es cuando se agregan clasificadores

60
00:02:52,475 --> 00:02:54,735
débiles para crear uno robusto.

61
00:02:54,735 --> 00:02:57,665
Por lo general, se hace
mediante el entrenamiento secuencial

62
00:02:57,665 --> 00:03:01,135
de cada clasificador
para corregir los problemas que tuvo.

63
00:03:01,135 --> 00:03:04,290
En los árboles potenciados,
a medida que se agregan más árboles

64
00:03:04,290 --> 00:03:07,065
al ensamble,
las predicciones suelen mejorar.

65
00:03:07,065 --> 00:03:11,545
Entonces, ¿continuamos agregando árboles
hasta el infinito? Claro que no.

66
00:03:11,545 --> 00:03:14,780
Pueden usar su conjunto de validación
para la interrupción anticipada

67
00:03:14,780 --> 00:03:17,110
y no sobreajustar los datos
de entrenamiento

68
00:03:17,110 --> 00:03:19,300
debido a la presencia
de demasiados árboles.

69
00:03:19,730 --> 00:03:21,945
Por último,
igual que con las redes neuronales

70
00:03:21,945 --> 00:03:24,010
podemos combinar clasificadores
(stacking),

71
00:03:24,010 --> 00:03:26,300
donde los metaclasificadores
aprenden qué hacer

72
00:03:26,300 --> 00:03:29,420
con las predicciones del ensamble,
que a su vez se pueden combinar

73
00:03:29,420 --> 00:03:31,160
en metaclasificadores, etcétera.

74
00:03:31,160 --> 00:03:34,690
Veremos la combinación de subcomponentes
y la reutilizaremos en las redes

75
00:03:34,690 --> 00:03:36,480
neuronales profundas dentro de poco.

76
00:03:36,480 --> 00:03:40,180
¿Cuál de las siguientes es probablemente
falsa sobre los bosques aleatorios

77
00:03:40,180 --> 00:03:43,275
cuando se los compara con los árboles
de decisión individuales?

78
00:03:45,115 --> 00:03:48,435
La respuesta correcta es
que es probablemente falso

79
00:03:48,435 --> 00:03:52,005
que los árboles aleatorios son
D. Más fáciles de interpretar visualmente.

80
00:03:52,005 --> 00:03:55,100
Igual que las redes neuronales,
mientras más capas de complejidad

81
00:03:55,100 --> 00:03:58,540
se agreguen al modelo,
más difícil será entender y explicar.

82
00:03:58,540 --> 00:04:02,390
Un bosque aleatorio es más complejo
que un árbol de decisión individual

83
00:04:02,390 --> 00:04:04,980
lo que lo hace
más difícil de interpretar visualmente.

84
00:04:04,980 --> 00:04:07,620
Las otras tres respuestas
son probablemente verdaderas.

85
00:04:07,620 --> 00:04:09,960
Los bosques aleatorios
tienen mejor generalización

86
00:04:09,960 --> 00:04:12,500
gracias a la agregación de bootstrap
y los subespacios

87
00:04:15,140 --> 00:04:16,930
o la agregación para la regresión

88
00:04:12,500 --> 00:04:15,140
y mediante un sistema
de votación para la clasificación

89
00:04:16,930 --> 00:04:20,290
el bosque puede tener
mejor rendimiento que un árbol individual.

90
00:04:20,290 --> 00:04:23,450
Finalmente, debido al muestreo aleatorio
de los bosques aleatorios

91
00:04:23,450 --> 00:04:26,390
mantiene un sesgo similar
al de un árbol individual

92
00:04:26,390 --> 00:04:28,600
pero también tiene menor varianza

93
00:04:28,600 --> 00:04:31,750
que, por lo general,
significa mejor generalización.