1
00:00:00,150 --> 00:00:03,410
Veamos un poco
de la historia del aprendizaje automático

2
00:00:03,410 --> 00:00:06,230
para ver su evolución en el tiempo
en las redes neuronales

3
00:00:06,230 --> 00:00:08,785
de aprendizaje profundo
que son tan populares hoy.

4
00:00:09,075 --> 00:00:11,985
Observarán que,
a pesar de que las redes neuronales

5
00:00:11,985 --> 00:00:15,895
estuvieron de moda de manera
intermitente en las últimas décadas

6
00:00:15,895 --> 00:00:19,395
los trucos y técnicas
que se desarrollaron para otros algoritmos

7
00:00:19,395 --> 00:00:23,745
se pueden aplicar a estas redes,
lo que las hace muy poderosas.

8
00:00:23,745 --> 00:00:27,370
La regresión lineal se inventó
para predecir el movimiento

9
00:00:27,370 --> 00:00:31,280
de los planetas y el tamaño
de las vainas según sus progenitores.

10
00:00:31,630 --> 00:00:35,540
Sir Francis Galton fue pionero
en el uso de los métodos estadísticos

11
00:00:35,540 --> 00:00:38,135
para medir fenómenos naturales.

12
00:00:38,755 --> 00:00:42,595
Buscaba datos sobre los tamaños
relativos de los progenitores y los hijos

13
00:00:42,595 --> 00:00:45,625
en varias especies,
incluidas las arvejillas.

14
00:00:45,945 --> 00:00:50,155
Observó un hecho muy extraño
que no era obvio de inmediato.

15
00:00:50,715 --> 00:00:54,995
Un progenitor más grande que el promedio
tiende a producir hijos más grandes

16
00:00:54,995 --> 00:00:58,400
que el promedio,
pero ¿cuánto más grande es el hijo

17
00:00:58,400 --> 00:01:01,425
con respecto al promedio
de los otros hijos de esa generación?

18
00:01:01,425 --> 00:01:05,285
Resultó que esta proporción
para los hijos es menor

19
00:01:05,285 --> 00:01:08,055
que la proporción
correspondiente para el progenitor.

20
00:01:08,055 --> 00:01:11,765
Por ejemplo, si el tamaño del progenitor
fuera 1.5 desviaciones estándar

21
00:01:11,765 --> 00:01:15,795
de la media en su propia generación;
entonces, predeciría que el tamaño

22
00:01:15,795 --> 00:01:19,820
del hijo sería menor que las 1.5
desviaciones estándar de la media

23
00:01:19,820 --> 00:01:21,725
en su grupo.

24
00:01:21,725 --> 00:01:26,025
Decimos que de generación
en generación, la naturaleza es regresiva

25
00:01:26,025 --> 00:01:31,070
o regresa a la media,
por eso el nombre de "regresión lineal".

26
00:01:31,760 --> 00:01:36,750
Este gráfico de 1877
es la primera regresión lineal realizada.

27
00:01:36,760 --> 00:01:38,010
Genial.

28
00:01:39,260 --> 00:01:42,915
El poder de computación
en el siglo XIX era bastante limitado

29
00:01:42,915 --> 00:01:46,310
y no se imaginaban lo bien
que esto funcionaría cuando hubiera

30
00:01:46,310 --> 00:01:48,280
grandes conjuntos de datos disponibles.

31
00:01:48,460 --> 00:01:52,165
Había una forma de solución cerrada
para resolver las regresiones lineales

32
00:01:52,165 --> 00:01:55,225
pero los métodos de descenso
de gradientes también se pueden usar

33
00:01:55,225 --> 00:01:58,465
cada uno con sus ventajas y desventajas,
según el conjunto de datos.

34
00:01:58,465 --> 00:02:01,455
Veamos con detalle
cómo funciona una regresión lineal.

35
00:02:02,575 --> 00:02:06,095
Tratemos de entender sus motivaciones.

36
00:02:06,685 --> 00:02:09,710
Comencemos con una ecuación lineal
que, según nuestra hipótesis,

37
00:02:09,710 --> 00:02:12,210
describe nuestro sistema,
mediante la multiplicación

38
00:02:12,210 --> 00:02:15,745
de varios pesos por los vectores
de nuestros atributos observados

39
00:02:15,745 --> 00:02:17,385
y, luego, lo sumamos todo.

40
00:02:17,385 --> 00:02:21,220
Podemos representarlo
en la primera ecuación para cada ejemplo

41
00:02:21,220 --> 00:02:27,630
en nuestro conjunto de datos:
y = w0x0 + w1x1 + w2x2

42
00:02:28,150 --> 00:02:31,020
y así para cada atributo
en nuestro modelo.

43
00:02:31,020 --> 00:02:35,215
Es decir, aplicamos esta ecuación
a cada fila en nuestro conjunto de datos

44
00:02:35,215 --> 00:02:37,685
en la que los valores
de los pesos son fijos

45
00:02:37,685 --> 00:02:40,940
y los valores de los atributos
pertenecen a cada columna asociada

46
00:02:40,940 --> 00:02:42,930
en nuestro conjunto de datos de AA.

47
00:02:42,930 --> 00:02:46,300
Esto se podría condensar muy bien
en la siguiente ecuación de la matriz

48
00:02:46,300 --> 00:02:48,395
y = Xw

49
00:02:49,905 --> 00:02:52,955
Esta ecuación
de la hipótesis es muy importante

50
00:02:52,955 --> 00:02:56,505
no solo para la regresión lineal,
sino también para otros modelos de AA

51
00:02:56,505 --> 00:02:59,860
como las redes neuronales profundas,
de las que hablaremos más tarde.

52
00:03:00,325 --> 00:03:03,125
Pero ¿cómo puedo determinar
si los pesos que elegí

53
00:03:03,125 --> 00:03:05,715
realizan predicciones buenas o malas?

54
00:03:06,075 --> 00:03:09,435
La respuesta es que necesitamos
crear una función de pérdida

55
00:03:09,435 --> 00:03:12,025
que es, en esencia,
simplemente la función objetivo

56
00:03:12,025 --> 00:03:13,680
que queremos optimizar.

57
00:03:13,680 --> 00:03:17,200
Como explicamos antes, por lo general,
en los problemas de regresión

58
00:03:17,200 --> 00:03:19,830
la función de pérdida
es el error cuadrático medio

59
00:03:19,830 --> 00:03:23,200
que se muestra
en forma de matriz en esta ecuación.

60
00:03:23,440 --> 00:03:27,340
Quité la constante, ya que desaparecerá
más tarde en la derivación.

61
00:03:28,100 --> 00:03:31,660
Primero, encontramos la diferencia
entre el valor real de las etiquetas

62
00:03:31,660 --> 00:03:34,675
y el valor pronosticado
de nuestra etiqueta, ŷ,

63
00:03:34,675 --> 00:03:37,360
que es simplemente Xw.

64
00:03:38,330 --> 00:03:42,725
Pero recuerden que mi objetivo
es reducir la pérdida lo más posible.

65
00:03:42,925 --> 00:03:45,350
Debo encontrar
una manera de minimizarla

66
00:03:45,350 --> 00:03:46,880
con respecto a los pesos.

67
00:03:46,880 --> 00:03:50,280
Para hacerlo, tomo la derivada
con respecto de los pesos

68
00:03:50,280 --> 00:03:53,765
en el caso de una dimensión
o, de manera general, el gradiente

69
00:03:53,765 --> 00:03:56,335
cuando tengo varias características.

70
00:03:56,745 --> 00:03:59,710
Puedo usar esto
para encontrar la mínima global.

71
00:04:00,360 --> 00:04:03,380
Esta ecuación,
en la que no haré derivación

72
00:04:03,380 --> 00:04:07,640
proporciona una forma cerrada de solución
analítica para la regresión lineal.

73
00:04:07,640 --> 00:04:12,015
Es decir, si incluyen los valores
de X y Y en esta fórmula,

74
00:04:12,015 --> 00:04:14,395
obtendrán los valores de los pesos.

75
00:04:14,825 --> 00:04:19,300
Pero esto no es muy práctico,
hay problemas con la inversa.

76
00:04:19,810 --> 00:04:23,674
Primero, suponemos
que la matriz de Gram, X transpuesta de X,

77
00:04:23,674 --> 00:04:26,870
no es singular, es decir,
que todas las columnas

78
00:04:26,870 --> 00:04:30,255
de nuestra matriz de atributos X
son linealmente independientes.

79
00:04:30,255 --> 00:04:35,560
Pero en conjuntos de datos del mundo real,
habrá datos duplicados o casi duplicados.

80
00:04:35,560 --> 00:04:38,260
El mismo cliente
que compra el mismo producto otra vez,

81
00:04:38,260 --> 00:04:41,700
dos fotos del mismo amanecer
tomadas con segundos de diferencia.

82
00:04:42,360 --> 00:04:45,960
Incluso si la matriz de Gram
es linealmente independiente técnicamente,

83
00:04:45,960 --> 00:04:48,165
igual podría estar mal condicionada

84
00:04:48,165 --> 00:04:51,200
y, por ende,
causar que la matriz no sea invertible

85
00:04:51,200 --> 00:04:53,095
y causarnos problemas.

86
00:04:53,465 --> 00:04:58,390
La inversa también tiene una complejidad
de tiempo de ON al cubo

87
00:04:59,010 --> 00:05:00,705
con el algoritmo Naïve

88
00:05:00,705 --> 00:05:04,290
pero incluso con estos algoritmos
sofisticados, no es mejor.

89
00:05:04,290 --> 00:05:07,410
Además, cada uno incluye
sus propios problemas numéricos.

90
00:05:07,410 --> 00:05:09,595
Lo mismo pasa incluso
con la multiplicación

91
00:05:09,595 --> 00:05:11,320
para crear la matriz de Gram.

92
00:05:11,320 --> 00:05:13,770
En vez, podríamos resolver
las ecuaciones normales

93
00:05:13,770 --> 00:05:16,795
con lo que se conoce como Cholesky
o una descomposición QR.

94
00:05:17,795 --> 00:05:25,040
Para ON al cubo o incluso ON a la 2.5,
cuando N es igual a 10,000 o más,

95
00:05:25,040 --> 00:05:27,385
el algoritmo puede ser muy lento.

96
00:05:27,755 --> 00:05:30,930
Entonces, sí.
Se puede resolver con los pesos

97
00:05:30,930 --> 00:05:34,150
de la ecuación normal
pero dependerá de sus datos,

98
00:05:34,150 --> 00:05:39,590
su modelo, qué algoritmos de matriz
de algebra lineal están usando, etcétera.

99
00:05:39,790 --> 00:05:43,640
Afortunadamente, existe el algoritmo
de optimización del descenso de gradientes

100
00:05:43,640 --> 00:05:46,910
que es, primero, menos costoso
en términos de computación en tiempo

101
00:05:46,910 --> 00:05:50,785
y memoria; segundo,
más práctico para la generalización leve

102
00:05:50,785 --> 00:05:54,275
y tercero, lo suficientemente genérico
para la mayoría de los problemas.

103
00:05:54,275 --> 00:05:58,535
En vez, en el descenso de gradientes
tenemos nuestra función de pérdida

104
00:05:58,535 --> 00:06:01,000
o, de manera general,
nuestra función objetivo

105
00:06:01,000 --> 00:06:03,905
que está parametrizada
por los pesos de nuestro modelo.

106
00:06:03,905 --> 00:06:08,430
En este espacio, hay picos y valles,
al igual que en la Tierra.

107
00:06:08,430 --> 00:06:11,335
No obstante, en muchos
problemas de aprendizaje automático

108
00:06:11,335 --> 00:06:14,900
habrá muchas más dimensiones
en el mundo espacial en 3D

109
00:06:14,900 --> 00:06:16,435
en el que vivimos.

110
00:06:16,435 --> 00:06:18,500
Ya que este
es un descenso de gradientes,

111
00:06:18,500 --> 00:06:21,390
minimización
en el gradiente, no ascenso,

112
00:06:21,390 --> 00:06:23,640
que en su lugar sería maximización,

113
00:06:23,640 --> 00:06:26,790
queremos atravesar
la hipersuperficie de pérdida

114
00:06:26,790 --> 00:06:28,840
para encontrar la mínima global.

115
00:06:28,840 --> 00:06:32,220
Es decir,
esperamos encontrar el valle más bajo

116
00:06:32,220 --> 00:06:35,050
sin importar dónde comencemos
en la hipersuperficie.

117
00:06:35,440 --> 00:06:37,755
Esto se puede hacer
si encontramos el gradiente

118
00:06:37,755 --> 00:06:41,375
de la función de pérdida
y lo multiplicamos por un hiperparámetro,

119
00:06:41,375 --> 00:06:44,670
la tasa de aprendizaje,
y luego restamos ese valor

120
00:06:44,670 --> 00:06:46,460
de los pesos actuales.

121
00:06:46,460 --> 00:06:49,185
Este proceso se repite
hasta la convergencia.

122
00:06:49,815 --> 00:06:53,235
Elegir la tasa de aprendizaje óptima
y esperar muchas repeticiones

123
00:06:53,235 --> 00:06:55,955
podría provocar que elijan
la ecuación normal en su lugar,

124
00:06:55,955 --> 00:06:58,015
si la cantidad
de atributos es pequeña

125
00:06:58,015 --> 00:07:00,175
y no hay problemas
de colinealidad, etcétera.

126
00:07:00,175 --> 00:07:02,590
O agregar un optimizador
de descenso de gradientes,

127
00:07:02,590 --> 00:07:05,965
como el momentum, o mediante
el descenso de la tasa de aprendizaje.

128
00:07:05,965 --> 00:07:09,705
Hablaremos más sobre el descenso
de gradientes en el próximo módulo.

129
00:07:10,055 --> 00:07:13,830
¿Qué hiperparámetro
ayuda a determinar el tamaño

130
00:07:13,830 --> 00:07:16,870
del paso del descenso
de gradientes en la hipersuperficie

131
00:07:16,870 --> 00:07:19,500
para acelerar la convergencia?

132
00:07:21,880 --> 00:07:24,415
La respuesta correcta es
B. La tasa de aprendizaje.

133
00:07:24,415 --> 00:07:27,990
Esta tasa, junto con otros
hiperparámetros que veremos

134
00:07:27,990 --> 00:07:31,045
en próximos módulos,
ayuda a determinar el tamaño del paso

135
00:07:31,045 --> 00:07:32,635
en el descenso de gradientes.

136
00:07:32,635 --> 00:07:36,230
Si es muy bajo, el descenso
de gradientes toma demasiado tiempo

137
00:07:36,230 --> 00:07:41,415
para llegar a la convergencia.
Si es muy alto, podría desviarse

138
00:07:41,415 --> 00:07:43,670
y aumentar mucho más la pérdida.

139
00:07:43,670 --> 00:07:46,590
Las otras tres respuestas
tienen que ver con la colinealidad

140
00:07:46,590 --> 00:07:48,895
y el condicionamiento,
que no deben preocuparnos

141
00:07:48,905 --> 00:07:52,150
en el descenso de gradientes,
a diferencia de la ecuación normal.