1
00:00:00,000 --> 00:00:03,410
Vamos dar uma olhada rápida
na história do aprendizado de máquina

2
00:00:03,410 --> 00:00:05,740
para ver como ele
evoluiu ao longo do tempo

3
00:00:05,740 --> 00:00:08,905
até chegar hoje às famosas redes
neurais de aprendizagem profunda.

4
00:00:08,905 --> 00:00:12,145
Você perceberá que,
mesmo com tantas redes neurais

5
00:00:12,145 --> 00:00:15,895
surgindo e desaparecendo
nas últimas décadas,

6
00:00:15,895 --> 00:00:19,395
os mesmos truques e técnicas
desenvolvidos para outros algoritmos

7
00:00:19,395 --> 00:00:21,935
se aplicam a redes neurais
de aprendizagem profunda,

8
00:00:21,935 --> 00:00:23,690
o que as torna muito poderosas.

9
00:00:23,690 --> 00:00:28,030
A regressão linear foi inventada
para prever o movimento dos planetas

10
00:00:28,030 --> 00:00:31,280
e o tamanho de vagens de ervilha
com base na aparência delas.

11
00:00:31,280 --> 00:00:34,470
Sir Francis Galton foi pioneiro
no uso de métodos estatísticos

12
00:00:34,470 --> 00:00:38,135
para avaliar fenômenos naturais.

13
00:00:38,135 --> 00:00:42,595
Ele analisava dados sobre
os tamanhos relativos de pais e filhos

14
00:00:42,595 --> 00:00:45,625
em várias espécies,
incluindo a ervilha-de-cheiro.

15
00:00:45,625 --> 00:00:50,155
Ele notou algo que não é
muito óbvio, algo bem estranho.

16
00:00:50,155 --> 00:00:55,985
Certo, um pai maior que a média costuma
produzir filhos maiores que a média.

17
00:00:55,985 --> 00:01:00,200
Mas o quanto esse filho é maior em relação
à média dos outros filhos desta geração?

18
00:01:01,200 --> 00:01:05,015
Acontece que a proporção do filho
em relação à sua geração é menor

19
00:01:05,015 --> 00:01:07,435
do que a proporção correspondente do pai.

20
00:01:07,435 --> 00:01:14,405
Se o tamanho do pai tem um desvio
padrão de 1,5 da média da sua geração,

21
00:01:14,405 --> 00:01:21,145
podemos prever que o tamanho do filho terá
um desvio menor que 1,5 da média do grupo.

22
00:01:21,145 --> 00:01:23,915
Pode-se dizer que, de geração em geração,

23
00:01:23,915 --> 00:01:28,225
a natureza regressa ou volta para a média.

24
00:01:28,225 --> 00:01:31,250
Por isso usamos o nome regressão linear.

25
00:01:31,250 --> 00:01:36,930
O gráfico exibido é de 1877 e mostra
a primeira regressão linear da história.

26
00:01:36,930 --> 00:01:38,485
Muito interessante!

27
00:01:38,485 --> 00:01:42,650
A computação no século XIX
era um tanto limitada,

28
00:01:42,650 --> 00:01:47,920
ninguém sabia como isso funcionaria
nos grandes conjuntos de dados.

29
00:01:47,920 --> 00:01:52,085
Havia uma solução de forma fechada
para resolver a regressão linear,

30
00:01:52,085 --> 00:01:55,005
mas também era possível
usar métodos de gradiente descendente,

31
00:01:55,005 --> 00:01:58,095
cada um com prós e contras,
dependendo do conjunto de dados.

32
00:01:58,095 --> 00:02:01,800
Vamos olhar mais de perto
como a regressão linear funciona.

33
00:02:01,800 --> 00:02:06,530
Vejamos em detalhes
para entender a motivação dela.

34
00:02:06,535 --> 00:02:11,145
Começamos com uma equação linear
que descreve nosso sistema como hipótese,

35
00:02:11,145 --> 00:02:17,150
multiplicando vários pesos pelos vetores
de recurso observados e somando tudo.

36
00:02:17,150 --> 00:02:21,985
É possível usar a equação superior
para cada exemplo no conjunto de dados,

37
00:02:21,985 --> 00:02:30,820
y = w0 . x0 + w1 . x1 + w2 . x2 + …
para cada recurso no modelo.

38
00:02:30,820 --> 00:02:35,355
Em outras palavras, a equação é aplicada
a todas as linhas no conjunto de dados,

39
00:02:35,355 --> 00:02:37,440
em que os valores de peso são fixos,

40
00:02:37,440 --> 00:02:40,420
e os valores de recurso têm
origem em cada coluna associada

41
00:02:40,420 --> 00:02:42,480
no nosso conjunto
do aprendizado de máquina.

42
00:02:42,480 --> 00:02:48,585
É possível condensá-la
na equação de matriz inferior, y = x . w.

43
00:02:49,635 --> 00:02:54,385
Essa equação hipotética é muito
importante não só na regressão linear,

44
00:02:54,385 --> 00:02:56,255
mas em outros modelos de aprendizado

45
00:02:56,255 --> 00:02:58,995
como redes neurais profundas,
que serão abordadas depois.

46
00:03:00,215 --> 00:03:05,715
Mas como determinar se os pesos escolhidos
criam hipóteses boas ou ruins?

47
00:03:05,715 --> 00:03:09,515
A resposta é que precisamos
criar uma função de perda,

48
00:03:09,515 --> 00:03:13,125
que é basicamente uma função
objetivo que queremos otimizar.

49
00:03:13,125 --> 00:03:16,970
Como já explicado,
nos problemas de regressão,

50
00:03:16,970 --> 00:03:19,490
a função de perda
é o erro quadrático médio,

51
00:03:19,490 --> 00:03:22,950
mostrado em forma de matriz nesta equação.

52
00:03:22,950 --> 00:03:27,700
A constante não aparece,
já que ela desaparecerá na derivação.

53
00:03:27,700 --> 00:03:31,660
Vamos primeiro encontrar a diferença
entre o valor real dos marcadores

54
00:03:31,660 --> 00:03:37,235
e o previsto, y-hat,
que é x multiplicado por w.

55
00:03:38,305 --> 00:03:42,840
Não se esqueça de que o objetivo
é reduzir a perda o máximo possível.

56
00:03:42,840 --> 00:03:46,590
Então precisamos encontrar um jeito
de minimizá-la considerando os pesos.

57
00:03:46,590 --> 00:03:52,330
Para isso, usamos a derivada considerando
os pesos no caso unidimensional

58
00:03:52,330 --> 00:03:56,335
ou, de modo geral, o gradiente quando
temos vários recursos.

59
00:03:56,335 --> 00:03:59,710
Depois, isso é usado para encontrar
o mínimo global.

60
00:03:59,710 --> 00:04:03,380
Nesta equação,
em que não abordaremos a derivação,

61
00:04:03,380 --> 00:04:07,270
temos uma solução analítica
de forma fechada para regressão linear.

62
00:04:07,270 --> 00:04:12,015
Isso significa que, ao aplicar
os valores de X e Y na fórmula,

63
00:04:12,015 --> 00:04:14,395
você encontra os valores dos pesos.

64
00:04:14,395 --> 00:04:19,660
No entanto, isso não é muito prático,
já que há problemas com a inversa.

65
00:04:19,660 --> 00:04:24,955
Consideramos que a matriz de Gram, 
X transposta X, não seja singular.

66
00:04:24,955 --> 00:04:29,890
Ou seja, todas as colunas da matriz de
recurso X são independentes linearmente.

67
00:04:29,890 --> 00:04:32,260
Mas em conjuntos de dados reais,

68
00:04:32,260 --> 00:04:35,320
você tem dados totalmente
ou quase duplicados.

69
00:04:35,320 --> 00:04:38,270
O mesmo cliente compra
um produto igual novamente,

70
00:04:38,270 --> 00:04:41,795
duas fotos do nascer do sol
tiradas em alguns segundos...

71
00:04:41,795 --> 00:04:45,830
Mesmo se a matriz de Gram
for independente linearmente,

72
00:04:45,830 --> 00:04:48,135
ela ainda poderá ser inadequada,

73
00:04:48,135 --> 00:04:53,240
o que a torna singular na computação,
e continua causando problemas para nós.

74
00:04:53,240 --> 00:04:58,610
A inversa também tem
complexidade de tempo de ON ao cubo

75
00:04:58,610 --> 00:05:00,630
usando algoritmo ingênuo.

76
00:05:00,630 --> 00:05:04,285
Ainda assim, algoritmos mais
complexos também não ajudam muito.

77
00:05:04,285 --> 00:05:07,270
Eles trazem
novos problemas numéricos.

78
00:05:07,270 --> 00:05:10,900
Isso também se aplica à multiplicação
usada para criar a matriz de Gram.

79
00:05:10,900 --> 00:05:15,285
É melhor resolver as equações
normais usando a fatoração de Cholesky

80
00:05:15,285 --> 00:05:17,165
ou decomposição QR.

81
00:05:17,165 --> 00:05:21,710
Em ON ao cubo ou até mesmo ON elevado a
2,5,

82
00:05:21,710 --> 00:05:24,840
quando N é igual a 10.000 ou mais,

83
00:05:24,840 --> 00:05:27,280
o algoritmo pode ser muito lento.

84
00:05:27,280 --> 00:05:31,890
Então, sim, é possível encontrar
os pesos usando a equação normal,

85
00:05:31,890 --> 00:05:35,370
mas isso depende muito
dos seus dados e seu modelo,

86
00:05:35,370 --> 00:05:39,175
além da álgebra linear
e algoritmos de matriz que você usa etc.

87
00:05:39,175 --> 00:05:42,895
Felizmente, há o algoritmo
de otimização por gradiente descendente

88
00:05:42,895 --> 00:05:47,480
que é mais econômico na computação
em termos de tempo e memória,

89
00:05:47,480 --> 00:05:50,775
mais suscetível a generalização moderada,

90
00:05:50,775 --> 00:05:54,400
além de ser genérico suficiente
para resolver a maioria dos problemas.

91
00:05:54,400 --> 00:05:56,665
Na verdade, no gradiente descendente,

92
00:05:56,665 --> 00:06:00,720
a função de perda, ou,
mais geralmente, a função objetivo

93
00:06:00,720 --> 00:06:03,400
são parametrizadas pelos pesos do modelo.

94
00:06:03,400 --> 00:06:08,460
Neste espaço,
há montanhas e vales, assim como a Terra.

95
00:06:08,460 --> 00:06:11,310
No entanto, em muitos
problemas de aprendizado de máquina,

96
00:06:11,310 --> 00:06:15,830
haverá muito mais dimensões
do que no mundo 3D em que vivemos.

97
00:06:15,830 --> 00:06:18,240
Como o gradiente é descendente,

98
00:06:18,240 --> 00:06:21,295
redução ao longo gradiente, 
e não ascendente,

99
00:06:21,295 --> 00:06:23,465
que no caso é aumento,

100
00:06:23,465 --> 00:06:26,330
queremos atravessar
a hipersuperfície de perda

101
00:06:26,330 --> 00:06:28,170
em busca do mínimo global.

102
00:06:28,170 --> 00:06:32,065
Em outras palavras, o objetivo
é encontrar o vale mais profundo,

103
00:06:32,065 --> 00:06:35,205
seja qual for
nosso início na hipersuperfície.

104
00:06:35,205 --> 00:06:38,705
Isso é feito ao encontrar
o gradiente da função de perda

105
00:06:38,705 --> 00:06:41,325
e multiplicá-lo pelo hiperparâmetro,

106
00:06:41,325 --> 00:06:45,975
ou taxa de aprendizado, e depois
subtrair esse valor dos pesos atuais.

107
00:06:45,975 --> 00:06:49,300
Esse processo é
iterado até a convergência.

108
00:06:49,300 --> 00:06:52,875
Para ter a melhor taxa
de aprendizado e esperar muitas iterações,

109
00:06:52,875 --> 00:06:55,745
você pode escolher usar a equação normal,

110
00:06:55,745 --> 00:06:57,800
desde que o número
de recursos seja pequeno,

111
00:06:57,800 --> 00:06:59,690
não haja colinearidade etc.,

112
00:06:59,690 --> 00:07:02,280
ou um otimizador adicional
de gradiente descendente,

113
00:07:02,280 --> 00:07:05,615
como momentum, ou uso
de uma taxa de aprendizado ruim.

114
00:07:05,615 --> 00:07:09,990
Falaremos muito mais sobre os detalhes do
gradiente descendente no próximo módulo.

115
00:07:09,990 --> 00:07:15,245
Qual é o hiperparâmetro que determina o
tamanho do passo do gradiente descendente,

116
00:07:15,245 --> 00:07:16,625
junto com a hipersuperfície,

117
00:07:16,625 --> 00:07:19,010
para acelerar a convergência?

118
00:07:21,610 --> 00:07:23,945
A resposta correta é "Taxa de 
aprendizado".

119
00:07:23,945 --> 00:07:27,000
A taxa de aprendizado junto
com alguns outros hiperparâmetros,

120
00:07:27,000 --> 00:07:29,330
que você conhecerá nos próximos módulos,

121
00:07:29,330 --> 00:07:32,035
dimensionam o tamanho
do passo no gradiente descendente.

122
00:07:32,035 --> 00:07:37,020
Se muito baixa, o gradiente descendente
levará muito tempo para convergir.

123
00:07:37,020 --> 00:07:38,480
Se muito alta,

124
00:07:38,480 --> 00:07:41,130
ele poderá até mesmo divergir

125
00:07:41,130 --> 00:07:43,560
e aumentar a perda cada vez mais.

126
00:07:43,560 --> 00:07:47,600
As outras três respostas têm a ver com
colinearidade e condicionamento,

127
00:07:47,600 --> 00:07:52,125
que não se aplicam ao gradiente
descendente, diferente da equação normal.