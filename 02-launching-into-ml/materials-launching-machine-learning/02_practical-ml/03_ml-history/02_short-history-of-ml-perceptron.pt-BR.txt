No que diz respeito ao
aprendizado a partir de dados, a Regressão Linear era basicamente isso. Até a década de 40, quando
um pesquisador chamado Frank Rosenblatt surge com um perceptron como
um modelo computacional de um neurônio no cérebro humano e mostra como ele
pode aprender funções simples. Tratava-se do que hoje
chamamos de Classificador Linear Binário, em que tentamos encontrar uma única linha que divide
os dados em duas classes. Uma camada de perceptrons seria a rede
neural feedforward mais simples possível. Entradas que alimentam perceptrons de uma
camada e uma soma ponderada serão feitas. Essa soma passaria
pela função de inativação, que é apenas 
uma função matemática que você aplica a cada elemento que
agora reside naquele neurônio. Mas, lembre-se de que, 
neste momento, isto é apenas um classificador linear. Portanto, a função de ativação,
que é linear neste caso, apenas retorna suas entradas. Comparar a saída dessa
função com um limite determinaria a qual classe
cada ponto pertence. Os erros seriam agregados e usados
para mudar os pesos usados na soma. E o processo se repetiria
até a convergência. Se você está tentando criar
um modelo simples de algo que aprende uma saída desejada
a partir de uma distribuição de entrada, não precisa ir longe. Nossos cérebros fazem isso o dia inteiro
para entender o mundo que nos cerca e todos os sinais que recebemos. Uma das principais
unidades do cérebro é o neurônio. As redes neurais são apenas
grupos de neurônios conectados em diferentes
padrões ou arquiteturas. Um neurônio biológico tem vários
componentes especializados em transmitir sinais elétricos que nos
permitem pensar, realizar ações e estudar o mundo fascinante do
aprendizado de máquina. Os sinais elétricos de outros neurônios,
como os sensoriais na retina dos olhos, são propagados de
neurônio em neurônio. O sinal de entrada é recebido
em uma das pontas do neurônio, que é composta de dendritos. Esses dendritos podem não só coletar
sinais elétricos de apenas um neurônio mas possivelmente de vários
que se somam em janelas a ponto de alterar
o potencial elétrico da célula. Um neurônio comum tem
um potencial elétrico em repouso de cerca de 70 milivolts negativos. À medida que os estímulos de entrada
recebidos nos dendritos aumentam, eles podem alcançar um limite
em torno de 55 milivolts negativos. Quando ocorre uma rápida
despolarização do axônio com várias portas de voltagem se abrindo
e permitindo um fluxo repentino de íons. Isso faz o neurônio disparar um potencial
de ação de corrente elétrica ao longo do axônio ajudado pela bainha de mielina
para melhor transmissão ao axônio. Aqui, os neurotransmissores
são liberados em sinapses que viajam pela fenda sináptica
para os dendritos de outros neurônios. Alguns dos neurotransmissores
são excitatórios e aumentam o potencial da próxima célula. Outros são inibitórios
e diminuem o potencial. O neurônio se repolariza a um potencial
menor que repouso por um tempo refratário. E o processo continua
no próximo neurônio até que alcança um neurônio motor e move a sua
mão para proteger os olhos do sol. E o que toda essa biologia e neurociência
têm a ver com o aprendizado de máquina? Parece familiar? Este é um perceptron de camada única. Assim como o neurônio, ele tem entradas que
multiplica por pesos e soma tudo. Aqui, o valor é comparado a um limite
e transformado por uma função de ativação. Por exemplo, se a soma
for maior ou igual a zero, ative ou pressione o valor de um. Do contrário, não ative
ou pressione um valor de zero. As entradas e os pesos são como os
neurotransmissores de um neurônio em que alguns podem
ser positivos e agregar à soma e outros podem ser
negativos e subtrair da soma. A função de etapa da unidade atua
como um limite de tudo ou nada. Se o limite for alcançado,
o sinal é transmitido. Do contrário,
nenhum sinal é transmitido. Por fim, há uma saída e,
como neurônios biológicos, isso pode realmente passar como entrada para outros neurônios em
um perceptron de várias camadas . Falaremos sobre isso em seguida. Isso tudo é muito legal, mas note que há funções
muito simples que não são aprendidas. Por exemplo, a função XOR. Marvin Minsky, um famoso
cientista da computação do MIT, ressaltou isso e ninguém
se interessou pela IA por 15 anos. Este não foi o primeiro
obstáculo das redes neurais que acabaram ficando
esquecidas por um tempo. Que componente do neurônio biológico é
análogo à entrada de um perceptron? A resposta correta
são os dendritos. Eles recebem estímulos
de outros neurônios, assim como em
uma rede neural artificial. Não é o axônio porque ele é
mais análogo à saída de um perceptron. Não é o núcleo porque ele
armazena o material genético celular e controla as atividades
das células. Não é a bainha de mielina porque
ela ajuda na transmissão do axônio, que, mais uma vez, fica na
saída do perceptron.