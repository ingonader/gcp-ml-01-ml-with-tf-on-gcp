1
00:00:00,000 --> 00:00:03,020
Voltando para a linha do tempo,
falamos agora das redes neurais,

2
00:00:03,020 --> 00:00:06,510
com ainda mais melhorias com
os saltos na capacidade computacional,

3
00:00:06,510 --> 00:00:08,860
além de uma enorme quantidade de dados.

4
00:00:08,860 --> 00:00:14,840
As DNNs começaram a superar outros
métodos de teste como visão computacional.

5
00:00:14,840 --> 00:00:17,680
Além do grande
avanço do hardware aprimorado,

6
00:00:17,680 --> 00:00:21,050
há muitas
novas técnicas e arquiteturas que

7
00:00:21,050 --> 00:00:24,695
otimizaram o treinamento de
redes neurais profundas como ReLUs,

8
00:00:24,695 --> 00:00:30,235
métodos melhores de inicialização, CNNs
(redes neurais convolucionais) e dropout.

9
00:00:31,745 --> 00:00:34,650
Falamos sobre algumas
dessas técnicas em outros métodos de ML.

10
00:00:34,650 --> 00:00:38,090
Sobre o uso de funções
de ativação não lineares como ReLUs,

11
00:00:38,090 --> 00:00:40,325
que agora costumam
ser definidas como padrão,

12
00:00:40,325 --> 00:00:42,890
falamos durante
a primeira abordagem de redes neurais.

13
00:00:43,600 --> 00:00:46,945
Camadas de dropout começaram
a ser usadas para ajudar a generalização,

14
00:00:46,945 --> 00:00:48,585
que é como métodos de combinação,

15
00:00:48,585 --> 00:00:51,920
abordados quando falamos de
florestas aleatórias e árvores de decisão.

16
00:00:52,220 --> 00:00:54,840
As camadas convolucionais
foram adicionadas para reduzir

17
00:00:54,840 --> 00:00:58,825
a carga computacional e de memória
devido à conectividade não total dela,

18
00:00:58,825 --> 00:01:01,990
além de se concentrar em aspectos locais.

19
00:01:01,990 --> 00:01:05,985
Por exemplo, em imagens, em vez de 
comparar fatores não relacionados nelas.

20
00:01:07,065 --> 00:01:10,850
Em outras palavras, todos os avanços
que surgiram em outros métodos de ML

21
00:01:10,850 --> 00:01:13,005
foram transformados em redes neurais.

22
00:01:13,005 --> 00:01:15,510
Vamos ver um exemplo
de rede neural profunda.

23
00:01:15,510 --> 00:01:18,475
Este incrível histórico
do aprendizado de máquina

24
00:01:18,475 --> 00:01:22,320
culminou na aprendizagem profunda
com redes neurais que contêm centenas

25
00:01:22,320 --> 00:01:26,350
de camadas e milhões de parâmetros,
mas com resultados excelentes.

26
00:01:26,350 --> 00:01:29,065
Este é o GoogLeNet ou Inception,

27
00:01:29,065 --> 00:01:31,150
um modelo de classificação de imagem.

28
00:01:31,150 --> 00:01:34,960
Ele foi treinado para o ImageNet 
Large Visual Recognition Challenge

29
00:01:34,960 --> 00:01:39,960
em 2014, usando dados de 2012,
para classificar imagens em

30
00:01:39,960 --> 00:01:43,970
milhares de classes com
1,2 milhão de imagens de treinamento.

31
00:01:43,970 --> 00:01:46,420
Ele tem 22 camadas profundas,

32
00:01:46,420 --> 00:01:48,550
27, se incluir o pool,

33
00:01:48,550 --> 00:01:50,480
que abordaremos mais adiante no curso,

34
00:01:50,480 --> 00:01:54,780
além de centenas de camadas, se dividi-lo
em blocos de construção independentes.

35
00:01:54,780 --> 00:01:58,150
Há mais de 11 milhões
de parâmetros treinados.

36
00:01:58,150 --> 00:02:01,355
Algumas camadas são totalmente conectadas,
outras não,

37
00:02:01,355 --> 00:02:04,225
como as convolucionais,
que abordaremos mais tarde.

38
00:02:04,225 --> 00:02:07,255
São usadas camadas de dropout
para aumentar a generalização,

39
00:02:07,255 --> 00:02:10,000
simulando uma combinação
de redes neurais profundas.

40
00:02:10,000 --> 00:02:12,410
Como vimos nas
redes neurais e empilhamento,

41
00:02:12,410 --> 00:02:15,845
cada caixa é uma unidade de
componentes em um grupo maior,

42
00:02:15,845 --> 00:02:17,845
como aquele que mostramos ampliado.

43
00:02:17,845 --> 00:02:21,420
Essa ideia de blocos de construção
que formam algo maior do que a soma

44
00:02:21,420 --> 00:02:25,720
das partes é um dos motivos
do sucesso da aprendizagem profunda.

45
00:02:25,720 --> 00:02:28,420
Claro, uma quantidade
de dados cada vez maior,

46
00:02:28,420 --> 00:02:31,805
uma capacidade de computação robusta e
mais memória também ajudam.

47
00:02:31,805 --> 00:02:34,830
Há agora diversas versões
que vão além disso,

48
00:02:34,830 --> 00:02:37,865
são muito maiores
e têm precisão ainda melhor.

49
00:02:37,865 --> 00:02:40,310
O principal ponto de
todo este histórico é que

50
00:02:40,310 --> 00:02:43,740
a pesquisa do aprendizado de máquina
reutiliza partes de técnicas

51
00:02:43,740 --> 00:02:47,150
de outros algoritmos antigos
e as combina para

52
00:02:47,150 --> 00:02:50,940
criar modelos muito avançados
e, principalmente, realizar testes.

53
00:02:50,940 --> 00:02:54,080
O que é importante ao
criar redes neurais profundas?

54
00:02:56,660 --> 00:02:59,255
Resposta correta: "Todas as opções acima".

55
00:02:59,255 --> 00:03:01,445
Esta não é uma lista detalhada,

56
00:03:01,445 --> 00:03:04,550
mas é muito importante
considerar estas três opções.

57
00:03:04,550 --> 00:03:07,790
Primeiro, você precisa ter muitos dados.

58
00:03:07,790 --> 00:03:10,680
Muitas pesquisas estão
sendo feitas para tentar reduzir

59
00:03:10,680 --> 00:03:12,650
os dados para aprendizagem profunda.

60
00:03:12,650 --> 00:03:15,460
Até lá, precisamos ter muitos deles.

61
00:03:15,460 --> 00:03:18,780
Isso acontece por conta
da alta capacidade do número

62
00:03:18,780 --> 00:03:22,080
de parâmetros que precisam
ser treinados nestes modelos enormes.

63
00:03:22,080 --> 00:03:24,360
Como o modelo é muito complexo,

64
00:03:24,360 --> 00:03:27,225
ele realmente precisa
internalizar a distribuição de dados.

65
00:03:27,225 --> 00:03:29,710
Portanto, são necessários muitos sinais.

66
00:03:29,710 --> 00:03:32,680
Lembre-se: o propósito do
aprendizado de máquina não é

67
00:03:32,680 --> 00:03:35,550
simplesmente treinar
um grupo de modelos sofisticados.

68
00:03:35,550 --> 00:03:38,855
É treiná-los para que
façam previsões bastante precisas.

69
00:03:38,855 --> 00:03:41,810
Se não é possível generalizar
novos dados para fazer previsões,

70
00:03:41,810 --> 00:03:43,895
então qual é o sentido desse modelo?

71
00:03:43,895 --> 00:03:48,030
Portanto, repetindo, ter
dados suficientes é importante para que

72
00:03:48,030 --> 00:03:52,085
não se sobreajustem a
um conjunto visto milhões de vezes,

73
00:03:52,085 --> 00:03:55,165
em vez de a um enorme, visto muito menos.

74
00:03:55,165 --> 00:03:56,730
Isso também possibilita ter

75
00:03:56,730 --> 00:03:59,915
conjuntos de teste e validação grandes
para ajustar modelos.

76
00:03:59,915 --> 00:04:03,860
Além disso, ao adicionar camadas
de dropout, executar aumento de dados,

77
00:04:03,860 --> 00:04:08,435
incluir ruído etc., você aprimora
ainda mais a generalização.

78
00:04:08,435 --> 00:04:12,410
Por fim, o aprendizado de máquina
é experimentação.

79
00:04:12,410 --> 00:04:14,760
Há vários tipos diferentes de algoritmo,

80
00:04:14,760 --> 00:04:18,084
hiperparâmetros e formas de
criar conjuntos de dados atualmente.

81
00:04:18,084 --> 00:04:20,610
Não há uma forma prioritária de saber,

82
00:04:20,610 --> 00:04:24,005
desde o início, quais as melhores
opções para quase todos os problemas.

83
00:04:24,005 --> 00:04:28,110
Com a experimentação e o acompanhamento
cuidadoso do que já foi realizado

84
00:04:28,110 --> 00:04:30,535
e avaliações de desempenho
para comparar modelos,

85
00:04:30,535 --> 00:04:35,170
você se divertirá muito e
criará algumas ferramentas poderosas.

86
00:04:35,820 --> 00:04:38,060
Em seguida, vamos falar
um pouco mais sobre como

87
00:04:38,060 --> 00:04:41,415
redes neurais continuam a se basear
no desempenho de modelos antigos.

88
00:04:42,185 --> 00:04:43,630
Este é o desempenho de

89
00:04:43,630 --> 00:04:47,050
versões específicas de modelo de
redes neurais profundas em vários anos.

90
00:04:47,050 --> 00:04:48,590
Como é possível ver no gráfico,

91
00:04:48,590 --> 00:04:50,970
um grande salto ocorreu em 2014,

92
00:04:50,970 --> 00:04:52,390
destacado em azul,

93
00:04:52,390 --> 00:04:54,390
em que o modelo "Inception" do Google

94
00:04:54,390 --> 00:04:57,375
evoluiu de 10% a 6,7% de taxa de erro.

95
00:04:57,375 --> 00:05:01,210
O desempenho das DNNs
continua melhorando a cada ano,

96
00:05:01,210 --> 00:05:04,160
usando a experiência
aprendida de modelos anteriores.

97
00:05:04,160 --> 00:05:07,060
Em 2015, uma terceira
versão do modelo "Inception"

98
00:05:07,060 --> 00:05:09,780
alcançou 3,5% de taxa de erro.

99
00:05:09,780 --> 00:05:14,045
O que levou estes modelos a
se aprimorar tanto em um período pequeno?

100
00:05:14,045 --> 00:05:18,465
Muitas vezes, quando pesquisadores criam
uma nova técnica ou método muito bons,

101
00:05:18,465 --> 00:05:22,110
outros usam essas ideias
como base de criação.

102
00:05:22,110 --> 00:05:28,110
Isso gera um grande salto na
experimentação para acelerar o progresso.

103
00:05:28,110 --> 00:05:31,750
Isso inclui melhores hiperparâmetros,
mais camadas,

104
00:05:31,750 --> 00:05:36,565
melhor generalização, subcomponentes 
aprimorados como camada convolucional etc.

105
00:05:36,565 --> 00:05:39,910
Explique como você
aplicaria o ML ao problema.

106
00:05:39,910 --> 00:05:42,505
Pode haver mais de uma resposta correta.

107
00:05:44,525 --> 00:05:47,880
Você é o dono de uma estação de
esqui e quer prever níveis de tráfego

108
00:05:47,880 --> 00:05:50,615
nas pistas com base em
quatro tipos de cliente,

109
00:05:50,615 --> 00:05:54,765
iniciante, intermediário, avançado
e especialista, que compraram bilhetes,

110
00:05:54,765 --> 00:05:56,715
e com base na quantidade passada de neve.

111
00:05:59,905 --> 00:06:01,725
Escreva agora sua resposta.

112
00:06:03,885 --> 00:06:07,265
Aplicam-se regressão ou classificação,

113
00:06:07,265 --> 00:06:11,350
já que o significado de níveis de tráfego
não foi exatamente especificado.

114
00:06:11,350 --> 00:06:15,170
Queremos dizer o número
de pessoas que usam a pista por hora?

115
00:06:15,170 --> 00:06:19,285
Ou algo mais categórico
como alto, médio e baixo?

116
00:06:19,285 --> 00:06:21,830
Para isso, começamos
com uma heurística de base

117
00:06:21,830 --> 00:06:24,690
como o número médio
de pessoas em cada pista

118
00:06:24,690 --> 00:06:28,400
e depois modelos de base
de regressão logística ou linear,

119
00:06:28,400 --> 00:06:32,645
dependendo do processo
de classificação ou regressão.

120
00:06:33,445 --> 00:06:35,545
Conforme o desempenho,
a quantidade de dados,

121
00:06:35,545 --> 00:06:38,265
talvez seja necessário usar redes neurais.

122
00:06:38,265 --> 00:06:40,240
Se houver outros recursos nos dados,

123
00:06:40,240 --> 00:06:43,155
é bom testá-los e monitorar o desempenho.

124
00:06:45,695 --> 00:06:48,790
No Google, segundo a última contagem,

125
00:06:48,790 --> 00:06:53,025
há mais de 4.000 modelos de ML profundo
de produção capacitando sistemas.

126
00:06:53,025 --> 00:06:56,610
Cada modelo e versão
melhora o desempenho ao se basear

127
00:06:56,610 --> 00:07:00,290
no sucesso e falhas de modelos antigos.

128
00:07:00,290 --> 00:07:03,585
Um dos mais usados
antigamente era o Sibyl,

129
00:07:03,585 --> 00:07:06,760
criado inicalmente para recomendar
vídeos relacionados do YouTube.

130
00:07:06,760 --> 00:07:09,670
Esse mecanismo funcionava tão bem

131
00:07:09,670 --> 00:07:13,365
que depois foi muito incorporado a
anúncios e outras partes do Google.

132
00:07:13,365 --> 00:07:15,720
O modelo era linear.

133
00:07:15,720 --> 00:07:19,460
Neste ano, outro modelo acabou se tornando

134
00:07:19,460 --> 00:07:23,980
o verdadeiro mecanismo de ajuste de
parâmetro de outros modelos e sistemas.

135
00:07:23,980 --> 00:07:27,020
O Google Brain é
a divisão de pesquisa de ML

136
00:07:27,020 --> 00:07:30,540
que criou um jeito de aproveitar
a capacidade computacional de milhares

137
00:07:30,540 --> 00:07:34,020
de CPUs para treinar grandes modelos
como redes neurais profundas.

138
00:07:35,140 --> 00:07:37,330
A experiência ao criar
e executar os modelos

139
00:07:37,330 --> 00:07:39,730
foi o que moldou
a criação do TensorFlow,

140
00:07:39,730 --> 00:07:42,410
uma biblioteca de código aberto de ML.

141
00:07:42,410 --> 00:07:47,210
O Google então criou o TFX ou
a plataforma de ML baseada no TensorFlow.

142
00:07:47,210 --> 00:07:50,600
Mostraremos a você como criar
 e implantar modelos de ML de produção

143
00:07:50,600 --> 00:07:55,040
com o TensorFlow e o
Cloud ML Engine, Dataflow e BigQuery.

144
00:07:55,040 --> 00:07:57,465
Recapitulando, ocorreu nas últimas décadas

145
00:07:57,465 --> 00:08:01,190
um aumento na adoção 
e no desempenho de redes neurais.

146
00:08:01,190 --> 00:08:02,700
Com a onipresença dos dados,

147
00:08:02,700 --> 00:08:06,905
estes modelos têm a vantagem de aprender
com cada vez mais exemplos de treinamento.

148
00:08:06,905 --> 00:08:10,410
O aumento nos dados e exemplos se aliou à

149
00:08:10,410 --> 00:08:15,900
infraestrutura escalonável para modelos
complexos distribuídos de várias camadas.

150
00:08:15,900 --> 00:08:18,320
Uma observação que
deixaremos com você é que,

151
00:08:18,320 --> 00:08:21,975
mesmo que o desempenho de redes neurais
seja ótimo em alguns aplicativos,

152
00:08:21,975 --> 00:08:25,750
elas são apenas algumas dos vários tipos
de modelo disponíveis para testar.

153
00:08:25,750 --> 00:08:28,070
A experimentação é o segredo para garantir

154
00:08:28,070 --> 00:08:31,710
o melhor desempenho usando
seus dados para superar seus desafios.