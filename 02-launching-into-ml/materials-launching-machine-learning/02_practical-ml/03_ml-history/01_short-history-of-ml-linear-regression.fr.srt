1
00:00:00,000 --> 00:00:03,410
Intéressons-nous brièvement
à l'histoire du machine learning

2
00:00:03,410 --> 00:00:07,080
pour voir comment il s'est transformé
en réseaux de neurones de deep learning

3
00:00:07,080 --> 00:00:08,905
qui sont aujourd'hui si populaires.

4
00:00:09,275 --> 00:00:15,805
Les réseaux de neurones ont été utilisés
au gré des modes ces dernières décennies,

5
00:00:15,805 --> 00:00:19,115
mais ces techniques développées
pour d'autres algorithmes

6
00:00:19,115 --> 00:00:22,165
peuvent être appliquées
aux réseaux de neurones de deep learning,

7
00:00:22,165 --> 00:00:23,595
qui sont ainsi très puissants.

8
00:00:23,595 --> 00:00:27,820
La régression linéaire a été inventée
pour prédire le mouvement des planètes

9
00:00:27,820 --> 00:00:30,980
et la taille des cosses
en fonction de leur apparence.

10
00:00:31,700 --> 00:00:35,600
Sir Francis Galton a été l'un des premiers
à utiliser les méthodes statistiques

11
00:00:35,600 --> 00:00:38,020
pour mesurer des phénomènes naturels.

12
00:00:38,830 --> 00:00:42,545
Il s'intéressait aux tailles respectives
des parents et de leurs enfants,

13
00:00:42,545 --> 00:00:45,435
pour différentes espèces,
dont les pois de senteur.

14
00:00:45,795 --> 00:00:50,205
Il a alors observé
quelque chose de très étrange :

15
00:00:50,525 --> 00:00:55,765
un parent plus grand que la moyenne a
tendance à produire un enfant plus grand,

16
00:00:55,895 --> 00:01:00,655
mais dans quelle mesure est-il plus grand
que la moyenne des autres enfants ?

17
00:01:01,135 --> 00:01:07,090
Ce ratio pour l'enfant est
en fait inférieur à celui du parent.

18
00:01:07,780 --> 00:01:12,705
Si la taille du parent a un écart type
de 1,5 par rapport à la moyenne,

19
00:01:12,705 --> 00:01:14,485
au sein de sa génération,

20
00:01:14,485 --> 00:01:17,205
cela prédit que la taille
de l'enfant sera inférieure

21
00:01:17,205 --> 00:01:20,725
aux écarts types de 1,5
observés au sein de sa cohorte.

22
00:01:21,535 --> 00:01:24,125
Nous disons que,
génération après génération,

23
00:01:24,125 --> 00:01:28,600
les choses de la nature régressent
ou reviennent à la moyenne,

24
00:01:28,600 --> 00:01:31,305
d'où le nom "régression linéaire".

25
00:01:31,985 --> 00:01:36,745
Ce graphique de 1877 est
la première régression linéaire.

26
00:01:36,965 --> 00:01:37,980
Remarquable.

27
00:01:39,340 --> 00:01:42,510
La puissance de calcul,
en 1800, était assez limitée.

28
00:01:42,720 --> 00:01:45,850
Ils n'ont donc même pas réalisé
que cela fonctionnerait très bien

29
00:01:45,850 --> 00:01:47,885
avec de grands ensembles de données.

30
00:01:48,505 --> 00:01:51,940
Il existait une solution analytique
pour résoudre la régression linéaire,

31
00:01:51,940 --> 00:01:55,280
mais les méthodes de descente
de gradient peuvent aussi être utilisées,

32
00:01:55,280 --> 00:01:57,375
chacune ayant
ses avantages et inconvénients,

33
00:01:57,375 --> 00:01:58,715
selon l'ensemble de données.

34
00:01:58,715 --> 00:02:01,175
Penchons-nous sur la régression linéaire.

35
00:02:02,285 --> 00:02:06,465
Attardons-nous sur les motivations
autour de la régression linéaire.

36
00:02:06,805 --> 00:02:08,755
Nous commençons par une équation linéaire

37
00:02:08,755 --> 00:02:11,330
qui, selon l'hypothèse de départ,
décrit notre système.

38
00:02:11,330 --> 00:02:13,760
Nous multiplions diverses pondérations

39
00:02:13,760 --> 00:02:17,135
par les vecteurs de caractéristiques
observés, puis en faisons la somme.

40
00:02:17,565 --> 00:02:20,085
Nous pouvons le représenter
dans l'équation ci-dessus,

41
00:02:20,085 --> 00:02:22,375
pour chaque exemple
de notre ensemble de données,

42
00:02:22,375 --> 00:02:29,040
y= w0 fois x0
+ w1 fois x1 plus w2 fois x2, etc.,

43
00:02:29,040 --> 00:02:31,200
pour chaque caractéristique
de notre modèle.

44
00:02:31,200 --> 00:02:35,125
Ainsi, nous appliquons cette équation
à chaque ligne de notre ensemble,

45
00:02:35,125 --> 00:02:37,150
quand les valeurs pondérales sont fixes,

46
00:02:37,150 --> 00:02:40,815
et que les valeurs des caractéristiques
proviennent de chaque colonne associée

47
00:02:40,815 --> 00:02:42,705
et de notre ensemble de données de ML.

48
00:02:42,945 --> 00:02:48,370
Cela pourrait être résumé
par l'équation de mesures y = X fois w.

49
00:02:50,100 --> 00:02:52,990
Cette équation d'hypothèse
est très importante

50
00:02:52,990 --> 00:02:56,410
pour la régression linéaire
et d'autres modèles de ML,

51
00:02:56,410 --> 00:02:59,885
comme les réseaux de neurones profonds
dont nous parlerons ultérieurement.

52
00:03:00,465 --> 00:03:05,375
Comment déterminer si mes pondérations
font de bonnes ou mauvaises suppositions ?

53
00:03:06,235 --> 00:03:09,510
Réponse : nous devons
créer une fonction de perte,

54
00:03:09,510 --> 00:03:12,895
qui est une fonction d'objectif
que nous voulons optimiser.

55
00:03:13,535 --> 00:03:17,135
Comme déjà expliqué, en général,
pour les problèmes de régression,

56
00:03:17,135 --> 00:03:19,715
la fonction de perte est
l'erreur quadratique moyenne,

57
00:03:19,715 --> 00:03:22,815
qui est représentée dans cette équation
sous forme matricielle.

58
00:03:23,625 --> 00:03:27,505
Je ne parle pas de la constante ici :
elle va disparaître dans la dérivation.

59
00:03:28,115 --> 00:03:31,590
Nous trouvons d'abord la différence
entre la valeur réelle des étiquettes

60
00:03:31,590 --> 00:03:34,620
et la valeur prédite de celles-ci,
y-accent circonflexe,

61
00:03:34,620 --> 00:03:37,390
qui est X fois w.

62
00:03:38,420 --> 00:03:42,880
Mon objectif est de réduire la perte
autant que faire se peut.

63
00:03:42,880 --> 00:03:45,220
Je dois trouver un moyen
de la réduire au maximum,

64
00:03:45,220 --> 00:03:46,795
en fonction des pondérations.

65
00:03:46,795 --> 00:03:50,150
Pour ce faire, je prends la dérivée
en fonction des pondérations,

66
00:03:50,150 --> 00:03:52,475
dans le cas de la 1D,

67
00:03:52,475 --> 00:03:56,490
ou, plus généralement, le gradient
quand j'ai plusieurs caractéristiques.

68
00:03:56,830 --> 00:03:59,690
Je peux ensuite utiliser ceci
pour trouver le minimum absolu.

69
00:04:00,310 --> 00:04:03,980
L'équation ici, je ne vais pas parler
de la dérivation, fournit

70
00:04:03,980 --> 00:04:07,185
une solution analytique
pour la régression linéaire.

71
00:04:07,795 --> 00:04:11,845
Ainsi, si vous ajoutez
les valeurs X et y à cette formule,

72
00:04:11,845 --> 00:04:14,540
vous obtiendrez
les valeurs des pondérations.

73
00:04:14,970 --> 00:04:17,760
Mais, ce n'est pas très pratique,

74
00:04:17,760 --> 00:04:19,980
il y a des problèmes avec l'inverse,

75
00:04:19,980 --> 00:04:25,275
nous supposons d'abord que la matrice
de Gram, X transposition X, est régulière.

76
00:04:25,275 --> 00:04:29,800
Toutes les colonnes de notre matrice X
sont donc linéairement indépendantes.

77
00:04:30,060 --> 00:04:32,634
Dans les ensembles de données
du monde réel, cependant,

78
00:04:32,634 --> 00:04:35,180
il y a des données en double,
ou presque en double.

79
00:04:35,850 --> 00:04:38,275
Rachat du même produit par le même client,

80
00:04:38,275 --> 00:04:41,940
deux photos du même coucher de soleil
à quelques secondes d'intervalle,...

81
00:04:42,070 --> 00:04:45,850
Même si la matrice de Gram est
techniquement linéairement indépendante,

82
00:04:45,850 --> 00:04:48,360
elle peut être mal conditionnée

83
00:04:48,360 --> 00:04:50,950
et être ainsi singulière
sur le plan des calculs,

84
00:04:50,950 --> 00:04:52,885
et nous causer des problèmes.

85
00:04:53,535 --> 00:05:00,890
L'inverse a aussi une complexité en temps
de O(n3) avec l'algorithme naïf,

86
00:05:00,890 --> 00:05:04,315
mais n'est pas meilleur
avec des algorithmes complexes.

87
00:05:04,315 --> 00:05:07,270
Et ces derniers apportent
leur lot de problèmes numériques.

88
00:05:07,270 --> 00:05:10,985
Il en va de même pour la multiplication
permettant de créer la matrice de Gram.

89
00:05:11,085 --> 00:05:13,760
À la place, nous pouvons
résoudre les équations normales

90
00:05:13,760 --> 00:05:16,740
à l'aide d'un Cholesky
ou d'une décomposition QR.

91
00:05:17,800 --> 00:05:24,915
Pour O(n3) ou même O(n2.5),
quand N est égal à 10 000 ou plus,

92
00:05:24,915 --> 00:05:27,450
l'algorithme peut être très lent.

93
00:05:27,630 --> 00:05:32,160
Oui, vous pouvez résoudre le problème
en utilisant l'équation normale,

94
00:05:32,160 --> 00:05:34,165
mais cela dépend
fortement de vos données,

95
00:05:34,165 --> 00:05:39,370
du modèle, de quels algorithmes matriciels
d'algèbre linéaire vous utilisez, etc.

96
00:05:39,640 --> 00:05:43,205
Heureusement, il existe un algorithme
d'optimisation de descente de gradient

97
00:05:43,205 --> 00:05:47,570
qui est moins onéreux en termes
de temps et de mémoire pour les calculs,

98
00:05:47,570 --> 00:05:50,935
plus souple pour la généralisation faible

99
00:05:50,935 --> 00:05:54,400
et assez générique
pour résoudre la plupart des problèmes.

100
00:05:54,540 --> 00:05:58,235
À la place, en descente de gradient,
nous avons notre fonction de perte

101
00:05:58,235 --> 00:06:00,910
ou, plus généralement,
notre fonction d'objectif,

102
00:06:00,910 --> 00:06:03,595
qui est paramétrée
par les pondérations de notre modèle.

103
00:06:03,675 --> 00:06:07,160
Au sein de cet espace,
il y a des collines et des vallées,

104
00:06:07,160 --> 00:06:08,500
tout comme sur la Terre.

105
00:06:08,500 --> 00:06:11,220
Cependant, dans de nombreux
problèmes de machine learning,

106
00:06:11,220 --> 00:06:15,680
il y aura de nombreuses autres dimensions,
dans le monde 3D dans lequel nous vivons.

107
00:06:16,420 --> 00:06:18,480
Puisqu'il s'agit
d'une descente en gradient,

108
00:06:18,480 --> 00:06:21,460
une minimisation avec le gradient,
et pas avec la montée,

109
00:06:21,460 --> 00:06:23,580
qui serait une maximisation,

110
00:06:23,580 --> 00:06:26,525
nous voulons parcourir
la dernière hypersurface,

111
00:06:26,525 --> 00:06:28,305
à la recherche du minimum absolu.

112
00:06:28,545 --> 00:06:32,100
Autrement dit, nous voulons
trouver la vallée la plus basse,

113
00:06:32,100 --> 00:06:34,850
quel que soit le point de départ
sur l'hypersurface.

114
00:06:35,650 --> 00:06:38,795
Pour ce faire, il faut trouver
le gradient de la fonction de perte,

115
00:06:38,795 --> 00:06:42,545
et le multiplier par un hyperparamètre,
le taux d'apprentissage,

116
00:06:42,545 --> 00:06:45,715
puis soustraire cette valeur
aux pondérations actuelles.

117
00:06:46,125 --> 00:06:49,285
Ce processus effectue
des itérations jusqu'à la convergence.

118
00:06:49,475 --> 00:06:52,905
Choisir le taux d'apprentissage optimal
et attendre la fin des itérations

119
00:06:52,905 --> 00:06:55,140
peut vous inciter
à utiliser l'équation normale,

120
00:06:55,140 --> 00:06:57,955
en présupposant que le nombre
de caractéristiques est petit,

121
00:06:57,955 --> 00:07:00,285
qu'il n'y a pas
de problèmes de colinéarité, etc.

122
00:07:00,285 --> 00:07:02,600
Ou ajouter un optimiseur
de descente de gradient,

123
00:07:02,600 --> 00:07:05,840
comme un momentum, ou utiliser
un taux d'apprentissage en diminution.

124
00:07:05,840 --> 00:07:09,800
Nous parlerons en détail de la descente
de gradient au module suivant.

125
00:07:09,800 --> 00:07:14,090
Qu'est-ce qu'un hyperparamètre qui aide
à déterminer le pas d'apprentissage

126
00:07:14,090 --> 00:07:16,705
de descente de gradient
le long de l'hypersurface

127
00:07:16,705 --> 00:07:19,010
pour accélérer la convergence ?

128
00:07:21,640 --> 00:07:24,105
La bonne réponse est
le taux d'apprentissage.

129
00:07:24,105 --> 00:07:27,195
Le taux d'apprentissage
et d'autres hyperparamètres,

130
00:07:27,195 --> 00:07:29,390
que vous découvrirez
dans les modules suivants,

131
00:07:29,390 --> 00:07:32,635
aident à dimensionner le pas
d'apprentissage de descente de gradient.

132
00:07:32,635 --> 00:07:35,730
S'il est trop faible, cette dernière
mettra beaucoup de temps

133
00:07:35,730 --> 00:07:37,160
à atteindre la convergence.

134
00:07:37,160 --> 00:07:41,225
S'il est trop élevé, la descente
de gradient pourrait même diverger,

135
00:07:41,225 --> 00:07:43,380
et augmenter de plus en plus la perte.

136
00:07:43,380 --> 00:07:47,460
Les trois autres réponses concernent
la colinéarité et le conditionnement,

137
00:07:47,460 --> 00:07:50,398
que nous n'avons pas à traiter
avec la descente de gradient,

138
00:07:50,398 --> 00:07:52,388
comme dans une équation normale.