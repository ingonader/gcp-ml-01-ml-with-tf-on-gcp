1
00:00:00,000 --> 00:00:03,370
2000年代になり処理能力が向上すると

2
00:00:03,370 --> 00:00:08,280
機械学習では多数のモデルの性能を
組み合わせるようになりました

3
00:00:08,280 --> 00:00:11,280
これをアンサンブル法といいます

4
00:00:11,280 --> 00:00:16,070
個々の学習器は弱くても
それを多数組み合わせることで

5
00:00:16,070 --> 00:00:19,960
誤差が個々に依存せず強力なMLになります

6
00:00:19,960 --> 00:00:23,470
DNNではドロップアウト層を使って
これを近似化し

7
00:00:23,470 --> 00:00:26,170
モデルを正規化して過剰適合を避けます

8
00:00:26,170 --> 00:00:28,680
たとえば前方パスの確率を使って

9
00:00:28,680 --> 00:00:32,245
いくつかのニューロンを
ランダムにオフにすると

10
00:00:32,245 --> 00:00:35,855
実質的に毎回異なるネットワークができます

11
00:00:36,585 --> 00:00:39,970
複雑な問題を解くには多くの場合

12
00:00:39,970 --> 00:00:44,195
1人の回答よりも何千人もの
回答を集めるべきです

13
00:00:44,195 --> 00:00:47,090
いわゆる群衆の叡智です

14
00:00:47,090 --> 00:00:49,150
機械学習も同じです

15
00:00:49,150 --> 00:00:53,560
分類でも回帰でも
多くの予測変数の結果を集計すると

16
00:00:53,560 --> 00:00:57,850
最適な1つのモデルより
優れていることが多いです

17
00:00:57,850 --> 00:01:00,710
このように組み合わせた予測変数グループは

18
00:01:00,710 --> 00:01:03,370
アンサンブル学習になります

19
00:01:03,370 --> 00:01:07,150
このような学習アルゴリズムを
アンサンブル法といいます

20
00:01:07,150 --> 00:01:11,430
アンサンブル学習の中で
ランダムフォレストが特に有名です

21
00:01:11,430 --> 00:01:15,870
トレーニングセット全体から
1つの決定ツリーを作る代わりに

22
00:01:15,870 --> 00:01:18,070
複数の決定ツリーを集めて

23
00:01:18,070 --> 00:01:21,400
それぞれにランダムなサブサンプルを与えます

24
00:01:21,400 --> 00:01:23,970
トレーニングセット全体は見えないので

25
00:01:23,970 --> 00:01:26,450
全体を記憶することができません

26
00:01:26,450 --> 00:01:29,955
サブセットで
すべてのツリーをトレーニングした後

27
00:01:29,955 --> 00:01:34,350
機械学習の最も重要な部分
つまり予測を実行します

28
00:01:34,350 --> 00:01:37,820
フォレストの各ツリーにテスト標本を送って

29
00:01:37,820 --> 00:01:39,830
その結果を集計します

30
00:01:39,830 --> 00:01:41,500
分類であれば

31
00:01:41,500 --> 00:01:43,785
すべてのツリーによる多数決が

32
00:01:43,785 --> 00:01:46,345
最終的な出力クラスになります

33
00:01:46,345 --> 00:01:49,900
回帰であれば平均値 最大値 中間値などを

34
00:01:49,900 --> 00:01:52,190
集計することができます

35
00:01:52,190 --> 00:01:57,740
より良い一般化のために
例や特徴をランダムサンプル化できます

36
00:01:57,740 --> 00:02:01,010
ランダムサンプリングの例には 置換

37
00:02:01,010 --> 00:02:02,835
バギング（ブートストラップ集計）

38
00:02:02,835 --> 00:02:05,820
置換なし貼り付けがあります

39
00:02:05,820 --> 00:02:08,660
データセット全体ではなく小さなサブセットで

40
00:02:08,660 --> 00:02:12,850
個々の予測変数がトレーニングされ
偏りが大きくなりますが

41
00:02:12,850 --> 00:02:15,975
集計することで偏りと分散が減ります

42
00:02:15,975 --> 00:02:17,637
アンサンブルの偏りは通常

43
00:02:17,637 --> 00:02:21,700
トレーニングセット全体での1つの予測変数
の場合とほぼ同じですが

44
00:02:21,700 --> 00:02:23,525
分散はより小さくなります

45
00:02:23,525 --> 00:02:25,800
一般化誤差を検証する優れた方法が

46
00:02:25,800 --> 00:02:28,830
out-of-bagデータです これを使えば

47
00:02:28,830 --> 00:02:32,760
あらかじめデータセットから
個別のセットを抜き出す必要がありません

48
00:02:33,120 --> 00:02:37,100
ランダムホールドアウトを使った
K分割検証を連想させます

49
00:02:37,100 --> 00:02:41,215
特徴から標本を得て しかも
例をランダムサンプリングすると

50
00:02:41,215 --> 00:02:44,890
ランダムサブ空間ができ
ランダムパッチと呼ばれます

51
00:02:44,890 --> 00:02:48,545
弱い学習器をたくさん集めて
強い学習器を作るときには

52
00:02:48,545 --> 00:02:54,100
適応的ブースティングや
勾配ブースティングなどを使います

53
00:02:54,100 --> 00:02:57,440
通常はそれぞれの学習器を
順番にトレーニングし

54
00:02:57,440 --> 00:03:00,835
1つ前の学習器の問題点を修正しようとします

55
00:03:00,835 --> 00:03:04,870
ブーストされたツリーでは
アンサンブルにツリーが加わるごとに

56
00:03:04,870 --> 00:03:07,405
通常は予測が改善します

57
00:03:07,405 --> 00:03:11,335
でも無制限にツリーを
追加し続けるわけではありません

58
00:03:11,335 --> 00:03:13,930
早期終了のために検証セットを使えば

59
00:03:13,930 --> 00:03:15,770
ツリーが多くなりすぎて

60
00:03:15,770 --> 00:03:19,180
トレーニングデータに
過剰適合するのを避けられます

61
00:03:19,180 --> 00:03:21,930
最後に ニューラルネットワークのときと同様

62
00:03:21,930 --> 00:03:23,335
スタッキングできます

63
00:03:23,335 --> 00:03:26,860
つまりアンサンブルの扱いを
メタ学習器に学習させ

64
00:03:26,860 --> 00:03:30,655
さらに その上に次々と
メタ学習器を積み重ねていきます

65
00:03:30,665 --> 00:03:35,565
DNNでのサブコンポーネントの積み重ねと
再使用については後で学びます

66
00:03:36,465 --> 00:03:39,260
この中で 個々の決定ツリーと比べて

67
00:03:39,260 --> 00:03:43,260
ランダムフォレストに
最も当てはまらないのはどれですか？

68
00:03:45,210 --> 00:03:48,280
正解は

69
00:03:48,280 --> 00:03:51,435
「視覚的に解釈しやすい」です

70
00:03:51,435 --> 00:03:53,220
ニューラルネットワークと同様

71
00:03:53,220 --> 00:03:55,510
モデルに層を追加すればするほど

72
00:03:55,510 --> 00:03:58,230
複雑で理解しにくくなります

73
00:03:58,620 --> 00:04:02,360
ランダムフォレストは通常
個々の決定ツリーより複雑ですから

74
00:04:02,360 --> 00:04:04,450
視覚的に解釈しにくいです

75
00:04:04,450 --> 00:04:06,770
他の3つは正しいです

76
00:04:06,770 --> 00:04:11,400
ランダムフォレストの一般化は
バギングとサブ空間で改善します

77
00:04:11,400 --> 00:04:15,355
また分類や回帰の集計で
投票システムを使うことで

78
00:04:15,355 --> 00:04:19,905
フォレストの性能が個々のツリーより
かなり良くなります

79
00:04:19,905 --> 00:04:23,825
最後にランダムフォレストの
ランダムサンプリングにより

80
00:04:23,825 --> 00:04:26,980
個々のツリーとほぼ同じ偏りになるだけでなく

81
00:04:26,980 --> 00:04:29,160
分散が小さくなります

82
00:04:29,160 --> 00:04:32,560
これも より良い一般化に役立ちます