Nas últimas décadas, já nos anos 2000, a pesquisa do ML adquiriu a capacidade
computacional para unir o desempenho em vários modelos do
que chamamos de método de combinação. Se os erros são independentes para uma
quantidade de aprendizes fracos simples, combinados, eles formam um aprendiz
forte. A DNN usa camadas de dropout para
aproximar esse fato, regularizando o modelo e evitando
o sobreajuste. Isso é simulado ao desligar neurônios
aleatoriamente na rede com alguma probabilidade em cada
transmissão direta, que basicamente criará uma nova rede
por vez. Com frequência, você responde melhor a
perguntas complexas quando são agregadas a partir de milhares de respostas, em vez
daquelas de apenas uma pessoa. Isso é conhecido como a sabedoria das
multidões. Ela é aplicada ao aprendizado de máquina. Agregando resultados de
preditores, classificadores, regressores, o grupo tem um desempenho melhor do que
o melhor modelo individual. Este grupo de preditores é uma combinação,
que quando feita deste jeito, gera aprendizado de combinação. O algoritmo que executa esse aprendizado
é um método de combinação. Um dos tipos mais famosos de aprendizado
de combinação é a floresta aleatória. Em vez de usar o conjunto de treinamento
para criar uma árvore de decisão, é possível ter um grupo delas em que cada recebe uma subamostra
aleatória dos dados de treinamento. Como não passaram por todo o conjunto de
treinamento, eles não terão memorizado tudo. Ao treinar árvores e transformá-las em
subconjuntos de dados, é possível executar a parte mais
importante do ML: previsões! Para isso, você passa a amostra de teste
por cada árvore na floresta e agrega os resultados. Se for classificação, poderá haver o voto de maioria em todas as árvores, sendo a classe de
saída final. Se for regressão, poderá ser a
agregação dos valores como média, máximo, mediana etc. A amostragem aleatória de exemplos e/ou
características melhora a generalização. Esses exemplos com substituição são
chamados de bagging, ou agregação via bootstrap, e pasting, quando não há substituição. Cada preditor individual tem alta
tendência e é treinado no menor subconjunto,
não no conjunto de dados total. Mas a agregação reduz a tendência e 
a variação. Isso proporciona à combinação a mesma tendência de um único preditor no
conjunto de treinamento, mas com variação menor. Um ótimo método de validação para o erro
de generalização é usar os dados out-of-bag, não um conjunto separado
extraído do conjunto antes do treinamento. É reminiscente da validação cruzada usando
holdouts aleatórios. São criados subespaços aleatórios ao fazer
a amostragem das características e, ao fazer a amostragem de exemplos
aleatórios, chamamos de patch aleatório. O aprimoramento adaptativo ou AdaBoost no
tipo gradiente são exemplos de boosting, que é quando agregamos uma quantidade de
aprendizes fracos para criar um forte. Geralmente, isso é feito ao treinar cada
aprendiz sequencialmente para corrigir qualquer
problema que ele já teve antes. Nas árvores aprimoradas, quanto mais
árvores adicionamos à combinação, maior é o aprimoramento das previsões. Então continuamos a adicionar árvores sem 
parar? Claro que não. Use o conjunto de validação para utilizar
interrupção antecipada. Isso evita o sobreajuste dos dados de
treinamento por conta de muitas árvores adicionadas. Por fim, como vimos nas redes neurais, é possível empilhar, treinando meta-aprendizes no que
fazer com imagens de combinação, que por sua vez podem ser empilhadas em
meta-aprendizes e assim por diante. Veremos em breve o empilhamento e reuso de
subcomponentes em redes neurais profundas. Qual das opções a seguir é falsa com
relação a florestas aleatórias, comparando com
árvores de decisão individual? A resposta correta para a questão é que florestas aleatórias são "Mais fáceis
de interpretar visualmente". Igualmente às redes neurais, quanto mais complexidade
você adiciona ao modelo, mais difícil é o entendimento e 
a explicação dele. A floresta aleatória é mais complexa do
que uma árvore de decisão individual, o que dificulta a interpretação visual. As outras três opções são verdadeiras. As florestas aleatórias têm melhor 
generalização via bagging e subespaços. E por usar um sistema de votação em
classificação ou agregação de regressão, a floresta tem desempenho muito melhor do
que a árvore individual. Por fim, devido à amostragem aleatória de
árvores desse tipo, a tendência é similar à da árvore
individual, mas também com menor variação que,
novamente, costuma levar à melhor generalização.