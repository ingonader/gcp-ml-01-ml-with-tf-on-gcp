1
00:00:00,000 --> 00:00:01,830
We looked at the tip status set,

2
00:00:01,830 --> 00:00:04,725
and said that we could use either the tip amount as the label,

3
00:00:04,725 --> 00:00:06,690
or the sex of the customer as a label.

4
00:00:06,690 --> 00:00:11,035
In option one, we are treating the tip amount as the label and want to predict it,

5
00:00:11,035 --> 00:00:13,240
given the other features in the data set.

6
00:00:13,240 --> 00:00:16,330
Let's assume that you are using only one feature,

7
00:00:16,330 --> 00:00:18,955
just the total bill amount to predict the tip.

8
00:00:18,955 --> 00:00:21,255
Because tip is a continuous number,

9
00:00:21,255 --> 00:00:23,175
this is a regression problem.

10
00:00:23,175 --> 00:00:25,640
In regression problems, the goal is to use

11
00:00:25,640 --> 00:00:28,490
mathematical functions of different combinations of features,

12
00:00:28,490 --> 00:00:31,055
to predict the continuous value of our label.

13
00:00:31,055 --> 00:00:33,370
This is shown by the line,

14
00:00:33,370 --> 00:00:36,920
where for a given total bill amount times the slope of the line,

15
00:00:36,920 --> 00:00:39,070
we get a continuous value for tip amount.

16
00:00:39,070 --> 00:00:43,340
Perhaps the average chip rate is 18 percent of the total bill,

17
00:00:43,340 --> 00:00:46,270
then the slope of the line will be zero point one eight.

18
00:00:46,270 --> 00:00:51,410
And by multiplying the bill amount by zero point one eight, we'll get the predicted tip.

19
00:00:51,410 --> 00:00:56,370
This linear progression with only one feature generalizes to additional features.

20
00:00:56,370 --> 00:00:59,570
In that case, we have a multi-dimensional problem,

21
00:00:59,570 --> 00:01:01,270
but the concept is the same.

22
00:01:01,270 --> 00:01:07,040
The value of each feature for each example is multiplied by the gradient of a hyperplane,

23
00:01:07,040 --> 00:01:11,610
which is just the generalization of the line to get a continuous value for the label.

24
00:01:11,610 --> 00:01:14,250
In regression problems, we want to minimize

25
00:01:14,250 --> 00:01:16,670
the error between our predicted continuous value,

26
00:01:16,670 --> 00:01:18,750
and the label's continuous value,

27
00:01:18,750 --> 00:01:21,900
usually using mean squared error.

28
00:01:21,900 --> 00:01:26,170
In option two, we are going to treat sex as our label,

29
00:01:26,170 --> 00:01:30,200
and predict the sex of the customer using data from the tip and total bill.

30
00:01:30,200 --> 00:01:34,325
Of course as you can see from the data, this is a bad idea.

31
00:01:34,325 --> 00:01:37,120
The data for men and women is not really separate,

32
00:01:37,120 --> 00:01:39,990
and we will get a terrible model if we did this.

33
00:01:39,990 --> 00:01:43,170
But, trying to do this helps me illustrate what

34
00:01:43,170 --> 00:01:47,780
happens when the thing you want to predict is categorical, and not continuous.

35
00:01:47,780 --> 00:01:50,040
The values of the sex column takes,

36
00:01:50,040 --> 00:01:51,480
at least in this data set,

37
00:01:51,480 --> 00:01:54,140
are discrete, male or female.

38
00:01:54,140 --> 00:01:55,970
Because sex is categorical,

39
00:01:55,970 --> 00:01:58,930
and we are using the sex column of the data set as our label,

40
00:01:58,930 --> 00:02:01,680
the problem is a classification problem.

41
00:02:01,680 --> 00:02:06,760
In classification problems, instead of trying to predict a continuous variable,

42
00:02:06,760 --> 00:02:11,560
we are trying to create a decision boundary that separates the different classes.

43
00:02:11,560 --> 00:02:16,710
So in this case, there are two classes of sex, female and male.

44
00:02:16,710 --> 00:02:22,030
A linear decision boundary will form a line or a hyperplane in higher dimensions,

45
00:02:22,030 --> 00:02:24,295
with each class on either side.

46
00:02:24,295 --> 00:02:27,190
For example, we might say that if the tip amount is

47
00:02:27,190 --> 00:02:30,735
greater than zero point one eight times the total bill amount,

48
00:02:30,735 --> 00:02:34,475
then we predict that the person making the payment was male.

49
00:02:34,475 --> 00:02:36,620
This is shown by the red line.

50
00:02:36,620 --> 00:02:39,455
But that doesn't work very well for this data set.

51
00:02:39,455 --> 00:02:42,640
Men seem to have higher variability,

52
00:02:42,640 --> 00:02:45,210
while women tend to tip in a more narrow band.

53
00:02:45,210 --> 00:02:48,285
This is an example of a non-linear decision boundary,

54
00:02:48,285 --> 00:02:50,425
shown by the yellow lips in the graph.

55
00:02:50,425 --> 00:02:53,175
How do we know the right decision boundary is bad,

56
00:02:53,175 --> 00:02:55,530
and the yellow decision boundary is better?

57
00:02:55,530 --> 00:02:59,390
In classification problems, we want to minimize the error or

58
00:02:59,390 --> 00:03:03,275
mis-classification between our predicted class and the labels class.

59
00:03:03,275 --> 00:03:06,485
This is done usually using cross entropy.

60
00:03:06,485 --> 00:03:08,860
Even if we are predicting the tip amount,

61
00:03:08,860 --> 00:03:11,305
perhaps we don't need to know the exact tip amount.

62
00:03:11,305 --> 00:03:16,940
Instead, we want to determine whether the tip will be high, average or low.

63
00:03:16,940 --> 00:03:20,770
We could define a high amount as greater than 25 percent,

64
00:03:20,770 --> 00:03:24,055
average tip amount as between 15 and 25 percent,

65
00:03:24,055 --> 00:03:26,890
and a low tip amount is being below 15 percent.

66
00:03:26,890 --> 00:03:30,485
In other words, we could discretize to the amount.

67
00:03:30,485 --> 00:03:33,980
And now, creating the tip amount or more appropriately,

68
00:03:33,980 --> 00:03:37,650
the tip class becomes a classification problem.

69
00:03:37,650 --> 00:03:43,255
In general, a raw continuous feature can be discretize into a categorical feature.

70
00:03:43,255 --> 00:03:45,865
Later in this specialization,

71
00:03:45,865 --> 00:03:48,105
we will talk about the reverse process.

72
00:03:48,105 --> 00:03:52,420
A categorical feature can be embedded into a continuous space.

73
00:03:52,420 --> 00:03:55,110
It really depends on the exact problem you're are trying to solve,

74
00:03:55,110 --> 00:03:56,480
and what works best.

75
00:03:56,480 --> 00:03:59,970
Machine learning is all about experimentation.

76
00:03:59,970 --> 00:04:03,645
Both of these problem types, regression and classification,

77
00:04:03,645 --> 00:04:06,095
can be thought of as prediction problems,

78
00:04:06,095 --> 00:04:11,125
in contrast to unsupervised problems which are like description problems.

79
00:04:11,125 --> 00:04:13,805
Now, where does all this data come from?

80
00:04:13,805 --> 00:04:16,965
This tip data set is what we call structured data,

81
00:04:16,965 --> 00:04:20,120
consisting of rows and columns.

82
00:04:20,120 --> 00:04:24,620
And a very common source of structure data for machine learning is your data warehouse.

83
00:04:24,620 --> 00:04:29,910
Unstructured data are things like pictures, audio, or video.

84
00:04:29,910 --> 00:04:32,775
Here, I'm showing you a natality data set,

85
00:04:32,775 --> 00:04:35,455
a public data set of medical information.

86
00:04:35,455 --> 00:04:38,490
It is a public data set in BigQuery,

87
00:04:38,490 --> 00:04:40,810
and you will use it later in the specialization.

88
00:04:40,810 --> 00:04:44,500
But for now, assume that this data set is in your data warehouse.

89
00:04:44,500 --> 00:04:47,750
Let's say we want to predict the gestation weeks of the baby.

90
00:04:47,750 --> 00:04:51,585
In other words, we want to predict when the baby is going to be born.

91
00:04:51,585 --> 00:04:57,110
You can do a SQL select statement in BigQuery to create an ML data set.

92
00:04:57,110 --> 00:04:59,310
We will choose input features of the model,

93
00:04:59,310 --> 00:05:00,820
things like mother's age,

94
00:05:00,820 --> 00:05:02,310
the weight gain in pounds,

95
00:05:02,310 --> 00:05:04,695
and the label, gestation weeks.

96
00:05:04,695 --> 00:05:08,040
Because gestation weeks is a continuous number,

97
00:05:08,040 --> 00:05:10,760
this is a regression problem.

98
00:05:10,760 --> 00:05:14,555
Making predictions from structured data is a very common place,

99
00:05:14,555 --> 00:05:18,370
and that is what we focused on on the first part of this specialization.

100
00:05:18,370 --> 00:05:23,125
Of course, this medical data set can be used to predict other things too.

101
00:05:23,125 --> 00:05:28,190
Perhaps, we want to predict baby weight using the other attributes as our features.

102
00:05:28,190 --> 00:05:30,990
Baby weight can be an indicator of health.

103
00:05:30,990 --> 00:05:33,815
When a baby is predicted to have a low birth weight,

104
00:05:33,815 --> 00:05:37,565
the hospital will usually have equipment such as an incubator handy,

105
00:05:37,565 --> 00:05:40,795
so it can be important to be able to predict the baby's weight.

106
00:05:40,795 --> 00:05:43,050
The label here will be baby weight,

107
00:05:43,050 --> 00:05:45,395
and it's a continuous variable.

108
00:05:45,395 --> 00:05:50,535
It's stored as a floating point number which should make this a regression problem.

109
00:05:50,535 --> 00:05:52,380
Is this data set,

110
00:05:52,380 --> 00:05:54,265
a good candidate for linear regression,

111
00:05:54,265 --> 00:05:57,740
and or linear classification?

112
00:05:57,740 --> 00:06:01,625
The correct answer is both.

113
00:06:01,625 --> 00:06:07,460
Let's investigate why. Let's step back and look at the data set with both classes mixed.

114
00:06:07,460 --> 00:06:10,210
Without the different colors and shapes to aid us,

115
00:06:10,210 --> 00:06:15,900
the data appears to be one noisy line with a negative slope and positive intercept.

116
00:06:15,900 --> 00:06:18,325
Since it appears quite linear,

117
00:06:18,325 --> 00:06:22,575
this will probably most likely be a good candidate for linear regression,

118
00:06:22,575 --> 00:06:26,675
where what we are trying to predict is the value for Y.

119
00:06:26,675 --> 00:06:30,300
Add even different colors and shapes back in,

120
00:06:30,300 --> 00:06:33,040
it is much more evident that this data set is actually

121
00:06:33,040 --> 00:06:36,490
two linear series with some Gaussian noise added.

122
00:06:36,490 --> 00:06:39,795
The lines have slightly different slopes and different intercepts,

123
00:06:39,795 --> 00:06:42,500
and the noise has different standard deviations.

124
00:06:42,500 --> 00:06:45,890
I've planned the lines here to show you that this is most

125
00:06:45,890 --> 00:06:50,370
definitely a linear data set by design. I'll be a little noisy.

126
00:06:50,370 --> 00:06:53,565
This would be a good candidate for linear regression.

127
00:06:53,565 --> 00:06:57,000
Despite there being two distinct linear series,

128
00:06:57,000 --> 00:07:00,350
let's first look at the result of a one dimensional linear regression,

129
00:07:00,350 --> 00:07:02,320
plotting Y from X,

130
00:07:02,320 --> 00:07:04,200
to start building an intuition,

131
00:07:04,200 --> 00:07:07,325
then we'll see if we can do better.

132
00:07:07,325 --> 00:07:12,605
The green line here is the fitted linear equation from linear regression.

133
00:07:12,605 --> 00:07:16,555
Notice that it is far away from each individual class distribution,

134
00:07:16,555 --> 00:07:21,575
because class B pulls the line away from class A, and vice versa.

135
00:07:21,575 --> 00:07:25,970
It ends up approximately bisecting the space between the two distributions.

136
00:07:25,970 --> 00:07:28,390
This makes sense since with regression,

137
00:07:28,390 --> 00:07:30,875
we optimize our loss of mean squared error.

138
00:07:30,875 --> 00:07:33,320
So with an equal pull from each class,

139
00:07:33,320 --> 00:07:37,485
the regression should have the lowest mean squared error in between the two classes,

140
00:07:37,485 --> 00:07:40,460
approximately equidistant from their means.

141
00:07:40,460 --> 00:07:44,965
Since each class is a different linear series with different slopes and intercepts,

142
00:07:44,965 --> 00:07:47,840
we would actually have a much better accuracy

143
00:07:47,840 --> 00:07:50,730
by performing a linear regression for each class,

144
00:07:50,730 --> 00:07:54,480
which should fit very closely to each of the lines plotted here.

145
00:07:54,480 --> 00:07:57,010
Even better, instead of performing

146
00:07:57,010 --> 00:08:01,770
a one dimensional linear regression predicting the value of Y from one feature X,

147
00:08:01,770 --> 00:08:07,435
we could perform a two dimensional linear regression predicting Y from two features,

148
00:08:07,435 --> 00:08:10,000
X and the class of the point.

149
00:08:10,000 --> 00:08:13,710
The class feature could be a one if the point belongs to class A,

150
00:08:13,710 --> 00:08:16,545
and a zero if the point belongs to class B.

151
00:08:16,545 --> 00:08:21,030
Instead of a line, it would for me 2D hyper plane.

152
00:08:21,030 --> 00:08:23,535
Let's see how that would look.

153
00:08:23,535 --> 00:08:27,165
Here are the results of the 2D linear regression.

154
00:08:27,165 --> 00:08:32,685
To predict our label Y, we used two features, X and class.

155
00:08:32,685 --> 00:08:36,040
As you can see, a 2D hyper plane has been formed between

156
00:08:36,040 --> 00:08:39,840
the two sets of data which are now separated by the class dimension.

157
00:08:39,840 --> 00:08:44,830
I've also included the true lines for both class A and class B,

158
00:08:44,830 --> 00:08:48,670
as well as the 1D linear regression's line of best fit.

159
00:08:48,670 --> 00:08:52,545
The plane doesn't completely contain any of the lines,

160
00:08:52,545 --> 00:08:55,870
due to the noises of the data tilting the two slopes of the plane.

161
00:08:55,870 --> 00:08:58,275
Otherwise, with no noise,

162
00:08:58,275 --> 00:09:01,390
all three lines would be perfectly on the plane.

163
00:09:01,390 --> 00:09:04,590
Also, we have already answered

164
00:09:04,590 --> 00:09:07,595
the other portion of the quiz question about linear classification.

165
00:09:07,595 --> 00:09:09,650
Because the linear regression line does

166
00:09:09,650 --> 00:09:12,960
a really great job already of separating the classes.

167
00:09:12,960 --> 00:09:17,005
This is a very good candidate for linear classification as well.

168
00:09:17,005 --> 00:09:20,600
But, would it produce a decision boundary exactly

169
00:09:20,600 --> 00:09:24,145
on the 1D linear regression's line of best fit? Let's find out.

170
00:09:24,145 --> 00:09:27,010
Plotted in yellow is the output of a one

171
00:09:27,010 --> 00:09:30,180
dimensional linear classifier, logistic regression.

172
00:09:30,180 --> 00:09:34,290
Notice, that it is very close to linear regression's green line,

173
00:09:34,290 --> 00:09:37,740
but not exactly. Why could this be?

174
00:09:37,740 --> 00:09:40,550
Remember, I mentioned that regression models

175
00:09:40,550 --> 00:09:43,820
usually use mean squared error as their loss function,

176
00:09:43,820 --> 00:09:47,230
whereas classification models tend to use cross entropy.

177
00:09:47,230 --> 00:09:49,545
So, what is the difference between the two?

178
00:09:49,545 --> 00:09:53,005
Without going into too much of the details just yet,

179
00:09:53,005 --> 00:09:55,680
there is a quadratic penalty for mean squared error,

180
00:09:55,680 --> 00:09:57,420
so it is essentially trying to minimize

181
00:09:57,420 --> 00:10:01,115
the euclidean distance between the actual label and the predicted label.

182
00:10:01,115 --> 00:10:04,845
On the other hand, with classifications cross entropy,

183
00:10:04,845 --> 00:10:09,880
the penalty is almost linear and the predicted probability is close to the actual label,

184
00:10:09,880 --> 00:10:13,375
but as it gets further away it becomes exponential,

185
00:10:13,375 --> 00:10:16,560
when it gets close to the predicting the opposite class of the label.

186
00:10:16,560 --> 00:10:19,320
Therefore, if you look closely at the plot,

187
00:10:19,320 --> 00:10:20,700
the most likely reason

188
00:10:20,700 --> 00:10:25,055
the classification decision boundary line has a slightly more negative slope,

189
00:10:25,055 --> 00:10:28,000
is so that some of those noisy red points,

190
00:10:28,000 --> 00:10:29,945
red being the noisy distribution,

191
00:10:29,945 --> 00:10:35,705
fall on the other side of the decision boundary and lose their high error contribution.

192
00:10:35,705 --> 00:10:38,005
Since they are so close to the line,

193
00:10:38,005 --> 00:10:41,615
their error contribution would be small for linear regression,

194
00:10:41,615 --> 00:10:45,150
because not only is the error quadratic,

195
00:10:45,150 --> 00:10:50,080
but there is no preference to be on one side of the line or the other for regression,

196
00:10:50,080 --> 00:10:53,015
as long as the distance stays small as possible.

197
00:10:53,015 --> 00:10:54,890
So, as you can see,

198
00:10:54,890 --> 00:10:59,970
this data set is a great fit for both linear regression and linear classification.

199
00:10:59,970 --> 00:11:02,340
Unlike, when we looked at the tips data set,

200
00:11:02,340 --> 00:11:04,680
where it was only acceptable for linear regression,

201
00:11:04,680 --> 00:11:08,000
and we better for a non-linear classification.