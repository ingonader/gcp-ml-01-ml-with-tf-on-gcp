チップの例では
ラベルをチップ金額にすることも 客の性別にすることもできます オプション1ではチップ金額をラベルにして データセットの他の特徴に基づき チップを予測してみます 1つの特徴だけを使うとしましょう たとえば請求金額だけからチップを予測します チップは連続的な数値ですから 回帰問題になります 回帰問題の目標は さまざまな特徴を組み合わせた数学関数を使用し ラベルの連続値を予測することです それが この直線に示されています 請求金額の合計に直線の傾きをかけると チップの連続値を予測できます おそらく チップの平均は請求額の18%でしょう すると 直線の傾きは0.18になります 請求額に0.18を掛けると
チップの予測値が得られます この線形回帰の「特徴」は 1つだけですが
特徴を追加して汎化します その場合 多次元問題になります でも概念は同じです それぞれ 1つの例の 1つの特徴の値に
超平面の勾配を掛けます こうして汎化された線形で
ラベルの連続値が得られます 回帰問題では 予測される連続値とラベルの連続値の間の 誤差を最小化するために 平均二乗誤差を使います オプション2では性別をラベルにして 請求額とチップ額から
お客さんの性別を予測します データを見れば明らかですが
もちろん この方法は良くありません 男性と女性のデータにあまり差がないからです このモデルではひどい結果が出るでしょう でも ここで皆さんに説明したい点は 連続値ではなく カテゴリ値を
予測するとどうなるか です 「性別」列の値は 少なくとも このデータセットでは 離散値「男性」「 女性」です 性別はカテゴリです このデータセットの性別列を
ラベルとして使うので これは分類問題になります 分類問題では連続値を予測する代わりに 異なるクラスを分ける
決定境界を作成しようと試みます この場合は2つの性別クラス
「女性」と「男性」があります 線形の決定境界では
直線または多次元の超平面を形成して 各クラスがどちらかの側に入ります たとえばチップ金額が 請求金額の0.18倍より大きいならば 支払った人は男性だと予測します この赤い線です でも このデータセットは不十分です 男性の方が「ばらつき」が
大きいように見えます 女性の方がばらつきが少ないです これは非線形決定境界の例です 黄色い唇のような曲線です 赤い境界よりも黄色い境界が良いと なぜ わかるのでしょうか 分類問題では「誤差」つまり 予測されるクラスと ラベルクラスの間の
誤分類を最小化します それには交差エントロピーを使います ところで チップを予測するとき 正確な金額を予測しなくてよいかもしれません チップが多いか 普通か 少ないかが
分かればよいのです たとえば 請求額の25%を超えたら
「多い」と定義できます 15～25%なら「平均的」 15%未満なら「少ない」です 言い換えると 金額を離散化できます こうしてチップ金額の予測 正確には チップクラスの予測は 分類問題になります 通常は 生の連続的「特徴」を
カテゴリ特徴に離散化できます この専門分野の後半では 逆のプロセスについてお話しします カテゴリ特徴を連続空間に
埋め込むことができるのです どちらを選ぶかは 問題の性質と得られる結果で決まります 機械学習では実験が欠かせません 回帰と分類のどちらのタイプの問題も 予測問題だと考えることができます 対照的に 教師なし問題は
記述問題のようなものです さて このデータの出所は？ このチップデータセットは 行と列からなる構造化データです ML用の構造化データは 多くの場合
データウェアハウスから得られます 画像 音声 ビデオなどは非構造化データです これは出産データを示しています 医療情報の公開データセットです この一般公開データはBigQueryにあり この専門分野でも後で使用しますが 今は これが皆さんの
データウェアハウスに入っているとします 妊娠期間（週）を予測しましょう つまり 赤ちゃんが
いつ生まれるかを予測するのです BigQueryで SQL SELECT文を使って
MLデータセットが得られます モデルの入力となる特徴を選びます たとえば 母親の年齢 体重の増加です 妊娠期間（週）がラベルです 妊娠期間は連続する数値なので これは回帰問題になります 構造化データから予測する手法は
よく使われます この専門分野の最初の部分でも
これに焦点を当てます もちろん この医療データセットを使って
他の事柄も予測できます たとえば 他の属性を「特徴」として使って
赤ちゃんの体重を予測できるでしょう 体重は健康状態の指標の1つです 赤ちゃんの体重が少ないと予測されるなら 病院は通常保育器などの機器を用意します ですから体重を予測できることは重要です ここでは赤ちゃんの体重がラベルです これは連続値で
浮動小数点値として格納されますから これは回帰問題になります さて このデータセットに使用できるのは 線形回帰 線形分類
または両方のどれでしょうか 正解は「両方」です なぜでしょうか データセットの色と直線を取り除くと 2つのクラスが混ざり このように多数のノイズがある
1本の直線に見えます 負の傾き 正の切片です 直線のように見えますから たぶん線形回帰が適しているでしょう Yの値を予測するのです 色と線を戻しました このデータセットが実際には
2本の直線であることが分かります ガウスノイズが少し含まれます 2つの線の傾きと切片は少し異なり
ノイズの標準偏差も異なります 私が皆さんに直線を見せている理由は ノイズ付き線形データセットとして
設計したと分かってもらうためです これは線形回帰のよい候補です 2本の直線に分かれていますが まず一次元の線形回帰の結果を見てみましょう XからYをプロットして 直感的な線を最初に作り その後で改善方法を見ていきます 緑の線は線形回帰から得られる
近似一次方程式です どちらのクラス分布からも遠く離れていますね クラスBとクラスAが
互いに直線を引っ張り合うからです 2つのクラス分布の間の空間を
切り離したような感じですね これは妥当な結果です なぜなら回帰では 平均二乗誤差の損失を最適にします 各クラスが均等に引っ張ると 2つのクラス間の平均二乗誤差が最小になり それぞれの平均値からほぼ等距離になります しかし 各クラスは別々の線形分布です
傾きも切片も異なりますから クラスごとに線形回帰すれば もっと正確な予測に改善できるでしょう ここに描いた それぞれの線に
かなり近づくでしょう 1つの特徴XからYの値を予測する 一次元の線形回帰よりもっと優れた方法として 二次元の線形回帰が可能です 2つの特徴「X」と「その点のクラス」から
Yを予測するのです その点がクラスAに属するなら
クラス特徴値は「1」 クラスBに属するなら「0」です 一本の直線ではなく二次元の超平面ができます どうなるか見てみましょう 2D線形回帰の結果です ラベルYを予測するために
Xとクラスの2つの特徴を使います このように2D超平面ができて 2つのデータセットが
クラス次元ごとに分かれます クラスAとBの本当の線も含めました 1D線形回帰の近似線もです 平面にはどの直線も完全には含まれません ノイズが原因で平面の2つの
傾きが歪んでいるためです もしノイズがなければ 3本の線は完全に平面に含まれるでしょう さらに 先程のクイズの もう1つの答えも出たことになります
「分類」です この線形回帰線が クラスを分けてくれたからです ですから 線形分類としても良い候補です でも1D線形回帰の近似線は
本当に正確な決定境界ですか？ 確かめましょう 黄色い線は一次元線形分類器 つまりロジスティック回帰の出力です 緑の線形回帰線に非常に近いですが そうではありません なぜでしょう 覚えていますか 回帰モデルでは損失関数として
平均二乗誤差を使います 分類モデルでは交差エントロピーを使います この2つの違いは？ 複雑な説明を避けると 平均二乗誤差には二次ペナルティがあります つまり 実際のラベルと予測ラベルの間の ユークリッド距離を最小化するのです 一方 分類の交差エントロピーでは 予測確率が実際のラベルに近いときは
ペナルティがほぼ線形です しかし遠くなると指数的に増えます そして 逆のクラスの予測に近づきます そこで この図をよく見ると 分類決定境界線の方が傾きが少し急です その理由は 赤い点の中にノイズが大きいものがあり 赤の分布ノイズが大きく 決定境界の向こう側に入り
高い誤差寄与の損失になるのです これらの点は直線にかなり近いので 線形回帰では誤差寄与が小さいでしょう なぜなら 誤差が二次関数であるだけでなく 回帰線のどちら側に点があっても
かまわないからです 距離を最小化できればいいのです ですから このデータセットは 線形回帰と線形分類の両方に適しています 一方 先程のチップ金額データセットは 線形回帰だけに適しており 分類は非線形にした方がよいでしょう