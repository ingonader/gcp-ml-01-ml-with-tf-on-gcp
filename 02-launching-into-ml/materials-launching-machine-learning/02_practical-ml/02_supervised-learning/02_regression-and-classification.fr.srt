1
00:00:00,000 --> 00:00:02,100
Nous avons examiné
l'ensemble des pourboires,

2
00:00:02,100 --> 00:00:04,765
et dit que nous pouvions
utiliser le montant du pourboire

3
00:00:04,765 --> 00:00:06,529
ou le sexe du client comme étiquette.

4
00:00:06,929 --> 00:00:09,655
Dans l'Option 1,
le montant est l'étiquette,

5
00:00:09,655 --> 00:00:11,110
et nous voulons le prédire

6
00:00:11,110 --> 00:00:13,820
en fonction des caractéristiques
de l'ensemble de données.

7
00:00:13,820 --> 00:00:16,579
Imaginons que vous n'utilisez
qu'une seule caractéristique,

8
00:00:16,579 --> 00:00:18,955
le montant de l'addition
pour prédire le pourboire.

9
00:00:19,155 --> 00:00:21,165
Puisque le pourboire
est un nombre continu,

10
00:00:21,175 --> 00:00:22,985
il s'agit d'un problème de régression.

11
00:00:23,365 --> 00:00:26,270
Dans ce type de problèmes,
le but est d'utiliser des fonctions

12
00:00:26,270 --> 00:00:28,520
de différentes
combinaisons de caractéristiques,

13
00:00:28,520 --> 00:00:30,945
pour prédire
la valeur continue de notre étiquette.

14
00:00:31,225 --> 00:00:33,300
C'est illustré par la droite,

15
00:00:33,300 --> 00:00:36,570
où, pour le total de l'addition
multiplié par la pente de la droite,

16
00:00:36,570 --> 00:00:39,520
nous obtenons une valeur continue
pour le montant du pourboire.

17
00:00:39,520 --> 00:00:43,190
Si le pourboire moyen
est 18 % du total de l'addition,

18
00:00:43,190 --> 00:00:46,140
la pente de la droite est alors de 0,18.

19
00:00:46,870 --> 00:00:51,340
Et en multipliant l'addition par 0,18,
nous obtenons le pourboire prédit.

20
00:00:51,790 --> 00:00:54,460
Cette progression linéaire
avec une seule caractéristique

21
00:00:54,460 --> 00:00:56,550
est généralisée
à d'autres caractéristiques.

22
00:00:56,550 --> 00:00:59,660
Dans ce cas,
le problème a plusieurs dimensions,

23
00:00:59,660 --> 00:01:01,090
mais le concept est identique.

24
00:01:01,700 --> 00:01:06,985
La valeur de chaque caractéristique est
multipliée par le gradient d'un hyperplan,

25
00:01:06,985 --> 00:01:09,320
qui est simplement
la généralisation de la droite

26
00:01:09,320 --> 00:01:11,650
pour obtenir
une valeur continue pour l'étiquette.

27
00:01:11,890 --> 00:01:14,840
Dans les problèmes de régression,
nous voulons réduire l'erreur

28
00:01:14,840 --> 00:01:18,620
entre la valeur continue prédite
et la valeur continue de l'étiquette,

29
00:01:18,630 --> 00:01:21,110
en général en utilisant
l'erreur quadratique moyenne.

30
00:01:23,200 --> 00:01:27,930
Dans l'Option 2, le sexe est l'étiquette,
et nous prédisons le sexe du client

31
00:01:27,930 --> 00:01:30,440
grâce aux données
du pourboire et de l'addition.

32
00:01:30,440 --> 00:01:34,490
Il s'agit bien sûr d'une mauvaise idée
comme vous le voyez dans les données.

33
00:01:34,490 --> 00:01:37,730
Les données des hommes et des femmes
ne sont pas vraiment distinctes,

34
00:01:37,730 --> 00:01:40,255
et nous obtiendrons
un mauvais modèle en faisant cela.

35
00:01:40,555 --> 00:01:43,920
Cela va cependant me permettre
d'illustrer ce qu'il se passe

36
00:01:43,920 --> 00:01:47,480
lorsque ce que vous voulez prédire
est catégorique, et pas continu.

37
00:01:48,080 --> 00:01:51,630
Les valeurs de la colonne "sex",
au moins dans cet ensemble de données,

38
00:01:51,630 --> 00:01:54,050
sont discrètes : homme ou femme.

39
00:01:54,460 --> 00:01:55,990
Puisque le sexe est catégorique,

40
00:01:55,990 --> 00:01:58,890
et que nous utilisons
la colonne "sex" comme notre étiquette,

41
00:01:58,890 --> 00:02:01,380
le problème est
un problème de classification.

42
00:02:02,080 --> 00:02:06,720
Dans les problèmes de classification,
au lieu de prédire une variable continue,

43
00:02:06,720 --> 00:02:11,180
nous créons une frontière de décision
qui sépare les différentes classes.

44
00:02:11,760 --> 00:02:16,350
Dans ce cas, il existe deux classes
de sexe : femme et homme.

45
00:02:17,190 --> 00:02:20,770
Une frontière de décision linéaire
forme une ligne ou un hyperplan

46
00:02:20,770 --> 00:02:24,460
pour les dimensions supérieures,
avec une classe de chaque côté.

47
00:02:24,460 --> 00:02:27,880
Par exemple, nous pouvons prédire
que si le pourboire est plus élevé

48
00:02:27,880 --> 00:02:30,700
que 0,18 fois le total de l'addition,

49
00:02:30,700 --> 00:02:34,125
la personne qui a payé est un homme.

50
00:02:34,965 --> 00:02:36,680
C'est illustré par la droite rouge.

51
00:02:36,680 --> 00:02:39,845
Mais cela ne fonctionne pas très bien
pour cet ensemble de données.

52
00:02:39,845 --> 00:02:42,515
La variabilité semble
plus grande pour les hommes,

53
00:02:42,515 --> 00:02:45,560
alors que le pourboire féminin
reste dans une bande plus étroite.

54
00:02:45,560 --> 00:02:48,355
C'est un exemple de frontière
de décision non linéaire,

55
00:02:48,355 --> 00:02:50,780
illustrée par les lèvres jaunes
sur notre graphique.

56
00:02:50,780 --> 00:02:53,350
Comment déterminer
que la frontière rouge est mauvaise,

57
00:02:53,350 --> 00:02:55,345
et que la jaune est meilleure ?

58
00:02:55,935 --> 00:02:59,215
Dans les problèmes de classification,
nous voulons réduire les erreurs

59
00:02:59,215 --> 00:03:03,095
de classification entre la classe
prédite et celle des étiquettes.

60
00:03:03,225 --> 00:03:06,260
Pour cela, on utilise
en général l'entropie croisée.

61
00:03:07,060 --> 00:03:08,780
Même si nous prédisons le pourboire,

62
00:03:08,780 --> 00:03:11,775
nous n'avons peut-être pas besoin
de connaître le montant exact.

63
00:03:11,775 --> 00:03:16,755
À la place, nous voulons déterminer
s'il sera élevé, moyen ou faible.

64
00:03:17,235 --> 00:03:20,540
Un pourboire élevé
serait supérieur à 25 %,

65
00:03:20,540 --> 00:03:24,065
un pourboire moyen
entre 15 et 25 %,

66
00:03:24,065 --> 00:03:26,780
et un pourboire faible en dessous de 15 %.

67
00:03:26,890 --> 00:03:30,180
Autrement dit, nous pourrions
discrétiser jusqu'au montant.

68
00:03:30,490 --> 00:03:32,935
Créer le montant du pourboire

69
00:03:32,935 --> 00:03:37,340
ou, plutôt, la classe du pourboire,
devient un problème de classification.

70
00:03:37,920 --> 00:03:40,375
Une caractéristique continue brute peut,

71
00:03:40,375 --> 00:03:43,380
en général, être discrétisée
en une caractéristique catégorique.

72
00:03:43,380 --> 00:03:48,130
Nous parlerons ultérieurement
du processus inverse.

73
00:03:48,130 --> 00:03:52,125
Une caractéristique catégorique
peut être intégrée à un espace continu.

74
00:03:52,415 --> 00:03:55,105
Cela dépend du problème
que vous voulez résoudre,

75
00:03:55,105 --> 00:03:56,635
et de ce qui est le plus adapté.

76
00:03:56,635 --> 00:03:59,560
En ML, tout est affaire d'expérimentation.

77
00:04:00,490 --> 00:04:03,500
Ces deux types de problèmes,
la régression et la classification,

78
00:04:03,500 --> 00:04:06,210
peuvent être considérés
comme des problèmes de prédiction,

79
00:04:06,210 --> 00:04:10,700
contrairement aux problèmes non supervisés
qui sont des problèmes de description.

80
00:04:11,120 --> 00:04:13,685
Mais d'où viennent toutes ces données ?

81
00:04:14,175 --> 00:04:17,105
Ces données sur les pourboires
sont des données structurées

82
00:04:17,105 --> 00:04:19,915
qui sont rangées en lignes et colonnes.

83
00:04:20,115 --> 00:04:24,195
Très souvent, les données structurées
proviennent d'un entrepôt de données.

84
00:04:24,615 --> 00:04:29,664
Les données non structurées sont
des images, des fichiers audio ou vidéo.

85
00:04:29,904 --> 00:04:32,540
Voici un ensemble
de données sur la natalité,

86
00:04:32,540 --> 00:04:35,830
un ensemble de données publiques
contenant des informations médicales.

87
00:04:35,830 --> 00:04:38,600
Il s'agit d'un ensemble
de données publiques dans BigQuery,

88
00:04:38,600 --> 00:04:40,675
et vous l'utiliserez ultérieurement.

89
00:04:40,805 --> 00:04:44,495
Pour le moment, partons du principe
que cet ensemble est dans votre entrepôt.

90
00:04:44,495 --> 00:04:47,750
Nous voulons prédire
les semaines de gestation du bébé.

91
00:04:47,750 --> 00:04:51,310
Autrement dit, nous voulons
prédire sa date de naissance.

92
00:04:51,590 --> 00:04:55,200
Vous pouvez effectuer une instruction
de sélection SQL dans BigQuery

93
00:04:55,200 --> 00:04:57,110
pour créer un ensemble de données de ML.

94
00:04:57,110 --> 00:04:59,780
Nous choisissons
les caractéristiques d'entrée du modèle,

95
00:04:59,780 --> 00:05:02,505
comme l'âge de la mère,
la prise de poids,

96
00:05:02,505 --> 00:05:05,170
et l'étiquette : semaines de gestation.

97
00:05:05,170 --> 00:05:07,870
Puisque les semaines de gestation
sont un nombre continu,

98
00:05:07,870 --> 00:05:10,550
il s'agit d'un problème de régression.

99
00:05:10,760 --> 00:05:14,320
Effectuer des prédictions à partir de
données structurées est très courant,

100
00:05:14,320 --> 00:05:18,175
et c'est ce que nous avons fait
dans la première partie.

101
00:05:18,365 --> 00:05:23,450
Cet ensemble de données médicales peut
être utilisé pour d'autres prédictions,

102
00:05:23,450 --> 00:05:25,920
comme prédire le poids du bébé

103
00:05:25,920 --> 00:05:28,590
en utilisant les autres attributs
comme caractéristiques.

104
00:05:28,590 --> 00:05:30,975
Le poids du bébé peut
indiquer son état de santé.

105
00:05:30,975 --> 00:05:34,070
Lorsqu'il est prédit qu'un bébé
aura un poids de naissance faible,

106
00:05:34,070 --> 00:05:37,555
l'hôpital prépare en général une couveuse.

107
00:05:37,555 --> 00:05:40,800
Il peut donc être important
de prédire le poids d'un bébé.

108
00:05:40,800 --> 00:05:45,250
L'étiquette est le poids du bébé,
et il s'agit d'une variable continue.

109
00:05:45,400 --> 00:05:48,015
Elle est stockée
en tant que nombre à virgule flottante,

110
00:05:48,015 --> 00:05:50,185
ce qui est en fait
un problème de régression.

111
00:05:51,005 --> 00:05:53,975
Cet ensemble de données est-il
adapté à la régression linéaire

112
00:05:53,975 --> 00:05:56,660
ou à la classification linéaire ?

113
00:05:59,000 --> 00:06:01,575
La réponse est : les deux.

114
00:06:01,625 --> 00:06:02,955
Voyons pourquoi.

115
00:06:03,695 --> 00:06:07,210
Observons l'ensemble de données
avec les deux classes mélangées.

116
00:06:07,800 --> 00:06:12,765
Sans couleurs ni formes pour nous aider,
les données sont une ligne brouillonne

117
00:06:12,765 --> 00:06:15,900
avec une pente négative
et un point d'intersection positif.

118
00:06:16,570 --> 00:06:18,475
Puisque la droite semble assez linéaire,

119
00:06:18,475 --> 00:06:22,750
la régression linéaire sera
sans doute la solution la plus adaptée

120
00:06:22,750 --> 00:06:25,860
pour prédire la valeur de Y.

121
00:06:27,710 --> 00:06:31,800
Si l'on ajoute des couleurs et des formes,
il est alors bien plus évident

122
00:06:31,800 --> 00:06:34,755
que cet ensemble de données
se compose de deux séries linéaires

123
00:06:34,755 --> 00:06:36,425
avec du bruit gaussien.

124
00:06:36,435 --> 00:06:39,915
Les droites ont des pentes et points
d'intersection légèrement différents,

125
00:06:39,915 --> 00:06:42,260
et le bruit présente
différents écarts types.

126
00:06:42,920 --> 00:06:45,020
J'ai utilisé des lignes ici
pour vous montrer

127
00:06:45,020 --> 00:06:49,870
qu'il s'agit d'un ensemble linéaire.
Je vais faire un peu de bruit.

128
00:06:50,840 --> 00:06:53,445
Il s'agit d'un bon exemple
pour la régression linéaire.

129
00:06:54,335 --> 00:06:56,700
Bien qu'il y ait
deux séries linéaires distinctes,

130
00:06:56,700 --> 00:07:00,460
penchons-nous d'abord sur le résultat
d'une régression linéaire dimensionnelle,

131
00:07:00,460 --> 00:07:02,460
en traçant point par point Y à partir de X

132
00:07:02,460 --> 00:07:04,525
pour commencer à construire une intuition.

133
00:07:04,525 --> 00:07:06,810
Nous verrons ensuite
si nous pouvons faire mieux.

134
00:07:08,620 --> 00:07:12,570
La droite verte est l'équation linéaire
ajustée de la régression linéaire.

135
00:07:12,950 --> 00:07:16,500
Remarquez qu'elle est éloignée
de chaque distribution de classe,

136
00:07:16,500 --> 00:07:21,370
car la classe B éloigne la droite
de la classe A, et vice versa.

137
00:07:22,080 --> 00:07:25,625
Elle bissecte approximativement
l'espace entre les deux distributions.

138
00:07:26,285 --> 00:07:28,385
Cela paraît sensé,
car, avec la régression,

139
00:07:28,385 --> 00:07:31,055
nous optimisons la perte
d'erreur quadratique moyenne.

140
00:07:31,055 --> 00:07:34,385
Avec un éloignement égal de chaque classe,
la régression devrait avoir

141
00:07:34,385 --> 00:07:37,570
l'erreur quadratique moyenne
la plus faible entre les deux classes,

142
00:07:37,570 --> 00:07:40,380
à peu près équidistante de leurs moyennes.

143
00:07:40,670 --> 00:07:43,245
Puisque chaque classe
est une série linéaire différente

144
00:07:43,245 --> 00:07:45,675
avec des pentes
et points d'intersection différents,

145
00:07:45,685 --> 00:07:47,830
nous obtiendrions
une précision bien meilleure

146
00:07:47,830 --> 00:07:50,535
en réalisant une régression linéaire
pour chaque classe

147
00:07:50,535 --> 00:07:54,330
qui devrait se trouver très près
de chacune des droites tracées ici.

148
00:07:54,830 --> 00:07:58,765
Mieux encore, au lieu d'effectuer
une régression linéaire unidimensionnelle,

149
00:07:58,765 --> 00:08:01,840
en prédisant la valeur de Y
à partir d'une caractéristique X,

150
00:08:01,840 --> 00:08:04,890
nous pourrions effectuer
une régression linéaire bidimensionnelle

151
00:08:04,890 --> 00:08:07,380
en prédisant Y
à partir de deux caractéristiques :

152
00:08:07,380 --> 00:08:09,980
X et la classe du point.

153
00:08:10,280 --> 00:08:14,050
La caractéristique de classe peut être
"1" si le point appartient à la classe A,

154
00:08:14,050 --> 00:08:16,455
et "0"
si le point appartient à la classe B.

155
00:08:16,895 --> 00:08:20,700
Au lieu d'une droite,
ce serait un hyperplan 2D.

156
00:08:21,430 --> 00:08:23,000
Voyons à quoi cela ressemblerait.

157
00:08:24,450 --> 00:08:27,175
Voici les résultats
d'une régression linéaire en 2D.

158
00:08:27,345 --> 00:08:30,820
Pour prédire notre étiquette Y,
nous avons utilisé deux caractéristiques :

159
00:08:30,820 --> 00:08:32,280
X et la classe.

160
00:08:33,050 --> 00:08:37,195
Un hyperplan 2D a été créé
entre les deux ensembles de données

161
00:08:37,195 --> 00:08:39,775
qui sont désormais
séparés par la dimension de classe.

162
00:08:40,255 --> 00:08:44,624
J'ai aussi ajouté les vraies droites
pour les classes A et B,

163
00:08:44,624 --> 00:08:48,149
et la droite de régression linéaire en 1D.

164
00:08:49,479 --> 00:08:52,470
L'hyperplan ne contient
aucune des droites entièrement,

165
00:08:52,470 --> 00:08:55,750
à cause des bruits des données
qui inclinent ses deux pentes.

166
00:08:56,290 --> 00:09:00,920
Sans bruit, les trois droites
se trouveraient dans l'hyperplan.

167
00:09:02,120 --> 00:09:06,045
Nous avons aussi déjà répondu
à l'autre partie de la question

168
00:09:06,045 --> 00:09:07,860
concernant la classification linéaire.

169
00:09:07,860 --> 00:09:12,765
Car la droite de régression linéaire
sépare déjà très bien les classes.

170
00:09:13,375 --> 00:09:16,910
Il s'agit donc aussi d'un très bon exemple
pour la classification linéaire.

171
00:09:17,620 --> 00:09:20,190
Mais est-ce que cela créerait
une frontière de décision

172
00:09:20,190 --> 00:09:24,080
exactement sur la droite de régression
linéaire 1D ? Découvrons-le.

173
00:09:24,570 --> 00:09:28,670
La droite jaune est le résultat
d'un classifieur linéaire 1D,

174
00:09:28,670 --> 00:09:30,110
la régression logistique.

175
00:09:30,620 --> 00:09:34,385
Remarquez qu'elle est très proche
de la droite verte de régression linéaire,

176
00:09:34,385 --> 00:09:37,540
mais pas complètement dessus.
Pour quelles raisons ?

177
00:09:38,200 --> 00:09:42,755
Les modèles de régression utilisent
en général l'erreur quadratique moyenne

178
00:09:42,755 --> 00:09:43,950
comme fonction de pertes,

179
00:09:43,950 --> 00:09:47,190
alors que les modèles de classification
utilisent l'entropie croisée.

180
00:09:47,650 --> 00:09:49,795
Quelle est donc
la différence entre les deux ?

181
00:09:50,155 --> 00:09:54,555
Sans trop entrer dans les détails,
il existe une pénalisation quadratique

182
00:09:54,555 --> 00:09:56,175
pour l'erreur quadratique moyenne.

183
00:09:56,175 --> 00:09:58,220
Il s'agit de réduire
la distance euclidienne

184
00:09:58,220 --> 00:10:00,965
entre la véritable étiquette
et l'étiquette prédite.

185
00:10:01,535 --> 00:10:06,680
Avec l'entropie croisée,
la pénalisation est presque linéaire,

186
00:10:06,680 --> 00:10:09,670
et la probabilité prédite est proche
de la véritable étiquette,

187
00:10:09,680 --> 00:10:13,260
mais lorsqu'elle continue,
elle devient exponentielle,

188
00:10:13,260 --> 00:10:16,835
lorsqu'elle s'approche de la prédiction
de la classe opposée de l'étiquette.

189
00:10:16,835 --> 00:10:19,350
Ainsi, si vous regardez
le tracé plus précisément,

190
00:10:19,350 --> 00:10:25,330
la pente de la frontière de décision est
probablement légèrement plus négative,

191
00:10:25,330 --> 00:10:30,320
car certains des points rouges,
le rouge étant la distribution du bruit,

192
00:10:30,320 --> 00:10:33,015
se trouvent de l'autre côté
de la frontière de décision

193
00:10:33,015 --> 00:10:35,865
et perdent
leur contribution élevée à l'erreur.

194
00:10:36,009 --> 00:10:38,169
Puisqu'ils sont si proches de la droite,

195
00:10:38,169 --> 00:10:41,709
leur contribution à l'erreur
serait petite pour la régression linéaire,

196
00:10:41,709 --> 00:10:46,800
car l'erreur est quadratique,
et il n'y a pas de préférence

197
00:10:46,800 --> 00:10:50,440
quant à leur position d'un côté ou l'autre
de la droite pour la régression,

198
00:10:50,440 --> 00:10:53,050
tant que la distance
est aussi faible que possible.

199
00:10:53,384 --> 00:10:59,744
Cet ensemble convient donc pour la
régression et la classification linéaires.

200
00:10:59,914 --> 00:11:02,394
Contrairement à l'ensemble
de données des pourboires,

201
00:11:02,394 --> 00:11:05,440
pour lequel nous ne pouvions
utiliser que la régression linéaire,

202
00:11:05,440 --> 00:11:08,250
et avons opté
pour une classification non linéaire.