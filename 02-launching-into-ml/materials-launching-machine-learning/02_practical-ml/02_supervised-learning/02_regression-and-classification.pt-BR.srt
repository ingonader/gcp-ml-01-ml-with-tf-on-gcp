1
00:00:00,000 --> 00:00:01,830
Vimos o conjunto de status da gorjeta

2
00:00:01,830 --> 00:00:04,725
e que podemos usar como marcador o valor 
da gorjeta

3
00:00:04,725 --> 00:00:06,689
ou o gênero do cliente.

4
00:00:06,689 --> 00:00:11,035
Na primeira opção, usamos o valor como
o marcador e queremos prevê-lo

5
00:00:11,035 --> 00:00:13,240
de acordo com os outros recursos
do conjunto.

6
00:00:13,240 --> 00:00:16,329
Suponha que você use apenas um recurso,

7
00:00:16,329 --> 00:00:18,955
apenas o total da conta para prever
a gorjeta.

8
00:00:18,955 --> 00:00:21,255
Como a gorjeta é um número contínuo,

9
00:00:21,255 --> 00:00:23,175
este é um problema de regressão.

10
00:00:23,175 --> 00:00:25,640
Nos problemas de regressão, o objetivo
é usar

11
00:00:25,640 --> 00:00:28,490
funções matemáticas de diferentes
combinações de recursos

12
00:00:28,490 --> 00:00:31,055
para prever o valor contínuo do marcador.

13
00:00:31,055 --> 00:00:33,370
Isso é demonstrado pela linha,

14
00:00:33,370 --> 00:00:36,920
em que, para um determinado total da conta
vezes a inclinação da linha,

15
00:00:36,920 --> 00:00:39,070
conseguimos um valor contínuo
para a gorjeta.

16
00:00:39,070 --> 00:00:43,340
Talvez a taxa média de gorjeta seja 18%
do total da conta,

17
00:00:43,340 --> 00:00:46,270
então a inclinação da linha será 0,18.

18
00:00:46,270 --> 00:00:51,410
Ao multiplicar a conta por 0,18,
conseguiremos prever a gorjeta.

19
00:00:51,410 --> 00:00:56,370
Esta regressão linear com apenas
um recurso generaliza outros.

20
00:00:56,370 --> 00:00:59,570
Nesse caso, temos um problema
multidimensional,

21
00:00:59,570 --> 00:01:01,270
mas o conceito é o mesmo.

22
00:01:01,270 --> 00:01:07,040
Multiplicamos o valor dos recursos dos
exemplos pelo gradiente de um hiperplano,

23
00:01:07,040 --> 00:01:11,610
que é apenas a generalização da linha para
conseguir um valor contínuo do marcador.

24
00:01:11,610 --> 00:01:14,250
Nos problemas de regressão, queremos
minimizar

25
00:01:14,250 --> 00:01:16,670
o erro entre o valor contínuo da previsão

26
00:01:16,670 --> 00:01:18,750
e o do marcador,

27
00:01:18,750 --> 00:01:21,900
geralmente usando o erro quadrático
médio.

28
00:01:21,900 --> 00:01:26,170
Na segunda opção, usaremos o gênero
como marcador

29
00:01:26,170 --> 00:01:30,200
para prever o gênero do cliente com dados
da gorjeta e do total da conta.

30
00:01:30,200 --> 00:01:34,325
Claro, como você pode ver nos dados,
esta não é uma boa ideia.

31
00:01:34,325 --> 00:01:37,120
Os dados de homens e mulheres
não estão separados,

32
00:01:37,120 --> 00:01:39,990
e criaremos um modelo incorreto
se fizermos isso.

33
00:01:39,990 --> 00:01:43,170
Mas a separação ajuda a ilustrar o que

34
00:01:43,170 --> 00:01:47,780
acontece quando o que você quer prever
é de categoria, e não contínuo.

35
00:01:47,780 --> 00:01:50,040
Os valores da coluna de gênero

36
00:01:50,040 --> 00:01:51,480
neste conjunto de dados

37
00:01:51,480 --> 00:01:54,140
são poucos, masculino ou feminino.

38
00:01:54,140 --> 00:01:55,970
Como o gênero é de categoria e

39
00:01:55,970 --> 00:01:58,930
usamos sua coluna do conjunto de dados
como marcador,

40
00:01:58,930 --> 00:02:01,680
o problema é de classificação.

41
00:02:01,680 --> 00:02:06,760
Nos problemas de classificação, em vez de
prever uma variável contínua,

42
00:02:06,760 --> 00:02:11,560
queremos criar um limite de decisão
que separa as diferentes classes.

43
00:02:11,560 --> 00:02:16,710
Nesse caso, há duas classes de gênero:
feminino e masculino.

44
00:02:16,710 --> 00:02:22,030
O limite de decisão linear forma uma linha
ou um hiperplano em dimensões maiores,

45
00:02:22,030 --> 00:02:24,295
com cada classe em ambos os lados.

46
00:02:24,295 --> 00:02:27,190
Por exemplo, podemos dizer que,
se o valor da gorjeta

47
00:02:27,190 --> 00:02:30,735
tiver sido maior que 0,18 vezes 
o total da conta,

48
00:02:30,735 --> 00:02:34,475
a previsão é de que o pagamento foi feito
por um homem.

49
00:02:34,475 --> 00:02:36,620
Isso é mostrado pela linha vermelha.

50
00:02:36,620 --> 00:02:39,455
Mas não funciona muito bem para este
conjunto de dados.

51
00:02:39,455 --> 00:02:42,640
Os homens parecem ter maior variabilidade,

52
00:02:42,640 --> 00:02:45,210
já mulheres dão gorjeta em uma faixa
mais estreita.

53
00:02:45,210 --> 00:02:48,285
Este é um exemplo de limite de decisão
não linear,

54
00:02:48,285 --> 00:02:50,425
mostrado pela elipse amarela no
gráfico.

55
00:02:50,425 --> 00:02:53,175
Como saber se o limite de decisão vermelho
é inválido

56
00:02:53,175 --> 00:02:55,530
e se o amarelo é mais apropriado?

57
00:02:55,530 --> 00:02:59,390
Nos problemas de classificação,
queremos minimizar o erro ou

58
00:02:59,390 --> 00:03:03,275
a classificação incorreta entre
a classe prevista e a classe do marcador.

59
00:03:03,275 --> 00:03:06,485
Isso costuma ser feito com a entropia
cruzada.

60
00:03:06,485 --> 00:03:08,860
Mesmo ao prever o valor da gorjeta,

61
00:03:08,860 --> 00:03:11,305
às vezes não precisamos saber 
a quantia exata.

62
00:03:11,305 --> 00:03:16,940
Na verdade, queremos determinar
se a gorjeta será alta, média ou baixa.

63
00:03:16,940 --> 00:03:20,770
Definimos um valor como alto se for maior
do que 25%,

64
00:03:20,770 --> 00:03:24,055
médio se estiver entre 15% e 25%

65
00:03:24,055 --> 00:03:26,890
e baixo se for inferior a 15%.

66
00:03:26,890 --> 00:03:30,485
Em outras palavras,
é possível discretizar o valor.

67
00:03:30,485 --> 00:03:33,980
Agora, criar o valor da gorjeta ou,
melhor dizendo,

68
00:03:33,980 --> 00:03:37,650
a classe dela se torna um problema
de classificação.

69
00:03:37,650 --> 00:03:43,255
Em geral, é possível discretizar um 
recurso bruto contínuo em um de categoria.

70
00:03:43,255 --> 00:03:45,865
Mais adiante neste curso,

71
00:03:45,865 --> 00:03:48,105
falaremos sobre o processo reverso.

72
00:03:48,105 --> 00:03:52,420
É possível incorporar um recurso
de categoria em um espaço contínuo.

73
00:03:52,420 --> 00:03:55,110
Isso depende do problema que
você quer resolver

74
00:03:55,110 --> 00:03:56,480
e do que funciona melhor.

75
00:03:56,480 --> 00:03:59,970
O aprendizado de máquina
é experimentação.

76
00:03:59,970 --> 00:04:03,645
Os dois tipos de problema, regressão
e classificação,

77
00:04:03,645 --> 00:04:06,095
são considerados como problemas de
previsão,

78
00:04:06,095 --> 00:04:11,125
ao contrário dos não supervisionados,
que são como problemas de descrição.

79
00:04:11,125 --> 00:04:13,805
Agora, de onde vêm todos estes dados?

80
00:04:13,805 --> 00:04:16,964
Chamamos este conjunto de dados de gorjeta
como dados estruturados,

81
00:04:16,964 --> 00:04:20,120
formados por linhas e colunas.

82
00:04:20,120 --> 00:04:24,620
Sua origem mais comum no aprendizado de
máquina é o armazenamento de dados.

83
00:04:24,620 --> 00:04:29,910
Os dados não estruturados são itens como
imagens, áudios ou vídeos.

84
00:04:29,910 --> 00:04:32,775
Aqui, você vê um conjunto de dados
de natalidade,

85
00:04:32,775 --> 00:04:35,455
que inclui informações médicas.

86
00:04:35,455 --> 00:04:38,490
Ele é um conjunto de dados público
no BigQuery,

87
00:04:38,490 --> 00:04:40,810
e você o usará mais tarde no curso.

88
00:04:40,810 --> 00:04:44,500
Agora, suponha que este conjunto de dados
esteja no armazenamento de dados.

89
00:04:44,500 --> 00:04:47,750
Vamos supor que você queira prever
as semanas de gestação de um bebê.

90
00:04:47,750 --> 00:04:51,585
Em outras palavras, queremos prever
quando o bebê vai nascer.

91
00:04:51,585 --> 00:04:57,110
Realize uma instrução SELECT de SQL no
BigQuery para criar dados de ML.

92
00:04:57,110 --> 00:04:59,310
Escolheremos os recursos de entrada
do modelo,

93
00:04:59,310 --> 00:05:00,820
como idade da mãe,

94
00:05:00,820 --> 00:05:02,310
o ganho de peso em quilos

95
00:05:02,310 --> 00:05:04,695
e o marcador, semanas de gestação.

96
00:05:04,695 --> 00:05:08,040
Como as semanas de gestação são um número
contínuo,

97
00:05:08,040 --> 00:05:10,760
este é um problema de regressão.

98
00:05:10,760 --> 00:05:14,555
Fazer previsões com dados
estruturados é algo muito comum,

99
00:05:14,555 --> 00:05:18,370
e este foi o foco da primeira parte deste
curso.

100
00:05:18,370 --> 00:05:23,125
Claro, é possível usar este conjunto de
dados médicos para prever outras coisas.

101
00:05:23,125 --> 00:05:28,190
Talvez seja preciso prever o peso do bebê
usando outros atributos como os recursos.

102
00:05:28,190 --> 00:05:30,990
O peso é um indicador de saúde.

103
00:05:30,990 --> 00:05:33,815
Ao prever que o bebê nascerá com
peso baixo,

104
00:05:33,815 --> 00:05:37,565
o hospital poderá contar com equipamentos
como uma incubadora,

105
00:05:37,565 --> 00:05:40,795
então é importante fazer esse tipo de
previsão.

106
00:05:40,795 --> 00:05:43,050
Aqui, o marcador é o peso do bebê,

107
00:05:43,050 --> 00:05:45,395
que é uma variável contínua.

108
00:05:45,395 --> 00:05:50,535
Ele é armazenado como ponto flutuante, 
tornando-se um problema de regressão.

109
00:05:50,535 --> 00:05:52,380
Este conjunto de dados é

110
00:05:52,380 --> 00:05:54,265
adequado para a regressão linear

111
00:05:54,265 --> 00:05:57,740
e/ou classificação linear?

112
00:05:57,740 --> 00:06:01,625
A resposta correta é "Ambos".

113
00:06:01,625 --> 00:06:07,460
Vamos descobrir por quê. Olhe novamente
para o conjunto com as classes misturadas.

114
00:06:07,460 --> 00:06:10,210
Sem as diferentes cores e formas para
nos ajudar,

115
00:06:10,210 --> 00:06:15,900
os dados são linhas com ruído, inclinadas
negativamente, com interceptação positiva.

116
00:06:15,900 --> 00:06:18,325
Como há uma aparência bem linear,

117
00:06:18,325 --> 00:06:22,575
este é provavelmente um caso adequado de
regressão linear,

118
00:06:22,575 --> 00:06:26,675
em que o previsto será o valor de Y.

119
00:06:26,675 --> 00:06:30,300
Adicionando formas e cores diferentes,

120
00:06:30,300 --> 00:06:33,040
fica muito mais evidente que este conjunto
de dados são

121
00:06:33,040 --> 00:06:36,490
duas séries lineares com algum ruído
gaussiano.

122
00:06:36,490 --> 00:06:39,795
As linhas têm inclinações
diferentes e interceptações distintas,

123
00:06:39,795 --> 00:06:42,500
e o ruído tem diferentes desvios padrão.

124
00:06:42,500 --> 00:06:45,890
As linhas foram aplicadas para mostrar
a você que este é definitivamente

125
00:06:45,890 --> 00:06:50,370
um conjunto de dados linear pelo design,
apesar de ter algum ruído.

126
00:06:50,370 --> 00:06:53,565
Este caso é adequado para a regressão
linear.

127
00:06:53,565 --> 00:06:57,000
Mesmo havendo duas séries lineares
diferentes,

128
00:06:57,000 --> 00:07:00,350
primeiro veremos o resultado de
uma regressão linear unidimensional,

129
00:07:00,350 --> 00:07:02,320
prevendo Y a partir de X,

130
00:07:02,320 --> 00:07:04,200
para começar a criar uma hipótese.

131
00:07:04,200 --> 00:07:07,325
Depois iremos ainda mais longe.

132
00:07:07,325 --> 00:07:12,605
A linha verde é a equação linear ajustada
de acordo com a regressão linear.

133
00:07:12,605 --> 00:07:16,555
Perceba que ela está distante de cada
distribuição de classe individual

134
00:07:16,555 --> 00:07:21,575
porque a classe B afasta a linha da classe
A e vice-versa.

135
00:07:21,575 --> 00:07:25,970
Isso acaba praticamente cortando o espaço
entre as duas distribuições.

136
00:07:25,970 --> 00:07:28,390
É algo que faz sentido já que,
na regressão,

137
00:07:28,390 --> 00:07:30,875
reduzimos a perda do erro quadrático
médio.

138
00:07:30,875 --> 00:07:33,320
Com o mesmo afastamento de cada classe,

139
00:07:33,320 --> 00:07:37,485
a regressão terá o menor erro quadrático
médio entre elas,

140
00:07:37,485 --> 00:07:40,460
sendo praticamente equidistante das
médias.

141
00:07:40,460 --> 00:07:44,965
Cada classe é uma série linear diferente
com inclinação e interceptação distintas,

142
00:07:44,965 --> 00:07:47,840
então teremos uma precisão ainda melhor

143
00:07:47,840 --> 00:07:50,730
ao realizar a regressão linear de
cada uma delas,

144
00:07:50,730 --> 00:07:54,480
que encaixará muito perto de cada linha
esboçada.

145
00:07:54,480 --> 00:07:57,010
Melhor ainda, em vez de realizar

146
00:07:57,010 --> 00:08:01,770
a regressão linear unidimensional
prevendo Y a partir de um recurso X,

147
00:08:01,770 --> 00:08:07,435
podemos fazer uma bidimensional para
prever o Y de dois recursos:

148
00:08:07,435 --> 00:08:10,000
X e a classe do ponto.

149
00:08:10,000 --> 00:08:13,710
O recurso pode ser um se o ponto pertencer
à classe A

150
00:08:13,710 --> 00:08:16,545
e zero se pertencer à classe B.

151
00:08:16,545 --> 00:08:21,030
Ele forma um hiperplano 2D em vez de uma
linha.

152
00:08:21,030 --> 00:08:23,535
Vamos ver como ele é.

153
00:08:23,535 --> 00:08:27,165
Estes são os resultados da
regressão linear 2D.

154
00:08:27,165 --> 00:08:32,684
Para prever o marcador Y, usamos dois 
recursos: X e a classe.

155
00:08:32,684 --> 00:08:36,039
Como você pode ver, foi formado
um hiperplano 2D entre

156
00:08:36,039 --> 00:08:39,840
os dois conjuntos de dados agora
separados pela dimensão de classe.

157
00:08:39,840 --> 00:08:44,830
Também estão incluídas as linhas reais
das classes A e B,

158
00:08:44,830 --> 00:08:48,670
além da linha de tendência da regressão 
linear 1D.

159
00:08:48,670 --> 00:08:52,545
O plano não contém as linhas por completo

160
00:08:52,545 --> 00:08:55,870
por conta dos ruídos dos dados que oscilam
em duas extremidades dele.

161
00:08:55,870 --> 00:08:58,275
Do contrário, sem nenhum ruído,

162
00:08:58,275 --> 00:09:01,390
todas as três linhas ficariam
perfeitamente no plano.

163
00:09:01,390 --> 00:09:04,590
Além disso, também respondemos

164
00:09:04,590 --> 00:09:07,595
à outra parte da pergunta sobre
classificação linear.

165
00:09:07,595 --> 00:09:09,650
Isso porque a linha da regressão linear

166
00:09:09,650 --> 00:09:12,960
faz um ótimo trabalho ao separar
as classes.

167
00:09:12,960 --> 00:09:17,005
Este também é um caso muito adequado para
a classificação linear.

168
00:09:17,005 --> 00:09:20,600
Mas ele produziria um limite de decisão
exatamente

169
00:09:20,600 --> 00:09:24,145
na linha de tendência da regressão linear
1D? Vamos descobrir.

170
00:09:24,145 --> 00:09:27,010
Em amarelo, está o resultado de

171
00:09:27,010 --> 00:09:30,180
um classificador linear unidimensional:
regressão logística.

172
00:09:30,180 --> 00:09:34,290
Perceba que está muito perto da linha
verde da regressão linear,

173
00:09:34,290 --> 00:09:37,740
mas não exatamente. Por quê?

174
00:09:37,740 --> 00:09:40,550
Lembre-se: você viu que os modelos de
regressão

175
00:09:40,550 --> 00:09:43,820
costumam usar o erro quadrático médio como
função de perda,

176
00:09:43,820 --> 00:09:47,230
enquanto os de classificação geralmente
usam entropia cruzada.

177
00:09:47,230 --> 00:09:49,545
Então qual é a diferença entre as duas?

178
00:09:49,545 --> 00:09:53,005
Sem mencionar muitos detalhes agora,

179
00:09:53,005 --> 00:09:55,680
há uma penalidade quadrática para erro
quadrático médio.

180
00:09:55,680 --> 00:09:57,420
Ela tenta basicamente minimizar

181
00:09:57,420 --> 00:10:01,115
a distância euclidiana entre o marcador
real e o previsto.

182
00:10:01,115 --> 00:10:04,845
Por outro lado, com a entropia cruzada
das classificações,

183
00:10:04,845 --> 00:10:09,880
a penalidade é quase linear, e a previsão
de probabilidade está perto do marcador.

184
00:10:09,880 --> 00:10:13,375
Mas ao se afastar, torna-se exponencial,

185
00:10:13,375 --> 00:10:16,560
quando chega perto de prever a classe
oposta do marcador.

186
00:10:16,560 --> 00:10:19,320
Portanto, ao analisar de perto o plano,

187
00:10:19,320 --> 00:10:20,700
o motivo mais provável

188
00:10:20,700 --> 00:10:25,055
para a linha do limite na classificação
ter uma inclinação mais negativa

189
00:10:25,055 --> 00:10:28,000
é que alguns dos pontos vermelhos de
ruído,

190
00:10:28,000 --> 00:10:29,945
tendo a distribuição com ruído essa cor,

191
00:10:29,945 --> 00:10:35,705
ficam do outro lado do limite de decisão, 
perdendo a alta contribuição de erro.

192
00:10:35,705 --> 00:10:38,005
Como eles estão muito perto da linha,

193
00:10:38,005 --> 00:10:41,615
a contribuição de erro seria pequena na
regressão linear,

194
00:10:41,615 --> 00:10:45,150
já que o erro é quadrático,

195
00:10:45,150 --> 00:10:50,080
e não há preferência de lado na linha para
a regressão,

196
00:10:50,080 --> 00:10:53,015
contanto que a distância permaneça a
menor possível.

197
00:10:53,015 --> 00:10:54,890
Então como você pode ver,

198
00:10:54,890 --> 00:10:59,970
este conjunto de dados é adequado para
regressão e classificação linear.

199
00:10:59,970 --> 00:11:02,340
Ao contrário do conjunto de dados
das gorjetas,

200
00:11:02,340 --> 00:11:04,680
em que a regressão linear era adequada,

201
00:11:04,680 --> 00:11:08,000
além da classificação não linear.