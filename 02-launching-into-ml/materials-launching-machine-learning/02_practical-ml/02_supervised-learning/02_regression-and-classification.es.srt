1
00:00:00,030 --> 00:00:02,100
En el conjunto de datos de las propinas

2
00:00:02,100 --> 00:00:04,625
vimos que podíamos usar
el importe de la propina

3
00:00:04,625 --> 00:00:06,869
o el género del cliente
como etiquetas.

4
00:00:06,869 --> 00:00:09,825
En la primera opción,
el importe de la propina es la etiqueta

5
00:00:09,825 --> 00:00:11,050
que queremos predecir

6
00:00:11,050 --> 00:00:13,629
dados los otros atributos
en el conjunto de datos.

7
00:00:13,629 --> 00:00:16,365
Supongamos que están usando
solo un atributo

8
00:00:16,365 --> 00:00:19,355
para predecir la propina:
el importe total de la factura.

9
00:00:19,365 --> 00:00:23,405
Ya que la propina es un número continuo,
este es un problema de regresión.

10
00:00:23,405 --> 00:00:26,600
En estos problemas, el objetivo
es usar funciones matemáticas

11
00:00:26,600 --> 00:00:28,750
de diferentes combinaciones de atributos

12
00:00:28,750 --> 00:00:31,055
para predecir el valor continuo
de la etiqueta.

13
00:00:31,535 --> 00:00:35,460
Esto se muestra en la línea, en la que
por un importe total dado de la factura

14
00:00:35,460 --> 00:00:38,190
por la pendiente de la línea,
obtenemos un valor continuo

15
00:00:38,190 --> 00:00:39,630
para el importe de la propina.

16
00:00:39,630 --> 00:00:43,340
Si el porcentaje de la contribución
es 18% de la factura total,

17
00:00:43,340 --> 00:00:46,270
entonces, la pendiente
de la línea será 0.18.

18
00:00:46,620 --> 00:00:49,950
Si multiplicamos el importe
de la factura por 0.18,

19
00:00:49,950 --> 00:00:51,880
obtendremos la predicción de la propina.

20
00:00:51,880 --> 00:00:55,080
Esta regresión lineal
con un solo atributo se generaliza

21
00:00:55,080 --> 00:00:56,570
a atributos adicionales.

22
00:00:56,570 --> 00:00:59,670
En ese caso,
tenemos un problema multidimensional

23
00:00:59,670 --> 00:01:01,610
pero el concepto es el mismo.

24
00:01:01,610 --> 00:01:05,480
El valor de cada atributo
para cada ejemplo se multiplica

25
00:01:05,480 --> 00:01:09,420
por el gradiente del hiperplano,
que es la generalización de la línea,

26
00:01:09,420 --> 00:01:11,810
para obtener un valor continuo
para la etiqueta.

27
00:01:12,080 --> 00:01:14,900
En los problemas de regresión,
queremos minimizar el error

28
00:01:14,900 --> 00:01:18,920
entre el valor continuo pronosticado
y el valor continuo de la etiqueta

29
00:01:18,920 --> 00:01:21,580
por lo general,
mediante el error cuadrático medio.

30
00:01:23,050 --> 00:01:26,215
En la segunda opción,
usaremos el género como la etiqueta

31
00:01:26,215 --> 00:01:29,240
y predeciremos el género del cliente
con los datos de la propina

32
00:01:29,240 --> 00:01:30,780
y el total de la factura.

33
00:01:30,780 --> 00:01:34,390
Por supuesto, como pueden ver
en los datos, no es una buena idea.

34
00:01:34,390 --> 00:01:37,710
Los datos de hombres
y mujeres no están separados

35
00:01:37,710 --> 00:01:40,090
y obtendremos un mal modelo si lo hacemos.

36
00:01:40,520 --> 00:01:44,100
Pero hacerlo ayuda a ilustrar qué pasa

37
00:01:44,100 --> 00:01:48,070
cuando lo que se quiere predecir
es categórico y no continuo.

38
00:01:48,070 --> 00:01:51,550
Los valores de las columnas de género,
al menos en este conjunto de datos,

39
00:01:51,550 --> 00:01:54,470
son discretos, masculino o femenino.

40
00:01:54,470 --> 00:01:57,470
Puesto que el género es categórico
y que usamos esta columna

41
00:01:57,470 --> 00:02:01,300
del conjunto de datos como la etiqueta,
el problema es de clasificación.

42
00:02:01,940 --> 00:02:04,060
En los problemas de clasificación,

43
00:02:04,060 --> 00:02:06,800
en lugar de intentar
predecir una variable continua,

44
00:02:06,800 --> 00:02:11,360
intentamos crear un límite de decisión
que separe las diferentes clases.

45
00:02:11,780 --> 00:02:14,655
En este caso, hay dos clases de géneros:

46
00:02:14,985 --> 00:02:16,835
femenino y masculino.

47
00:02:17,225 --> 00:02:21,050
Un límite de decisión lineal
formará una línea o un hiperplano

48
00:02:21,050 --> 00:02:24,465
en dimensiones más altas,
con cada clase en uno de los lados.

49
00:02:24,465 --> 00:02:27,365
Por ejemplo, podríamos decir
que si el importe de la propina

50
00:02:27,365 --> 00:02:30,880
es mayor que 0.18 veces
el importe total de la factura,

51
00:02:30,880 --> 00:02:34,365
entonces predecimos que la persona
que hizo el pago fue un hombre.

52
00:02:35,055 --> 00:02:37,030
La línea roja muestra esto.

53
00:02:37,030 --> 00:02:39,850
Pero eso no funciona muy bien
con este conjunto de datos.

54
00:02:39,850 --> 00:02:42,675
Las propinas de los hombres
parecen tener mayor variabilidad

55
00:02:42,675 --> 00:02:45,745
y las de las mujeres tienden
a estar en una franja más estrecha.

56
00:02:45,745 --> 00:02:48,600
Este es un ejemplo
de un límite de decisión no lineal

57
00:02:48,600 --> 00:02:50,970
que se ve
en las franjas amarillas en el gráfico.

58
00:02:50,970 --> 00:02:53,725
¿Cómo sabemos que el límite
de decisión rojo no es bueno?

59
00:02:53,725 --> 00:02:56,065
¿Y que el límite
de decisión amarillo es mejor?

60
00:02:56,065 --> 00:02:59,130
En problemas de clasificación,
queremos minimizar el error

61
00:02:59,130 --> 00:03:02,155
o la clasificación incorrecta
entre nuestra clase pronosticada

62
00:03:02,155 --> 00:03:03,740
y la clase de la etiqueta.

63
00:03:03,740 --> 00:03:06,570
Por lo general, esto se logra
mediante la entropía cruzada.

64
00:03:06,860 --> 00:03:09,145
Aun si predecimos el importe de la propina

65
00:03:09,145 --> 00:03:11,710
tal vez no necesitamos
conocer el importe exacto.

66
00:03:11,710 --> 00:03:16,675
En vez, queremos determinar
si la propina será alta, media o baja.

67
00:03:17,185 --> 00:03:20,680
Podemos definir
que un importe alto sea mayor que 25%,

68
00:03:21,080 --> 00:03:24,050
medio esté entre 15% y 25%

69
00:03:24,050 --> 00:03:27,045
y bajo sea menos que 15%.

70
00:03:27,045 --> 00:03:30,875
Es decir,
podemos discretizar el importe.

71
00:03:30,875 --> 00:03:32,935
Ahora, crear el importe de la propina

72
00:03:32,935 --> 00:03:35,410
o, mejor dicho, la clase de la propina

73
00:03:35,410 --> 00:03:37,630
se convierte
en un problema de clasificación.

74
00:03:38,100 --> 00:03:40,790
En general,
un atributo continuo, sin procesar

75
00:03:40,790 --> 00:03:43,390
se puede discretizar
en un atributo categórico.

76
00:03:43,900 --> 00:03:47,970
Más adelante en esta especialización,
hablaremos sobre el proceso contrario.

77
00:03:47,970 --> 00:03:52,445
Un atributo categórico
se puede incrustar en un espacio continuo.

78
00:03:52,445 --> 00:03:55,335
Depende del problema
que estén tratando de resolver

79
00:03:55,335 --> 00:03:56,815
y de qué funciona mejor.

80
00:03:56,815 --> 00:04:00,105
El aprendizaje automático
se trata de la experimentación.

81
00:04:00,625 --> 00:04:03,924
Ambos tipos de problemas,
la regresión y la clasificación,

82
00:04:03,924 --> 00:04:06,270
se pueden considerar
como problemas de predicción,

83
00:04:06,270 --> 00:04:09,040
a diferencia
de los problemas no supervisados

84
00:04:09,040 --> 00:04:11,060
que son problemas descriptivos.

85
00:04:11,450 --> 00:04:14,105
Ahora, ¿de dónde vienen estos datos?

86
00:04:14,105 --> 00:04:15,735
El conjunto de datos de propinas

87
00:04:15,735 --> 00:04:19,950
es lo que llamamos datos estructurados,
compuesto de filas y columnas.

88
00:04:20,360 --> 00:04:23,420
Una fuente común de estos datos
para el aprendizaje automático

89
00:04:23,420 --> 00:04:25,040
es su almacén de datos.

90
00:04:25,040 --> 00:04:29,490
Los datos no estructurados
son elementos como fotos, audio o video.

91
00:04:30,230 --> 00:04:33,125
Este es un conjunto de datos de natalidad,

92
00:04:33,125 --> 00:04:35,510
un conjunto público de información médica.

93
00:04:36,080 --> 00:04:38,750
Es un conjunto
de datos públicos en BigQuery

94
00:04:38,750 --> 00:04:40,820
y lo usarán más tarde
en la especialización.

95
00:04:40,820 --> 00:04:44,120
Por ahora, supongamos
que está en su almacén de datos.

96
00:04:44,875 --> 00:04:48,220
Digamos que queremos predecir
las semanas de gestación del bebé.

97
00:04:48,220 --> 00:04:51,320
Es decir, cuándo nacerá el bebé.

98
00:04:51,960 --> 00:04:55,345
Pueden ejecutar una instrucción
SELECT de SQL en BigQuery

99
00:04:55,345 --> 00:04:57,430
para crear un conjunto de datos de AA.

100
00:04:57,430 --> 00:04:59,685
Elegiremos los atributos
de entrada del modelo,

101
00:04:59,685 --> 00:05:02,120
como edad de la madre,
aumento de peso en libras

102
00:05:02,660 --> 00:05:05,025
y la etiqueta, "gestation_weeks".

103
00:05:05,025 --> 00:05:10,675
Ya que este es un número continuo,
es un problema de regresión.

104
00:05:11,085 --> 00:05:14,745
Realizar predicciones
a partir de datos estructurados es común

105
00:05:14,745 --> 00:05:17,280
y eso es en lo que nos enfocamos
en la primera parte

106
00:05:17,280 --> 00:05:18,785
de esta especialización.

107
00:05:18,785 --> 00:05:23,275
Este conjunto de datos se puede usar
para realizar otras predicciones también.

108
00:05:23,275 --> 00:05:25,870
Tal vez queremos predecir el peso del bebé

109
00:05:25,870 --> 00:05:28,595
mediante otros atributos.

110
00:05:28,595 --> 00:05:31,500
El peso del bebé
puede ser un indicador de salud.

111
00:05:31,500 --> 00:05:34,165
Cuando se predice
que un bebé tendrá un bajo peso,

112
00:05:34,165 --> 00:05:38,040
por lo general, el hospital tendrá equipo
preparado, como una incubadora,

113
00:05:38,040 --> 00:05:41,280
por lo que puede ser importante
predecir el peso del bebé.

114
00:05:41,280 --> 00:05:45,320
La etiqueta será "baby_weight"
y es una variable continua.

115
00:05:45,780 --> 00:05:48,035
Se almacena
como un número de punto flotante

116
00:05:48,035 --> 00:05:50,715
que debería convertir esto
en un problema de regresión.

117
00:05:51,125 --> 00:05:53,465
¿Es este conjunto
de datos un buen candidato

118
00:05:53,465 --> 00:05:56,550
para una regresión lineal?
¿O para una clasificación lineal?

119
00:05:59,430 --> 00:06:02,000
La respuesta correcta es
C. Ambas.

120
00:06:02,000 --> 00:06:03,370
Investiguemos por qué.

121
00:06:03,790 --> 00:06:07,525
Observemos el conjunto de datos
con ambas clases mezcladas.

122
00:06:07,765 --> 00:06:10,470
Sin los diferentes colores
y formas para guiarnos

123
00:06:10,470 --> 00:06:14,460
los datos parecen ser una línea
con ruido, con una pendiente negativa

124
00:06:14,460 --> 00:06:16,170
y una ordenada al origen positiva.

125
00:06:16,630 --> 00:06:21,385
Ya que parece bastante lineal,
probablemente será un buen candidato

126
00:06:21,385 --> 00:06:25,780
para una regresión lineal, en la que
intentamos predecir el valor de Y.

127
00:06:27,890 --> 00:06:32,930
Si agregamos diferentes colores y formas,
es más claro que este conjunto de datos

128
00:06:32,930 --> 00:06:36,690
tiene en realidad dos series lineales
con un poco de ruido gaussiano.

129
00:06:36,690 --> 00:06:39,980
Las líneas tienen ordenadas
y pendientes diferentes

130
00:06:39,980 --> 00:06:42,590
y el ruido tiene diferentes
desviaciones estándar.

131
00:06:42,950 --> 00:06:46,320
Hice que estas líneas muestren
que este conjunto de datos

132
00:06:46,320 --> 00:06:50,230
está diseñado para ser lineal
y tendrá un poco de ruido.

133
00:06:50,960 --> 00:06:54,205
Sería un buen candidato
para la regresión lineal.

134
00:06:54,205 --> 00:06:57,045
A pesar de existir
dos series lineales distintas,

135
00:06:57,045 --> 00:07:00,490
primero veamos el resultado
de una regresión lineal de una dimensión

136
00:07:00,490 --> 00:07:04,375
si trazamos Y desde X
para comenzar a crear una intuición.

137
00:07:04,650 --> 00:07:07,765
Luego, veremos si podemos hacerlo mejor.

138
00:07:08,795 --> 00:07:12,650
La línea verde es la ecuación lineal
ajustada de la regresión lineal.

139
00:07:12,970 --> 00:07:16,875
Observen que está lejos
de cada distribución de clase individual

140
00:07:16,875 --> 00:07:21,430
porque la clase B aleja la línea
de la clase A y viceversa.

141
00:07:21,900 --> 00:07:25,760
Cruza el espacio
entre las dos distribuciones.

142
00:07:26,070 --> 00:07:29,630
Esto tiene sentido, ya que
con la regresión optimizamos la pérdida

143
00:07:29,630 --> 00:07:31,080
del error cuadrático medio.

144
00:07:31,080 --> 00:07:33,350
Con un alejamiento parejo de cada clase

145
00:07:33,350 --> 00:07:36,365
la regresión debería tener
el error cuadrático medio más bajo

146
00:07:36,365 --> 00:07:40,400
entre las dos clases, aproximadamente
equidistante de sus medias.

147
00:07:40,710 --> 00:07:44,420
Ya que cada clase es una serie
lineal diferente con ordenadas

148
00:07:44,420 --> 00:07:48,035
y pendientes diferentes,
tendríamos una mejor precisión

149
00:07:48,035 --> 00:07:51,110
si realizáramos una regresión
lineal para cada clase

150
00:07:51,110 --> 00:07:54,605
que debería ajustarse muy cerca
a cada una de las líneas trazadas aquí.

151
00:07:54,765 --> 00:07:57,795
Aún mejor,
en vez de realizar una regresión lineal

152
00:07:57,795 --> 00:08:00,364
de una dimensión
para predecir el valor de Y

153
00:08:00,364 --> 00:08:04,109
a partir de un atributo de X,
podemos realizar una regresión lineal

154
00:08:04,109 --> 00:08:07,490
de dos dimensiones para predecir Y
a partir de dos atributos

155
00:08:07,490 --> 00:08:10,320
X y la clase del punto.

156
00:08:10,320 --> 00:08:12,370
El atributo de la clase podría ser uno

157
00:08:12,370 --> 00:08:16,855
si el punto pertenece a la clase A
y cero si el punto pertenece a la clase B.

158
00:08:16,855 --> 00:08:20,720
En vez de una línea,
formaría un hiperplano 2D.

159
00:08:21,190 --> 00:08:23,445
Veamos cómo se vería.

160
00:08:24,495 --> 00:08:27,790
Estos son los resultados
de la regresión lineal 2D.

161
00:08:27,790 --> 00:08:30,740
Para predecir la etiqueta Y,
usamos dos atributos

162
00:08:30,740 --> 00:08:32,665
X y la clase.

163
00:08:33,045 --> 00:08:35,940
Cómo pueden ver,
se formó un hiperplano 2D

164
00:08:35,940 --> 00:08:38,660
entre los dos conjuntos de datos
que ahora están separados

165
00:08:38,660 --> 00:08:40,405
por la dimensión de la clase.

166
00:08:40,405 --> 00:08:44,960
También incluí las líneas
verdaderas para las clases A y B

167
00:08:44,960 --> 00:08:48,945
al igual que la línea
de mejor ajuste de la regresión lineal 1D.

168
00:08:49,335 --> 00:08:52,870
El plano no contiene
ninguna de las líneas por completo

169
00:08:52,870 --> 00:08:56,340
debido al ruido de los datos
que inclinan las dos pendientes del plano.

170
00:08:56,340 --> 00:08:59,600
De otro modo, sin ruido,
las tres líneas entrarían

171
00:08:59,600 --> 00:09:01,980
perfectamente en el plano.

172
00:09:01,980 --> 00:09:05,390
También, ya respondimos
a la otra parte de la pregunta

173
00:09:05,390 --> 00:09:08,010
del cuestionario
sobre la clasificación lineal.

174
00:09:08,010 --> 00:09:10,055
Puesto que la línea de la regresión lineal

175
00:09:10,055 --> 00:09:12,925
ya logra separar las clases.

176
00:09:13,435 --> 00:09:17,210
Entonces, también es un buen candidato
para la clasificación lineal.

177
00:09:17,600 --> 00:09:21,710
Pero, ¿produciría un límite de decisión
exacto en la línea de mejor ajuste

178
00:09:21,710 --> 00:09:23,475
de la regresión lineal 1D?

179
00:09:23,475 --> 00:09:24,475
Averigüémoslo.

180
00:09:24,475 --> 00:09:27,975
El trazo amarillo es la salida
de un clasificador lineal

181
00:09:27,975 --> 00:09:30,700
de una dimensión:
una regresión logística.

182
00:09:30,700 --> 00:09:34,455
Observen que está muy cerca
de la línea verde de la regresión lineal

183
00:09:34,455 --> 00:09:37,640
pero no exactamente. ¿Por qué?

184
00:09:38,150 --> 00:09:40,670
Si recuerdan, mencioné
que los modelos de regresión

185
00:09:40,670 --> 00:09:44,140
por lo general, usan el error cuadrático
medio como su función de pérdida,

186
00:09:44,140 --> 00:09:47,755
mientras que los modelos de clasificación
usan la entropía cruzada.

187
00:09:47,755 --> 00:09:50,200
¿Cuál es la diferencia entre ambos?

188
00:09:50,200 --> 00:09:54,705
Sin profundizar demasiado aún,
existe una penalización cuadrática

189
00:09:54,705 --> 00:09:57,625
en el error cuadrático medio,
por lo que intenta minimizar

190
00:09:57,625 --> 00:09:59,785
la distancia euclidiana
entre la etiqueta real

191
00:09:59,785 --> 00:10:01,645
y la etiqueta de la predicción.

192
00:10:01,645 --> 00:10:05,180
Por otro lado, con la entropía
cruzada de las clasificaciones

193
00:10:05,180 --> 00:10:08,260
la penalización es casi lineal
y la probabilidad pronosticada

194
00:10:08,260 --> 00:10:11,815
es cercana a la etiqueta real,
pero a medida que se aleja

195
00:10:11,815 --> 00:10:14,980
se convierte en exponencial
cuando se acerca a la predicción

196
00:10:14,980 --> 00:10:16,950
de la clase opuesta de la etiqueta.

197
00:10:16,950 --> 00:10:19,600
Por lo tanto,
si miran el gráfico cuidadosamente

198
00:10:19,600 --> 00:10:22,840
la razón más probable
por la que la línea del límite de decisión

199
00:10:22,840 --> 00:10:25,720
de la clasificación
tiene una pendiente ligeramente negativa

200
00:10:25,720 --> 00:10:28,360
es porque algunos
de esos puntos rojos de ruido

201
00:10:28,360 --> 00:10:30,290
es decir, la distribución ruidosa,

202
00:10:30,290 --> 00:10:33,140
se ubican en el otro lado
del límite de decisión

203
00:10:33,140 --> 00:10:35,580
y pierden su alta contribución al error.

204
00:10:36,090 --> 00:10:40,460
Puesto que están tan cerca de la línea,
su contribución al error sería pequeña

205
00:10:40,460 --> 00:10:44,870
en la regresión lineal,
no solo porque el error es cuadrático

206
00:10:45,250 --> 00:10:49,980
sino porque no importa estar en un lado
u otro de la línea en la regresión lineal

207
00:10:50,560 --> 00:10:53,530
mientras la distancia permanezca
lo más pequeña posible.

208
00:10:53,530 --> 00:10:57,030
Como pueden ver,
este conjunto de datos es excelente

209
00:10:57,030 --> 00:11:00,230
tanto para la regresión lineal
como para la clasificación lineal.

210
00:11:00,230 --> 00:11:02,870
A diferencia
del conjunto de datos de propinas

211
00:11:02,870 --> 00:11:05,440
que solo era aceptable
para la regresión lineal

212
00:11:05,440 --> 00:11:07,740
y mejor para la clasificación no lineal.