1
00:00:00,000 --> 00:00:04,170
チップの例では
ラベルをチップ金額にすることも

2
00:00:04,170 --> 00:00:06,659
客の性別にすることもできます

3
00:00:06,659 --> 00:00:09,615
オプション1ではチップ金額をラベルにして

4
00:00:09,615 --> 00:00:11,745
データセットの他の特徴に基づき

5
00:00:11,745 --> 00:00:13,380
チップを予測してみます

6
00:00:13,380 --> 00:00:16,039
1つの特徴だけを使うとしましょう

7
00:00:16,039 --> 00:00:19,055
たとえば請求金額だけからチップを予測します

8
00:00:19,055 --> 00:00:21,345
チップは連続的な数値ですから

9
00:00:21,345 --> 00:00:23,205
回帰問題になります

10
00:00:23,205 --> 00:00:24,980
回帰問題の目標は

11
00:00:24,980 --> 00:00:28,490
さまざまな特徴を組み合わせた数学関数を使用し

12
00:00:28,490 --> 00:00:31,055
ラベルの連続値を予測することです

13
00:00:31,055 --> 00:00:33,370
それが この直線に示されています

14
00:00:33,370 --> 00:00:36,660
請求金額の合計に直線の傾きをかけると

15
00:00:36,660 --> 00:00:39,070
チップの連続値を予測できます

16
00:00:39,070 --> 00:00:43,080
おそらく チップの平均は請求額の18%でしょう

17
00:00:43,080 --> 00:00:46,270
すると 直線の傾きは0.18になります

18
00:00:46,270 --> 00:00:51,410
請求額に0.18を掛けると
チップの予測値が得られます

19
00:00:51,410 --> 00:00:56,560
この線形回帰の「特徴」は 1つだけですが
特徴を追加して汎化します

20
00:00:56,560 --> 00:00:59,340
その場合 多次元問題になります

21
00:00:59,340 --> 00:01:01,320
でも概念は同じです

22
00:01:01,320 --> 00:01:07,060
それぞれ 1つの例の 1つの特徴の値に
超平面の勾配を掛けます

23
00:01:07,060 --> 00:01:11,610
こうして汎化された線形で
ラベルの連続値が得られます

24
00:01:11,610 --> 00:01:13,140
回帰問題では

25
00:01:13,140 --> 00:01:16,670
予測される連続値とラベルの連続値の間の

26
00:01:16,670 --> 00:01:18,760
誤差を最小化するために

27
00:01:18,760 --> 00:01:21,520
平均二乗誤差を使います

28
00:01:23,070 --> 00:01:26,170
オプション2では性別をラベルにして

29
00:01:26,170 --> 00:01:29,750
請求額とチップ額から
お客さんの性別を予測します

30
00:01:29,750 --> 00:01:34,095
データを見れば明らかですが
もちろん この方法は良くありません

31
00:01:34,095 --> 00:01:37,120
男性と女性のデータにあまり差がないからです

32
00:01:37,120 --> 00:01:39,990
このモデルではひどい結果が出るでしょう

33
00:01:39,990 --> 00:01:43,050
でも ここで皆さんに説明したい点は

34
00:01:43,050 --> 00:01:47,780
連続値ではなく カテゴリ値を
予測するとどうなるか です

35
00:01:47,780 --> 00:01:49,690
「性別」列の値は

36
00:01:49,690 --> 00:01:51,690
少なくとも このデータセットでは

37
00:01:51,690 --> 00:01:54,140
離散値「男性」「 女性」です

38
00:01:54,140 --> 00:01:55,910
性別はカテゴリです

39
00:01:55,910 --> 00:01:59,080
このデータセットの性別列を
ラベルとして使うので

40
00:01:59,080 --> 00:02:01,400
これは分類問題になります

41
00:02:02,110 --> 00:02:06,670
分類問題では連続値を予測する代わりに

42
00:02:06,670 --> 00:02:11,560
異なるクラスを分ける
決定境界を作成しようと試みます

43
00:02:11,560 --> 00:02:16,710
この場合は2つの性別クラス
「女性」と「男性」があります

44
00:02:16,710 --> 00:02:21,900
線形の決定境界では
直線または多次元の超平面を形成して

45
00:02:21,900 --> 00:02:24,715
各クラスがどちらかの側に入ります

46
00:02:24,715 --> 00:02:27,090
たとえばチップ金額が

47
00:02:27,090 --> 00:02:31,205
請求金額の0.18倍より大きいならば

48
00:02:31,205 --> 00:02:34,475
支払った人は男性だと予測します

49
00:02:34,475 --> 00:02:36,620
この赤い線です

50
00:02:36,620 --> 00:02:39,455
でも このデータセットは不十分です

51
00:02:39,455 --> 00:02:42,780
男性の方が「ばらつき」が
大きいように見えます

52
00:02:42,780 --> 00:02:45,210
女性の方がばらつきが少ないです

53
00:02:45,210 --> 00:02:48,285
これは非線形決定境界の例です

54
00:02:48,285 --> 00:02:50,815
黄色い唇のような曲線です

55
00:02:50,815 --> 00:02:53,435
赤い境界よりも黄色い境界が良いと

56
00:02:53,435 --> 00:02:55,590
なぜ わかるのでしょうか

57
00:02:55,590 --> 00:02:58,180
分類問題では「誤差」つまり

58
00:02:58,180 --> 00:03:03,275
予測されるクラスと ラベルクラスの間の
誤分類を最小化します

59
00:03:03,275 --> 00:03:06,395
それには交差エントロピーを使います

60
00:03:06,395 --> 00:03:08,440
ところで チップを予測するとき

61
00:03:08,440 --> 00:03:11,805
正確な金額を予測しなくてよいかもしれません

62
00:03:11,805 --> 00:03:16,940
チップが多いか 普通か 少ないかが
分かればよいのです

63
00:03:16,940 --> 00:03:20,770
たとえば 請求額の25%を超えたら
「多い」と定義できます

64
00:03:20,770 --> 00:03:23,925
15～25%なら「平均的」

65
00:03:23,925 --> 00:03:26,990
15%未満なら「少ない」です

66
00:03:26,990 --> 00:03:30,535
言い換えると 金額を離散化できます

67
00:03:30,535 --> 00:03:33,890
こうしてチップ金額の予測 正確には

68
00:03:33,890 --> 00:03:37,560
チップクラスの予測は 分類問題になります

69
00:03:37,560 --> 00:03:43,222
通常は 生の連続的「特徴」を
カテゴリ特徴に離散化できます

70
00:03:43,225 --> 00:03:45,585
この専門分野の後半では

71
00:03:45,585 --> 00:03:48,105
逆のプロセスについてお話しします

72
00:03:48,105 --> 00:03:52,420
カテゴリ特徴を連続空間に
埋め込むことができるのです

73
00:03:52,420 --> 00:03:53,890
どちらを選ぶかは

74
00:03:53,890 --> 00:03:56,680
問題の性質と得られる結果で決まります

75
00:03:56,680 --> 00:03:59,890
機械学習では実験が欠かせません

76
00:04:00,370 --> 00:04:03,375
回帰と分類のどちらのタイプの問題も

77
00:04:03,375 --> 00:04:06,095
予測問題だと考えることができます

78
00:04:06,095 --> 00:04:11,125
対照的に 教師なし問題は
記述問題のようなものです

79
00:04:11,125 --> 00:04:13,805
さて このデータの出所は？

80
00:04:13,805 --> 00:04:16,724
このチップデータセットは

81
00:04:16,724 --> 00:04:20,070
行と列からなる構造化データです

82
00:04:20,070 --> 00:04:24,700
ML用の構造化データは 多くの場合
データウェアハウスから得られます

83
00:04:24,700 --> 00:04:29,910
画像 音声 ビデオなどは非構造化データです

84
00:04:29,910 --> 00:04:32,875
これは出産データを示しています

85
00:04:32,875 --> 00:04:35,495
医療情報の公開データセットです

86
00:04:35,495 --> 00:04:38,490
この一般公開データはBigQueryにあり

87
00:04:38,490 --> 00:04:40,810
この専門分野でも後で使用しますが

88
00:04:40,810 --> 00:04:44,680
今は これが皆さんの
データウェアハウスに入っているとします

89
00:04:44,680 --> 00:04:47,910
妊娠期間（週）を予測しましょう

90
00:04:47,910 --> 00:04:51,585
つまり 赤ちゃんが
いつ生まれるかを予測するのです

91
00:04:51,585 --> 00:04:57,110
BigQueryで SQL SELECT文を使って
MLデータセットが得られます

92
00:04:57,110 --> 00:04:59,380
モデルの入力となる特徴を選びます

93
00:04:59,380 --> 00:05:00,920
たとえば 母親の年齢

94
00:05:00,920 --> 00:05:02,310
体重の増加です

95
00:05:02,310 --> 00:05:04,825
妊娠期間（週）がラベルです

96
00:05:04,825 --> 00:05:08,040
妊娠期間は連続する数値なので

97
00:05:08,040 --> 00:05:10,760
これは回帰問題になります

98
00:05:10,760 --> 00:05:14,555
構造化データから予測する手法は
よく使われます

99
00:05:14,555 --> 00:05:18,470
この専門分野の最初の部分でも
これに焦点を当てます

100
00:05:18,470 --> 00:05:23,125
もちろん この医療データセットを使って
他の事柄も予測できます

101
00:05:23,125 --> 00:05:28,120
たとえば 他の属性を「特徴」として使って
赤ちゃんの体重を予測できるでしょう

102
00:05:28,120 --> 00:05:30,990
体重は健康状態の指標の1つです

103
00:05:30,990 --> 00:05:33,815
赤ちゃんの体重が少ないと予測されるなら

104
00:05:33,815 --> 00:05:37,445
病院は通常保育器などの機器を用意します

105
00:05:37,445 --> 00:05:40,755
ですから体重を予測できることは重要です

106
00:05:40,755 --> 00:05:43,400
ここでは赤ちゃんの体重がラベルです

107
00:05:43,400 --> 00:05:47,515
これは連続値で
浮動小数点値として格納されますから

108
00:05:47,515 --> 00:05:50,625
これは回帰問題になります

109
00:05:51,525 --> 00:05:53,280
さて このデータセットに使用できるのは

110
00:05:53,280 --> 00:05:57,995
線形回帰 線形分類
または両方のどれでしょうか

111
00:05:59,550 --> 00:06:01,625
正解は「両方」です

112
00:06:01,625 --> 00:06:03,250
なぜでしょうか

113
00:06:03,970 --> 00:06:07,320
データセットの色と直線を取り除くと

114
00:06:07,320 --> 00:06:09,390
2つのクラスが混ざり

115
00:06:09,390 --> 00:06:13,450
このように多数のノイズがある
1本の直線に見えます

116
00:06:13,450 --> 00:06:16,450
負の傾き 正の切片です

117
00:06:16,450 --> 00:06:18,655
直線のように見えますから

118
00:06:18,655 --> 00:06:22,365
たぶん線形回帰が適しているでしょう

119
00:06:22,365 --> 00:06:26,365
Yの値を予測するのです

120
00:06:27,485 --> 00:06:30,010
色と線を戻しました

121
00:06:30,010 --> 00:06:33,850
このデータセットが実際には
2本の直線であることが分かります

122
00:06:33,850 --> 00:06:36,570
ガウスノイズが少し含まれます

123
00:06:36,570 --> 00:06:42,460
2つの線の傾きと切片は少し異なり
ノイズの標準偏差も異なります

124
00:06:42,460 --> 00:06:45,400
私が皆さんに直線を見せている理由は

125
00:06:45,400 --> 00:06:50,910
ノイズ付き線形データセットとして
設計したと分かってもらうためです

126
00:06:50,910 --> 00:06:53,635
これは線形回帰のよい候補です

127
00:06:53,635 --> 00:06:56,640
2本の直線に分かれていますが

128
00:06:56,640 --> 00:07:00,350
まず一次元の線形回帰の結果を見てみましょう

129
00:07:00,350 --> 00:07:02,320
XからYをプロットして

130
00:07:02,320 --> 00:07:04,200
直感的な線を最初に作り

131
00:07:04,200 --> 00:07:06,695
その後で改善方法を見ていきます

132
00:07:08,575 --> 00:07:12,795
緑の線は線形回帰から得られる
近似一次方程式です

133
00:07:12,795 --> 00:07:16,555
どちらのクラス分布からも遠く離れていますね

134
00:07:16,555 --> 00:07:21,475
クラスBとクラスAが
互いに直線を引っ張り合うからです

135
00:07:21,475 --> 00:07:25,920
2つのクラス分布の間の空間を
切り離したような感じですね

136
00:07:25,920 --> 00:07:28,480
これは妥当な結果です なぜなら回帰では

137
00:07:28,480 --> 00:07:30,995
平均二乗誤差の損失を最適にします

138
00:07:30,995 --> 00:07:33,540
各クラスが均等に引っ張ると

139
00:07:33,540 --> 00:07:37,415
2つのクラス間の平均二乗誤差が最小になり

140
00:07:37,415 --> 00:07:40,500
それぞれの平均値からほぼ等距離になります

141
00:07:40,500 --> 00:07:45,015
しかし 各クラスは別々の線形分布です
傾きも切片も異なりますから

142
00:07:45,015 --> 00:07:47,520
クラスごとに線形回帰すれば

143
00:07:47,520 --> 00:07:50,540
もっと正確な予測に改善できるでしょう

144
00:07:50,540 --> 00:07:54,480
ここに描いた それぞれの線に
かなり近づくでしょう

145
00:07:54,480 --> 00:07:58,010
1つの特徴XからYの値を予測する

146
00:07:58,010 --> 00:08:01,770
一次元の線形回帰よりもっと優れた方法として

147
00:08:01,770 --> 00:08:04,485
二次元の線形回帰が可能です

148
00:08:04,485 --> 00:08:10,000
2つの特徴「X」と「その点のクラス」から
Yを予測するのです

149
00:08:10,000 --> 00:08:13,860
その点がクラスAに属するなら
クラス特徴値は「1」

150
00:08:13,860 --> 00:08:16,545
クラスBに属するなら「0」です

151
00:08:16,545 --> 00:08:21,030
一本の直線ではなく二次元の超平面ができます

152
00:08:21,030 --> 00:08:23,365
どうなるか見てみましょう

153
00:08:24,285 --> 00:08:27,645
2D線形回帰の結果です

154
00:08:27,645 --> 00:08:32,604
ラベルYを予測するために
Xとクラスの2つの特徴を使います

155
00:08:32,604 --> 00:08:36,319
このように2D超平面ができて

156
00:08:36,319 --> 00:08:39,840
2つのデータセットが
クラス次元ごとに分かれます

157
00:08:39,840 --> 00:08:44,720
クラスAとBの本当の線も含めました

158
00:08:44,720 --> 00:08:48,140
1D線形回帰の近似線もです

159
00:08:49,260 --> 00:08:52,545
平面にはどの直線も完全には含まれません

160
00:08:52,545 --> 00:08:55,980
ノイズが原因で平面の2つの
傾きが歪んでいるためです

161
00:08:55,980 --> 00:08:58,235
もしノイズがなければ

162
00:08:58,235 --> 00:09:01,840
3本の線は完全に平面に含まれるでしょう

163
00:09:01,840 --> 00:09:04,080
さらに 先程のクイズの

164
00:09:04,080 --> 00:09:07,865
もう1つの答えも出たことになります
「分類」です

165
00:09:07,865 --> 00:09:09,910
この線形回帰線が

166
00:09:09,910 --> 00:09:12,960
クラスを分けてくれたからです

167
00:09:12,960 --> 00:09:16,875
ですから 線形分類としても良い候補です

168
00:09:16,875 --> 00:09:22,440
でも1D線形回帰の近似線は
本当に正確な決定境界ですか？

169
00:09:22,440 --> 00:09:24,285
確かめましょう

170
00:09:25,805 --> 00:09:28,350
黄色い線は一次元線形分類器

171
00:09:28,350 --> 00:09:30,800
つまりロジスティック回帰の出力です

172
00:09:30,800 --> 00:09:34,340
緑の線形回帰線に非常に近いですが

173
00:09:34,340 --> 00:09:37,740
そうではありません なぜでしょう

174
00:09:37,740 --> 00:09:39,450
覚えていますか

175
00:09:39,450 --> 00:09:43,880
回帰モデルでは損失関数として
平均二乗誤差を使います

176
00:09:43,880 --> 00:09:47,230
分類モデルでは交差エントロピーを使います

177
00:09:47,230 --> 00:09:49,715
この2つの違いは？

178
00:09:50,095 --> 00:09:52,425
複雑な説明を避けると

179
00:09:52,425 --> 00:09:55,720
平均二乗誤差には二次ペナルティがあります

180
00:09:55,720 --> 00:09:58,290
つまり 実際のラベルと予測ラベルの間の

181
00:09:58,290 --> 00:10:01,245
ユークリッド距離を最小化するのです

182
00:10:01,245 --> 00:10:04,485
一方 分類の交差エントロピーでは

183
00:10:04,485 --> 00:10:10,110
予測確率が実際のラベルに近いときは
ペナルティがほぼ線形です

184
00:10:10,110 --> 00:10:13,215
しかし遠くなると指数的に増えます

185
00:10:13,215 --> 00:10:16,610
そして 逆のクラスの予測に近づきます

186
00:10:16,610 --> 00:10:19,320
そこで この図をよく見ると

187
00:10:19,320 --> 00:10:23,470
分類決定境界線の方が傾きが少し急です

188
00:10:23,470 --> 00:10:25,115
その理由は

189
00:10:25,115 --> 00:10:28,000
赤い点の中にノイズが大きいものがあり

190
00:10:28,000 --> 00:10:30,115
赤の分布ノイズが大きく

191
00:10:30,115 --> 00:10:35,535
決定境界の向こう側に入り
高い誤差寄与の損失になるのです

192
00:10:35,535 --> 00:10:38,005
これらの点は直線にかなり近いので

193
00:10:38,005 --> 00:10:41,615
線形回帰では誤差寄与が小さいでしょう

194
00:10:41,615 --> 00:10:45,150
なぜなら 誤差が二次関数であるだけでなく

195
00:10:45,150 --> 00:10:50,080
回帰線のどちら側に点があっても
かまわないからです

196
00:10:50,080 --> 00:10:53,295
距離を最小化できればいいのです

197
00:10:53,295 --> 00:10:56,000
ですから このデータセットは

198
00:10:56,000 --> 00:10:59,970
線形回帰と線形分類の両方に適しています

199
00:10:59,970 --> 00:11:02,450
一方 先程のチップ金額データセットは

200
00:11:02,450 --> 00:11:04,680
線形回帰だけに適しており

201
00:11:04,680 --> 00:11:08,250
分類は非線形にした方がよいでしょう