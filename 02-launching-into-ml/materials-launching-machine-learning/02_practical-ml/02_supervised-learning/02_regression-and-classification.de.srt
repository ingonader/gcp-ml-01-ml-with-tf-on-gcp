1
00:00:00,000 --> 00:00:01,830
Wir haben uns das
Trinkgeld-Dataset bereits angesehen

2
00:00:01,830 --> 00:00:04,725
und gesagt, dass wir entweder den
Trinkgeldbetrag

3
00:00:04,725 --> 00:00:06,689
oder das Geschlecht des Kunden
als Label verwenden können.

4
00:00:06,689 --> 00:00:11,035
In Option 1 nehmen wir den Trinkgeldbetrag
als Label und möchten ihn anhand

5
00:00:11,035 --> 00:00:13,240
der anderen Merkmale
im Dataset vorhersagen.

6
00:00:13,240 --> 00:00:16,239
Nehmen wir an,
Sie verwenden nur ein Merkmal,

7
00:00:16,239 --> 00:00:18,955
nämlich den Gesamtrechnungsbetrag,
zur Vorhersage des Trinkgelds.

8
00:00:18,955 --> 00:00:21,255
Da das Trinkgeld eine
kontinuierliche Zahl ist,

9
00:00:21,255 --> 00:00:23,175
ist dies ein Regressionsproblem.

10
00:00:23,175 --> 00:00:25,640
Das Ziel bei Regressionsproblemen ist es,

11
00:00:25,640 --> 00:00:28,330
mit mathematischen Funktionen
und verschiedenen Kombinationen

12
00:00:28,330 --> 00:00:31,375
von Merkmalen den kontinuierlichen
Wert des Labels vorherzusagen.

13
00:00:31,375 --> 00:00:33,370
Das wird durch die Linie dargestellt,

14
00:00:33,370 --> 00:00:36,920
wobei ein Rechnungsbetrag
multipliziert mit der Kurvensteigung

15
00:00:36,920 --> 00:00:39,240
einen kontinuierlichen Wert als
Trinkgeld ergibt.

16
00:00:39,240 --> 00:00:43,340
Nehmen wir an, die durchschnittliche
Trinkgeldhöhe beträgt 18 % des Rechnungsbetrags.

17
00:00:43,340 --> 00:00:46,270
Dann ist die Kurvensteigung 0,18.

18
00:00:46,270 --> 00:00:51,410
Die Multiplikation des Rechnungsbetrags
mit 0,18 ergibt die Trinkgeldvorhersage.

19
00:00:51,410 --> 00:00:56,370
Diese Regression mit einem Merkmal lässt
sich auf weitere Merkmale ausweiten.

20
00:00:56,370 --> 00:00:59,570
In diesem Fall haben wir ein
mehrdimensionales Problem,

21
00:00:59,570 --> 00:01:01,270
aber das Prinzip ist dasselbe.

22
00:01:01,270 --> 00:01:07,040
Der Wert jedes Merkmals multipliziert
mit der Steigung einer Hyperebene,

23
00:01:07,040 --> 00:01:11,610
der Generalisierung einer Geraden, ergibt
einen kontinuierlichen Wert für das Label.

24
00:01:11,610 --> 00:01:14,040
Bei Regressionsproblemen möchten wir den

25
00:01:14,040 --> 00:01:16,670
Fehler zwischen dem
vorausgesagten kontinuierlichen Wert

26
00:01:16,670 --> 00:01:18,750
und dem kontinuierlichen Wert des Labels

27
00:01:18,750 --> 00:01:21,900
mithilfe der mittleren quadratischen
Abweichung minimieren.

28
00:01:21,900 --> 00:01:26,170
In Option 2 verwenden wir
das Geschlecht als Label und

29
00:01:26,170 --> 00:01:30,200
sagen das Geschlecht des Kunden mit Daten
aus Trinkgeld und Rechnungsbetrag voraus.

30
00:01:30,200 --> 00:01:34,325
Wie wir an den Daten sehen,
ist das natürlich eine schlechte Idee.

31
00:01:34,325 --> 00:01:37,170
Die Daten für Männer und Frauen
sind nicht wirklich getrennt

32
00:01:37,170 --> 00:01:39,990
und wir würden ein
fürchterliches Modell erhalten.

33
00:01:39,990 --> 00:01:43,170
Aber der Versuch hilft mir
zu veranschaulichen, was

34
00:01:43,170 --> 00:01:47,780
passiert, wenn die Zielantwort nicht
kontinuierlich, sondern kategorial ist.

35
00:01:47,780 --> 00:01:50,040
Die möglichen Werte
für die Spalte "Geschlecht",

36
00:01:50,040 --> 00:01:51,480
zumindest in diesem Dataset,

37
00:01:51,480 --> 00:01:54,140
sind überschaubar: männlich oder weiblich.

38
00:01:54,140 --> 00:01:55,970
Da das Geschlecht kategorisch ist

39
00:01:55,970 --> 00:01:59,040
und wir die Spalte "Geschlecht"
des Datasets als Label nehmen,

40
00:01:59,040 --> 00:02:01,680
ist dies ein Klassifikationsproblem.

41
00:02:01,680 --> 00:02:06,760
Bei Klassifikationsproblemen
suchen wir keine kontinuierliche Variable,

42
00:02:06,760 --> 00:02:11,560
sondern möchten eine Entscheidungsgrenze
zur Trennung der Klassen erstellen.

43
00:02:11,560 --> 00:02:16,710
In diesem Fall haben wir also
zwei Klassen: weiblich und männlich.

44
00:02:16,710 --> 00:02:22,030
Eine lineare Entscheidungsgrenze bildet
eine Gerade oder Hyperebene (bei mehreren

45
00:02:22,030 --> 00:02:24,295
Dimensionen) mit einer Klasse auf jeder Seite.

46
00:02:24,295 --> 00:02:27,190
Wenn zum Beispiel das Trinkgeld höher als

47
00:02:27,190 --> 00:02:30,735
0,18 multipliziert mit 
dem Rechnungsbetrag ist,

48
00:02:30,735 --> 00:02:34,475
ergibt die Prognose,
dass ein Mann bezahlt hat.

49
00:02:34,475 --> 00:02:36,620
Das zeigt die rote Linie.

50
00:02:36,620 --> 00:02:39,455
Das funktioniert aber bei
diesem Dataset nicht sehr gut.

51
00:02:39,455 --> 00:02:42,640
Männer haben einen
höheren Schwankungsbereich,

52
00:02:42,640 --> 00:02:45,210
während Frauen ähnlicheres Trinkgeld geben.

53
00:02:45,210 --> 00:02:48,285
Dies ist ein Beispiel für eine
nicht lineare Entscheidungsgrenze,

54
00:02:48,285 --> 00:02:50,425
dargestellt durch die gelben Bögen im Diagramm.

55
00:02:50,425 --> 00:02:53,175
Woher wissen wir,
dass die rote Grenze schlecht

56
00:02:53,175 --> 00:02:55,530
und die gelbe besser ist?

57
00:02:55,530 --> 00:02:59,390
Bei Klassifikationsproblemen möchten wir
den Fehler bzw. die Fehlklassifikation

58
00:02:59,390 --> 00:03:03,275
zwischen der vorhergesagten Klasse 
und der Klasse des Labels minimieren.

59
00:03:03,275 --> 00:03:06,485
Dazu verwendet man
normalerweise Kreuzentropie.

60
00:03:06,485 --> 00:03:08,860
Auch wenn wir
das Trinkgeld voraussagen,

61
00:03:08,860 --> 00:03:11,305
müssen wir vielleicht nicht
die genaue Höhe wissen.

62
00:03:11,305 --> 00:03:16,940
Vielmehr interessiert uns, ob das
Trinkgeld hoch, mittel oder niedrig ist.

63
00:03:16,940 --> 00:03:20,770
Als hoch könnten wir
Beträge über 25 % definieren,

64
00:03:20,770 --> 00:03:24,055
als mittel Beträge zwischen 15 und 25 %

65
00:03:24,055 --> 00:03:26,890
und als niedriges Trinkgeld
Beträge unter 15 %.

66
00:03:26,890 --> 00:03:30,485
Sprich, wir könnten
in Richtung des Betrags diskretisieren.

67
00:03:30,485 --> 00:03:33,980
Und jetzt wird die
Prognose der Trinkgeldhöhe oder besser

68
00:03:33,980 --> 00:03:37,650
der Trinkgeldklasse
zu einem Klassifikaitonsproblem.

69
00:03:37,650 --> 00:03:43,255
Ein rohes kontinuierliches Merkmal lässt
sich in ein kategorisches diskretisieren.

70
00:03:43,255 --> 00:03:45,865
Im Verlauf dieses Kurses

71
00:03:45,865 --> 00:03:48,105
gehen wir noch genauer auf
den umgekehrten Prozess ein.

72
00:03:48,105 --> 00:03:52,420
Ein kategorisches Merkmal kann in einen
kontinuierlichen Raum eingebettet werden.

73
00:03:52,420 --> 00:03:55,110
Das hängt von dem Problem ab,
das Sie zu lösen versuchen,

74
00:03:55,110 --> 00:03:56,860
und davon, was am besten funktioniert.

75
00:03:56,860 --> 00:03:59,970
Beim maschinellen Lernen
geht es um Versuche.

76
00:03:59,970 --> 00:04:03,645
Beide Problemtypen,
Regression und Klassifikation,

77
00:04:03,645 --> 00:04:06,095
können wir uns als
Vorhersageprobleme vorstellen.

78
00:04:06,095 --> 00:04:11,125
Unüberwachte Probleme hingegen
eher als Beschreibungsprobleme.

79
00:04:11,125 --> 00:04:13,805
Doch wo kommen eigentlich all diese Daten her?

80
00:04:13,805 --> 00:04:16,964
Etwas wie dieses Trinkgeld-Dataset
nennen wir strukturierte Daten,

81
00:04:16,964 --> 00:04:20,120
die aus Zeilen und Spalten bestehen.

82
00:04:20,120 --> 00:04:24,620
Eine sehr übliche Quelle für strukturierte
Daten bei ML ist Ihr Data Warehouse.

83
00:04:24,620 --> 00:04:29,910
Unstrukturierte Daten sind zum Beispiel
Bilder, Audio- oder Videodaten.

84
00:04:29,910 --> 00:04:32,775
Hier zeige ich Ihnen ein
Geburtenraten-Dataset,

85
00:04:32,775 --> 00:04:35,455
ein öffentliches Dataset
mit medizinischen Daten.

86
00:04:35,455 --> 00:04:38,420
Dabei handelt es sich um ein
öffentlich verfügbares Dataset in BigQuery,

87
00:04:38,420 --> 00:04:40,810
das wir später in diesem Kurs verwenden.

88
00:04:40,810 --> 00:04:44,500
Vorerst nehmen wir an, dass dieses
Dataset in unserem Data Warehouse ist.

89
00:04:44,500 --> 00:04:47,750
Wir möchten nun für ein Baby
die Schwangerschaftswoche vorhersagen.

90
00:04:47,750 --> 00:04:51,585
Genauer gesagt, vorhersagen,
wann das Baby geboren wird.

91
00:04:51,585 --> 00:04:57,110
Sie können mit einer SQL-SELECT-Anweisung
in BigQuery ein ML-Dataset erstellen.

92
00:04:57,110 --> 00:04:59,310
Wir wählen Eingabemerkmale
für das Modell aus,

93
00:04:59,310 --> 00:05:00,820
wie Alter der Mutter,

94
00:05:00,820 --> 00:05:02,310
Gewichtszunahme

95
00:05:02,310 --> 00:05:04,695
und das Label, die Schwangerschaftswoche.

96
00:05:04,695 --> 00:05:08,040
Da die Schwangerschaftswoche
eine kontinuierliche Zahl ist,

97
00:05:08,040 --> 00:05:10,760
ist dies ein Regressionsproblem.

98
00:05:10,760 --> 00:05:14,555
Vorhersagen aufgrund von strukturierten
Daten zu treffen, ist sehr gebräuchlich

99
00:05:14,555 --> 00:05:18,370
und darauf haben wir uns im ersten Teil
dieses Kurses konzentriert.

100
00:05:18,370 --> 00:05:23,125
Natürlich dient dieses medizinische
Dataset auch für andere Prognosen.

101
00:05:23,125 --> 00:05:28,190
Wir möchten vielleicht das Babygewicht
mit den anderen Attributen voraussagen.

102
00:05:28,190 --> 00:05:30,990
Das Babygewicht kann ein
Gesundheitsindikator sein.

103
00:05:30,990 --> 00:05:33,815
Wenn ein niedriges
Geburtsgewicht erwartet wird,

104
00:05:33,815 --> 00:05:37,565
stellt das Krankenhaus normalerweise
Geräte wie einen Inkubator bereit.

105
00:05:37,565 --> 00:05:40,795
Es kann also wichtig sein, das
Babygewicht voraussagen zu können.

106
00:05:40,795 --> 00:05:43,050
Das Label ist hier das Babygewicht

107
00:05:43,050 --> 00:05:45,055
und das ist eine kontinuierliche Variable.

108
00:05:45,055 --> 00:05:51,405
Sie ist als Gleitkommazahl gespeichert,
also ist das ein Regressionsproblem.

109
00:05:51,405 --> 00:05:52,380
Ist dieses Dataset

110
00:05:52,380 --> 00:05:54,525
ein guter Kandidat für lineare Regression

111
00:05:54,525 --> 00:05:57,920
oder für lineare Klassifikation?

112
00:05:57,920 --> 00:06:01,625
Die Antwort ist: beides.

113
00:06:01,625 --> 00:06:07,460
Schauen wir, weshalb. Hier sind
beide Klassen im Dataset vermischt.

114
00:06:07,460 --> 00:06:10,210
Ohne die verschiedenen
Farben und Formen als Hilfe

115
00:06:10,210 --> 00:06:15,900
ist das eine unklare Linie mit negativer 
Steigung und positivem Achsenabschnitt.

116
00:06:15,900 --> 00:06:18,325
Da es ziemlich linear aussieht,

117
00:06:18,325 --> 00:06:22,575
ist dies wahrscheinlich ein guter
Kandidat für lineare Regression,

118
00:06:22,575 --> 00:06:26,675
bei der unser Prognoseziel
der Wert für Y ist.

119
00:06:26,675 --> 00:06:30,300
Bringen wir wieder Farben und Formen ein,

120
00:06:30,300 --> 00:06:33,040
wird noch deutlicher,
dass dieses Dataset eigentlich aus

121
00:06:33,040 --> 00:06:36,490
zwei linearen Serien mit etwas
Gaußschem Rauschen besteht.

122
00:06:36,490 --> 00:06:39,795
Die Geraden haben leicht andere
Steigungen und andere Achsenabschnitte

123
00:06:39,795 --> 00:06:42,500
und das Rauschen unterschiedliche
Standardabweichungen.

124
00:06:42,500 --> 00:06:45,890
Ich habe die Geraden so festgelegt,
um Ihnen zu zeigen, dass dies ganz

125
00:06:45,890 --> 00:06:50,370
eindeutig ein lineares Dataset ist,
nur etwas verrauscht.

126
00:06:50,370 --> 00:06:53,565
Dies wäre ein guter
Kandidat für lineare Regression.

127
00:06:53,565 --> 00:06:56,430
Es liegen uns zwar zwei
unterschiedliche lineare Serien vor,

128
00:06:56,430 --> 00:07:00,350
aber wir betrachten zuerst das Ergebnis
einer eindimensionalen linearen Regression

129
00:07:00,350 --> 00:07:02,320
und plotten Y von X,

130
00:07:02,320 --> 00:07:04,200
um eine Intuition aufzubauen.

131
00:07:04,200 --> 00:07:07,325
Dann sehen wir, ob es noch besser geht.

132
00:07:07,325 --> 00:07:12,605
Die grüne Linie hier ist die angepasste
lineare Gleichung aus linearer Regression.

133
00:07:12,605 --> 00:07:16,555
Sie ist weit von den einzelnen
Klassenverteilungen entfernt,

134
00:07:16,555 --> 00:07:21,575
da Klasse B die Linie von Klasse A
wegzieht und umgekehrt.

135
00:07:21,575 --> 00:07:25,970
Sie halbiert ungefähr den Raum
zwischen den zwei Verteilungen.

136
00:07:25,970 --> 00:07:27,900
Das ist sinnvoll, da wir bei Regression

137
00:07:27,900 --> 00:07:30,875
unseren Verlust der mittleren
quadratischen Abweichung optimieren.

138
00:07:30,875 --> 00:07:33,180
So sollte bei gleichem
Zug durch jede Klasse

139
00:07:33,180 --> 00:07:37,485
die Regression die niedrigste MQA
zwischen den zwei Klassen haben,

140
00:07:37,485 --> 00:07:40,460
die etwa gleich weit von ihren
Mittelwerten entfernt sind.

141
00:07:40,460 --> 00:07:44,935
Da die Klassen unterschiedliche lineare
Serien mit eigener Steigung und eigenem

142
00:07:44,935 --> 00:07:47,840
Achsenabschnitt sind, hätten wir eine
viel höhere Genauigkeit,

143
00:07:47,840 --> 00:07:50,730
wenn wir eine lineare Regression
pro Klasse vornehmen würden,

144
00:07:50,730 --> 00:07:54,480
die ziemlich genau auf die hier
dargestellten Linien passen sollte.

145
00:07:54,480 --> 00:07:57,010
Noch besser als die Durchführung einer

146
00:07:57,010 --> 00:08:01,770
eindimensionalen linearen Regression zur
Prognose des Werts von Y aus dem Merkmal X

147
00:08:01,770 --> 00:08:07,435
wäre eine zweidimensionale lineare
Regression zur Prognose von Y aus den zwei

148
00:08:07,435 --> 00:08:10,000
Merkmalen X und der Klasse des Punkts.

149
00:08:10,000 --> 00:08:13,710
Das Klassenmerkmal könnte eine 1 sein,
wenn der Punkt zu Klasse A gehört,

150
00:08:13,710 --> 00:08:16,545
und 0, wenn er zu Klasse B gehört.

151
00:08:16,545 --> 00:08:21,030
Anstelle einer Linie würde
dies eine 2D-Hyperebene bilden.

152
00:08:21,030 --> 00:08:23,535
Schauen wir einmal, wie das aussieht.

153
00:08:23,535 --> 00:08:27,165
Hier sind die Ergebnisse
der zweidimensionalen linearen Regression.

154
00:08:27,165 --> 00:08:32,684
Zur Prognose des Labels Y haben wir
die zwei Merkmale X und Klasse verwendet.

155
00:08:32,684 --> 00:08:36,039
Wie Sie sehen, hat sich
eine 2D-Hyperebene zwischen den

156
00:08:36,039 --> 00:08:39,840
zwei Datasets gebildet, die nun
durch die Klassendimension getrennt sind.

157
00:08:39,840 --> 00:08:44,830
Ich habe auch die Linien für
Klasse A und Klasse B eingebunden

158
00:08:44,830 --> 00:08:48,670
sowie die Linie
der eindimensionalen linearen Regression.

159
00:08:48,670 --> 00:08:52,545
Die Ebene enthält keine der Geraden ganz,

160
00:08:52,545 --> 00:08:55,870
da das Datenrauschen
die zwei Steigungen der Ebene kippt.

161
00:08:55,870 --> 00:08:58,275
Anderenfalls, ohne Rauschen,

162
00:08:58,275 --> 00:09:01,390
lägen alle drei
Linien perfekt in der Ebene.

163
00:09:01,390 --> 00:09:04,590
Außerdem haben wir bereits eine Antwort

164
00:09:04,590 --> 00:09:07,595
auf den anderen Teil der
Quizfrage zur linearen Klassifikation,

165
00:09:07,595 --> 00:09:09,650
denn die lineare Regressionsgerade

166
00:09:09,650 --> 00:09:12,960
trennt die Klassen
auf effektive Art und Weise.

167
00:09:12,960 --> 00:09:17,005
Dies ist also auch ein sehr guter
Kandidat für die lineare Klassifikation.

168
00:09:17,005 --> 00:09:20,600
Würde sie aber eine 
Entscheidungsgrenze genau

169
00:09:20,600 --> 00:09:24,145
auf der Linie der
eindimensionalen linearen Regression bilden? Mal sehen.

170
00:09:24,145 --> 00:09:27,010
Die gelbe Linie ist das Ergebnis eines

171
00:09:27,010 --> 00:09:30,180
eindimensionalen linearen Klassifikators:
logistische Regression.

172
00:09:30,180 --> 00:09:34,290
Sie liegt sehr nahe an der
Geraden der linearen Regression,

173
00:09:34,290 --> 00:09:37,740
aber nicht genau darauf. 
Woran könnte das liegen?

174
00:09:37,740 --> 00:09:40,550
Ich habe bereits erwähnt,
dass Regressionsmodelle

175
00:09:40,550 --> 00:09:43,820
oft die mittlere quadratische
Abweichung als Verlustfunktion verwenden

176
00:09:43,820 --> 00:09:47,230
und Klassifikationsmodelle
normalerweise die Kreuzentropie.

177
00:09:47,230 --> 00:09:49,545
Wodurch unterscheiden sich nun die beiden?

178
00:09:49,545 --> 00:09:53,005
Ohne zu sehr ins Detail zu gehen,

179
00:09:53,005 --> 00:09:55,680
gibt es bei der MQA
einen quadratischen Abzug.

180
00:09:55,680 --> 00:09:57,340
Es wird also im Grunde versucht,

181
00:09:57,340 --> 00:10:01,095
den euklidischen Abstand zwischen Antwort
und vorausgesagtem Ziel zu minimieren.

182
00:10:01,095 --> 00:10:06,255
Bei der Kreuzentropie der Klassifikation
hingegen ist der Abzug fast linear

183
00:10:06,255 --> 00:10:10,070
und die vorausgesagte Wahrscheinlichkeit
liegt nahe an der tatsächlichen Antwort.

184
00:10:10,070 --> 00:10:13,375
Aber mit zunehmender Distanz
wird sie exponentiell,

185
00:10:13,375 --> 00:10:16,840
bei Annäherung an die Vorhersage der
entgegengesetzten Klasse des Labels.

186
00:10:16,840 --> 00:10:19,200
Betrachten wir die Darstellung,

187
00:10:19,200 --> 00:10:20,700
ist der wahrscheinlichste Grund

188
00:10:20,700 --> 00:10:25,055
für die leicht negativere Steigung der
Entscheidungsgrenze der Klassifizierung

189
00:10:25,055 --> 00:10:28,000
der, dass einige der roten Punkte

190
00:10:28,000 --> 00:10:29,945
(wobei Rot die Rauschverteilung ist)

191
00:10:29,945 --> 00:10:35,705
jenseits der Entscheidungsgrenze liegen
und ihren hohen Fehlerbeitrag verlieren.

192
00:10:35,705 --> 00:10:38,005
Da sie so nahe an der Geraden liegen,

193
00:10:38,005 --> 00:10:41,615
wäre ihr Fehlerbeitrag bei
linearer Regression geringer,

194
00:10:41,615 --> 00:10:45,150
weil der Fehler quadratisch ist

195
00:10:45,150 --> 00:10:50,080
und bei Regression egal ist, auf welcher
Seite der Geraden ein Punkt liegt,

196
00:10:50,080 --> 00:10:53,015
solange der Abstand
so klein wie möglich ist.

197
00:10:53,015 --> 00:10:54,890
Wie Sie also sehen,

198
00:10:54,890 --> 00:10:59,970
ist dieses Dataset perfekt für lineare
Regression und lineare Klassifikation.

199
00:10:59,970 --> 00:11:02,340
Im Gegensatz zum Trinkgeld-Dataset,

200
00:11:02,340 --> 00:11:04,680
das für lineare Regression
lediglich akzeptabel und

201
00:11:04,680 --> 00:11:08,000
besser für eine nicht
lineare Klassifikation geeignet war.