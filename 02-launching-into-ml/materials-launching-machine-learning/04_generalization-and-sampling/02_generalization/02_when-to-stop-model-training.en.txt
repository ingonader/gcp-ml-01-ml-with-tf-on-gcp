So, in addition to helping you choose between two different ML models, like, should I use linear regression or a neural network? You can also use your validation data-set to help fine tune those hyper-parameters of a single model, which if you recall, those hyper-parameters are set before training. This tuning process is accomplished through successive training runs, and then comparing those training runs against that independent validation data-set to check for over-fitting. Here's how your validation set will actually be used during training. As you saw when we covered during optimization, training the model is where we start to calculate random weights, calculate that derivative, look at the direction down the gradient descent loss curve, minimize your loss metric, and then repeat. And periodically, you want to assess the performance of a model against data that has not yet seen in training, which is where we use the validation data-set. After a completed training run has happened, validate that model's results against your validation data-set to see if those hyper-parameters are any good, or if you can tune them a little bit more. And if there's not a significant divergence between the loss metrics from the training run, and the loss metrics for the validation data-set run, then we could potentially go back and optimize our hyper-parameters a little bit more. Now, once the loss metrics from our model have been sufficiently optimized and passed the validation data-set, remember when you start to see that divergence and you confirm that the model is not over-fitting, that's when we know we need to stop and say, our model is tuned, ready for production. Now, you can use a loop similar to this one to also figure out what model parameters for your individual models, like what we did for the hyper-parameters that we set before training. For example, if the layers of a network or the number of nodes that you should use. Essentially, you'll train with one configuration like six nodes in your neural network, and then train against another one, and then evaluate to see which one performs better on your validation data-set. And you're going to end up choosing a model configuration that results in a lower loss in the validation data-set, not the model configuration that results in a lower loss on the training one. Now later in this specialization, we're going to show you how Cloud ML engine can carry out a bayesian short search through a hyper-parameter space, so you don't have to do this kind of experimentation one hyper-parameter at a time. Now, Cloud Machine Learning engine can help us do this sort of experimentation in a parallel fashion using a different optimized strategy. Now, once you've done your training, you need to tell your boss how well is your model doing. What data-set are you going to use for that final go or no-go evaluation? Can you just simply report the loss or the error on your validation data-set even if it's consistent with your training data-set? Actually you can't. Why not? Well, because you used your validation data-set to choose when you should stop the training. It's no longer independent. The model has seen it. So, what do you have to do? Well, you actually have to split your data into three parts, training, validation, and a brand new completely isolated silo called testing. Once your model has been trained and validated, then you can write it once, and only once against the independent test data-set. And that's the loss metric that you can report to your boss. And it's the loss metric that then on your testing data-set, decides whether or not you want to use this model in production. So what happens if you fail on your testing data-set, even though you passed validation? Means you can't retest the same ML model, and you've got to either retrain a brand new Machine Learning model, or go back to the drawing board and collect more data samples to provide new data for your ML model. Now, while this is a good approach, there's one tiny problem. Nobody likes to waste data, and it seems like the test data is essentially wasted. I'm only using it once, it's held-out. Can't you use all your data in training, and still get a reasonable indication of how well your model is going to perform? Well, the answer is you can. The compromise between these methods, is to do a training validation split and do that many different times. Train and then compute the loss in the validation data-set, keeping in mind this validation set could consist of points that were not used in training the first time, then split the data again. Now you're training data might include some points that you used in your original validation on that first run, but you're completely doing multiple iterations. And then finally after a few rounds of this, this blending, you average the validation loss metrics across the board. And you'll get a standard deviation of the validation losses, and it'd be able to help you analyze that spread and go with the final number. This process is called bootstrapping or cross-validation. The upside is, you get to use all data, but you have to train lots and lots more times because you're creating more of the splits. So, at the end of the day, here's what you have to remember. If you have lots of data, use the approach of having a completely independent held-out test data-set, that's like go or no-go decision. If you don't have that much data, use the cross-validation approach. So, how do you go about actually splitting these large data-sets into these silos that we keep talking about? Well, that's the topic for our next lesson on sampling.