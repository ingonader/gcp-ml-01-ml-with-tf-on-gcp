1
00:00:00,000 --> 00:00:04,275
So, in addition to helping you choose between two different ML models,

2
00:00:04,275 --> 00:00:06,865
like, should I use linear regression or a neural network?

3
00:00:06,865 --> 00:00:09,570
You can also use your validation data-set to help

4
00:00:09,570 --> 00:00:12,160
fine tune those hyper-parameters of a single model,

5
00:00:12,160 --> 00:00:13,620
which if you recall,

6
00:00:13,620 --> 00:00:15,765
those hyper-parameters are set before training.

7
00:00:15,765 --> 00:00:19,780
This tuning process is accomplished through successive training runs,

8
00:00:19,780 --> 00:00:22,315
and then comparing those training runs against

9
00:00:22,315 --> 00:00:26,470
that independent validation data-set to check for over-fitting.

10
00:00:26,470 --> 00:00:30,645
Here's how your validation set will actually be used during training.

11
00:00:30,645 --> 00:00:33,225
As you saw when we covered during optimization,

12
00:00:33,225 --> 00:00:36,470
training the model is where we start to calculate random weights,

13
00:00:36,470 --> 00:00:40,625
calculate that derivative, look at the direction down the gradient descent loss curve,

14
00:00:40,625 --> 00:00:43,290
minimize your loss metric, and then repeat.

15
00:00:43,290 --> 00:00:46,130
And periodically, you want to assess the performance of

16
00:00:46,130 --> 00:00:49,115
a model against data that has not yet seen in training,

17
00:00:49,115 --> 00:00:52,595
which is where we use the validation data-set.

18
00:00:52,595 --> 00:00:54,985
After a completed training run has happened,

19
00:00:54,985 --> 00:00:57,880
validate that model's results against

20
00:00:57,880 --> 00:01:01,220
your validation data-set to see if those hyper-parameters are any good,

21
00:01:01,220 --> 00:01:02,490
or if you can tune them a little bit more.

22
00:01:02,490 --> 00:01:03,690
And if there's not

23
00:01:03,690 --> 00:01:06,520
a significant divergence between the loss metrics from the training run,

24
00:01:06,520 --> 00:01:09,400
and the loss metrics for the validation data-set run,

25
00:01:09,400 --> 00:01:13,520
then we could potentially go back and optimize our hyper-parameters a little bit more.

26
00:01:13,520 --> 00:01:16,530
Now, once the loss metrics from our model have been

27
00:01:16,530 --> 00:01:19,500
sufficiently optimized and passed the validation data-set,

28
00:01:19,500 --> 00:01:21,405
remember when you start to see that divergence

29
00:01:21,405 --> 00:01:23,465
and you confirm that the model is not over-fitting,

30
00:01:23,465 --> 00:01:25,950
that's when we know we need to stop and say,

31
00:01:25,950 --> 00:01:29,290
our model is tuned, ready for production.

32
00:01:29,290 --> 00:01:32,500
Now, you can use a loop similar to this one to also figure

33
00:01:32,500 --> 00:01:35,305
out what model parameters for your individual models,

34
00:01:35,305 --> 00:01:38,190
like what we did for the hyper-parameters that we set before training.

35
00:01:38,190 --> 00:01:42,500
For example, if the layers of a network or the number of nodes that you should use.

36
00:01:42,500 --> 00:01:46,340
Essentially, you'll train with one configuration like six nodes in your neural network,

37
00:01:46,340 --> 00:01:47,820
and then train against another one,

38
00:01:47,820 --> 00:01:51,600
and then evaluate to see which one performs better on your validation data-set.

39
00:01:51,600 --> 00:01:54,020
And you're going to end up choosing a model configuration

40
00:01:54,020 --> 00:01:56,725
that results in a lower loss in the validation data-set,

41
00:01:56,725 --> 00:02:00,995
not the model configuration that results in a lower loss on the training one.

42
00:02:00,995 --> 00:02:03,050
Now later in this specialization,

43
00:02:03,050 --> 00:02:05,740
we're going to show you how Cloud ML engine can carry

44
00:02:05,740 --> 00:02:08,920
out a bayesian short search through a hyper-parameter space,

45
00:02:08,920 --> 00:02:13,490
so you don't have to do this kind of experimentation one hyper-parameter at a time.

46
00:02:13,490 --> 00:02:16,140
Now, Cloud Machine Learning engine can help us do this sort of

47
00:02:16,140 --> 00:02:21,395
experimentation in a parallel fashion using a different optimized strategy.

48
00:02:21,395 --> 00:02:23,870
Now, once you've done your training,

49
00:02:23,870 --> 00:02:27,165
you need to tell your boss how well is your model doing.

50
00:02:27,165 --> 00:02:31,365
What data-set are you going to use for that final go or no-go evaluation?

51
00:02:31,365 --> 00:02:34,430
Can you just simply report the loss or the error on your validation

52
00:02:34,430 --> 00:02:37,835
data-set even if it's consistent with your training data-set?

53
00:02:37,835 --> 00:02:40,920
Actually you can't. Why not?

54
00:02:40,920 --> 00:02:43,950
Well, because you used your validation

55
00:02:43,950 --> 00:02:47,055
data-set to choose when you should stop the training.

56
00:02:47,055 --> 00:02:48,980
It's no longer independent.

57
00:02:48,980 --> 00:02:50,335
The model has seen it.

58
00:02:50,335 --> 00:02:52,605
So, what do you have to do?

59
00:02:52,605 --> 00:02:56,960
Well, you actually have to split your data into three parts, training,

60
00:02:56,960 --> 00:03:02,365
validation, and a brand new completely isolated silo called testing.

61
00:03:02,365 --> 00:03:05,830
Once your model has been trained and validated,

62
00:03:05,830 --> 00:03:07,190
then you can write it once,

63
00:03:07,190 --> 00:03:09,900
and only once against the independent test data-set.

64
00:03:09,900 --> 00:03:12,350
And that's the loss metric that you can report to your boss.

65
00:03:12,350 --> 00:03:15,140
And it's the loss metric that then on your testing data-set,

66
00:03:15,140 --> 00:03:17,665
decides whether or not you want to use this model in production.

67
00:03:17,665 --> 00:03:20,780
So what happens if you fail on your testing data-set,

68
00:03:20,780 --> 00:03:22,500
even though you passed validation?

69
00:03:22,500 --> 00:03:24,950
Means you can't retest the same ML model,

70
00:03:24,950 --> 00:03:28,585
and you've got to either retrain a brand new Machine Learning model,

71
00:03:28,585 --> 00:03:30,520
or go back to the drawing board and collect

72
00:03:30,520 --> 00:03:35,035
more data samples to provide new data for your ML model.

73
00:03:35,035 --> 00:03:37,215
Now, while this is a good approach,

74
00:03:37,215 --> 00:03:39,145
there's one tiny problem.

75
00:03:39,145 --> 00:03:40,650
Nobody likes to waste data,

76
00:03:40,650 --> 00:03:42,980
and it seems like the test data is essentially wasted.

77
00:03:42,980 --> 00:03:44,910
I'm only using it once, it's held-out.

78
00:03:44,910 --> 00:03:47,060
Can't you use all your data in training,

79
00:03:47,060 --> 00:03:51,060
and still get a reasonable indication of how well your model is going to perform?

80
00:03:51,060 --> 00:03:53,290
Well, the answer is you can.

81
00:03:53,290 --> 00:03:55,330
The compromise between these methods,

82
00:03:55,330 --> 00:03:59,185
is to do a training validation split and do that many different times.

83
00:03:59,185 --> 00:04:02,475
Train and then compute the loss in the validation data-set,

84
00:04:02,475 --> 00:04:04,470
keeping in mind this validation set could

85
00:04:04,470 --> 00:04:07,230
consist of points that were not used in training the first time,

86
00:04:07,230 --> 00:04:09,090
then split the data again.

87
00:04:09,090 --> 00:04:11,350
Now you're training data might include some points that

88
00:04:11,350 --> 00:04:13,815
you used in your original validation on that first run,

89
00:04:13,815 --> 00:04:16,965
but you're completely doing multiple iterations.

90
00:04:16,965 --> 00:04:19,750
And then finally after a few rounds of this, this blending,

91
00:04:19,750 --> 00:04:23,405
you average the validation loss metrics across the board.

92
00:04:23,405 --> 00:04:26,140
And you'll get a standard deviation of the validation losses,

93
00:04:26,140 --> 00:04:29,510
and it'd be able to help you analyze that spread and go with the final number.

94
00:04:29,510 --> 00:04:33,075
This process is called bootstrapping or cross-validation.

95
00:04:33,075 --> 00:04:34,970
The upside is, you get to use all data,

96
00:04:34,970 --> 00:04:36,790
but you have to train lots and lots more

97
00:04:36,790 --> 00:04:38,600
times because you're creating more of the splits.

98
00:04:38,600 --> 00:04:40,645
So, at the end of the day,

99
00:04:40,645 --> 00:04:42,100
here's what you have to remember.

100
00:04:42,100 --> 00:04:43,735
If you have lots of data,

101
00:04:43,735 --> 00:04:48,070
use the approach of having a completely independent held-out test data-set,

102
00:04:48,070 --> 00:04:50,180
that's like go or no-go decision.

103
00:04:50,180 --> 00:04:51,720
If you don't have that much data,

104
00:04:51,720 --> 00:04:54,700
use the cross-validation approach.

105
00:04:54,700 --> 00:04:58,140
So, how do you go about actually splitting

106
00:04:58,140 --> 00:05:01,245
these large data-sets into these silos that we keep talking about?

107
00:05:01,245 --> 00:05:05,000
Well, that's the topic for our next lesson on sampling.