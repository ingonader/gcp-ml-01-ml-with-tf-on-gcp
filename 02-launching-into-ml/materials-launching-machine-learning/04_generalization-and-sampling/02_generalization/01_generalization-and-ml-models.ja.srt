1
00:00:00,000 --> 00:00:01,780
まず一般化に取り組みましょう

2
00:00:01,780 --> 00:00:03,629
一般化は ある問題に答えてくれます

3
00:00:03,629 --> 00:00:08,064
最も正確なMLモデルが
常に最適な選択ではないという問題です

4
00:00:08,064 --> 00:00:12,135
今度もおなじみの出生率の
データセットを使いましょう

5
00:00:12,135 --> 00:00:15,760
ただし今回は母親の体重増加をX軸にして

6
00:00:15,760 --> 00:00:18,825
妊娠の経過日数をY軸で予測します

7
00:00:18,825 --> 00:00:21,850
どんなパターンがデータで確認できますか？

8
00:00:21,850 --> 00:00:24,470
強い相関性があるように見えます

9
00:00:24,470 --> 00:00:27,230
体重が増えると経過日数も長いです

10
00:00:27,230 --> 00:00:31,125
赤ちゃんが大きくなるので
直感的に納得がいきますよね

11
00:00:31,125 --> 00:00:34,785
これをモデル化して相関性を証明するには

12
00:00:34,785 --> 00:00:38,410
どのモデルを最初に使うのが
一般的でしょうか？

13
00:00:38,410 --> 00:00:41,820
線形回帰モデルと答えた人は正解です

14
00:00:41,835 --> 00:00:44,600
回帰の問題で説明しましたが

15
00:00:44,600 --> 00:00:46,875
最適化したい損失の指標は

16
00:00:46,875 --> 00:00:49,730
通常は平均二乗誤差（MSE）か

17
00:00:49,730 --> 00:00:53,570
平均二乗平方根誤差（RMSE）です

18
00:00:53,570 --> 00:00:58,825
平均二乗誤差では 回帰直線と
点の近さがわかります

19
00:00:58,825 --> 00:01:03,610
そのために 点から実際の回帰直線までの
距離を測ります

20
00:01:03,610 --> 00:01:07,245
この距離を誤差といいますが
これを二乗します

21
00:01:07,245 --> 00:01:10,615
二乗が必要なのは
負号を取り除くためです

22
00:01:10,615 --> 00:01:15,265
MSEは線からの距離が大きいものに
重み付けします

23
00:01:15,265 --> 00:01:19,380
MSEの平方根を取るとRMSEが求められます

24
00:01:19,380 --> 00:01:25,405
これはデータポイントと適合線の
垂直方向の距離の平均です

25
00:01:25,405 --> 00:01:29,740
RMSEはY軸の測定単位で直接解釈されます

26
00:01:29,740 --> 00:01:33,575
このため 相関関数よりも
適合度を測るのに適しています

27
00:01:33,575 --> 00:01:35,580
両方の誤差の測定で

28
00:01:35,580 --> 00:01:39,260
値が低い方がパフォーマンスがいいモデルです

29
00:01:39,260 --> 00:01:42,390
誤差がゼロに近いほど優れています

30
00:01:42,390 --> 00:01:45,835
ここでは線形回帰モデルを使っています

31
00:01:45,835 --> 00:01:50,105
データに最も適合する線を引き
誤差を最小化します

32
00:01:50,105 --> 00:01:53,095
最終的なRMSEは2.224です

33
00:01:53,095 --> 00:01:57,205
今見ている問題に対しては
とてもいい出来でしょう

34
00:01:57,205 --> 00:01:59,595
しかしこれを見てください

35
00:01:59,595 --> 00:02:02,245
もっと複雑なモデルを使うとどうでしょうか？

36
00:02:02,245 --> 00:02:05,830
モデルが複雑になると
自由パラメータが増える可能性があります

37
00:02:05,830 --> 00:02:08,345
この例ではこの自由パラメータによって

38
00:02:08,345 --> 00:02:11,670
ご覧のようにデータセットの凸凹を
すべてとらえています

39
00:02:11,670 --> 00:02:13,820
RMSEを減らしてゼロにすると

40
00:02:13,820 --> 00:02:15,980
モデルは完全に正確になりました

41
00:02:15,980 --> 00:02:17,725
これで完了でしょうか？

42
00:02:17,725 --> 00:02:21,107
これは最適なモデルでしょうか？
本番で使えますか？

43
00:02:21,107 --> 00:02:24,885
2番目のモデルは何か怪しいと
感じるかもしれません

44
00:02:24,885 --> 00:02:27,190
しかし それはどうすればわかりますか？

45
00:02:27,190 --> 00:02:30,340
MLではデータが大量で
そんな直感が働かないことがよくあります

46
00:02:30,340 --> 00:02:34,265
8個のノードのニューラルネットワークが
12個より優れているでしょうか？

47
00:02:34,265 --> 00:02:38,455
16個のノードの方がRMSEが低いです
それを選ぶべきですか？

48
00:02:38,455 --> 00:02:40,390
ご覧になっているこの例は

49
00:02:40,390 --> 00:02:44,905
100次の多項式または数百のノードを持つ
ニューラルネットワークです

50
00:02:44,905 --> 00:02:48,820
スパイラルの例を
最適化の最終講義の終わりで見ましたが

51
00:02:48,820 --> 00:02:52,265
モデルが複雑になると
最適化できるパラメータが増えます

52
00:02:52,265 --> 00:02:55,970
モデルはスパイラルのように
さらに複雑なデータに適合できます

53
00:02:55,970 --> 00:02:59,870
シンプルで小さなデータセットを
記憶することもできます

54
00:02:59,870 --> 00:03:03,930
では モデルのトレーニング時に
データセットの記憶や過学習を防ぐには

55
00:03:03,930 --> 00:03:08,250
どのタイミングで止めればよいのでしょう？

56
00:03:08,250 --> 00:03:11,020
モデルの品質評価の最適な方法は

57
00:03:11,020 --> 00:03:15,820
未知の新しいデータセットで
パフォーマンスを確認することです

58
00:03:15,820 --> 00:03:20,905
新しいデータポイントで
そのモデルが適切に一般化されているか判断できます

59
00:03:20,905 --> 00:03:23,640
未知のデータセットは
現実世界の本番データの代わりになります

60
00:03:23,640 --> 00:03:27,160
では 線形回帰モデルと
ニューラルネットワークモデルの

61
00:03:27,160 --> 00:03:29,970
出来栄えをもう一度確認しましょう

62
00:03:29,970 --> 00:03:32,885
新しいデータポイントに対して線形回帰モデルは

63
00:03:32,885 --> 00:03:34,760
十分に一般化されています

64
00:03:34,760 --> 00:03:37,710
RMSEは以前のものと似ていますが

65
00:03:37,710 --> 00:03:40,025
驚きがないのはいいことです

66
00:03:40,025 --> 00:03:44,800
モデルのトレーニングと評価で
パフォーマンスが一貫しているのが理想です

67
00:03:44,800 --> 00:03:46,860
2番目のモデルを見直してみると

68
00:03:46,860 --> 00:03:50,010
新しいデータセットでは
うまく一般化されていません

69
00:03:50,010 --> 00:03:51,960
これは懸念です

70
00:03:51,960 --> 00:03:54,770
RMSEが0から3.2に大きく増えています

71
00:03:54,770 --> 00:03:56,520
これは大きな問題です

72
00:03:56,520 --> 00:04:01,260
モデルがトレーニング用データセットを
完全に過学習しています

73
00:04:01,260 --> 00:04:05,355
これでは新しいデータに対して
一般化できず当てになりません

74
00:04:05,355 --> 00:04:07,645
モデルが過学習しないように

75
00:04:07,645 --> 00:04:11,660
トレーニングを止めるタイミングを
知るにはどうすればよいでしょうか

76
00:04:11,660 --> 00:04:14,295
その答えは驚くほどシンプルです

77
00:04:14,295 --> 00:04:16,300
データを分割するのです

78
00:04:16,300 --> 00:04:21,385
元のデータセットを分割して
まったく別のグループにします

79
00:04:21,385 --> 00:04:25,020
トレーニング用データセットで
モデルをトレーニングします

80
00:04:25,020 --> 00:04:26,990
トレーニングが完了したら

81
00:04:26,990 --> 00:04:31,445
分けておいた評価用データセットでの
パフォーマンスと比較します

82
00:04:31,445 --> 00:04:33,255
一般化がうまくいったモデルは

83
00:04:33,255 --> 00:04:37,605
損失の指標や誤差の値が
トレーニングと評価でほぼ同じになります

84
00:04:37,605 --> 00:04:40,950
評価用データセットでの
モデルのパフォーマンスが

85
00:04:40,950 --> 00:04:42,315
低下してきたら

86
00:04:42,315 --> 00:04:44,885
たとえば 損失の指標が
上昇し始めたら

87
00:04:44,885 --> 00:04:47,200
そこが止めるタイミングです

88
00:04:47,200 --> 00:04:50,160
MLモデルのトレーニングと評価では

89
00:04:50,160 --> 00:04:53,790
一般化できるモデルと
モデルのパラメータを見つけます

90
00:04:53,790 --> 00:04:57,810
トレーニングデータセットに適合しつつも
記憶はしていないモデルです

91
00:04:57,810 --> 00:05:02,100
この線形モデルはシンプルすぎて
データに適合していません

92
00:05:02,100 --> 00:05:04,800
どんなにひどいか
見ればわかりますね

93
00:05:04,800 --> 00:05:08,940
傾向線から外れている点が
たくさんあります

94
00:05:08,940 --> 00:05:12,040
これを未学習と呼びます

95
00:05:12,040 --> 00:05:13,630
これと反対で

96
00:05:13,630 --> 00:05:16,830
より危険なのがすでに説明した過学習です

97
00:05:16,830 --> 00:05:18,430
これは右下に示されています

98
00:05:18,430 --> 00:05:21,850
ここでは線型モデルの複雑さを
かなり増やしました

99
00:05:21,850 --> 00:05:24,090
n次多項式に解釈され

100
00:05:24,090 --> 00:05:28,670
先ほど見たとおり
データのすべての凸凹に適合しています

101
00:05:28,670 --> 00:05:31,235
ここで評価用データセットの出番です

102
00:05:31,235 --> 00:05:34,805
モデルのパラメータが
過学習していないか確認しましょう

103
00:05:34,805 --> 00:05:36,245
複雑すぎていませんか

104
00:05:36,245 --> 00:05:39,010
トレーニング用データセットへの
過剰な適合は

105
00:05:39,010 --> 00:05:42,445
それなりに適合するモデルより
ずっと問題です

106
00:05:42,445 --> 00:05:44,470
本番まで気付かないこともあります

107
00:05:44,470 --> 00:05:46,575
私たちはそれを検証しました

108
00:05:46,575 --> 00:05:51,490
未学習と過学習の間に
適切な複雑さのモデルが存在します

109
00:05:51,490 --> 00:05:55,030
では 評価用データセットの使い方を確認し

110
00:05:55,030 --> 00:06:00,420
トレーニングを止める時を知って
過学習を防ぐ方法を見ていきましょう