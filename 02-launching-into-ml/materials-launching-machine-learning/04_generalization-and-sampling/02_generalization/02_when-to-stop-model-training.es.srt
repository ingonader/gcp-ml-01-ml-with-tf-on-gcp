1
00:00:00,920 --> 00:00:04,105
Además de ayudarlos a elegir
entre dos modelos de AA diferentes

2
00:00:04,105 --> 00:00:06,865
es decir, ¿debo usar regresión lineal
o una red neuronal?

3
00:00:06,865 --> 00:00:09,510
también pueden usar su conjunto
de datos de validación

4
00:00:09,510 --> 00:00:12,160
para ajustar los hiperparámetros
de un único modelo.

5
00:00:12,160 --> 00:00:14,660
Si recuerdan,
esos hiperparámetros se configuran

6
00:00:14,660 --> 00:00:16,075
antes del entrenamiento.

7
00:00:16,075 --> 00:00:19,390
Este proceso de ajuste
se logra mediante ejecuciones sucesivas

8
00:00:19,390 --> 00:00:22,315
de entrenamientos y, luego,
la comparación de esas ejecuciones

9
00:00:22,315 --> 00:00:25,010
con el conjunto de datos
de validación independiente

10
00:00:25,010 --> 00:00:27,215
para verificar si hay sobreajuste.

11
00:00:27,215 --> 00:00:30,815
Así se usará su conjunto de validación
durante el entrenamiento.

12
00:00:31,045 --> 00:00:34,460
Como vieron durante la optimización,
el entrenamiento del modelo

13
00:00:34,460 --> 00:00:37,055
es cuando se comienzan
a calcular los pesos aleatorios

14
00:00:37,055 --> 00:00:39,860
la derivada,
la dirección de la curva de pérdida

15
00:00:39,860 --> 00:00:42,740
del descenso de gradientes,
se minimiza la métrica de pérdida

16
00:00:42,740 --> 00:00:44,115
y se repite.

17
00:00:44,115 --> 00:00:47,305
De manera periódica,
deben evaluar el rendimiento del modelo

18
00:00:47,305 --> 00:00:49,565
con los datos no vistos
durante el entrenamiento

19
00:00:49,565 --> 00:00:52,880
que es cuando usamos el conjunto
de datos de validación.

20
00:00:52,880 --> 00:00:55,120
Luego de completar un entrenamiento

21
00:00:55,120 --> 00:00:59,300
validen los resultados de este modelo
con el conjunto de validación

22
00:00:59,300 --> 00:01:03,020
para verificar si esos parámetros sirven
o si pueden ajustarlos un poco más.

23
00:01:03,020 --> 00:01:06,380
Si no hay una divergencia significativa
entre las métricas de pérdida

24
00:01:06,380 --> 00:01:09,480
del entrenamiento y las de la validación

25
00:01:09,480 --> 00:01:13,380
podríamos optimizar
nuestros hiperparámetros un poco más.

26
00:01:14,230 --> 00:01:17,410
Una vez que las métricas de pérdida
del modelo se han optimizado

27
00:01:17,410 --> 00:01:19,880
lo suficiente con el conjunto
de datos de validación

28
00:01:19,880 --> 00:01:22,305
cuando comiencen
a ver la divergencia, y confirmen

29
00:01:22,305 --> 00:01:25,685
que el modelo no se está sobreajustando,
es cuando nos detenemos

30
00:01:25,685 --> 00:01:29,600
y podemos decir que nuestro modelo
está ajustado, listo para producción.

31
00:01:30,200 --> 00:01:33,880
Pueden usar un bucle similar a este
para descubrir qué parámetros usar

32
00:01:33,880 --> 00:01:37,270
en sus modelos individuales,
como hicimos con los hiperparámetros

33
00:01:37,270 --> 00:01:40,845
que configuramos antes del entrenamiento.
Por ejemplo, las capas de una red

34
00:01:40,845 --> 00:01:42,860
o la cantidad de nodos que deberían usar.

35
00:01:42,860 --> 00:01:45,790
Básicamente, entrenarán
con una configuración, como seis nodos

36
00:01:45,790 --> 00:01:48,370
en su red neuronal
y luego entrenarán con otra.

37
00:01:48,370 --> 00:01:50,710
Luego,
evaluarán cuál tiene mejor rendimiento.

38
00:01:50,710 --> 00:01:52,710
con el conjunto de datos de validación.

39
00:01:52,710 --> 00:01:56,170
Terminarán eligiendo una configuración
de modelo que genere menos pérdida

40
00:01:56,170 --> 00:01:59,015
en el conjunto de datos de validación
y no la configuración

41
00:01:59,015 --> 00:02:01,825
que genere menos pérdida
en el conjunto de entrenamiento.

42
00:02:01,825 --> 00:02:05,460
Más adelante en esta especialización
les mostraremos cómo Cloud ML Engine

43
00:02:05,460 --> 00:02:07,910
puede realizar
una búsqueda bayesiana corta

44
00:02:07,910 --> 00:02:11,049
en el espacio de hiperparámetros,
de modo que no tengan que hacer

45
00:02:11,049 --> 00:02:13,820
este tipo de experimentación
un hiperparámetro a la vez.

46
00:02:13,820 --> 00:02:17,070
Cloud Machine Learning Engine
puede ayudarnos a realizar este tipo

47
00:02:17,070 --> 00:02:20,985
de experimentación de forma paralela
con una estrategia optimizada diferente.

48
00:02:22,450 --> 00:02:25,745
Una vez que terminen el entrenamiento,
deben compartir con su jefe

49
00:02:25,745 --> 00:02:27,555
cómo le está yendo a su modelo.

50
00:02:27,555 --> 00:02:30,060
¿Qué conjunto de datos
usarán para la decisión final

51
00:02:30,060 --> 00:02:31,925
sobre la evaluación?

52
00:02:31,925 --> 00:02:34,990
¿Pueden simplemente informar
la pérdida o el error en su conjunto

53
00:02:34,990 --> 00:02:38,240
de validación? ¿Incluso
si es coherente con el de entrenamiento?

54
00:02:38,325 --> 00:02:41,270
En realidad, no pueden. ¿Por qué?

55
00:02:42,110 --> 00:02:45,325
Porque usaron su conjunto de datos
de validación para decidir

56
00:02:45,325 --> 00:02:47,405
cuándo detener el entrenamiento.

57
00:02:47,405 --> 00:02:50,520
Ya no es independiente.
El modelo ya lo vio.

58
00:02:51,330 --> 00:02:53,185
¿Qué deben hacer?

59
00:02:53,185 --> 00:02:58,040
Deben dividir sus datos en tres partes:
entrenamiento, validación

60
00:02:58,040 --> 00:03:03,050
y un nuevo grupo aislado
completamente llamado "prueba".

61
00:03:03,420 --> 00:03:06,010
Una vez que su modelo
se entrenó y validó

62
00:03:06,010 --> 00:03:09,300
pueden ejecutarlo solo una vez
con el conjunto independiente

63
00:03:09,300 --> 00:03:10,450
de datos de prueba.

64
00:03:10,450 --> 00:03:13,120
Y esa es la métrica de pérdida
que informarán a su jefe.

65
00:03:13,130 --> 00:03:16,195
Y es la métrica de pérdida que,
en su conjunto de datos de prueba

66
00:03:16,195 --> 00:03:18,550
decide si deben usar
este modelo en producción.

67
00:03:18,550 --> 00:03:21,190
¿Qué pasa si el modelo falla
con el conjunto de prueba

68
00:03:21,190 --> 00:03:23,350
a pesar de que pasó la validación?

69
00:03:23,350 --> 00:03:26,395
Quiere decir que no pueden
probar de nuevo el mismo modelo de AA

70
00:03:26,395 --> 00:03:29,160
y tendrán que entrenar
un nuevo modelo de AA

71
00:03:29,160 --> 00:03:32,415
o volver atrás
y recolectar más muestras de datos

72
00:03:32,415 --> 00:03:34,875
para proporcionar nuevos datos
al modelo de AA.

73
00:03:36,005 --> 00:03:39,405
Si bien este es un buen enfoque,
hay un pequeño problema.

74
00:03:39,405 --> 00:03:41,180
A nadie le gusta desperdiciar datos

75
00:03:41,180 --> 00:03:43,440
y parece que eso pasa
con los datos de prueba.

76
00:03:43,440 --> 00:03:45,630
Solo los estoy usando una vez;
están retenidos.

77
00:03:45,630 --> 00:03:48,070
¿No podrían usar todos los datos
en el entrenamiento

78
00:03:48,070 --> 00:03:51,990
y aun así obtener una indicación razonable
del rendimiento que tendrá el modelo?

79
00:03:51,990 --> 00:03:53,290
La respuesta es sí.

80
00:03:53,290 --> 00:03:57,440
La diferencia entre estos dos métodos
es que harán la división de entrenamiento

81
00:03:57,440 --> 00:03:59,665
y validación muchas veces.

82
00:03:59,665 --> 00:04:03,255
Entrenar y luego calcular la pérdida
en el conjunto de datos de validación

83
00:04:03,255 --> 00:04:06,710
teniendo en cuenta que este conjunto
podría consistir en puntos no usados

84
00:04:06,710 --> 00:04:10,030
en el entrenamiento la primera vez.
Luego, dividir los datos de nuevo.

85
00:04:10,030 --> 00:04:12,420
Los datos de entrenamiento
podrían incluir puntos

86
00:04:12,420 --> 00:04:15,105
usados en la validación original
en esa primera ejecución

87
00:04:15,105 --> 00:04:17,414
pero están realizando
iteraciones múltiples.

88
00:04:17,414 --> 00:04:20,150
Finalmente, luego de unas veces
de hacer esta mezcla

89
00:04:20,150 --> 00:04:23,715
se obtiene el promedio de las métricas
de pérdida de la validación general.

90
00:04:23,715 --> 00:04:27,050
Y obtendrán una desviación estándar
de las pérdidas de la validación

91
00:04:27,050 --> 00:04:30,040
los ayudará a analizar la dispersión
y decidir la cifra final.

92
00:04:30,040 --> 00:04:33,245
Este proceso se llama
bootstrapping o validación cruzada.

93
00:04:33,245 --> 00:04:35,540
Lo bueno es que pueden usar
todos los datos

94
00:04:35,540 --> 00:04:39,190
pero deben entrenar muchas veces
porque crearán muchas divisiones.

95
00:04:40,060 --> 00:04:42,530
Al final,
esto es lo que deben recordar.

96
00:04:42,530 --> 00:04:46,025
Si tienen muchos datos,
usen el enfoque del conjunto de datos

97
00:04:46,025 --> 00:04:48,410
de prueba completamente
independiente y retenido

98
00:04:48,410 --> 00:04:50,795
que significará la decisión
de usarlo o no.

99
00:04:50,795 --> 00:04:54,690
Si no tienen tantos datos,
usen la validación cruzada.

100
00:04:56,480 --> 00:04:59,130
¿Cómo dividir estos grandes
conjuntos de datos

101
00:04:59,130 --> 00:05:01,770
en estos grupos
aislados de los que hablamos?

102
00:05:01,770 --> 00:05:05,070
Ese es el tema de nuestra siguiente
lección: el muestreo.