Okay, so here we are with the repeatable splitting lab. So, if you haven't taken a look at this lab yet already, and given an attempt yourself, highly recommend you do that, and then come back and watch this solution walkthrough video. And here we go. So ultimately, what we're looking to do is predict the arrival delay of an aircraft if it's been delayed in departure, and what the relationship is for those depending upon the different airports, like arrival and departure airports. So, if you have a flight that's coming out of New York that's 30 minutes delayed and it's going to Los Angeles, is it going to be delayed on arrival as well? And ultimately, what we're looking to do there is predict that relationship. So, to get at that relationship or model of that relationship, we have the formula as you see here, our cloud DataLab notebook. And that's the delay in arrival is equal to a parameter as is your alpha times the delay in departure. And to estimate the alpha, you see the formula here. And ultimately, what we're trying to do is feed in a bunch of departure delay information, and ultimately predict out whether or not that's going to make you delayed on arrival. But before we do any of this machine re-modeling, we need to set up our testing and validation environments for our model. And to do that, since it's such a large dataset, we're going to use Google Big Query, invoking it from within cloud DataLab. Now, what we're going to be doing is we're going to be creating a pair of arrival and departure airports, so you can basically see whether or not there is a strong correlation for this particular airport, that particular leg as it were, whether or not you're going to be delayed an arrival if you're departed at a later time than you expected to. And for the purposes of this notebook, we're just going to be looking at Denver and Los Angeles. So, that all aside, that's the context for the lab, very important to understand what we're trying to do. We need to get a repeatable dataset sample created inside of Big Query in order to do that. So, let's cover a lot of the ways that you shouldn't do it first, and if you see this code or naive random sampling in yours or your colleagues' notebooks, you can point to this and basically say like, "Hey, if I wanted to run your code, I might not necessarily get the same results as you saw on the lecture that we had." So first up is that naive random split. So, we're going to go and execute this code. I've already executed it and I've noticed that the alpha that we have is highly correlated, 0.97 for between Denver and Los Angeles. I'm going to run to this cell. And let's get the coefficient for this. The last three numbers are 784. And to make this repeatable, if I did the same exact thing again, what do you think is going to happen? Am I going to get 784? No, I got 919. So, I haven't changed anything nor have I did any type of training or anything like that yet, so I want it to, ultimately, should be the same data, right? Well, you can see in the code here at line seven, you're selecting this random function as your splitting field. So, every time I'm executing this as you saw on a lecture, the random is applying a different number between zero to one. So you're splitting on that. So it's not necessarily repeatable at all. So, let's escalate it a little bit better. And this random is actually executed every time a row is executed inside of Big Query. So, let's make this little bit more obvious. So, if you then actually use that for your loss metric, root mean squared error between your arrival delay and departure delay, and split this into the different datasets as training and evaluation. So, say, it was done immediately for every single record, you've done this random less than 0.8 for all of these, let's see if we get the same RMSE. So, I'm just going to run this code cell. And the RMSE, we have 13.098 for training, and evaluation 13.027. So, it's sort of relatively consistent, but let's see if it's repeatable if I did the exact same thing. 13.098 is what we're looking for training. And as you see here, in training, we get 13.089 which is different than the 98 that we had before, and 13.063. So behind the scenes, even though we're running the exact same code, you're getting different results for your RMSE. And again, the culprit here, as a lot of you might be already screaming in your monitors, is this random function that we're doing here. So even splitting randomly into training and evaluation is not going to work. So, what you might be thinking is, "All right, well, how do I actually do this? If I set up all my data inside of training and evaluation using the random function once, and I have it correctly split as 80 percent, 20 percent, why is it constantly changing? How do I get above and beyond just having random being executed each time?" And that's where we need to completely do a mindset shift. So, here's another example where you have the training on the random, and you do that first. You've got a little bit of a subquery, or you have training and evaluation and you're splitting that into training and evaluation as a sub dataset. And then, you're running those queries as well but see here, you can run the exact same problem again where you have your RMSE 13.037 from my run, which is probably different from your runs as well. And that's on the evaluation. And I'm going to click here and go run to this cell, which is going to rerun everything up into this. And as soon as this is executed, we're looking for 13.037, and we're waiting for it to run. And we have 13.087, the alpha is different, it was 0.977 I believe before. As you can see, if you using random anywhere inside of your Big Query code, you execute that, your data is automatically going to be changing underneath you. So, how do we get away from using random as we were mentioning? Well, instead of using random, we use that hash function that we demoed a little bit earlier, and that's exactly what you're going to see here. So, we want to do is split inside of the where clause instead of doing like a random function less than 0.8. What we're using now is, all right, well, hash on the date. The date is not going to change. The date is just going to be whatever the date is in your training dataset. And then look for that remainder. And if that's going to fall into one particular category, in this particular case, we want to take anything that's less than eight and then dump it into our training bucket. So this is an 80 percent split, and that's likely this is going to be used for training. So we have 0.975, let's just all the way look at the end, it's 403. So, let's keep running this. And see what we get. All the way at the end, you get 403 here as well again. So you see, this is a repeatable way to do it. And intuitively, it makes sense. There's no functions that are changing underneath the hood while you're executing this code. So, now, what we can do is we can take that and actually add a little bit more SQL and then do your root mean squared error. Root mean squared error instead of SQL, again, you're just taking the average of that function that you saw before, and taking the square root all the way at the top, and your training dataset is 13.16072. So 13.160712 is the exact same result every time you get when you execute this. So, what did you learn? Pretty much, any time you're creating a repeatable data sample here, you need to use a hash function instead of using something like a naive random sample. And even doing something as you saw here, which is a little bit more sophisticated, but still equally as dangerous as pre-separating your data inside of training and evaluation, say you wanted to carve that outright, say you wanted to actually create, you might ask, "Well, Evan, if I actually did this, run this once and stored the results in two separate tables for a training evaluation, and then use those immediately, that's great because then you've only done that once and you've split it into a 80-20 split. But what happens if you get more data in the future? What happens if somebody wants to repeat your analysis in the original dataset?" Just because you created that one time hard coded split of data inside of 80-20 doesn't mean you're going to be able ever to repeat that in the future, especially if your dataset expands or contracts, or you wanted to actually make a different split than the 80-20. So it's much more flexible, much more repeatable to use something like a field to bucketize and hash your data. And you can see that here. So you get a familiarity with practicing with this because this is ultimately going to be one of those fundamental steps and building blocks that you need to do before you actually run machine learning models. And that's create these data buckets that your model can then learn from, validate from, and then ultimately, make that go or no go decision with test data to put your machine learning model in production. All right, that's it. What we're going to do at the end is cover a little bit more material and then work on an end-to-end lab, predicting taxi cab fare. We'll see you there.