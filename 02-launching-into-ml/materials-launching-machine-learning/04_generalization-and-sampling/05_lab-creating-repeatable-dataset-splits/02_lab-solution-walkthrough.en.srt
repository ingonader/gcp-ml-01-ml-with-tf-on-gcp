1
00:00:00,000 --> 00:00:03,255
Okay, so here we are with the repeatable splitting lab.

2
00:00:03,255 --> 00:00:05,980
So, if you haven't taken a look at this lab yet already,

3
00:00:05,980 --> 00:00:08,410
and given an attempt yourself, highly recommend you do that,

4
00:00:08,410 --> 00:00:11,035
and then come back and watch this solution walkthrough video.

5
00:00:11,035 --> 00:00:12,620
And here we go.

6
00:00:12,620 --> 00:00:15,835
So ultimately, what we're looking to do is predict

7
00:00:15,835 --> 00:00:20,295
the arrival delay of an aircraft if it's been delayed in departure,

8
00:00:20,295 --> 00:00:24,090
and what the relationship is for those depending upon the different airports,

9
00:00:24,090 --> 00:00:25,795
like arrival and departure airports.

10
00:00:25,795 --> 00:00:27,730
So, if you have a flight that's coming out of New York that's

11
00:00:27,730 --> 00:00:29,600
30 minutes delayed and it's going to Los Angeles,

12
00:00:29,600 --> 00:00:31,845
is it going to be delayed on arrival as well?

13
00:00:31,845 --> 00:00:35,320
And ultimately, what we're looking to do there is predict that relationship.

14
00:00:35,320 --> 00:00:38,510
So, to get at that relationship or model of that relationship,

15
00:00:38,510 --> 00:00:39,990
we have the formula as you see here,

16
00:00:39,990 --> 00:00:41,365
our cloud DataLab notebook.

17
00:00:41,365 --> 00:00:43,780
And that's the delay in arrival is equal to a parameter as is

18
00:00:43,780 --> 00:00:47,730
your alpha times the delay in departure.

19
00:00:47,730 --> 00:00:49,640
And to estimate the alpha,

20
00:00:49,640 --> 00:00:50,735
you see the formula here.

21
00:00:50,735 --> 00:00:53,260
And ultimately, what we're trying to do is feed in

22
00:00:53,260 --> 00:00:56,170
a bunch of departure delay information,

23
00:00:56,170 --> 00:01:00,170
and ultimately predict out whether or not that's going to make you delayed on arrival.

24
00:01:00,170 --> 00:01:01,990
But before we do any of this machine re-modeling,

25
00:01:01,990 --> 00:01:06,425
we need to set up our testing and validation environments for our model.

26
00:01:06,425 --> 00:01:08,555
And to do that, since it's such a large dataset,

27
00:01:08,555 --> 00:01:10,080
we're going to use Google Big Query,

28
00:01:10,080 --> 00:01:12,600
invoking it from within cloud DataLab.

29
00:01:12,600 --> 00:01:15,500
Now, what we're going to be doing is we're going to be creating

30
00:01:15,500 --> 00:01:18,625
a pair of arrival and departure airports,

31
00:01:18,625 --> 00:01:20,330
so you can basically see whether or not there is

32
00:01:20,330 --> 00:01:23,340
a strong correlation for this particular airport,

33
00:01:23,340 --> 00:01:26,350
that particular leg as it were,

34
00:01:26,350 --> 00:01:28,460
whether or not you're going to be delayed an arrival if you're

35
00:01:28,460 --> 00:01:31,510
departed at a later time than you expected to.

36
00:01:31,510 --> 00:01:33,610
And for the purposes of this notebook,

37
00:01:33,610 --> 00:01:35,865
we're just going to be looking at Denver and Los Angeles.

38
00:01:35,865 --> 00:01:37,630
So, that all aside,

39
00:01:37,630 --> 00:01:39,130
that's the context for the lab,

40
00:01:39,130 --> 00:01:41,095
very important to understand what we're trying to do.

41
00:01:41,095 --> 00:01:43,510
We need to get a repeatable dataset sample

42
00:01:43,510 --> 00:01:45,595
created inside of Big Query in order to do that.

43
00:01:45,595 --> 00:01:49,085
So, let's cover a lot of the ways that you shouldn't do it first,

44
00:01:49,085 --> 00:01:53,785
and if you see this code or naive random sampling in yours or your colleagues' notebooks,

45
00:01:53,785 --> 00:01:56,270
you can point to this and basically say like, "Hey,

46
00:01:56,270 --> 00:01:57,400
if I wanted to run your code,

47
00:01:57,400 --> 00:02:00,530
I might not necessarily get the same results as you saw on the lecture that we had."

48
00:02:00,530 --> 00:02:03,505
So first up is that naive random split.

49
00:02:03,505 --> 00:02:06,815
So, we're going to go and execute this code.

50
00:02:06,815 --> 00:02:09,210
I've already executed it and I've noticed that

51
00:02:09,210 --> 00:02:12,280
the alpha that we have is highly correlated,

52
00:02:12,280 --> 00:02:16,210
0.97 for between Denver and Los Angeles.

53
00:02:16,210 --> 00:02:19,975
I'm going to run to this cell.

54
00:02:19,975 --> 00:02:25,200
And let's get the coefficient for this.

55
00:02:25,200 --> 00:02:28,335
The last three numbers are 784.

56
00:02:28,335 --> 00:02:29,750
And to make this repeatable,

57
00:02:29,750 --> 00:02:31,960
if I did the same exact thing again,

58
00:02:31,960 --> 00:02:34,400
what do you think is going to happen?

59
00:02:34,400 --> 00:02:39,285
Am I going to get 784? No, I got 919.

60
00:02:39,285 --> 00:02:41,910
So, I haven't changed anything nor have I

61
00:02:41,910 --> 00:02:45,730
did any type of training or anything like that yet,

62
00:02:45,730 --> 00:02:48,595
so I want it to, ultimately, should be the same data, right?

63
00:02:48,595 --> 00:02:51,500
Well, you can see in the code here at line seven,

64
00:02:51,500 --> 00:02:54,605
you're selecting this random function as your splitting field.

65
00:02:54,605 --> 00:02:57,780
So, every time I'm executing this as you saw on a lecture,

66
00:02:57,780 --> 00:03:01,230
the random is applying a different number between zero to one.

67
00:03:01,230 --> 00:03:04,030
So you're splitting on that. So it's not necessarily repeatable at all.

68
00:03:04,030 --> 00:03:08,755
So, let's escalate it a little bit better.

69
00:03:08,755 --> 00:03:14,330
And this random is actually executed every time a row is executed inside of Big Query.

70
00:03:14,330 --> 00:03:19,820
So, let's make this little bit more obvious.

71
00:03:19,820 --> 00:03:23,140
So, if you then actually use that for your loss metric,

72
00:03:23,140 --> 00:03:26,600
root mean squared error between your arrival delay and departure delay,

73
00:03:26,600 --> 00:03:31,750
and split this into the different datasets as training and evaluation.

74
00:03:31,750 --> 00:03:36,200
So, say, it was done immediately for every single record,

75
00:03:36,200 --> 00:03:38,575
you've done this random less than 0.8 for all of these,

76
00:03:38,575 --> 00:03:41,590
let's see if we get the same RMSE.

77
00:03:41,590 --> 00:03:45,620
So, I'm just going to run this code cell.

78
00:03:47,860 --> 00:03:56,050
And the RMSE, we have 13.098 for training, and evaluation 13.027.

79
00:03:56,050 --> 00:03:58,910
So, it's sort of relatively consistent,

80
00:03:58,910 --> 00:04:01,950
but let's see if it's repeatable if I did the exact same thing.

81
00:04:01,950 --> 00:04:06,430
13.098 is what we're looking for training.

82
00:04:06,850 --> 00:04:10,710
And as you see here, in training,

83
00:04:10,710 --> 00:04:17,520
we get 13.089 which is different than the 98 that we had before, and 13.063.

84
00:04:17,520 --> 00:04:18,790
So behind the scenes,

85
00:04:18,790 --> 00:04:21,730
even though we're running the exact same code,

86
00:04:21,730 --> 00:04:23,755
you're getting different results for your RMSE.

87
00:04:23,755 --> 00:04:25,070
And again, the culprit here,

88
00:04:25,070 --> 00:04:27,415
as a lot of you might be already screaming in your monitors,

89
00:04:27,415 --> 00:04:29,760
is this random function that we're doing here.

90
00:04:29,760 --> 00:04:33,585
So even splitting randomly into training and evaluation is not going to work.

91
00:04:33,585 --> 00:04:35,555
So, what you might be thinking is,

92
00:04:35,555 --> 00:04:38,085
"All right, well, how do I actually do this?

93
00:04:38,085 --> 00:04:42,430
If I set up all my data inside of training and evaluation using the random function once,

94
00:04:42,430 --> 00:04:45,270
and I have it correctly split as 80 percent,

95
00:04:45,270 --> 00:04:48,645
20 percent, why is it constantly changing?

96
00:04:48,645 --> 00:04:52,865
How do I get above and beyond just having random being executed each time?"

97
00:04:52,865 --> 00:04:56,710
And that's where we need to completely do a mindset shift.

98
00:04:56,710 --> 00:04:59,090
So, here's another example where you have

99
00:04:59,090 --> 00:05:02,060
the training on the random, and you do that first.

100
00:05:02,060 --> 00:05:04,155
You've got a little bit of a subquery,

101
00:05:04,155 --> 00:05:07,410
or you have training and evaluation and you're splitting that into

102
00:05:07,410 --> 00:05:11,580
training and evaluation as a sub dataset.

103
00:05:11,580 --> 00:05:14,910
And then, you're running those queries as well but see here,

104
00:05:14,910 --> 00:05:21,150
you can run the exact same problem again where you have your RMSE 13.037 from my run,

105
00:05:21,150 --> 00:05:23,995
which is probably different from your runs as well.

106
00:05:23,995 --> 00:05:26,400
And that's on the evaluation.

107
00:05:26,400 --> 00:05:31,700
And I'm going to click here and go run to this cell,

108
00:05:31,700 --> 00:05:34,965
which is going to rerun everything up into this.

109
00:05:34,965 --> 00:05:37,350
And as soon as this is executed,

110
00:05:37,350 --> 00:05:40,010
we're looking for 13.037,

111
00:05:40,010 --> 00:05:46,005
and we're waiting for it to run.

112
00:05:46,005 --> 00:05:49,410
And we have 13.087,

113
00:05:49,410 --> 00:05:52,585
the alpha is different, it was 0.977 I believe before.

114
00:05:52,585 --> 00:05:57,105
As you can see, if you using random anywhere inside of your Big Query code,

115
00:05:57,105 --> 00:06:00,990
you execute that, your data is automatically going to be changing underneath you.

116
00:06:00,990 --> 00:06:04,105
So, how do we get away from using random as we were mentioning?

117
00:06:04,105 --> 00:06:05,460
Well, instead of using random,

118
00:06:05,460 --> 00:06:07,760
we use that hash function that we demoed a little bit earlier,

119
00:06:07,760 --> 00:06:09,710
and that's exactly what you're going to see here.

120
00:06:09,710 --> 00:06:13,100
So, we want to do is split

121
00:06:13,100 --> 00:06:16,555
inside of the where clause instead of doing like a random function less than 0.8.

122
00:06:16,555 --> 00:06:18,530
What we're using now is, all right,

123
00:06:18,530 --> 00:06:20,550
well, hash on the date.

124
00:06:20,550 --> 00:06:21,980
The date is not going to change.

125
00:06:21,980 --> 00:06:25,610
The date is just going to be whatever the date is in your training dataset.

126
00:06:25,610 --> 00:06:27,090
And then look for that remainder.

127
00:06:27,090 --> 00:06:31,100
And if that's going to fall into one particular category, in this particular case,

128
00:06:31,100 --> 00:06:33,700
we want to take anything that's less than eight and then

129
00:06:33,700 --> 00:06:36,310
dump it into our training bucket.

130
00:06:36,310 --> 00:06:38,130
So this is an 80 percent split,

131
00:06:38,130 --> 00:06:40,240
and that's likely this is going to be used for training.

132
00:06:40,240 --> 00:06:43,650
So we have 0.975,

133
00:06:43,650 --> 00:06:46,275
let's just all the way look at the end, it's 403.

134
00:06:46,275 --> 00:06:48,815
So, let's keep running this.

135
00:06:48,815 --> 00:06:51,005
And see what we get.

136
00:06:51,005 --> 00:06:52,340
All the way at the end,

137
00:06:52,340 --> 00:06:54,110
you get 403 here as well again.

138
00:06:54,110 --> 00:06:55,865
So you see, this is a repeatable way to do it.

139
00:06:55,865 --> 00:06:57,330
And intuitively, it makes sense.

140
00:06:57,330 --> 00:06:58,745
There's no functions that are

141
00:06:58,745 --> 00:07:02,085
changing underneath the hood while you're executing this code.

142
00:07:02,085 --> 00:07:05,170
So, now, what we can do is we can take that and actually

143
00:07:05,170 --> 00:07:09,215
add a little bit more SQL and then do your root mean squared error.

144
00:07:09,215 --> 00:07:12,180
Root mean squared error instead of SQL, again,

145
00:07:12,180 --> 00:07:16,770
you're just taking the average of that function that you saw before,

146
00:07:16,770 --> 00:07:19,390
and taking the square root all the way at the top,

147
00:07:19,390 --> 00:07:25,525
and your training dataset is 13.16072.

148
00:07:25,525 --> 00:07:32,665
So 13.160712 is the exact same result

149
00:07:32,665 --> 00:07:36,750
every time you get when you execute this. So, what did you learn?

150
00:07:36,750 --> 00:07:42,570
Pretty much, any time you're creating a repeatable data sample here,

151
00:07:42,570 --> 00:07:46,565
you need to use a hash function instead of using something like a naive random sample.

152
00:07:46,565 --> 00:07:48,440
And even doing something as you saw here,

153
00:07:48,440 --> 00:07:50,490
which is a little bit more sophisticated,

154
00:07:50,490 --> 00:07:52,400
but still equally as dangerous as

155
00:07:52,400 --> 00:07:55,620
pre-separating your data inside of training and evaluation,

156
00:07:55,620 --> 00:07:57,310
say you wanted to carve that outright,

157
00:07:57,310 --> 00:07:58,880
say you wanted to actually create,

158
00:07:58,880 --> 00:08:00,260
you might ask, "Well,

159
00:08:00,260 --> 00:08:02,140
Evan, if I actually did this,

160
00:08:02,140 --> 00:08:06,210
run this once and stored the results in two separate tables for a training evaluation,

161
00:08:06,210 --> 00:08:07,855
and then use those immediately,

162
00:08:07,855 --> 00:08:10,600
that's great because then you've only done that

163
00:08:10,600 --> 00:08:13,270
once and you've split it into a 80-20 split.

164
00:08:13,270 --> 00:08:15,450
But what happens if you get more data in the future?

165
00:08:15,450 --> 00:08:18,700
What happens if somebody wants to repeat your analysis in the original dataset?"

166
00:08:18,700 --> 00:08:22,310
Just because you created that one time hard coded split of data

167
00:08:22,310 --> 00:08:26,160
inside of 80-20 doesn't mean you're going to be able ever to repeat that in the future,

168
00:08:26,160 --> 00:08:28,400
especially if your dataset expands or contracts,

169
00:08:28,400 --> 00:08:31,515
or you wanted to actually make a different split than the 80-20.

170
00:08:31,515 --> 00:08:32,770
So it's much more flexible,

171
00:08:32,770 --> 00:08:36,070
much more repeatable to use something like a field to bucketize and hash your data.

172
00:08:36,070 --> 00:08:37,190
And you can see that here.

173
00:08:37,190 --> 00:08:38,760
So you get a familiarity with

174
00:08:38,760 --> 00:08:40,970
practicing with this because this is ultimately going to be one of

175
00:08:40,970 --> 00:08:43,010
those fundamental steps and building blocks

176
00:08:43,010 --> 00:08:45,480
that you need to do before you actually run machine learning models.

177
00:08:45,480 --> 00:08:49,225
And that's create these data buckets that your model can then learn from,

178
00:08:49,225 --> 00:08:51,090
validate from, and then ultimately,

179
00:08:51,090 --> 00:08:52,525
make that go or no go decision with

180
00:08:52,525 --> 00:08:55,380
test data to put your machine learning model in production.

181
00:08:55,380 --> 00:08:58,220
All right, that's it. What we're going to do at the end is

182
00:08:58,220 --> 00:09:00,955
cover a little bit more material and then work on an end-to-end lab,

183
00:09:00,955 --> 00:09:06,470
predicting taxi cab fare. We'll see you there.