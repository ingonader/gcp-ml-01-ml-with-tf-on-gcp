1
00:00:00,000 --> 00:00:02,870
Okay. This is the last lab that we're going to look at

2
00:00:02,870 --> 00:00:05,210
as part of this module in generalization and sampling,

3
00:00:05,210 --> 00:00:07,110
and it's pretty comprehensive.

4
00:00:07,110 --> 00:00:09,320
So, if it took you quite a while to work through it

5
00:00:09,320 --> 00:00:11,680
and work through all the steps, that's completely expected.

6
00:00:11,680 --> 00:00:13,805
So, let's take a look at a solution walk through right now.

7
00:00:13,805 --> 00:00:15,255
If you haven't attempted it yet,

8
00:00:15,255 --> 00:00:17,445
go ahead and try to pull up the data lab notebook,

9
00:00:17,445 --> 00:00:21,515
IPython notebook yourself, and run through the code that you see there in the cells,

10
00:00:21,515 --> 00:00:23,890
and then come back to this solution walk through video.

11
00:00:23,890 --> 00:00:25,925
Alright. Those of you sticking around,

12
00:00:25,925 --> 00:00:27,740
let's go ahead and take a look at what we got here.

13
00:00:27,740 --> 00:00:33,560
So here I have pulled up the Google Cloud taxicab estimation notebook and ultimately,

14
00:00:33,560 --> 00:00:36,730
what we want to do, is we want to explore,

15
00:00:36,730 --> 00:00:37,980
remember those three steps, right,

16
00:00:37,980 --> 00:00:39,535
we have to explore the data,

17
00:00:39,535 --> 00:00:41,650
we have to create those data sets,

18
00:00:41,650 --> 00:00:44,895
so you're now getting really familiar with how to deal with those house functions,

19
00:00:44,895 --> 00:00:48,620
and that again those three steps are the training data set,

20
00:00:48,620 --> 00:00:50,480
the evaluation data set and the testing data set,

21
00:00:50,480 --> 00:00:52,890
and the last thing that you might not have seen already,

22
00:00:52,890 --> 00:00:54,895
is how to create a benchmark,

23
00:00:54,895 --> 00:00:58,495
so that we can pummel it later when you learn a lot more about machine learning,

24
00:00:58,495 --> 00:01:00,680
and beat that simplistic model with some of

25
00:01:00,680 --> 00:01:03,030
the more advanced things that you're going to learn in future courses,

26
00:01:03,030 --> 00:01:05,525
like how to build deep neural network using tensor flow.

27
00:01:05,525 --> 00:01:07,030
So before we do that,

28
00:01:07,030 --> 00:01:09,170
we got to start from zero and go all the way,

29
00:01:09,170 --> 00:01:10,415
work our way up from the bottom.

30
00:01:10,415 --> 00:01:12,435
So the first thing we need to do as you see here,

31
00:01:12,435 --> 00:01:15,110
is get the data sample.

32
00:01:15,110 --> 00:01:18,805
So, great thing about BigQuery is it got a lot of public data sets.

33
00:01:18,805 --> 00:01:20,625
And much like the flight data,

34
00:01:20,625 --> 00:01:23,420
the taxicab data here is also there.

35
00:01:23,420 --> 00:01:28,785
So what we're going to be doing is pulling all of the cab fares for New York City.

36
00:01:28,785 --> 00:01:30,950
And that's in this public data set,

37
00:01:30,950 --> 00:01:33,320
and the fields that we want to look at, right?

38
00:01:33,320 --> 00:01:35,860
This is a little bit of feature engineering designing what we're

39
00:01:35,860 --> 00:01:38,650
going to explore and eventually make its way into our model.

40
00:01:38,650 --> 00:01:42,440
Well, if you want to think about the problem of predicting cab fare,

41
00:01:42,440 --> 00:01:44,665
what would be some of the things you'd be interested in?

42
00:01:44,665 --> 00:01:47,880
Well, you want to know potentially when they were picked up,

43
00:01:47,880 --> 00:01:52,255
what's the exact point like the latitude and longitude of the pick up and drop off,

44
00:01:52,255 --> 00:01:54,280
how many people were in the taxicab?

45
00:01:54,280 --> 00:01:56,730
Maybe there of course, there's multiple different fees or

46
00:01:56,730 --> 00:01:59,625
a fair amount tiered structure for the number of occupants,

47
00:01:59,625 --> 00:02:03,750
how long you went, what happens if you cross any of the bridges in New York?

48
00:02:03,750 --> 00:02:05,530
That's the total amount and then you have

49
00:02:05,530 --> 00:02:08,790
the fare amount plus any tips or discretionary spending,

50
00:02:08,790 --> 00:02:10,140
and that's how you get to that total amount.

51
00:02:10,140 --> 00:02:12,840
So we are going to see which of these factors ultimately

52
00:02:12,840 --> 00:02:16,190
play into determining the final fare of a cab ride,

53
00:02:16,190 --> 00:02:17,390
even before you get in,

54
00:02:17,390 --> 00:02:18,795
step foot into that door.

55
00:02:18,795 --> 00:02:21,175
So the first thing we needed to do is get the data.

56
00:02:21,175 --> 00:02:23,650
So to get data here in Cloud data lab,

57
00:02:23,650 --> 00:02:26,130
we're going to invoke a BigQuery query as you see here,

58
00:02:26,130 --> 00:02:28,335
and this is from the BigQuery sample.

59
00:02:28,335 --> 00:02:31,315
So you've got New York City, yellow cab trips,

60
00:02:31,315 --> 00:02:35,940
you pulled all those fields that I just mentioned and we're going

61
00:02:35,940 --> 00:02:40,760
to take a look at the very small part of the data.

62
00:02:40,760 --> 00:02:42,880
So we're just going to use much like we use

63
00:02:42,880 --> 00:02:47,305
the one percent sampling in our flights data for the last lab as you saw.

64
00:02:47,305 --> 00:02:50,400
We're going to be using just a small subset of the city here.

65
00:02:50,400 --> 00:02:52,045
So here's the initial query,

66
00:02:52,045 --> 00:02:55,250
and what we want to use is just say,

67
00:02:55,250 --> 00:03:02,395
a 100,000 maybe even we have a 100,000 records to choose from.

68
00:03:02,395 --> 00:03:08,920
Let's see if we can pull out just maybe 10,000 taxicab rides from that.

69
00:03:08,920 --> 00:03:09,975
All right.

70
00:03:09,975 --> 00:03:13,995
So we have kind of parameterized the SQL query.

71
00:03:13,995 --> 00:03:17,420
You can parameterize that much like you would string replacement, right?

72
00:03:17,420 --> 00:03:19,175
So the query is, take

73
00:03:19,175 --> 00:03:23,710
the raw data query because we specified this as raw data appear as you see there,

74
00:03:23,710 --> 00:03:27,835
replace every n, this is grab a record, right?

75
00:03:27,835 --> 00:03:30,385
Sample it every n,

76
00:03:30,385 --> 00:03:34,290
and then the total size we're looking at is a 100,000 records,

77
00:03:34,290 --> 00:03:36,975
and then you're ultimately going to print that query and then execute it.

78
00:03:36,975 --> 00:03:39,195
So this is the query that's executed,

79
00:03:39,195 --> 00:03:45,965
and then we're sampling against this where the remainder of that operation is one,

80
00:03:45,965 --> 00:03:49,305
and so now we're down to only 10,000 taxicab rides.

81
00:03:49,305 --> 00:03:51,850
The reason why we want to do the sampling effort again is,

82
00:03:51,850 --> 00:03:56,000
you don't want to just necessarily take the first 1,000 because that could be ordered,

83
00:03:56,000 --> 00:03:58,050
and you get out bias in your data like say,

84
00:03:58,050 --> 00:04:00,800
a great example for taxicab data is,

85
00:04:00,800 --> 00:04:04,825
it might be sorted by the most recent cab rides have been first.

86
00:04:04,825 --> 00:04:09,550
So if you start looking and exploring your data on the most recent like 3,000 rides,

87
00:04:09,550 --> 00:04:12,330
you can get bias introducing your results because maybe there was

88
00:04:12,330 --> 00:04:16,665
a change or a fare increase that was captured recently,

89
00:04:16,665 --> 00:04:20,105
or a fare decrease that you wouldn't know just by looking at that.

90
00:04:20,105 --> 00:04:22,320
We call it recency bias.

91
00:04:22,320 --> 00:04:24,445
So we've sampled effectively,

92
00:04:24,445 --> 00:04:26,385
and here is what we have.

93
00:04:26,385 --> 00:04:28,340
And this is just, we haven't done anything yet.

94
00:04:28,340 --> 00:04:31,770
This is just the field that we returned from the data sets.

95
00:04:31,770 --> 00:04:34,070
The next step is we want to actually explore it.

96
00:04:34,070 --> 00:04:36,080
So you see we have passenger counts,

97
00:04:36,080 --> 00:04:38,425
you see 1-5 here and some of the examples.

98
00:04:38,425 --> 00:04:41,450
You've got how far you've gotten. Really interesting.

99
00:04:41,450 --> 00:04:45,310
You got zero distance if this is miles for trip distance.

100
00:04:45,310 --> 00:04:46,800
That looks kind of weird.

101
00:04:46,800 --> 00:04:48,995
Zero tolls that can be expected,

102
00:04:48,995 --> 00:04:52,825
fare amount $2.50, and the total amount of $2.50.

103
00:04:52,825 --> 00:04:55,300
OK. So the data looks interesting.

104
00:04:55,300 --> 00:04:57,765
Let's see if we can explore it a little bit quicker.

105
00:04:57,765 --> 00:05:01,470
And the best way to do that, is to say create a data visualization.

106
00:05:01,470 --> 00:05:03,215
So oftentimes in machine learning,

107
00:05:03,215 --> 00:05:07,355
we'll create a scatter plot and take a look at some of the points that we have.

108
00:05:07,355 --> 00:05:11,120
So, here we've plotted trip distance against the fare amount.

109
00:05:11,120 --> 00:05:12,200
So you might be thinking,

110
00:05:12,200 --> 00:05:13,885
well the longer you travel in a cab,

111
00:05:13,885 --> 00:05:15,945
the more that meter rate is going to tick up.

112
00:05:15,945 --> 00:05:19,215
So here we see the longer the trip.

113
00:05:19,215 --> 00:05:23,115
So even a trip distance of 40 here,

114
00:05:23,115 --> 00:05:25,830
you see a general high fare amount of $100.

115
00:05:25,830 --> 00:05:27,890
But you notice two strange,

116
00:05:27,890 --> 00:05:30,655
maybe couple of strange anomalies in the data that you see here.

117
00:05:30,655 --> 00:05:33,705
Well, there's a ton of extremely small trips,

118
00:05:33,705 --> 00:05:35,040
or even trips that could be zero,

119
00:05:35,040 --> 00:05:36,185
because they're right on this line.

120
00:05:36,185 --> 00:05:39,210
That's an anomaly. We want to filter that off the dataset.

121
00:05:39,210 --> 00:05:41,195
I don't know how you can have a cab ride that doesn't go anywhere.

122
00:05:41,195 --> 00:05:43,195
Maybe you go in and you get immediately kicked out.

123
00:05:43,195 --> 00:05:47,990
And so, you want to look at the points that are zero against this line.

124
00:05:47,990 --> 00:05:51,220
And then maybe any points that have

125
00:05:51,220 --> 00:05:56,285
a look at this kind of solid line that's going up here diagonally here.

126
00:05:56,285 --> 00:05:58,205
It looks almost like a line, but it's actually

127
00:05:58,205 --> 00:06:00,780
a ton of points collected across that line.

128
00:06:00,780 --> 00:06:02,545
That's because of the nature of the data.

129
00:06:02,545 --> 00:06:06,600
It's interesting because in New York when you exit JFK, one of the airports there,

130
00:06:06,600 --> 00:06:10,650
you could actually get a flat rate cab and go pretty much anywhere inside of Manhattan.

131
00:06:10,650 --> 00:06:12,505
And that will actually be a flat rate.

132
00:06:12,505 --> 00:06:14,745
So that's how based on the distance that you're traveling,

133
00:06:14,745 --> 00:06:16,250
it's actually known at that time.

134
00:06:16,250 --> 00:06:17,960
And that's why it's very easy to model

135
00:06:17,960 --> 00:06:20,160
that relationship and that relationship is just a line.

136
00:06:20,160 --> 00:06:23,460
But we want to predict not just folks coming from JFK,

137
00:06:23,460 --> 00:06:26,285
we want to predict folks that are traveling anywhere within New York.

138
00:06:26,285 --> 00:06:29,055
So, interesting things right.

139
00:06:29,055 --> 00:06:32,000
Let's take a look at some of the ways we can preprocess and clean

140
00:06:32,000 --> 00:06:35,840
that data before we ultimately bucket into the training datasets,

141
00:06:35,840 --> 00:06:37,965
the validation datasets and the testing datasets.

142
00:06:37,965 --> 00:06:40,320
And again you don't want to jump to creating those datasets

143
00:06:40,320 --> 00:06:42,620
splits before you clean your data first.

144
00:06:42,620 --> 00:06:43,830
So garbage in garbage out.

145
00:06:43,830 --> 00:06:45,290
If you start splitting data that's horrible,

146
00:06:45,290 --> 00:06:46,820
you're going to get a horrible model results,

147
00:06:46,820 --> 00:06:50,330
and you're going to not be able to model any behavior that's out there in the real world.

148
00:06:50,330 --> 00:06:52,885
And a good rule of thumb is all data is dirty.

149
00:06:52,885 --> 00:06:54,600
You want to clean and make sure that it's actually in

150
00:06:54,600 --> 00:06:56,500
a good form before you feed it into your model.

151
00:06:56,500 --> 00:06:58,520
Your model only wants good high quality data.

152
00:06:58,520 --> 00:07:02,860
That's what it loves. OK. So, looking at some of the rides here.

153
00:07:02,860 --> 00:07:06,860
So, let's look at anything that has crossed a bridge.

154
00:07:06,860 --> 00:07:09,260
So tolls amount greater than zero.

155
00:07:09,260 --> 00:07:11,900
And then we have a particular day that we're looking at the pickup time.

156
00:07:11,900 --> 00:07:14,780
In this case it's May 20, 2014.

157
00:07:14,780 --> 00:07:17,590
One interesting thing just gazing at the data here,

158
00:07:17,590 --> 00:07:19,280
pick up longitude of zero,

159
00:07:19,280 --> 00:07:21,275
or pick up latitude of zero,

160
00:07:21,275 --> 00:07:25,305
that is clearly wrong, or dirty data.

161
00:07:25,305 --> 00:07:29,210
We need to filter out anything that doesn't actually have a valid pick up location.

162
00:07:29,210 --> 00:07:32,720
So, you want to have a dataset that ultimately makes sense

163
00:07:32,720 --> 00:07:37,075
and doesn't have any records that just look very strange.

164
00:07:37,075 --> 00:07:41,200
Another thing that you might notice here is that the total amount,

165
00:07:41,200 --> 00:07:45,405
nowhere in here do we actually say of the columns that are available to us,

166
00:07:45,405 --> 00:07:48,395
what the customer uses a tip because or

167
00:07:48,395 --> 00:07:51,800
and any cash amount as a tip as well that's not recorded in here.

168
00:07:51,800 --> 00:07:56,525
So, for the purposes of our model since that's unknown and since tips are discretionary,

169
00:07:56,525 --> 00:07:59,445
that's not really included as part of the fare originally.

170
00:07:59,445 --> 00:08:01,025
We're actually not going to predict that.

171
00:08:01,025 --> 00:08:03,930
So, what we're going to do is set the new total amount with

172
00:08:03,930 --> 00:08:08,620
a new fare amount to be just the total amount for the distance that you travel,

173
00:08:08,620 --> 00:08:11,525
plus any tolls that you have.

174
00:08:11,525 --> 00:08:15,780
So, in this particular example the fare amount of 8.5 here is

175
00:08:15,780 --> 00:08:20,055
just the distance that you traveled 2.22 $2 and change,

176
00:08:20,055 --> 00:08:24,490
plus you went over a bridge which is $5.33 that get you that total fare amount.

177
00:08:24,490 --> 00:08:26,295
So, we going to recalculate that,

178
00:08:26,295 --> 00:08:27,990
just by adding those two together.

179
00:08:27,990 --> 00:08:29,210
And that's going to be a new total amount.

180
00:08:29,210 --> 00:08:32,195
So ignoring tips. All right.

181
00:08:32,195 --> 00:08:36,025
So, an interesting function that you can do is just a.describe,

182
00:08:36,025 --> 00:08:39,620
and that will give you a familiarity for what are some of the bounds,

183
00:08:39,620 --> 00:08:42,420
or some of the ranges of data for the columns that you have,

184
00:08:42,420 --> 00:08:44,070
very useful in statistics.

185
00:08:44,070 --> 00:08:47,670
So, let's take a look at the minimum and maximum for values.

186
00:08:47,670 --> 00:08:49,530
In case it wasn't clear for something like the pickup

187
00:08:49,530 --> 00:08:52,165
longitude or latitude when that was zero,

188
00:08:52,165 --> 00:08:53,960
you can see the max value is zero,

189
00:08:53,960 --> 00:08:55,010
minimum value is zero.

190
00:08:55,010 --> 00:08:57,285
So you can start to look at very strange things.

191
00:08:57,285 --> 00:08:59,765
Some of the things that immediately might become apparent,

192
00:08:59,765 --> 00:09:03,740
is if you have a minimum value for a cab fare that's negative 10.

193
00:09:03,740 --> 00:09:07,240
You can't have a negative cab fare.

194
00:09:07,240 --> 00:09:09,000
No one's paying you money to enter

195
00:09:09,000 --> 00:09:11,245
the cab and take the trip, you got to pay for the ride.

196
00:09:11,245 --> 00:09:13,925
So, and anything that looks like say,

197
00:09:13,925 --> 00:09:16,795
let's find the maximum of passenger count.

198
00:09:16,795 --> 00:09:18,390
Thankfully, this is six right here.

199
00:09:18,390 --> 00:09:21,230
But if you had a max passenger count of say twelve,

200
00:09:21,230 --> 00:09:24,520
that is not a cab vehicle unless it's a bus that was included in this.

201
00:09:24,520 --> 00:09:25,980
That's going to be there as well.

202
00:09:25,980 --> 00:09:28,415
So what we're slowly zeroing in on,

203
00:09:28,415 --> 00:09:30,090
is shaving and cleaning

204
00:09:30,090 --> 00:09:33,610
our whole dataset through an exercise that's called preprocessing.

205
00:09:33,610 --> 00:09:37,300
And then ultimately getting it ready to split into those three buckets,

206
00:09:37,300 --> 00:09:39,670
and then ultimately create a very simple benchmark off of

207
00:09:39,670 --> 00:09:42,220
that the ramp to beat later on. All right.

208
00:09:42,220 --> 00:09:45,290
So, once you've slogged your way through understanding the data.

209
00:09:45,290 --> 00:09:47,180
And by the way this process could take weeks.

210
00:09:47,180 --> 00:09:48,410
If you're unfamiliar, or you're not

211
00:09:48,410 --> 00:09:51,135
a subject matter expert in the dataset that you're looking at,

212
00:09:51,135 --> 00:09:53,955
and this could be hundreds of columns,

213
00:09:53,955 --> 00:09:55,740
or billions of records,

214
00:09:55,740 --> 00:09:57,440
then engage with, ask me,

215
00:09:57,440 --> 00:09:59,475
or subject matter expert that knows the data really well.

216
00:09:59,475 --> 00:10:02,250
And then really understand what the relationships are in the data,

217
00:10:02,250 --> 00:10:03,620
and then visualize it,

218
00:10:03,620 --> 00:10:06,605
use different visualizations, use statistical functions,

219
00:10:06,605 --> 00:10:09,080
even before you get to the machine learning side.

220
00:10:09,080 --> 00:10:11,775
You have to fundamentally understand what's going on in the data.

221
00:10:11,775 --> 00:10:14,105
So, although that took us only five minutes,

222
00:10:14,105 --> 00:10:16,225
the exploration part of machine learning,

223
00:10:16,225 --> 00:10:19,125
understand the datasets could take weeks or even months.

224
00:10:19,125 --> 00:10:23,310
OK. Let's look at some of the individual trips.

225
00:10:23,310 --> 00:10:26,180
So, here we are actually plotting these which is pretty cool,

226
00:10:26,180 --> 00:10:30,480
and you can see the trips themselves where we have the latitude and longitude.

227
00:10:30,480 --> 00:10:32,295
This is the trip lines.

228
00:10:32,295 --> 00:10:37,230
And then you can see that, lines that could be longer typically involve a toll.

229
00:10:37,230 --> 00:10:40,370
And that intuitively makes sense because you're crossing a bridge,

230
00:10:40,370 --> 00:10:42,005
you might be going a longer distance.

231
00:10:42,005 --> 00:10:45,420
It's not like somebody would get in a cab at the beginning of the bridge and

232
00:10:45,420 --> 00:10:49,365
then get out of the cab immediately after the bridge is done.

233
00:10:49,365 --> 00:10:51,260
So that's a good insight.

234
00:10:51,260 --> 00:10:55,020
OK. So, here is actually how we're going to clean up all this data.

235
00:10:55,020 --> 00:10:57,990
So, these are the five insights that we talked a little bit about before.

236
00:10:57,990 --> 00:11:00,770
So, we honed in that New York City longitudes and

237
00:11:00,770 --> 00:11:04,410
latitudes should be within this range negative 74 to 41.

238
00:11:04,410 --> 00:11:06,545
You can't have zero passengers.

239
00:11:06,545 --> 00:11:11,000
So, and arguably you shouldn't have more than a certain set amount,

240
00:11:11,000 --> 00:11:13,820
but we'll just have a baseline of saying no zero passengers.

241
00:11:13,820 --> 00:11:16,310
And then just like what we talked about with the tips,

242
00:11:16,310 --> 00:11:18,660
we're going to recalculate that total amount to

243
00:11:18,660 --> 00:11:22,520
just be fare amount plus the tolls amount as you see here.

244
00:11:22,520 --> 00:11:24,865
And then what we going to do is,

245
00:11:24,865 --> 00:11:27,450
we know the pick up and drop off locations,

246
00:11:27,450 --> 00:11:29,750
but not the trip distance.

247
00:11:29,750 --> 00:11:33,300
So, this is an interesting pitfall that a lot of

248
00:11:33,300 --> 00:11:37,160
people run into when creating training datasets for machine learning models.

249
00:11:37,160 --> 00:11:38,600
It can't be known.

250
00:11:38,600 --> 00:11:41,445
If it's not known during production time, you can't train on it.

251
00:11:41,445 --> 00:11:44,350
So you can't say something like, all right,

252
00:11:44,350 --> 00:11:48,050
the trip distance was 5 miles, 5.5 miles.

253
00:11:48,050 --> 00:11:51,050
I'm going to say it was a dollar per mile, so therefore,

254
00:11:51,050 --> 00:11:56,035
a very easy simplistic model is that the ultimate trip is going to cost $5.50.

255
00:11:56,035 --> 00:11:57,950
That's because when you start getting new data,

256
00:11:57,950 --> 00:12:00,520
like say I've requested a taxicab.

257
00:12:00,520 --> 00:12:02,690
And then the model asks,

258
00:12:02,690 --> 00:12:04,590
"Okay cool. How long did you travel?"

259
00:12:04,590 --> 00:12:06,660
And you're like, wait a minute. I didn't get in the taxicab.

260
00:12:06,660 --> 00:12:08,750
It's trying to know the future before it happens.

261
00:12:08,750 --> 00:12:09,950
So, you can't date it up,

262
00:12:09,950 --> 00:12:11,940
train on data that happens in the future.

263
00:12:11,940 --> 00:12:14,340
So, that's where we're actually dropping this from there,

264
00:12:14,340 --> 00:12:16,465
from the features data set as well.

265
00:12:16,465 --> 00:12:17,910
That's a really important point.

266
00:12:17,910 --> 00:12:20,150
So, think about data that exists,

267
00:12:20,150 --> 00:12:23,445
will exist when you actually launch this inside of production.

268
00:12:23,445 --> 00:12:28,830
So, lots of where clause filters for the bigQuery query that you see here.

269
00:12:28,830 --> 00:12:30,720
We're recalculating the fare_amount.

270
00:12:30,720 --> 00:12:32,970
We have the different columns as you can see here.

271
00:12:32,970 --> 00:12:34,680
We're renaming them with aliases,

272
00:12:34,680 --> 00:12:37,435
and we're creating this function, which basically says,

273
00:12:37,435 --> 00:12:38,960
this is going to be

274
00:12:38,960 --> 00:12:41,290
a parameterized query that we're going to

275
00:12:41,290 --> 00:12:44,180
ultimately sample between these particular ranges.

276
00:12:44,180 --> 00:12:47,890
So, here is all of our filters as we talked about a little bit above.

277
00:12:47,890 --> 00:12:52,390
Here's our modulo operators in the form fingerprint hash functions.

278
00:12:52,390 --> 00:12:54,675
We're hashing on pickup_datetime,

279
00:12:54,675 --> 00:12:58,430
and that means the all important message is that whatever you hash on,

280
00:12:58,430 --> 00:13:00,160
be prepared to lose.

281
00:13:00,160 --> 00:13:02,815
So we're willing to part with pickup_date time,

282
00:13:02,815 --> 00:13:05,000
in order for that column to be used in

283
00:13:05,000 --> 00:13:07,815
service for creating the barriers between those buckets.

284
00:13:07,815 --> 00:13:10,520
Training, evaluation, and testing.

285
00:13:10,520 --> 00:13:14,980
And that's ultimately we're saying that the time of day is not

286
00:13:14,980 --> 00:13:21,125
going to have predictive power ultimately in how much the cab fare is going to be.

287
00:13:21,125 --> 00:13:24,930
All right. So, we've created a query that can be parameterized,

288
00:13:24,930 --> 00:13:26,520
and we're going to say,

289
00:13:26,520 --> 00:13:29,240
if that were in training, and ultimately,

290
00:13:29,240 --> 00:13:32,490
what you can think down the road when I loop through this query three times, right?

291
00:13:32,490 --> 00:13:34,180
You got to create three data sets,

292
00:13:34,180 --> 00:13:36,030
training, evaluation and test.

293
00:13:36,030 --> 00:13:37,785
So, if we're in training,

294
00:13:37,785 --> 00:13:39,715
we want 70 percent of the data,

295
00:13:39,715 --> 00:13:42,195
subsample between zero and 70.

296
00:13:42,195 --> 00:13:46,750
And as you can see, sample_between is the query that we created earlier the a, b.

297
00:13:46,750 --> 00:13:50,365
And the a, b get plugged into a and b here,

298
00:13:50,365 --> 00:13:56,640
and that works out on modulo operator that you see there for every end.

299
00:13:56,640 --> 00:14:03,510
So, for training, that 70 percent validation is between 70 and 85 subtracting those 2,

300
00:14:03,510 --> 00:14:07,350
which means it's an additional 15 percent of the training data set we have available in

301
00:14:07,350 --> 00:14:13,595
the last 15 percent or 85 through 100 is going to be your testing.

302
00:14:13,595 --> 00:14:16,000
Okay. So that gets that all ready to run.

303
00:14:16,000 --> 00:14:19,830
Here's what a query would look like if we ran it,

304
00:14:19,830 --> 00:14:23,215
and now what we're actually going to do,

305
00:14:23,215 --> 00:14:26,270
is specify what the outputs of that are going to be stored.

306
00:14:26,270 --> 00:14:28,310
Because ultimately, we need some kind of say,

307
00:14:28,310 --> 00:14:32,000
CSV files or some other way that the machine learning model can reach out,

308
00:14:32,000 --> 00:14:35,080
and touch and access this training evaluation and testing data.

309
00:14:35,080 --> 00:14:38,480
And to do that, we need to create a function that's going to create these CSVs.

310
00:14:38,480 --> 00:14:41,570
In this particular case, we're training locally.

311
00:14:41,570 --> 00:14:42,820
So within Data Lab,

312
00:14:42,820 --> 00:14:44,895
we're actually going to be storing and creating these CSVs,

313
00:14:44,895 --> 00:14:48,345
in future modules when you get more familiar with cloud machine learning engine,

314
00:14:48,345 --> 00:14:50,910
and using other scalable,

315
00:14:50,910 --> 00:14:54,340
little bit of a prototyping step,

316
00:14:54,340 --> 00:14:57,145
what we're doing here is trying to do all this locally within Cloud data lab.

317
00:14:57,145 --> 00:15:00,505
But you see they can actually reference data directly from

318
00:15:00,505 --> 00:15:07,695
the query and from Google Cloud stores directly through like at Google storage bucket.

319
00:15:07,695 --> 00:15:10,010
Okay. So, here's the CSVs that we're creating.

320
00:15:10,010 --> 00:15:12,140
We're asking them to remove the fare amount,

321
00:15:12,140 --> 00:15:14,975
and then update it with the new one that we have inside of the CSV.

322
00:15:14,975 --> 00:15:16,650
Here's all the features that we're dumping in,

323
00:15:16,650 --> 00:15:21,745
which is pretty much everything that was included in the query up above.

324
00:15:21,745 --> 00:15:23,745
And then here's the all important loop.

325
00:15:23,745 --> 00:15:27,230
For phase in, train, validation,

326
00:15:27,230 --> 00:15:33,015
and testing invoke that query over the sample of 100,000,

327
00:15:33,015 --> 00:15:35,705
and then actually execute that bigQuery query,

328
00:15:35,705 --> 00:15:39,770
and then return the results for data frame that we can then iterate and operate over.

329
00:15:39,770 --> 00:15:42,225
And with those results,

330
00:15:42,225 --> 00:15:48,760
we restore that data frame with a prefix taxi dash hyphen,

331
00:15:48,760 --> 00:15:51,060
and then this is going to be your name of your data set.

332
00:15:51,060 --> 00:15:53,890
It's like taxi-train, taxi-validation,

333
00:15:53,890 --> 00:15:58,010
taxi-test inside of storage for the CSVs.

334
00:15:58,010 --> 00:16:00,800
And you can see that's exactly what happens here.

335
00:16:00,800 --> 00:16:03,120
So trust, but verify.

336
00:16:03,120 --> 00:16:05,920
We need make sure that those data sets actually do exist.

337
00:16:05,920 --> 00:16:08,780
So, just doing a simple ls here on the files that we have,

338
00:16:08,780 --> 00:16:15,550
and we see that there is 58,000 cab rides inside of the testing data set.

339
00:16:15,550 --> 00:16:18,890
And we have 400,000 in the training,

340
00:16:18,890 --> 00:16:21,390
and then 100,000 in the validation as well.

341
00:16:21,390 --> 00:16:26,385
So that reflects that split of what we have at the top,

342
00:16:26,385 --> 00:16:29,420
70, 15, and 15.

343
00:16:29,420 --> 00:16:34,260
The interesting part, if you are asking about why the testing

344
00:16:34,260 --> 00:16:39,000
and validation could be different and that's because of the distribution of the data.

345
00:16:39,000 --> 00:16:40,870
And it might not be normally distributed,

346
00:16:40,870 --> 00:16:43,280
so if you have a lot of dates that are clumped together,

347
00:16:43,280 --> 00:16:46,210
and again hashing on one day like January 1,

348
00:16:46,210 --> 00:16:49,410
2018, is going to return the same result as well.

349
00:16:49,410 --> 00:16:50,625
So, the data is not noisy enough,

350
00:16:50,625 --> 00:16:53,465
even if you tell it to be 70, 15, 15,

351
00:16:53,465 --> 00:16:58,700
it's going to hash it in blocks because you might have a lot of

352
00:16:58,700 --> 00:17:01,395
taxicabs that took place on New Year's Day

353
00:17:01,395 --> 00:17:04,380
that it has to fit into one of the different buckets, right?

354
00:17:04,380 --> 00:17:07,615
It can't be bold because you cannot split

355
00:17:07,615 --> 00:17:14,110
one single date when you're hashing on into two different places. Okay, great.

356
00:17:14,110 --> 00:17:18,650
So, let's take a look at the splits.

357
00:17:18,650 --> 00:17:26,305
We do that here, and now that we have all the data ready in those three silo buckets,

358
00:17:26,305 --> 00:17:30,030
it's finally, it's time to actually start creating a,

359
00:17:30,030 --> 00:17:31,570
I'll call this like a dummy model.

360
00:17:31,570 --> 00:17:32,745
This is your benchmark.

361
00:17:32,745 --> 00:17:38,880
So, if you had a simplistic guess of what the cab fare was going to be.

362
00:17:38,880 --> 00:17:41,430
So this is not taking into account whether,

363
00:17:41,430 --> 00:17:44,595
it's not taking into account whether or not you're coming from an airport.

364
00:17:44,595 --> 00:17:46,800
So again, all these more and more complex

365
00:17:46,800 --> 00:17:49,390
features and intuition that you can build into an advanced model,

366
00:17:49,390 --> 00:17:52,210
we're going to save that for later when we learn how to do TensorFlow,

367
00:17:52,210 --> 00:17:54,035
when you learn how to do proper feature engineering.

368
00:17:54,035 --> 00:17:57,680
Right now, is we want to create a pretty simplistic model that says, 'Hey,

369
00:17:57,680 --> 00:18:01,670
your advanced model better beat the RMSE or the loss

370
00:18:01,670 --> 00:18:05,840
metric for the model that we're running here as a benchmark."

371
00:18:05,840 --> 00:18:08,410
So, what does that simple model going to be?

372
00:18:08,410 --> 00:18:10,630
Well we're going to take a look at the,

373
00:18:10,630 --> 00:18:13,310
we're going to need to predict the trip distance first of all.

374
00:18:13,310 --> 00:18:14,910
So, a simple model is going to do that.

375
00:18:14,910 --> 00:18:17,530
And they're going to take the total fare amount,

376
00:18:17,530 --> 00:18:19,370
and divide it over the distance.

377
00:18:19,370 --> 00:18:20,935
And we'll just use a rate,

378
00:18:20,935 --> 00:18:23,350
per mile or per kilometer or something like that.

379
00:18:23,350 --> 00:18:27,150
And then, we want to say, based on the training data set what we know, and again,

380
00:18:27,150 --> 00:18:28,640
in the training data set it is labeled,

381
00:18:28,640 --> 00:18:32,275
meaning we actually do at the end of the day know the fare amount.

382
00:18:32,275 --> 00:18:35,740
That's how we can calculate the loss metric of the data,

383
00:18:35,740 --> 00:18:39,580
and we're going to use RMSE because it's a linear model so floating.

384
00:18:39,580 --> 00:18:42,670
Here's how we actually do that. Okay.

385
00:18:42,670 --> 00:18:45,560
So, we're going to define a couple of different functions to take

386
00:18:45,560 --> 00:18:49,975
the distances between the latitudes and longitudes or the pick up and drop off points.

387
00:18:49,975 --> 00:18:54,055
We're going to then estimate the distance between those two and then get

388
00:18:54,055 --> 00:18:58,965
a figure for how many miles that the taxicab actually drove.

389
00:18:58,965 --> 00:19:02,140
And again, we do know that information in training,

390
00:19:02,140 --> 00:19:03,340
but since we're predicting it,

391
00:19:03,340 --> 00:19:04,760
we cannot use that columns.

392
00:19:04,760 --> 00:19:05,780
We're actually going to predict that again.

393
00:19:05,780 --> 00:19:11,000
And then you compute the RMSE as you see with the equation as you see listed there.

394
00:19:11,000 --> 00:19:12,960
And then we are going to print it out,

395
00:19:12,960 --> 00:19:14,890
going to parse in our features to our model.

396
00:19:14,890 --> 00:19:16,950
We actually do want to predict our target.

397
00:19:16,950 --> 00:19:18,795
What we're actually predicting is the fare amount.

398
00:19:18,795 --> 00:19:20,615
We're going to list the features,

399
00:19:20,615 --> 00:19:22,980
and then ultimately, what we're going to do is,

400
00:19:22,980 --> 00:19:26,940
we're going to define where our data frames for training, validation, and test,

401
00:19:26,940 --> 00:19:28,770
those three data sets exist,

402
00:19:28,770 --> 00:19:31,800
and then we're going to train.

403
00:19:31,800 --> 00:19:33,905
Train a very simple model, which says,

404
00:19:33,905 --> 00:19:41,300
predict me a fare amount as the average divided by the,

405
00:19:41,300 --> 00:19:46,050
so the rate that we're calculating is just simply the average of the costs.

406
00:19:46,050 --> 00:19:50,295
So, it was like a t10 dollar cab ride divided by the average of how far it went.

407
00:19:50,295 --> 00:19:52,740
So, the line 28 is

408
00:19:52,740 --> 00:19:57,680
the only place right here where you see any kind of modeling actually happen.

409
00:19:57,680 --> 00:19:59,050
So, we spent 15,

410
00:19:59,050 --> 00:20:00,680
20 minutes going through this lab demo already,,

411
00:20:00,680 --> 00:20:04,605
and line 28 is the only place we actually doing the prediction or doing the modeling.

412
00:20:04,605 --> 00:20:11,410
So this, it took this long to create the data sets to do the cleaning and preprocessing.

413
00:20:11,410 --> 00:20:15,905
To do the setup of the CSV files for ingestion for the model to make it super easy,

414
00:20:15,905 --> 00:20:19,995
and then ultimately, to have this model be the benchmark for future model performance.

415
00:20:19,995 --> 00:20:24,875
Now, this ratio of 99 percent exploration,

416
00:20:24,875 --> 00:20:27,000
cleaning up, creating in the new data sets,

417
00:20:27,000 --> 00:20:30,235
establishing the benchmarks 99 to one percent of the actual model,

418
00:20:30,235 --> 00:20:33,990
that's going to shift as we start to get into more of model building,

419
00:20:33,990 --> 00:20:35,830
and how to create more sophisticated models,

420
00:20:35,830 --> 00:20:37,565
and how to do feature engineering in the future.

421
00:20:37,565 --> 00:20:40,065
So right now, this just can be our benchmark.

422
00:20:40,065 --> 00:20:44,730
Okay. So, this is the rate per kilometer that we actually get in at the end of the day.

423
00:20:44,730 --> 00:20:52,230
We have a rate of $2.60 per kilometer ride inside of your taxicab.

424
00:20:52,230 --> 00:20:54,665
And here are the RMSEs that you see here,

425
00:20:54,665 --> 00:21:02,585
so we have a training loss metric of 7.45 validation of 9.35,

426
00:21:02,585 --> 00:21:08,530
and actually when we tested it on was surprisingly the best out of all the three at 5.44.

427
00:21:08,530 --> 00:21:14,500
Now, whether or not that is our benchmark, globally saying,

428
00:21:14,500 --> 00:21:17,150
your taxi cab ride is going to be

429
00:21:17,150 --> 00:21:23,515
2.61 per kilometer no matter where you were going doesn't take into account traffic,

430
00:21:23,515 --> 00:21:26,050
doesn't take into account where you're going in Manhattan,

431
00:21:26,050 --> 00:21:27,760
doesn't take into account bridge tolls.

432
00:21:27,760 --> 00:21:29,220
We don't have any parameters in here

433
00:21:29,220 --> 00:21:31,190
for whether or not you're gonna be going over a bridge.

434
00:21:31,190 --> 00:21:33,085
It doesn't take into account time of day.

435
00:21:33,085 --> 00:21:36,240
So, all these things that you were just thinking in the back of your head, hey,

436
00:21:36,240 --> 00:21:38,845
you can't hard code 2.6 times the kilometers,

437
00:21:38,845 --> 00:21:41,920
all the intuition that we're going to build in more sophisticated models,

438
00:21:41,920 --> 00:21:43,280
and at the end of the day,

439
00:21:43,280 --> 00:21:45,905
they better do a much better job hopefully

440
00:21:45,905 --> 00:21:48,985
with all of the additional advanced insights that we're going to build into there,

441
00:21:48,985 --> 00:21:53,880
and we'll revisit this in the future to beat ultimately 5.44.

442
00:21:53,880 --> 00:21:58,690
So, that is your benchmark RMSE to beat. And that's it.

443
00:21:58,690 --> 00:22:00,940
So, the RMSE ultimately,

444
00:22:00,940 --> 00:22:04,725
if we took 5.44 times the actual rate,

445
00:22:04,725 --> 00:22:07,540
this is where you get that nine point.

446
00:22:07,540 --> 00:22:09,070
No, no, sorry.

447
00:22:09,070 --> 00:22:11,520
So, this was actually a little bit different.

448
00:22:11,520 --> 00:22:14,330
This is the 5.44 for this data set here.

449
00:22:14,330 --> 00:22:17,105
And you might get a little bit of that different response.

450
00:22:17,105 --> 00:22:20,175
All right, excellent. So, that wraps up this, and end lab,

451
00:22:20,175 --> 00:22:24,495
and I encourage you to continue taking the courses in the specialization.

452
00:22:24,495 --> 00:22:27,675
So ultimately, now that you have, you can't stop here.

453
00:22:27,675 --> 00:22:29,625
Now, that you know how to clean the data,

454
00:22:29,625 --> 00:22:31,020
get the data, adjust it,

455
00:22:31,020 --> 00:22:32,305
build the benchmarking model,

456
00:22:32,305 --> 00:22:34,600
you ultimately need to figure out, well, okay cool.

457
00:22:34,600 --> 00:22:37,220
I'm ready to actually do more sophisticated models and program in

458
00:22:37,220 --> 00:22:40,610
all those cool learning things that the model can

459
00:22:40,610 --> 00:22:42,525
do to make more sophisticated insights

460
00:22:42,525 --> 00:22:44,930
and beat ultimately this model with this RMSE here.

461
00:22:44,930 --> 00:22:48,860
So, stick around for future courses on TensorFlow,

462
00:22:48,860 --> 00:22:52,500
and how to actually beat this RMSE here and feel free.

463
00:22:52,500 --> 00:22:53,960
You have three attempts to do this lab.

464
00:22:53,960 --> 00:22:55,560
So, feel free to repeat it, and edit

465
00:22:55,560 --> 00:22:58,040
the code as you see fit in your cab data lab notebooks.

466
00:22:58,040 --> 00:23:00,000
All right, we'll see around. Nice job.