1
00:00:00,780 --> 00:00:05,240
Este é o último laboratório do módulo
sobre generalização e amostragem.

2
00:00:05,240 --> 00:00:07,110
Ele é bastante abrangente.

3
00:00:07,110 --> 00:00:11,660
Portanto, é normal levar mais tempo
para trabalhar e concluir todas as etapas.

4
00:00:11,660 --> 00:00:13,805
Vejamos o passo a passo da solução.

5
00:00:13,805 --> 00:00:17,465
Se você ainda não tiver tentado,
use o notebook de dados,

6
00:00:17,465 --> 00:00:21,515
o notebook IPython e execute o
código que vê nas células.

7
00:00:21,515 --> 00:00:23,890
Depois, volte a este vídeo
com o passo a passo.

8
00:00:23,890 --> 00:00:27,675
Aos que me acompanham, vamos
em frente e vejamos o que temos aqui.

9
00:00:27,675 --> 00:00:33,560
Extraí os dados do notebook
de estimativa de táxi do Google Cloud.

10
00:00:33,560 --> 00:00:36,730
Nosso objetivo é explorar os dados.

11
00:00:36,730 --> 00:00:37,980
Vocês se lembram das três etapas?

12
00:00:37,980 --> 00:00:39,535
Precisamos explorar os dados,

13
00:00:39,535 --> 00:00:41,650
precisamos criar aqueles
conjuntos de dados.

14
00:00:41,650 --> 00:00:45,125
Portanto, agora você está realmente
entendendo como lidar com essas funções

15
00:00:45,125 --> 00:00:50,500
além daquelas três etapas: o conjunto
de dados de treinamento, avaliação e teste.

16
00:00:50,500 --> 00:00:52,890
E o último ponto que talvez
você ainda não tenha visto

17
00:00:52,890 --> 00:00:54,895
é como criar um benchmark.

18
00:00:54,895 --> 00:00:56,665
Podemos voltar nisso mais tarde,

19
00:00:56,665 --> 00:00:59,250
quando você souber mais
sobre aprendizado de máquina e

20
00:00:59,250 --> 00:01:03,030
trocar o modelo simplista por
algo mais avançado mais tarde,

21
00:01:03,030 --> 00:01:05,525
como criar uma rede neural
profunda com o TensorFlow.

22
00:01:05,525 --> 00:01:09,140
Antes disso, precisamos começar
do zero e percorrer todo o caminho

23
00:01:09,140 --> 00:01:10,415
desde o começo.

24
00:01:10,415 --> 00:01:15,075
A primeira coisa que precisamos
fazer é conseguir a amostra dos dados.

25
00:01:15,075 --> 00:01:18,805
O bom do BigQuery é que
ele tem muitos conjuntos de dados públicos.

26
00:01:18,805 --> 00:01:23,435
E, assim como os dados de voos,
os dados de táxi também estão disponíveis.

27
00:01:23,435 --> 00:01:28,785
Extrairemos todas as tarifas de
táxi para a cidade de Nova York.

28
00:01:28,785 --> 00:01:33,320
E isso está neste conjunto de dados públicos
e nos campos que vamos verificar.

29
00:01:33,320 --> 00:01:35,860
Aqui entra um pouco de 
engenharia de recursos

30
00:01:35,860 --> 00:01:38,650
para elaborar o que vamos explorar
e, no fim, criar nosso modelo.

31
00:01:38,650 --> 00:01:42,440
Se você quiser pensar sobre
o problema da previsão da tarifa de táxi,

32
00:01:42,440 --> 00:01:44,665
quais seriam seus pontos de interesse?

33
00:01:44,665 --> 00:01:47,880
Provavelmente, você vai querer
saber onde as corridas começaram,

34
00:01:47,880 --> 00:01:52,255
a latitude e longitude exatas
dos pontos de início e fim da corrida

35
00:01:52,255 --> 00:01:54,320
e quantas pessoas estavam no táxi.

36
00:01:54,320 --> 00:01:58,200
Talvez haja várias tarifas diferentes
ou uma estrutura de valores justa,

37
00:01:58,200 --> 00:02:00,045
que considere o número de passageiros,

38
00:02:00,045 --> 00:02:03,750
a distância da corrida ou
o que acontece ao cruzar uma ponte.

39
00:02:03,750 --> 00:02:08,790
Esse é o valor total: a soma do valor
da tarifa mais gorjetas e despesas opcionais.

40
00:02:08,790 --> 00:02:10,419
É assim que chegamos a esse valor.

41
00:02:10,419 --> 00:02:13,420
Veremos quais desses fatores contribuem

42
00:02:13,420 --> 00:02:16,300
para determinar a tarifa final da corrida,

43
00:02:16,300 --> 00:02:18,550
antes mesmo de você entrar no táxi.

44
00:02:18,795 --> 00:02:21,175
Antes de tudo,
precisamos obter os dados.

45
00:02:21,175 --> 00:02:23,700
Como se vê aqui, para obter
os dados no Cloud Datalab,

46
00:02:23,700 --> 00:02:28,275
chamaremos uma consulta
do BigQuery a partir da amostra.

47
00:02:28,275 --> 00:02:31,315
Você tem as viagens
de táxi na cidade de Nova York

48
00:02:31,315 --> 00:02:34,030
e extraiu todos os campos
que acabei de mencionar.

49
00:02:35,340 --> 00:02:40,760
Agora, vejamos uma parte
pequena dos dados.

50
00:02:40,760 --> 00:02:43,690
Usaremos uma amostra de 1%,

51
00:02:43,690 --> 00:02:47,305
assim como fizemos
com os dados de voos no último laboratório.

52
00:02:47,305 --> 00:02:50,400
Usaremos apenas
um pequeno subconjunto da cidade aqui.

53
00:02:50,400 --> 00:02:52,045
Aqui está a consulta inicial.

54
00:02:52,045 --> 00:02:56,790
Queremos usar 100.000 registros.

55
00:02:56,790 --> 00:03:02,395
Na verdade, temos
100.000 opções de registros para escolher.

56
00:03:02,395 --> 00:03:08,920
Vamos ver se conseguimos ficar com
apenas 10.000 corridas desse conjunto.

57
00:03:09,350 --> 00:03:10,055
Ok.

58
00:03:10,055 --> 00:03:13,995
Definimos parâmetros para a consulta SQL.

59
00:03:13,995 --> 00:03:17,420
Você pode definir parâmetros
da mesma forma que substitui strings, certo?

60
00:03:17,420 --> 00:03:21,075
A consulta consiste
em pegar a consulta de dados brutos,

61
00:03:21,075 --> 00:03:23,420
já que os especificamos como brutos,

62
00:03:23,420 --> 00:03:27,835
e substituir todos os n.
Assim, conseguimos um registro.

63
00:03:27,835 --> 00:03:30,385
Faça a amostra de cada n,

64
00:03:30,385 --> 00:03:34,290
e depois o tamanho total,
que é de 100.000 registros.

65
00:03:34,290 --> 00:03:36,975
Depois, você imprimirá a
consulta e a executará.

66
00:03:36,975 --> 00:03:39,195
Esta é a consulta,

67
00:03:39,195 --> 00:03:45,965
estamos separando uma amostra
em que o resto da operação é 1.

68
00:03:45,965 --> 00:03:49,305
E agora estamos
com apenas 10.000 corridas de táxi.

69
00:03:49,305 --> 00:03:54,820
Fizemos essa nova amostragem porque não
queremos pegar os primeiros 1.000 registros.

70
00:03:54,820 --> 00:03:58,050
Eles podem estar ordenados,
e isso causaria um viés nos dados.

71
00:03:58,050 --> 00:04:04,830
Neste caso, elas podem ter sido
ordenadas pelas corridas mais recentes.

72
00:04:04,830 --> 00:04:09,550
Se você começar a explorar as
3.000 corridas mais recentes, por exemplo,

73
00:04:09,550 --> 00:04:12,310
os resultados podem ser influenciados

74
00:04:12,310 --> 00:04:17,085
por uma mudança, aumento
ou diminuição recente na tarifa

75
00:04:17,085 --> 00:04:20,105
que talvez você não percebesse
se olhasse apenas esses registros.

76
00:04:20,105 --> 00:04:22,320
Chamamos isso de viés de recência.

77
00:04:22,320 --> 00:04:24,445
Fizemos a
amostragem de maneira eficiente,

78
00:04:24,445 --> 00:04:26,385
e este é o nosso resultado.

79
00:04:26,385 --> 00:04:28,340
E nós ainda não fizemos nada.

80
00:04:28,340 --> 00:04:31,770
Este é apenas
o campo retornado dos conjuntos de dados.

81
00:04:31,770 --> 00:04:34,040
Na próxima etapa,
começaremos a explorá-los.

82
00:04:34,040 --> 00:04:36,080
Veja que temos o número de passageiros,

83
00:04:36,080 --> 00:04:38,425
de 1 a 5 aqui, e alguns dos exemplos.

84
00:04:38,425 --> 00:04:40,080
Há também a distância percorrida.

85
00:04:40,080 --> 00:04:41,450
Muitos dados interessantes.

86
00:04:41,450 --> 00:04:45,310
A distância será zero se ela for
calculada em milhas.

87
00:04:45,310 --> 00:04:46,800
Isso parece um pouco estranho.

88
00:04:46,800 --> 00:04:48,995
Podemos esperar pedágios zerados,

89
00:04:48,995 --> 00:04:52,825
tarifas de US$ 2,50
e o valor total de US$ 2,50.

90
00:04:52,825 --> 00:04:55,300
Ok. Os dados são interessantes.

91
00:04:55,300 --> 00:04:57,765
Vejamos se é possível explorá-los
um pouco mais rápido.

92
00:04:57,765 --> 00:05:01,470
A melhor maneira de fazer isso é
criar uma visualização dos dados.

93
00:05:01,470 --> 00:05:03,445
Muitas vezes, no aprendizado de máquina,

94
00:05:03,445 --> 00:05:07,355
criamos gráficos de dispersão
e verificamos alguns pontos.

95
00:05:07,355 --> 00:05:11,120
Aqui, plotamos a distância
da viagem em relação ao valor da tarifa.

96
00:05:11,120 --> 00:05:12,310
Você deve estar pensando

97
00:05:12,310 --> 00:05:15,705
"quanto maior a corrida,
maior a tarifa no taxímetro".

98
00:05:16,115 --> 00:05:19,215
Aqui vemos que quanto maior a corrida...

99
00:05:19,215 --> 00:05:23,115
Mesmo que a distância da corrida seja 40,

100
00:05:23,115 --> 00:05:25,830
você verá um valor
de tarifa geral alto de US$ 100,00.

101
00:05:25,830 --> 00:05:27,890
Mas é possível notar dois desvios,

102
00:05:27,890 --> 00:05:30,845
talvez duas anomalias estranhas
nos dados exibidos aqui.

103
00:05:30,845 --> 00:05:34,955
Há inúmeras corridas muito curtas,
e mesmo corridas que poderiam ser zero

104
00:05:34,955 --> 00:05:36,290
por estarem bem nesta linha.

105
00:05:36,290 --> 00:05:39,260
Isso é uma anomalia.
Precisamos removê-la do conjunto de dados.

106
00:05:39,260 --> 00:05:43,205
Não existe uma corrida sem um destino,
talvez o passageiro tenha sido expulso.

107
00:05:43,205 --> 00:05:47,990
Você deve olhar os pontos
que estão no zero nesta linha.

108
00:05:47,990 --> 00:05:55,850
E talvez para os pontos que formam
esta linha que cresce na diagonal.

109
00:05:56,285 --> 00:06:00,775
Parece uma linha, mas, na verdade,
são muitos pontos coletados nessa linha.

110
00:06:00,780 --> 00:06:02,545
Isso se deve à natureza dos dados.

111
00:06:02,545 --> 00:06:06,600
É interessante porque em Nova York
quando você sai do aeroporto JFK,

112
00:06:06,600 --> 00:06:10,650
você pode pegar um táxi com valor fixo
para qualquer lugar de Manhattan.

113
00:06:10,650 --> 00:06:12,505
Isso representa uma tarifa fixa.

114
00:06:12,505 --> 00:06:16,255
Por isso, a distância percorrida
já estará definida naquele momento.

115
00:06:16,255 --> 00:06:20,170
E é por isso que é fácil modelar
este caso, que é representado por uma linha.

116
00:06:20,170 --> 00:06:23,460
Mas não queremos prever
apenas as corridas que saem do JFK,

117
00:06:23,460 --> 00:06:26,285
queremos prever as corridas de
qualquer lugar de Nova York.

118
00:06:26,285 --> 00:06:29,055
Interessante, não?

119
00:06:29,055 --> 00:06:32,000
Vejamos algumas formas de
pré-processar e limpar os dados

120
00:06:32,000 --> 00:06:37,550
antes de agrupá-los nos conjuntos
de dados de treinamento, validação e teste.

121
00:06:38,005 --> 00:06:42,410
Você não quer passar para as divisões
de conjuntos sem limpar os dados antes.

122
00:06:42,410 --> 00:06:43,870
O lixo que chega tem que sair.

123
00:06:43,870 --> 00:06:46,830
Se você dividir dados ruins,
terá modelos ruins como resultado.

124
00:06:46,830 --> 00:06:50,330
Não vai conseguir reproduzir um
modelo de comportamento do mundo real.

125
00:06:50,330 --> 00:06:52,885
A regra geral é que
todos os dados estão sujos.

126
00:06:52,885 --> 00:06:56,580
Limpe-os e garanta que estão
corretos antes de integrá-los ao modelo.

127
00:06:56,580 --> 00:06:59,530
Seu modelo quer dados
de alta qualidade. É disso que ele gosta.

128
00:07:00,030 --> 00:07:02,920
Vamos observar algumas corridas.

129
00:07:02,920 --> 00:07:06,860
Vejamos agora algumas
das corridas que cruzaram pontes.

130
00:07:06,860 --> 00:07:09,220
Elas têm pedágio maior que zero.

131
00:07:09,220 --> 00:07:11,990
Em um determinado dia,
olhamos a hora de início da corrida.

132
00:07:11,990 --> 00:07:14,780
Neste exemplo, estamos em 20/05/2014.

133
00:07:14,780 --> 00:07:20,710
É interessante notar valores de
longitude ou latitude de partida iguais a 0.

134
00:07:21,435 --> 00:07:25,305
Obviamente, trata-se
de dados errados ou sujos.

135
00:07:25,305 --> 00:07:29,210
Precisamos remover dados
que representam locais de partida inválidos.

136
00:07:29,210 --> 00:07:32,380
Você precisa de um conjunto
de dados que faça sentido

137
00:07:32,380 --> 00:07:36,445
e que não tenha registros muito estranhos.

138
00:07:37,145 --> 00:07:41,200
Outro ponto que se pode
notar aqui é o valor total,

139
00:07:41,200 --> 00:07:45,405
em nenhum momento falamos
das colunas disponíveis para nós,

140
00:07:45,405 --> 00:07:48,395
o que o cliente usa como gorjeta ou

141
00:07:48,395 --> 00:07:51,800
qualquer valor em dinheiro
como uma gorjeta que não está registrada.

142
00:07:51,800 --> 00:07:53,655
Portanto, para os fins do nosso modelo,

143
00:07:53,655 --> 00:07:57,065
como não temos essa informação
e as gorjetas são opcionais,

144
00:07:57,065 --> 00:07:59,445
elas não são levadas em conta na tarifa.

145
00:07:59,445 --> 00:08:01,025
Não vamos prever isso.

146
00:08:01,025 --> 00:08:03,920
Agora, nós vamos definir o novo valor total

147
00:08:03,920 --> 00:08:11,085
como o valor total da distância percorrida
acrescido de quaisquer pedágios.

148
00:08:11,815 --> 00:08:13,760
Neste exemplo específico,

149
00:08:13,760 --> 00:08:20,055
o valor de US$ 8,50 refere-se
à distância percorrida, que é de 2,22,

150
00:08:20,055 --> 00:08:24,490
o valor de US$ 5,33 refere-se
ao pedágio da ponte atravessada.

151
00:08:24,490 --> 00:08:27,905
Vamos recalcular isso
somando os dois valores.

152
00:08:27,905 --> 00:08:29,210
Este será o valor total.

153
00:08:29,210 --> 00:08:30,435
Sem considerar gorjetas.

154
00:08:30,925 --> 00:08:31,795
Certo.

155
00:08:32,435 --> 00:08:36,025
Uma função interessante que você
pode aplicar é a .describe.

156
00:08:36,025 --> 00:08:39,450
Ela permite que você
conheça alguns dos limites

157
00:08:39,450 --> 00:08:42,419
ou intervalos de dados
para as suas colunas.

158
00:08:42,419 --> 00:08:44,070
É muito útil em estatística.

159
00:08:44,070 --> 00:08:47,670
Vamos conferir os valores mínimo e máximo.

160
00:08:47,670 --> 00:08:49,010
Caso eles não estejam claros

161
00:08:49,010 --> 00:08:52,165
para a longitude ou latitude
do local de partida quando era zero,

162
00:08:52,165 --> 00:08:53,960
você pode ver que o
valor máximo é zero e

163
00:08:53,960 --> 00:08:55,010
valor mínimo é zero.

164
00:08:55,010 --> 00:08:57,285
Agora você pode começar a
ver coisas muito estranhas.

165
00:08:57,285 --> 00:08:59,765
Uma que salta logo aos olhos é que,

166
00:08:59,765 --> 00:09:03,740
há um valor mínimo de corrida, que é -10.

167
00:09:03,740 --> 00:09:07,240
É impossível haver uma tarifa negativa.

168
00:09:07,240 --> 00:09:11,240
O motorista não paga a alguém
para entrar no carro, e sim o passageiro.

169
00:09:11,240 --> 00:09:13,925
Vamos procurar agora alguns dados...

170
00:09:13,925 --> 00:09:16,795
vamos ver o número máximo de passageiros.

171
00:09:16,795 --> 00:09:18,390
Felizmente, aqui são seis.

172
00:09:18,390 --> 00:09:24,120
Se esse número fosse 12,
não se trataria de um táxi, mas de um ônibus.

173
00:09:24,520 --> 00:09:25,980
Isso também vai aparecer.

174
00:09:25,980 --> 00:09:31,405
Aos poucos, estamos concentrando,
reduzindo e limpando o conjunto

175
00:09:31,405 --> 00:09:33,610
com um exercício de pré-processamento.

176
00:09:33,610 --> 00:09:37,300
Estamos nos preparando
para dividi-lo em três intervalos,

177
00:09:37,300 --> 00:09:40,930
para, então, criar um benchmark
simples que será superado mais tarde.

178
00:09:41,275 --> 00:09:41,975
Ok.

179
00:09:42,330 --> 00:09:47,090
O processo de entender
os dados pode demorar semanas.

180
00:09:47,090 --> 00:09:51,040
Se você não conhecer ou não for
um especialista no assunto dos conjuntos,

181
00:09:51,040 --> 00:09:55,725
você estará diante de centenas
de colunas ou bilhões de registros.

182
00:09:55,725 --> 00:09:59,495
Nesse caso, converse com
um especialista nos dados.

183
00:09:59,495 --> 00:10:03,600
Você precisa entender e visualizar
que relacionamentos estão presentes ali.

184
00:10:03,600 --> 00:10:06,605
Use visualizações diferentes
e funções estatísticas

185
00:10:06,605 --> 00:10:09,080
mesmo antes de entrar
no aprendizado de máquina.

186
00:10:09,080 --> 00:10:11,775
Você precisa entender
o que acontece nos dados.

187
00:10:11,775 --> 00:10:16,105
Ainda que tenhamos levado apenas
cinco minutos para fazer a exploração do ML,

188
00:10:16,105 --> 00:10:19,125
entender os conjuntos de dados
pode levar semanas ou meses.

189
00:10:19,125 --> 00:10:23,310
Vamos analisar algumas
corridas específicas.

190
00:10:23,310 --> 00:10:26,180
Aqui estamos plotando essas,
que são muito interessantes,

191
00:10:26,180 --> 00:10:30,480
você pode ver as corridas
em que temos a latitude e a longitude.

192
00:10:30,480 --> 00:10:32,295
Estas são as linhas das corridas.

193
00:10:32,295 --> 00:10:37,230
Veja que as linhas mais longas
normalmente incluem um pedágio.

194
00:10:37,230 --> 00:10:39,040
Intuitivamente, isso faz sentido.

195
00:10:39,040 --> 00:10:42,005
Se você está atravessando uma ponte,
a distância percorrida pode ser maior.

196
00:10:42,005 --> 00:10:45,420
Dificilmente alguém vai
entrar no táxi no começo da ponte,

197
00:10:45,420 --> 00:10:49,365
atravessá-la e encerrar
a viagem logo após o fim da ponte.

198
00:10:49,365 --> 00:10:51,260
Esse é um bom insight.

199
00:10:51,260 --> 00:10:55,020
Ok. Veja como limparemos
todos esses dados.

200
00:10:55,020 --> 00:10:57,990
Estes são os cinco insights
sobre os quais conversamos antes.

201
00:10:57,990 --> 00:11:00,770
Nós especificamos que as 
longitudes e latitudes da cidade

202
00:11:00,770 --> 00:11:04,240
de Nova York devem estar entre -74 e 41.

203
00:11:04,240 --> 00:11:06,675
Não é possível
que o valor de passageiro seja zero.

204
00:11:06,675 --> 00:11:10,820
Da mesma forma, há um
limite máximo de passageiros,

205
00:11:10,820 --> 00:11:13,990
mas atenha-se ao parâmetro
que impede uma corrida sem passageiro.

206
00:11:13,990 --> 00:11:16,310
E, como conversamos sobre as gorjetas,

207
00:11:16,310 --> 00:11:18,660
recalcularemos o valor total considerando

208
00:11:18,660 --> 00:11:21,880
apenas o valor da tarifa e os
pedágios conforme visto aqui.

209
00:11:22,990 --> 00:11:27,385
Feito isso, saberemos os
locais de partida e chegada,

210
00:11:27,385 --> 00:11:29,750
mas não a distância da corrida.

211
00:11:29,750 --> 00:11:33,290
Essa é uma dificuldade
que muitas pessoas encontram

212
00:11:33,290 --> 00:11:37,190
ao criar conjuntos de dados de treinamento
para modelos de aprendizado de máquina.

213
00:11:37,190 --> 00:11:40,580
A distância não é conhecida e se
não for descoberta no tempo de produção

214
00:11:40,580 --> 00:11:41,625
não será treinável.

215
00:11:41,625 --> 00:11:48,040
Portanto, você não pode especular
que a distância era de 5 ou 5,5 milhas.

216
00:11:48,040 --> 00:11:50,040
Digamos que tenha custado US$ 1 por milha.

217
00:11:50,040 --> 00:11:56,035
Portanto, um modelo simplista fácil
mostra que a viagem final custará US$ 5,50.

218
00:11:56,035 --> 00:12:00,400
Digamos que eu tenha pedido um táxi.

219
00:12:00,400 --> 00:12:02,550
Logo em seguida, o modelo pergunta:

220
00:12:02,550 --> 00:12:04,590
"Ok, legal. Por quanto tempo você viajou?"

221
00:12:04,590 --> 00:12:06,730
E você diz:
"mas eu ainda não entrei no táxi".

222
00:12:06,730 --> 00:12:08,750
É como querer adivinhar o futuro.

223
00:12:08,750 --> 00:12:11,940
Não é possível antecipar,
treinar em dados que ainda não existem.

224
00:12:11,940 --> 00:12:14,290
É daí que estamos retirando os dados,

225
00:12:14,290 --> 00:12:16,515
do conjunto de dados de recursos.

226
00:12:16,515 --> 00:12:18,060
Esta é uma questão importante.

227
00:12:18,060 --> 00:12:19,780
Pense nos dados que existem,

228
00:12:19,780 --> 00:12:23,445
se eles continuarão a existir
quando você colocar isso em produção.

229
00:12:23,445 --> 00:12:28,830
Aqui vemos muitos filtros de cláusula
WHERE para a consulta do BigQuery.

230
00:12:28,830 --> 00:12:30,720
Estamos recalculando o valor da tarifa.

231
00:12:30,720 --> 00:12:32,970
Temos as colunas
diferentes como se vê aqui.

232
00:12:32,970 --> 00:12:36,350
Estamos renomeando-as
com aliases e criando esta função

233
00:12:36,350 --> 00:12:40,425
que indica que isto será
uma consulta parametrizada

234
00:12:40,425 --> 00:12:44,200
com que faremos amostragem
entre esses intervalos específicos.

235
00:12:44,200 --> 00:12:47,890
Estes são todos os nossos filtros,
como falamos um pouco antes.

236
00:12:47,890 --> 00:12:52,390
Aqui estão nossos operadores de módulo 
nas funções hash FARM_FINGERPRINT.

237
00:12:52,390 --> 00:12:54,675
Estamos gerando hash em pickup_datetime,

238
00:12:54,675 --> 00:13:00,170
e isso significa que haverá uma perda
nas mensagens em que você aplica hash.

239
00:13:00,170 --> 00:13:05,015
Abriremos mão de pickup_datetime
para que essa coluna seja usada

240
00:13:05,015 --> 00:13:07,815
para criar as barreiras
entre esses intervalos.

241
00:13:07,815 --> 00:13:10,520
Treinamento, avaliação e teste.

242
00:13:10,520 --> 00:13:20,650
Estamos dizendo que o horário
não interfere na previsão da tarifa.

243
00:13:21,275 --> 00:13:24,930
Criamos uma consulta
que pode ser parametrizada.

244
00:13:24,930 --> 00:13:28,360
Digamos que isso fosse no treinamento,

245
00:13:28,360 --> 00:13:32,490
o que você pode pensar que aconteceria
se eu processasse esta consulta 3 vezes?

246
00:13:32,490 --> 00:13:34,180
Você criará três conjuntos de dados:

247
00:13:34,180 --> 00:13:36,030
treinamento, avaliação e teste.

248
00:13:36,030 --> 00:13:37,785
Se estamos no treinamento,

249
00:13:37,785 --> 00:13:39,715
queremos 70% dos dados,

250
00:13:39,715 --> 00:13:42,195
uma subamostra entre 0 e 70.

251
00:13:42,195 --> 00:13:46,750
Como se vê aqui, sample_between é a
consulta criada antes de a,b.

252
00:13:46,750 --> 00:13:50,365
E a,b se conectam a a e b aqui.

253
00:13:50,365 --> 00:13:56,640
Isso funciona no operador
de módulo que você vê a cada fim.

254
00:13:56,640 --> 00:14:03,510
No treinamento, aquela validação
de 70% fica entre 70 e 85 menos esses 2.

255
00:14:03,510 --> 00:14:07,350
Ou seja, é um adicional dos últimos 15% do
conjunto de dados de treinamento disponível.

256
00:14:07,350 --> 00:14:13,595
Isso significa que seu teste
terá de 85% a 100% dos dados.

257
00:14:13,595 --> 00:14:16,000
Ok. Tudo pronto para ser executado.

258
00:14:16,000 --> 00:14:18,600
Veja como seria
uma consulta se a executássemos.

259
00:14:20,950 --> 00:14:26,275
E agora vamos especificar o local
em que os resultados serão armazenados.

260
00:14:26,275 --> 00:14:30,110
Basicamente, precisamos de alguns
arquivos CSV ou algum outro formato

261
00:14:30,110 --> 00:14:35,080
que permita que o aprendizado
de máquina entenda e acesse esses dados.

262
00:14:35,080 --> 00:14:38,510
Para isso, temos que criar uma função
para gerar esses CSVs.

263
00:14:39,420 --> 00:14:41,570
Neste caso específico,
o treinamento é local.

264
00:14:41,570 --> 00:14:44,900
Portanto, armazenaremos
e criaremos esses CSVs no Datalab.

265
00:14:44,900 --> 00:14:48,345
Em módulos futuros, quando
você entender melhor o mecanismo de ML,

266
00:14:48,345 --> 00:14:54,210
você pode usar outra
etapa escalável com mais prototipagem.

267
00:14:54,210 --> 00:14:57,145
Aqui tentaremos fazer tudo
localmente no Cloud Datalab.

268
00:14:57,145 --> 00:15:02,105
Mas você vê que pode haver
referência direta aos dados na consulta

269
00:15:02,105 --> 00:15:07,695
além de nas lojas e nos intervalos
de armazenamento do Google Cloud.

270
00:15:07,695 --> 00:15:10,010
Ok. Estes são os CSVs que estamos criando.

271
00:15:10,010 --> 00:15:14,990
Agora, precisamos que o valor da tarifa
seja excluído e atualizado com o novo.

272
00:15:14,990 --> 00:15:16,980
Estes são os recursos que despejamos.

273
00:15:16,980 --> 00:15:21,745
É praticamente tudo que foi
incluído na consulta anterior.

274
00:15:21,745 --> 00:15:23,745
E aqui está o loop importante.

275
00:15:23,745 --> 00:15:28,040
Para a introdução gradual
a treino, validação e teste,

276
00:15:28,040 --> 00:15:33,015
invoque essa consulta
na amostra de 100.000.

277
00:15:33,015 --> 00:15:35,555
Em seguida, execute
a consulta do BigQuery.

278
00:15:35,555 --> 00:15:37,610
Depois, retorne
os resultados a um dataframe

279
00:15:37,610 --> 00:15:39,955
com que possamos interagir e operar.

280
00:15:39,955 --> 00:15:42,225
Com esses resultados,

281
00:15:42,225 --> 00:15:48,760
armazenamos aquele
dataframe com um prefixo táxi-{},

282
00:15:48,760 --> 00:15:51,060
e esse será o nome
do seu conjunto de dados.

283
00:15:51,060 --> 00:15:57,980
É como táxi-treino, táxi-validação,
táxi-teste no armazenamento dos CSVs.

284
00:15:58,430 --> 00:16:00,800
É isso que acontece aqui.

285
00:16:00,800 --> 00:16:03,120
Confie, mas verifique.

286
00:16:03,120 --> 00:16:05,920
É preciso ter certeza de que
os conjuntos de dados existem.

287
00:16:05,920 --> 00:16:08,780
Fazemos um ls simples nos nossos arquivos,

288
00:16:08,780 --> 00:16:15,550
e vemos que há 58.000 corridas
de táxi no conjunto de dados de teste,

289
00:16:15,550 --> 00:16:18,890
400.000 corridas no treinamento

290
00:16:18,890 --> 00:16:21,390
e 100.000 na validação.

291
00:16:21,390 --> 00:16:25,975
Isso reflete a divisão que fizemos antes.

292
00:16:25,975 --> 00:16:29,420
70, 15 e 15.

293
00:16:29,420 --> 00:16:35,660
Se você está se perguntando por que o teste
e a validação poderiam ser diferentes,

294
00:16:35,660 --> 00:16:39,000
isso se deve à distribuição dos dados.

295
00:16:39,000 --> 00:16:40,870
E a distribuição pode não ser normal.

296
00:16:40,870 --> 00:16:47,180
Se você tiver muitas datas próximas e
gerar hash em um dia como 01/01/2018,

297
00:16:47,180 --> 00:16:49,250
o resultado retornado será o mesmo.

298
00:16:49,250 --> 00:16:51,115
Os dados não são
ruidosos o suficiente.

299
00:16:51,115 --> 00:16:53,465
Mesmo que você estipule 70, 15, 15,

300
00:16:53,465 --> 00:16:56,710
o hash será gerado em blocos

301
00:16:56,710 --> 00:17:01,295
porque talvez tenham ocorrido
muitas corridas de táxi no Ano Novo

302
00:17:01,295 --> 00:17:04,290
e elas precisaram ser agrupadas
em intervalos diferentes, certo?

303
00:17:04,290 --> 00:17:05,685
Elas não podem estar nos dois

304
00:17:05,685 --> 00:17:10,109
porque não é possível dividir uma única data

305
00:17:10,109 --> 00:17:14,109
quando se gera hash
em dois lugares diferentes.

306
00:17:14,109 --> 00:17:19,790
Vejamos as divisões.

307
00:17:21,280 --> 00:17:26,305
Agora que temos todos os dados prontos
nestes três intervalos separados,

308
00:17:26,305 --> 00:17:31,570
é hora de começar a criar
o que chamo de modelo fictício.

309
00:17:31,570 --> 00:17:32,745
Este é o seu benchmark.

310
00:17:32,745 --> 00:17:38,880
Se você tivesse uma simples intuição
sobre o valor da corrida do táxi...

311
00:17:38,880 --> 00:17:44,590
Sem considerar o clima,
se você está saindo de um aeroporto, etc.

312
00:17:44,595 --> 00:17:47,250
Todas essas percepções
e recursos mais complexos

313
00:17:47,250 --> 00:17:49,390
que você pode aplicar a um
modelo avançado

314
00:17:49,390 --> 00:17:52,210
serão usados depois,
quando aprendermos o TensorFlow

315
00:17:52,210 --> 00:17:54,035
e a fazer engenharia
de recursos corretamente.

316
00:17:54,035 --> 00:17:56,480
Agora queremos criar um modelo simples

317
00:17:56,480 --> 00:18:05,590
que supere o RMSE e a métrica de perda
do modelo que usamos como benchmark.

318
00:18:05,990 --> 00:18:08,410
Como será esse modelo simples?

319
00:18:08,410 --> 00:18:10,470
Vamos dar uma olhada.

320
00:18:10,470 --> 00:18:13,310
Antes de tudo, precisaremos
prever a distância da corrida.

321
00:18:13,310 --> 00:18:14,910
Um modelo simples fará isso.

322
00:18:14,910 --> 00:18:19,300
Ele também pegará o valor
total da tarifa e o dividirá pela distância.

323
00:18:19,300 --> 00:18:23,365
Usaremos uma taxa por milha,
quilômetro ou algo semelhante.

324
00:18:23,365 --> 00:18:27,040
Depois, com base no conjunto
de dados de treinamento que conhecemos,

325
00:18:27,040 --> 00:18:28,640
no conjunto de dados marcados,

326
00:18:28,640 --> 00:18:32,275
no fim das contas, nós
saberemos o valor da tarifa.

327
00:18:32,275 --> 00:18:35,740
É assim que podemos calcular
a métrica de perda dos dados.

328
00:18:35,740 --> 00:18:39,580
E usaremos o RMSE porque
temos um modelo linear bem flutuante.

329
00:18:39,580 --> 00:18:41,250
Veja como fazemos isso.

330
00:18:41,730 --> 00:18:45,560
Definiremos algumas funções
diferentes para medir as distâncias

331
00:18:45,560 --> 00:18:49,975
entre as latitudes e longitudes
dos pontos de partida e chegada.

332
00:18:49,975 --> 00:18:58,815
Estimaremos esta distância e descobrir
a distância que o táxi percorreu.

333
00:18:59,245 --> 00:19:03,150
Nós temos esta informação no treinamento,
mas, como estamos fazendo a previsão,

334
00:19:03,150 --> 00:19:04,760
não podemos usar aquelas colunas.

335
00:19:04,760 --> 00:19:06,130
Faremos uma nova previsão.

336
00:19:06,130 --> 00:19:11,000
Em seguida, calcula-se o valor de RMSE
como se vê na equação listada.

337
00:19:11,000 --> 00:19:14,960
Depois, vamos imprimir
e analisar nossos recursos no modelo.

338
00:19:14,960 --> 00:19:16,950
Na verdade, queremos prever nossa meta.

339
00:19:16,950 --> 00:19:18,795
Estamos prevendo o valor da tarifa.

340
00:19:18,795 --> 00:19:20,615
Vamos listar os recursos,

341
00:19:20,615 --> 00:19:26,750
depois, definir nossos dataframes
para treinamento, validação e teste.

342
00:19:26,750 --> 00:19:29,290
Aqueles três conjuntos de dados.

343
00:19:29,290 --> 00:19:31,800
Por fim, faremos o treinamento.

344
00:19:31,800 --> 00:19:35,265
Treinaremos um modelo simples
que faz a previsão do valor da tarifa

345
00:19:35,265 --> 00:19:42,330
como a divisão da média
pela taxa que estávamos calculando,

346
00:19:42,330 --> 00:19:46,050
que é a média dos custos.

347
00:19:46,050 --> 00:19:50,295
Algo como uma corrida de 10 dólares
dividida pela média da distância percorrida.

348
00:19:50,295 --> 00:19:57,450
A linha 28 é a única
em que há algum tipo de modelagem.

349
00:19:57,450 --> 00:20:00,680
Já gastamos de 15 a 20 minutos
na demonstração deste laboratório

350
00:20:00,680 --> 00:20:04,605
e a linha 28 é a única em que
fazemos a previsão ou modelagem.

351
00:20:04,605 --> 00:20:11,410
Levamos esse tempo para criar, limpar
e pré-processar os conjuntos de dados,

352
00:20:11,410 --> 00:20:15,905
definir a configuração dos CSVs para
ingestão do modelo e facilitar o trabalho

353
00:20:15,905 --> 00:20:19,995
e finalmente ter esse modelo como
benchmark para o desempenho futuro.

354
00:20:19,995 --> 00:20:26,995
Esse índice de 99% de exploração,
limpeza e criação de novos conjuntos

355
00:20:27,000 --> 00:20:30,985
que estabelece os benchmarks de 99
para 1% do modelo real vai mudar

356
00:20:30,985 --> 00:20:33,860
à medida que começarmos
a pensar na criação de modelos,

357
00:20:33,860 --> 00:20:37,650
em como criar modelos mais sofisticados
e fazer engenharia de recursos no futuro.

358
00:20:37,650 --> 00:20:40,065
Neste momento, este pode
ser o nosso benchmark.

359
00:20:40,065 --> 00:20:44,730
Esta é a taxa por quilômetro
a que chegamos, no fim das contas.

360
00:20:44,730 --> 00:20:52,230
Temos uma taxa de US$ 2,60 por quilômetro.

361
00:20:52,230 --> 00:20:54,665
E aqui estão os valores de RMSE.

362
00:20:54,665 --> 00:21:02,515
Temos uma métrica de perda
de treinamento de 7,45 e validação de 9,35.

363
00:21:02,515 --> 00:21:08,530
Quando testamos, ficamos surpresos em ver
que foi o melhor de todos os três: 5,44.

364
00:21:08,530 --> 00:21:12,270
Agora, de qualquer forma,
este é o nosso benchmark.

365
00:21:12,270 --> 00:21:20,420
De modo geral, pode-se dizer que a tarifa
do táxi custará US$ 2,61 por quilômetro

366
00:21:20,420 --> 00:21:25,785
independentemente de destino, trânsito,
destino em Manhattan,

367
00:21:25,785 --> 00:21:27,850
além de desconsiderar pedágios nas pontes.

368
00:21:27,850 --> 00:21:31,200
Não temos parâmetros aqui para saber
se você vai passar por uma ponte.

369
00:21:31,200 --> 00:21:32,995
O horário também não é considerado.

370
00:21:32,995 --> 00:21:36,230
Portanto, com relação a todos
esses fatores em que você estava pensando,

371
00:21:36,230 --> 00:21:39,095
não é possível aplicar
uma codificação rígida a 2,6 x kms.

372
00:21:39,095 --> 00:21:42,170
Toda a intuição que desenvolvermos
em modelos mais sofisticados,

373
00:21:42,170 --> 00:21:49,000
no fim das contas, funcionará muito melhor
com outros insights avançados que tivermos.

374
00:21:49,000 --> 00:21:53,880
Voltaremos a isso no futuro
e melhoraremos o 5,44.

375
00:21:53,880 --> 00:21:58,690
Este é seu valor de RMSE
de benchmark a ser melhorado.

376
00:21:58,690 --> 00:22:00,940
No fim, o valor de RMSE,

377
00:22:00,940 --> 00:22:04,725
se pegarmos 5,44 vezes a taxa atual,

378
00:22:04,725 --> 00:22:08,270
chegaremos a 9,...

379
00:22:08,270 --> 00:22:11,520
Na verdade, isto é um pouquinho diferente.

380
00:22:11,520 --> 00:22:14,330
Este é o 5,44 para este
conjunto de dados aqui.

381
00:22:14,330 --> 00:22:17,105
E talvez você tenha uma
resposta um pouco diferente.

382
00:22:17,105 --> 00:22:20,235
Ok. Agora chegamos ao fim do laboratório.

383
00:22:20,235 --> 00:22:24,495
Eu incentivo você a continuar
fazendo os cursos da especialização.

384
00:22:24,495 --> 00:22:27,675
Agora que você começou,
não pode parar aqui.

385
00:22:27,675 --> 00:22:31,025
Agora que você aprendeu
a obter, limpar e ajustar os dados,

386
00:22:31,025 --> 00:22:34,385
além de criar o modelo de benchmark,
você não pode parar por aqui.

387
00:22:34,385 --> 00:22:37,220
Você está pronto para criar
modelos mais sofisticados e programar

388
00:22:37,220 --> 00:22:40,550
usando todos os recursos interessantes

389
00:22:40,550 --> 00:22:42,525
que o modelo pode oferecer para
obter insights mais elaborados

390
00:22:42,525 --> 00:22:45,540
e superar este modelo
com este valor de RMSE.

391
00:22:45,540 --> 00:22:51,130
Acompanhe os próximos cursos sobre
TensorFlow e aprenda a superar esse valor.

392
00:22:51,130 --> 00:22:54,600
Fique à vontade, você tem três
tentativas para completar este laboratório.

393
00:22:54,600 --> 00:22:58,230
Repita e edite os códigos o quanto
quiser usando os notebooks do Datalab.

394
00:22:58,230 --> 00:23:00,000
Até a próxima. Bom trabalho.