1
00:00:00,000 --> 00:00:02,970
As you just learned, splitting your data set allows for

2
00:00:02,970 --> 00:00:06,245
testing your modeling against the simulated real world data set,

3
00:00:06,245 --> 00:00:09,075
by holding out those subsets of data from training.

4
00:00:09,075 --> 00:00:12,710
But how do we know actually where to divide our original data set?

5
00:00:12,710 --> 00:00:15,540
What if the data set itself is massive?

6
00:00:15,540 --> 00:00:18,775
Do we need to train and test across every single data point?

7
00:00:18,775 --> 00:00:20,455
In this lesson and sampling,

8
00:00:20,455 --> 00:00:22,020
I'll guide you through how to actually,

9
00:00:22,020 --> 00:00:24,305
split your data set in a repeatable way,

10
00:00:24,305 --> 00:00:28,405
using Google Big Query and those several key pitfalls that you should avoid.

11
00:00:28,405 --> 00:00:34,280
Then you'll practice this yourself in the next lab. Let's get started.

12
00:00:34,280 --> 00:00:37,455
Now, before we discuss splitting our data sets,

13
00:00:37,455 --> 00:00:39,425
we first need to get one to split.

14
00:00:39,425 --> 00:00:41,230
For this example, we'll use

15
00:00:41,230 --> 00:00:46,505
Airline Ontime performance data from the U.S. Bureau of Transportation and Statistics.

16
00:00:46,505 --> 00:00:49,640
Google has made this public data available to all users in

17
00:00:49,640 --> 00:00:53,435
Big Query, as the airlineontimedata.flightsdataset.

18
00:00:53,435 --> 00:00:56,320
Now this data set has tracked the arrival and departure delays for

19
00:00:56,320 --> 00:01:00,705
flights and a new order of 70 million flights.

20
00:01:00,705 --> 00:01:03,855
Let's discuss how we can effectively sample training,

21
00:01:03,855 --> 00:01:07,010
validation, and testing data from this data set,

22
00:01:07,010 --> 00:01:09,615
in a uniform and repeatable way.

23
00:01:09,615 --> 00:01:12,315
SQL, this is structured query language,

24
00:01:12,315 --> 00:01:13,500
and then therefore Big Query,

25
00:01:13,500 --> 00:01:15,020
because it's what you execute SQL on,

26
00:01:15,020 --> 00:01:16,540
has the function Rand,

27
00:01:16,540 --> 00:01:19,750
and that'll generate a value between zero and one.

28
00:01:19,750 --> 00:01:23,720
You can very easily get 80 percent of your data set by just applying a simple SQL,

29
00:01:23,720 --> 00:01:25,865
where clause as shown here.

30
00:01:25,865 --> 00:01:28,845
You might notice some obvious issues with this.

31
00:01:28,845 --> 00:01:30,710
Think about whether or not this process will be

32
00:01:30,710 --> 00:01:32,590
repeatable if a colleague wanted to repeat

33
00:01:32,590 --> 00:01:36,750
your experiment with the same 80 percent training data set that you used.

34
00:01:36,750 --> 00:01:39,115
And assuming that it is set with 70 million flights,

35
00:01:39,115 --> 00:01:41,750
would they get the same 56 million flights or

36
00:01:41,750 --> 00:01:45,220
80 percent in the same training dataset that you did?

37
00:01:45,220 --> 00:01:48,660
Well, we need a better way of knowing which data

38
00:01:48,660 --> 00:01:52,635
belongs in which bucket training validation and testing,

39
00:01:52,635 --> 00:01:56,960
and this will enable us and our colleagues to repeat our experiments,

40
00:01:56,960 --> 00:01:59,570
using the exact same data for each bucket.

41
00:01:59,570 --> 00:02:01,450
Now, as you might have guessed,

42
00:02:01,450 --> 00:02:03,670
a simple random function would just grab

43
00:02:03,670 --> 00:02:06,935
a new set of five randomly selected rows shown here,

44
00:02:06,935 --> 00:02:08,565
each time you run the query.

45
00:02:08,565 --> 00:02:10,580
This makes it extremely difficult,

46
00:02:10,580 --> 00:02:12,475
almost impossible to identify and split

47
00:02:12,475 --> 00:02:16,170
the remaining 20 percent of your data for those validation and testing buckets.

48
00:02:16,170 --> 00:02:18,445
In addition, the dataset might also be sorted,

49
00:02:18,445 --> 00:02:19,950
which could add bias into your sample.

50
00:02:19,950 --> 00:02:22,600
And simply adding an order by also comes with

51
00:02:22,600 --> 00:02:26,785
its own problems when doing something like Mini-batch gradient descent.

52
00:02:26,785 --> 00:02:30,020
Now, for machine learning, you want to be able

53
00:02:30,020 --> 00:02:33,515
fundamentally to create these repeatable samples of data.

54
00:02:33,515 --> 00:02:36,590
One way to achieve this, is use the last few digits of

55
00:02:36,590 --> 00:02:41,175
a hash function on the field that you are using to split or bucketize your data.

56
00:02:41,175 --> 00:02:43,240
Once the hash function available publicly in

57
00:02:43,240 --> 00:02:45,900
BigQuery is called Farm fingerprint, just the hash function.

58
00:02:45,900 --> 00:02:49,735
Farm fingerprint will take a value like December 10 2018,

59
00:02:49,735 --> 00:02:52,470
turn into a long string of digits,

60
00:02:52,470 --> 00:02:54,370
and this hash value will be identical for

61
00:02:54,370 --> 00:02:57,445
every other December 10 2018 value in the dataset.

62
00:02:57,445 --> 00:03:01,575
Now, let's say you're building a machine learning algorithm to predict arrival delays.

63
00:03:01,575 --> 00:03:04,060
You might want to split up your data by date and get

64
00:03:04,060 --> 00:03:08,985
approximately 80 percent of the days in one data set, your training data set.

65
00:03:08,985 --> 00:03:10,775
Now this is actually repeatable,

66
00:03:10,775 --> 00:03:13,560
because the Farm fingerprint hash function returns

67
00:03:13,560 --> 00:03:17,430
the exact same value anytime it's vote on a specific date.

68
00:03:17,430 --> 00:03:20,300
You can be sure they're going to get the exact same 80 percent

69
00:03:20,300 --> 00:03:23,315
or roughly 80 percent of the data each time.

70
00:03:23,315 --> 00:03:25,810
If you split your data by arrival airport,

71
00:03:25,810 --> 00:03:28,270
set 80 percent of the airports are in the training data

72
00:03:28,270 --> 00:03:31,335
set and the others are in test and validation,

73
00:03:31,335 --> 00:03:34,495
then you'd use the hash function on arrival airport instead.

74
00:03:34,495 --> 00:03:36,410
So, looking at the query here,

75
00:03:36,410 --> 00:03:39,745
how would you get a new 10 percent sample for evaluation?

76
00:03:39,745 --> 00:03:44,160
Well, you would change the less than eight to equals eight for testing data,

77
00:03:44,160 --> 00:03:49,495
or equals eight or equals nine for another 10 percent for evaluation or testing.

78
00:03:49,495 --> 00:03:51,600
That's how you split up those buckets.

79
00:03:51,600 --> 00:03:55,720
So, say we wanted to predict flight delays based on air carrier,

80
00:03:55,720 --> 00:03:58,430
time of day, weather and airport characteristics,

81
00:03:58,430 --> 00:04:00,280
like the number of runways the airport has.

82
00:04:00,280 --> 00:04:02,395
What fields should we split our data set on?

83
00:04:02,395 --> 00:04:05,055
Date? Airport? Carrier name?

84
00:04:05,055 --> 00:04:07,280
So, be sure to split your data into

85
00:04:07,280 --> 00:04:10,745
those valid sets based on a calm that you can afford to loose.

86
00:04:10,745 --> 00:04:13,620
As an example, if you're looking to split on date to predict

87
00:04:13,620 --> 00:04:17,120
arrival delays and your data set only had flights for two days,

88
00:04:17,120 --> 00:04:19,675
you wouldn't be able to split it any more granular than 50-50.

89
00:04:19,675 --> 00:04:22,485
Remember the hash function is one way so it's just going to give you one value.

90
00:04:22,485 --> 00:04:25,115
You can't get 80-20 if you have just two dates.

91
00:04:25,115 --> 00:04:28,100
So, let's look at each of these options one by one.

92
00:04:28,100 --> 00:04:33,840
Okay? What if we bucketize or hash and split on date? Okay, that's fine.

93
00:04:33,840 --> 00:04:35,650
But understand that you can no longer make

94
00:04:35,650 --> 00:04:38,250
predictions based on something like the holidays,

95
00:04:38,250 --> 00:04:39,870
like Christmas or Thanksgiving.

96
00:04:39,870 --> 00:04:43,630
Be sure the primary drivers in your prediction have nothing to do with date,

97
00:04:43,630 --> 00:04:46,395
because that's how you bucketed and created those buckets.

98
00:04:46,395 --> 00:04:50,085
All right. What happens if we hash and split on airport name?

99
00:04:50,085 --> 00:04:52,650
Okay, that's fine, so long as it's distributed and noisy,

100
00:04:52,650 --> 00:04:56,580
understand that you can no longer make predictions that airport specific, for example.

101
00:04:56,580 --> 00:04:59,650
So, flights out of JFK at 5.00 pm are always late.

102
00:04:59,650 --> 00:05:02,880
You can no longer use JFK in that airport because you split on it.

103
00:05:02,880 --> 00:05:05,120
What if we hash and split on carrier name?

104
00:05:05,120 --> 00:05:10,245
And while there was only like 11 airline carriers and if you want to split your data,

105
00:05:10,245 --> 00:05:14,410
it's not well enough distributed to get a fine grained split.

106
00:05:14,410 --> 00:05:16,170
So, instead of an 80-20,

107
00:05:16,170 --> 00:05:20,185
you might just get like 60-40, which may not be good enough for you.

108
00:05:20,185 --> 00:05:22,670
Starting out with ML development,

109
00:05:22,670 --> 00:05:26,630
is best to develop your Tensorflow code on a small subset of data.

110
00:05:26,630 --> 00:05:30,605
Then later scale it out to the cloud for true productization.

111
00:05:30,605 --> 00:05:33,300
So, imagining you're developing an ML application,

112
00:05:33,300 --> 00:05:36,720
every time you make a change you have to rerun the application.

113
00:05:36,720 --> 00:05:38,460
If you use the full data set,

114
00:05:38,460 --> 00:05:40,460
this honestly could take hours or even days.

115
00:05:40,460 --> 00:05:44,165
You're talking petabytes of data and you can't develop software that way.

116
00:05:44,165 --> 00:05:48,130
You want a small dataset so that you can quickly run through your code,

117
00:05:48,130 --> 00:05:50,345
debug it, and then rerun it.

118
00:05:50,345 --> 00:05:52,625
Then, once the application is working properly,

119
00:05:52,625 --> 00:05:55,900
you can run it once or however many times you want on the full dataset,

120
00:05:55,900 --> 00:05:57,490
it's like prototyping, right?

121
00:05:57,490 --> 00:06:00,370
Next, let's see how we can uniformly sample

122
00:06:00,370 --> 00:06:03,780
a smaller subset of our airline data set to be used before.

123
00:06:03,780 --> 00:06:04,940
So, we've created the buckets,

124
00:06:04,940 --> 00:06:06,900
but now we want less data.

125
00:06:06,900 --> 00:06:11,110
So, the flight's data is 70 million rows.

126
00:06:11,110 --> 00:06:13,265
Perhaps you wanted a smaller dataset,

127
00:06:13,265 --> 00:06:16,310
say one million flights, again it has got to be repeatable.

128
00:06:16,310 --> 00:06:21,350
How would you pick one in 70 flights and then 80 percent of those as training?

129
00:06:21,350 --> 00:06:26,535
So, you can't pick one in 70 and then pick one in 10 from the results there.

130
00:06:26,535 --> 00:06:31,720
Can you figure out why? Well, if you're picking numbers that are divisible by 70,

131
00:06:31,720 --> 00:06:34,170
of course they're also going to be divisible by 10.

132
00:06:34,170 --> 00:06:36,280
That second modulo operation here,

133
00:06:36,280 --> 00:06:38,320
as you see, is useless.

134
00:06:38,320 --> 00:06:42,000
So, let's do a quick demo and I'll show you what this means.