1
00:00:00,000 --> 00:00:02,239
データセットを分割することで

2
00:00:02,239 --> 00:00:06,245
現実世界をシミュレーションした
データセットでモデルをテストできます

3
00:00:06,245 --> 00:00:10,115
そのために トレーニングには使わない
データのサブセットを分けておきます

4
00:00:10,115 --> 00:00:13,210
では 元のデータセットは
どこで分割したらよいでしょうか？

5
00:00:13,210 --> 00:00:15,390
データセット自体が巨大だったら？

6
00:00:15,390 --> 00:00:19,575
すべてのデータポイントで
トレーニングとテストが必要でしょうか？

7
00:00:19,575 --> 00:00:22,045
このサンプリングのレッスンでは

8
00:00:22,045 --> 00:00:26,465
Google BigQueryによる
繰り返し可能なデータセットの分割方法と

9
00:00:26,465 --> 00:00:29,475
避けるべき重要な落し穴を説明します

10
00:00:29,475 --> 00:00:32,880
そして 次のラボで実際に練習します

11
00:00:32,880 --> 00:00:34,280
では始めましょう

12
00:00:34,280 --> 00:00:37,455
データセットの分割を説明する前に

13
00:00:37,455 --> 00:00:39,425
分割するデータが必要です

14
00:00:39,425 --> 00:00:41,230
この例では

15
00:00:41,230 --> 00:00:46,115
アメリカ交通統計局による
航空会社の定時運航率を使います

16
00:00:46,115 --> 00:00:48,730
Googleではこの一般公開データを

17
00:00:48,730 --> 00:00:52,715
BigQueryのすべてのユーザーに提供しています
airlineontimedata.flightsdatasetです

18
00:00:52,715 --> 00:00:56,890
このデータセットは
フライトの到着と出発の遅延などを

19
00:00:56,890 --> 00:01:00,535
7,000万のフライトについて追跡しています

20
00:01:00,535 --> 00:01:02,165
このデータセットから

21
00:01:02,165 --> 00:01:06,880
トレーニング、評価、テスト用のデータを
サンプリングする方法を見つけましょう

22
00:01:06,880 --> 00:01:09,615
統一された繰り返し可能な方法が必要です

23
00:01:09,615 --> 00:01:14,325
構造化照会言語であるSQLと
SQLを実行する BigQueryは

24
00:01:14,325 --> 00:01:17,210
Rand関数を持っています

25
00:01:17,210 --> 00:01:19,450
これは 0と1の間の値を生成します

26
00:01:19,450 --> 00:01:23,940
データの80%を取得するのに
単純なSQLを適用できます

27
00:01:23,940 --> 00:01:25,865
ご覧のとおりWHERE節です

28
00:01:25,865 --> 00:01:28,965
これには明らかに問題があることに
気付くかもしれません

29
00:01:28,965 --> 00:01:31,310
この処理は繰り返し可能でしょうか

30
00:01:31,310 --> 00:01:34,800
あなたが使ったのと同じ80%の
トレーニング用データセットで

31
00:01:34,800 --> 00:01:36,820
同僚が同じ実験をしたいとしたらどうでしょう

32
00:01:36,820 --> 00:01:39,735
さらにこれは 7,000万ものフライトの
データセットです

33
00:01:39,735 --> 00:01:42,960
同僚の使用する5600万のフライト
つまり80%は

34
00:01:42,960 --> 00:01:46,400
あなたがトレーニングに使った80%のデータと
同じになるでしょうか？

35
00:01:46,400 --> 00:01:48,160
もっとよい方法が必要ですね

36
00:01:48,160 --> 00:01:53,325
どのデータが トレーニング用、評価用、テスト用の
バケットに属しているのか確認すべきです

37
00:01:53,325 --> 00:01:56,140
そうすれば自分と同僚が実験を繰り返すときに

38
00:01:56,140 --> 00:01:59,570
各バケットで同じデータを使うことができます

39
00:01:59,570 --> 00:02:01,770
さて お気づきの方もいるでしょう

40
00:02:01,770 --> 00:02:03,670
シンプルなランダム関数では

41
00:02:03,670 --> 00:02:06,515
5つのランダムに選択された行のセットを

42
00:02:06,515 --> 00:02:08,565
クエリを実行するたびに取得します

43
00:02:08,565 --> 00:02:12,310
この方法では 残り20%のデータを特定して

44
00:02:12,310 --> 00:02:15,885
評価用とテスト用のバケットに分割するのは
ほぼ不可能なほど困難です

45
00:02:15,885 --> 00:02:18,735
さらに データセットはソートされる場合もあり

46
00:02:18,735 --> 00:02:21,140
サンプルが偏る可能性があります

47
00:02:21,140 --> 00:02:22,910
order byを追加しても

48
00:02:22,910 --> 00:02:26,785
ミニバッチ勾配降下法などを行った場合に
問題が生じます

49
00:02:26,785 --> 00:02:28,250
機械学習では基本的に

50
00:02:28,250 --> 00:02:32,675
繰り返し可能なサンプルデータを
作成できる必要があります

51
00:02:32,675 --> 00:02:37,620
そのために フィールドのハッシュ関数の
最後の数桁を利用して

52
00:02:37,620 --> 00:02:40,385
データを分割またはバケット化します

53
00:02:40,385 --> 00:02:43,240
BigQueryで一般公開されている
ハッシュ関数は

54
00:02:43,240 --> 00:02:45,900
FARM_FINGERPRINTといいますが
普通のハッシュ関数です

55
00:02:45,900 --> 00:02:49,735
FARM_FINGERPRINTは
2018年12月10日のような値を

56
00:02:49,735 --> 00:02:52,470
桁数の大きな文字列に変えます

57
00:02:52,470 --> 00:02:54,370
このハッシュ値は

58
00:02:54,370 --> 00:02:57,445
データセットの他のすべての
2018年12月10日で同一です

59
00:02:57,445 --> 00:03:02,235
さて 機械学習アルゴリズムを構築して
到着時刻の遅れを推測するとします

60
00:03:02,235 --> 00:03:04,060
データを日付で分割して

61
00:03:04,060 --> 00:03:08,985
トレーニング用データセットに
日数の約80%を取得します

62
00:03:08,985 --> 00:03:10,775
これは繰り返し可能です

63
00:03:10,775 --> 00:03:13,560
FARM_FINGERPRINTハッシュ関数が

64
00:03:13,560 --> 00:03:17,430
同じ日付では常に同じ値を返すからです

65
00:03:17,430 --> 00:03:23,150
いつも全く同じか ほぼ同じ80%の
データを取得できます

66
00:03:23,150 --> 00:03:25,810
データを到着空港で分割する場合

67
00:03:25,810 --> 00:03:29,020
空港の80%が
トレーニング用データセットで

68
00:03:29,020 --> 00:03:31,115
残りがテスト用と評価用になります

69
00:03:31,115 --> 00:03:33,935
その場合 ハッシュ関数を到着空港に使います

70
00:03:33,935 --> 00:03:35,990
ここでクエリを見てください

71
00:03:35,990 --> 00:03:40,675
評価用に新しい10%のサンプルを
取得するにはどうしたらよいでしょう？

72
00:03:40,675 --> 00:03:44,790
テスト用データでは「8未満」から
「8に等しい」に変更します

73
00:03:44,790 --> 00:03:49,895
あるいは 残り10%の評価用、テスト用に
「8に等しい」「9に等しい」に変更します

74
00:03:49,895 --> 00:03:52,430
こうやってバケットを分割します

75
00:03:52,430 --> 00:03:55,130
フライトの遅延を予測する基準として

76
00:03:55,130 --> 00:03:59,870
航空会社、時刻、天候、滑走路の数などの
空港の特徴を使用するとします

77
00:03:59,870 --> 00:04:02,955
どのフィールドでデータセットを
分割したらよいでしょう？

78
00:04:02,955 --> 00:04:04,875
日付？空港？航空会社名？

79
00:04:04,875 --> 00:04:10,170
データを有効なセットに分割する際は
失ってもよい列を使用してください

80
00:04:10,170 --> 00:04:14,080
たとえば日付を分割して
到着時刻の遅れを予測しようとしています

81
00:04:14,080 --> 00:04:16,680
データセットは2日間のフライトだけです

82
00:04:16,680 --> 00:04:19,675
この場合50-50よりも
細かく分割することはできません

83
00:04:19,675 --> 00:04:22,485
ハッシュ関数は一方向なので
1つの値しか得られません

84
00:04:22,485 --> 00:04:25,115
日付が2つしかなければ
80-20にすることはできません

85
00:04:25,115 --> 00:04:28,100
これらのオプションを1つずつ確認しましょう

86
00:04:28,100 --> 00:04:33,840
日付に基づいて バケット化あるいはハッシュ分割したら
どうでしょう？ 問題ないですね

87
00:04:33,840 --> 00:04:35,650
ただ これにより

88
00:04:35,650 --> 00:04:39,880
クリスマスなどの休日に基づいた予測は
できなくなりました

89
00:04:39,880 --> 00:04:42,760
日付に基づいてバケットを作成したので

90
00:04:42,760 --> 00:04:46,395
予測の主要因は
日付と無関係なものにしなければなりません

91
00:04:46,395 --> 00:04:50,085
空港名でハッシュ分割したら
どうでしょうか？

92
00:04:50,085 --> 00:04:52,760
分散されていて
ノイズのあるデータなら問題ありません

93
00:04:52,760 --> 00:04:56,580
ただし 空港に関する予測はできなくなります

94
00:04:56,580 --> 00:05:00,240
たとえば JFKの午後5時のフライトは
いつも遅れます

95
00:05:00,240 --> 00:05:03,840
空港の JFKはもう使えません
それに基づいて分割したからです

96
00:05:03,840 --> 00:05:07,740
航空会社名でハッシュして分割する場合は
どうでしょうか？

97
00:05:07,740 --> 00:05:10,375
航空会社は11社しかないので

98
00:05:10,375 --> 00:05:14,410
十分な粒度の分割を行えるほど
分散されていません

99
00:05:14,410 --> 00:05:16,170
そこで80-20の代わりに

100
00:05:16,170 --> 00:05:20,185
60-40などにしますが
それでも十分ではないかもしれません

101
00:05:20,185 --> 00:05:22,190
MLの開発を始めるとき

102
00:05:22,190 --> 00:05:26,630
一番いいのはTensorFlowのコードを
小さいデータのサブセットで開発することです

103
00:05:26,630 --> 00:05:30,225
製品化する際にはクラウドに
スケールアウトします

104
00:05:30,225 --> 00:05:33,300
MLアプリケーションを開発しているとしましょう

105
00:05:33,300 --> 00:05:36,720
変更するたびにアプリケーションを
再実行する必要があります

106
00:05:36,720 --> 00:05:38,930
完全なデータセットを使っていると

107
00:05:38,930 --> 00:05:40,990
数時間や数日かかることがあります

108
00:05:40,990 --> 00:05:44,995
ペタバイトのデータを扱っていては
ソフトウェア開発はできません

109
00:05:44,995 --> 00:05:48,540
小さなデータセットを使えば
コードをすばやく実行して

110
00:05:48,540 --> 00:05:50,345
デバックし 再実行できます

111
00:05:50,345 --> 00:05:52,985
そして アプリケーションがきちんと動いたら

112
00:05:52,985 --> 00:05:55,900
完全なデータセットで好きなだけ
実行できます

113
00:05:55,900 --> 00:05:57,910
プロトタイピングに似ていますね

114
00:05:57,910 --> 00:06:01,470
では先ほどのフライトデータセットを
小さいサブセットに

115
00:06:01,470 --> 00:06:04,510
均一にサンプリングする方法を見てみましょう

116
00:06:04,510 --> 00:06:06,240
バケットを作成しましたが

117
00:06:06,240 --> 00:06:08,140
今度はデータを減らします

118
00:06:08,380 --> 00:06:11,110
フライトのデータは7,000万行です

119
00:06:11,110 --> 00:06:13,365
もっと小さなデータセットにしましょう

120
00:06:13,365 --> 00:06:16,250
100万件のフライトにします
繰り返し可能でなければなりません

121
00:06:16,250 --> 00:06:18,600
フライト70件につき1件を取得し

122
00:06:18,600 --> 00:06:22,410
さらにその80%をトレーニング用にするには
どうすればよいでしょうか

123
00:06:22,410 --> 00:06:26,755
70件中1件取得して その結果から
10件中1件を取得することはできません

124
00:06:26,755 --> 00:06:29,020
なぜかわかりますか？

125
00:06:29,020 --> 00:06:34,100
70で割り切れる数は
10でも割り切れるからです

126
00:06:34,100 --> 00:06:35,430
2番目のモジュロ演算は

127
00:06:35,430 --> 00:06:37,480
ご覧のとおり役に立ちません

128
00:06:37,480 --> 00:06:42,000
では簡単なデモを行い
どういうことなのかお見せしましょう