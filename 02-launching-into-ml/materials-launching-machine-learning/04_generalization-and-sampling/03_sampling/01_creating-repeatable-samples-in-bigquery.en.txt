As you just learned, splitting your data set allows for testing your modeling against the simulated real world data set, by holding out those subsets of data from training. But how do we know actually where to divide our original data set? What if the data set itself is massive? Do we need to train and test across every single data point? In this lesson and sampling, I'll guide you through how to actually, split your data set in a repeatable way, using Google Big Query and those several key pitfalls that you should avoid. Then you'll practice this yourself in the next lab. Let's get started. Now, before we discuss splitting our data sets, we first need to get one to split. For this example, we'll use Airline Ontime performance data from the U.S. Bureau of Transportation and Statistics. Google has made this public data available to all users in Big Query, as the airlineontimedata.flightsdataset. Now this data set has tracked the arrival and departure delays for flights and a new order of 70 million flights. Let's discuss how we can effectively sample training, validation, and testing data from this data set, in a uniform and repeatable way. SQL, this is structured query language, and then therefore Big Query, because it's what you execute SQL on, has the function Rand, and that'll generate a value between zero and one. You can very easily get 80 percent of your data set by just applying a simple SQL, where clause as shown here. You might notice some obvious issues with this. Think about whether or not this process will be repeatable if a colleague wanted to repeat your experiment with the same 80 percent training data set that you used. And assuming that it is set with 70 million flights, would they get the same 56 million flights or 80 percent in the same training dataset that you did? Well, we need a better way of knowing which data belongs in which bucket training validation and testing, and this will enable us and our colleagues to repeat our experiments, using the exact same data for each bucket. Now, as you might have guessed, a simple random function would just grab a new set of five randomly selected rows shown here, each time you run the query. This makes it extremely difficult, almost impossible to identify and split the remaining 20 percent of your data for those validation and testing buckets. In addition, the dataset might also be sorted, which could add bias into your sample. And simply adding an order by also comes with its own problems when doing something like Mini-batch gradient descent. Now, for machine learning, you want to be able fundamentally to create these repeatable samples of data. One way to achieve this, is use the last few digits of a hash function on the field that you are using to split or bucketize your data. Once the hash function available publicly in BigQuery is called Farm fingerprint, just the hash function. Farm fingerprint will take a value like December 10 2018, turn into a long string of digits, and this hash value will be identical for every other December 10 2018 value in the dataset. Now, let's say you're building a machine learning algorithm to predict arrival delays. You might want to split up your data by date and get approximately 80 percent of the days in one data set, your training data set. Now this is actually repeatable, because the Farm fingerprint hash function returns the exact same value anytime it's vote on a specific date. You can be sure they're going to get the exact same 80 percent or roughly 80 percent of the data each time. If you split your data by arrival airport, set 80 percent of the airports are in the training data set and the others are in test and validation, then you'd use the hash function on arrival airport instead. So, looking at the query here, how would you get a new 10 percent sample for evaluation? Well, you would change the less than eight to equals eight for testing data, or equals eight or equals nine for another 10 percent for evaluation or testing. That's how you split up those buckets. So, say we wanted to predict flight delays based on air carrier, time of day, weather and airport characteristics, like the number of runways the airport has. What fields should we split our data set on? Date? Airport? Carrier name? So, be sure to split your data into those valid sets based on a calm that you can afford to loose. As an example, if you're looking to split on date to predict arrival delays and your data set only had flights for two days, you wouldn't be able to split it any more granular than 50-50. Remember the hash function is one way so it's just going to give you one value. You can't get 80-20 if you have just two dates. So, let's look at each of these options one by one. Okay? What if we bucketize or hash and split on date? Okay, that's fine. But understand that you can no longer make predictions based on something like the holidays, like Christmas or Thanksgiving. Be sure the primary drivers in your prediction have nothing to do with date, because that's how you bucketed and created those buckets. All right. What happens if we hash and split on airport name? Okay, that's fine, so long as it's distributed and noisy, understand that you can no longer make predictions that airport specific, for example. So, flights out of JFK at 5.00 pm are always late. You can no longer use JFK in that airport because you split on it. What if we hash and split on carrier name? And while there was only like 11 airline carriers and if you want to split your data, it's not well enough distributed to get a fine grained split. So, instead of an 80-20, you might just get like 60-40, which may not be good enough for you. Starting out with ML development, is best to develop your Tensorflow code on a small subset of data. Then later scale it out to the cloud for true productization. So, imagining you're developing an ML application, every time you make a change you have to rerun the application. If you use the full data set, this honestly could take hours or even days. You're talking petabytes of data and you can't develop software that way. You want a small dataset so that you can quickly run through your code, debug it, and then rerun it. Then, once the application is working properly, you can run it once or however many times you want on the full dataset, it's like prototyping, right? Next, let's see how we can uniformly sample a smaller subset of our airline data set to be used before. So, we've created the buckets, but now we want less data. So, the flight's data is 70 million rows. Perhaps you wanted a smaller dataset, say one million flights, again it has got to be repeatable. How would you pick one in 70 flights and then 80 percent of those as training? So, you can't pick one in 70 and then pick one in 10 from the results there. Can you figure out why? Well, if you're picking numbers that are divisible by 70, of course they're also going to be divisible by 10. That second modulo operation here, as you see, is useless. So, let's do a quick demo and I'll show you what this means.