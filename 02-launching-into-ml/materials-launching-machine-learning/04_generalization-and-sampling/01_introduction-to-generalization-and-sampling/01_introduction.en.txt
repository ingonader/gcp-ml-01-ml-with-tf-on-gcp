Welcome back, I'm Evan Jones, a technical
curriculum developer for Google Cloud, and I love all things big data. Let's continue launching into ML
with generalization and sampling. Now so far in this course,
we've discussed ML model training and experimented with model training
inside of that Tensor Flow playground. So now it's time to answer
a rather weird question. When is the most accurate ML
model not the right one to pick? And as we hinted at in the last
model on optimization, simply because a model has a loss metric
of zero for your training dataset does not mean that it'll perform well on new
data out there in the real world. Now what you've got to realize is that the
best ML model is not necessarily the one that performs best on just
your training dataset, but it's the one that performs
best on unseen data. Your main concern should be how well
your model performs in production. And this implies data that
your model has yet to see. So how well would your model
perform on unknown data? Well, first you need to get some
data that’s not shown to the model during training. And after you successfully
train the model, you can then evaluate it
on this held-out dataset. You’ll learn how to assess whether or
not your model is overfitting, and how to gauge when to actually
stop model training. Now, the second part of this module is how
you can create this unknown dataset in the first place. Naturally, you don’t have unknown data,
but what you do have is a training dataset, that you can then split into
separate training and evaluation datasets. You can then experiment and
train your model with one dataset. And then when you're ready,
you can measure the model performance in the real world against an evaluation or
test dataset. So you learn actually how to create
these repeatable training evaluation and test datasets and
actually establish performance benchmarks.