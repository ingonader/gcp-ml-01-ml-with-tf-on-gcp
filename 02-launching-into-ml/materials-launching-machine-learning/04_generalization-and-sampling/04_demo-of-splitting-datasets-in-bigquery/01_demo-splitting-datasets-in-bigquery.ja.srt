1
00:00:00,000 --> 00:00:03,510
理論を説明し
SQLのコードも少し確認しましたが

2
00:00:03,510 --> 00:00:05,690
BigQuery内でいくつも実行することで

3
00:00:05,690 --> 00:00:09,025
データセットをバケットに分割する方法の
知識を固められます

4
00:00:09,025 --> 00:00:14,790
ただ モジュロ演算子のコードや
SQLのWHERE節によるデータ分割を実践する前に

5
00:00:14,790 --> 00:00:18,190
その方法を詳しく見ていきましょう

6
00:00:18,190 --> 00:00:19,840
この図をご覧ください

7
00:00:19,840 --> 00:00:23,920
合計7,000万件のフライトです
もっと大きなデータセットの場合もあります

8
00:00:23,920 --> 00:00:26,070
とにかくこの大きなデータセットを

9
00:00:26,070 --> 00:00:28,840
繰り返し可能な方法で
小さくしましょう

10
00:00:28,840 --> 00:00:31,535
単純なランダムのサンプリングは使えません

11
00:00:31,535 --> 00:00:34,190
WHERE節のフィルターを使う方法を
お見せします

12
00:00:34,190 --> 00:00:36,620
ここで1～2%取るとしましょう

13
00:00:36,620 --> 00:00:39,880
オレンジのボックスが約1%です
これを分割します

14
00:00:39,880 --> 00:00:42,135
この1%の50%を

15
00:00:42,135 --> 00:00:44,770
トレーニング用データにしましょう

16
00:00:44,770 --> 00:00:48,130
残りの50%を
評価用データセットにして

17
00:00:48,130 --> 00:00:52,260
評価用データセットの半分を
テスト用データセットにします

18
00:00:52,260 --> 00:00:55,725
これでモデルの成功または失敗を
チェックします

19
00:00:55,725 --> 00:00:58,150
Google BigQuery内では
どのように機能するでしょうか

20
00:00:58,150 --> 00:01:01,870
このコードは今後のプロジェクトで利用できます
では見てみましょう

21
00:01:01,870 --> 00:01:03,460
Google BigQuery内に

22
00:01:03,460 --> 00:01:05,530
あらかじめクエリを作成しておきました

23
00:01:05,530 --> 00:01:07,330
手順を順番にお見せしましょう

24
00:01:07,330 --> 00:01:09,510
これがGoogle BigQueryの
インターフェースです

25
00:01:09,510 --> 00:01:11,565
すでにご覧になっているかもしれません

26
00:01:11,565 --> 00:01:16,200
まず 各フライトの行が
7,000万行あることがわかりますが

27
00:01:16,200 --> 00:01:19,410
ソースデータテーブルの情報を
もう少し取得します

28
00:01:19,410 --> 00:01:25,140
[show]オプション内で
[legacy SQL]を無効にしてください

29
00:01:25,140 --> 00:01:31,260
これでCommandまたはWindowsキーを押す
機能が使えるようになります

30
00:01:32,130 --> 00:01:36,330
SQL内の任意の場所で
テーブルをクリックすると

31
00:01:36,330 --> 00:01:38,845
テーブルの詳細にすばやくアクセスできます

32
00:01:38,845 --> 00:01:41,200
ご覧のように
ここにすべてのフィールドがあり

33
00:01:41,200 --> 00:01:42,670
詳細をクリックすると

34
00:01:42,670 --> 00:01:44,995
フライトのレコード数を確認できます

35
00:01:44,995 --> 00:01:48,440
ここでデータセット内の
7,000万件のフライトを取得できます

36
00:01:48,440 --> 00:01:50,260
ただし1ギガバイトあります

37
00:01:50,260 --> 00:01:52,835
このデータセットはプレビューして
確認できます

38
00:01:52,835 --> 00:01:55,070
これは各フライトの日付

39
00:01:55,070 --> 00:01:56,200
出発空港

40
00:01:56,200 --> 00:02:00,900
他にも出発場所などの
運行に関するさまざまな情報を確認できます

41
00:02:00,900 --> 00:02:06,070
さて 基本的なフィールドのデータを取り出し
プレビューしましたが

42
00:02:06,070 --> 00:02:08,645
さらに3つを追加しました

43
00:02:08,645 --> 00:02:15,230
次に 17行目のWHERE節の下で
フィルタリングを行いますが

44
00:02:15,230 --> 00:02:18,300
その前に このサンプルをお見せします

45
00:02:18,300 --> 00:02:24,615
コードをハイライト表示し
下矢印をクリックして クエリを実行します

46
00:02:24,615 --> 00:02:30,340
これにより この日が何なのかがわかります

47
00:02:30,340 --> 00:02:32,420
これをご覧ください

48
00:02:32,420 --> 00:02:35,015
2008年6月30日です

49
00:02:35,015 --> 00:02:39,665
以前に説明したとおり
これがFARM_FINGERPRINT関数の動作です

50
00:02:39,665 --> 00:02:45,100
日付の文字列を取得して連続する数字に変えます

51
00:02:45,100 --> 00:02:47,690
一方向のハッシュ関数です

52
00:02:47,690 --> 00:02:51,025
この数字は後でいろいろと利用できます

53
00:02:51,025 --> 00:02:59,440
どの場合でも2008年6月30日は
必ず同じ値になるので とても便利です

54
00:02:59,440 --> 00:03:03,040
このようにFARM_FINGERPRINTを使った
ハッシュが完了しました

55
00:03:03,040 --> 00:03:06,770
5行目と6行目で1つ違うのは

56
00:03:06,770 --> 00:03:15,660
ハッシュが70で割り切れるか
700で割り切れるか確認した点です

57
00:03:15,660 --> 00:03:18,535
これを使う理由は

58
00:03:18,535 --> 00:03:23,960
余りが0になる値で
レコードを70件毎に1件取り出すためです

59
00:03:23,960 --> 00:03:28,950
これにより7,000万のフライトの
1～2%をフィルターして

60
00:03:28,950 --> 00:03:33,460
サブデータセットに使います

61
00:03:33,460 --> 00:03:35,225
ご覧のように

62
00:03:35,225 --> 00:03:39,085
70で割った余りを示したフィールドがあり

63
00:03:39,085 --> 00:03:43,610
これが0になるレコードが
70件毎に1件あります

64
00:03:43,610 --> 00:03:46,250
これを最初のフィルターに設定します

65
00:03:46,250 --> 00:03:50,340
わかりやすいように
LIMITを下に移動しますね

66
00:03:50,340 --> 00:03:55,640
SQLレコードのフィルタリングは
15行目からのWHERE節で行います

67
00:03:55,640 --> 00:03:57,560
ここにコメントがありますね

68
00:03:57,560 --> 00:03:59,325
70行毎に1行取得します

69
00:03:59,325 --> 00:04:04,580
先ほど下のフィールドで見たように
70で割った余りが0に等しいです

70
00:04:04,580 --> 00:04:06,820
次にLIMIT 10を含めて実行します

71
00:04:06,820 --> 00:04:12,430
この列に表示される 70で割った余りの値が
すべて0になるはずです

72
00:04:12,430 --> 00:04:14,170
できました

73
00:04:14,170 --> 00:04:21,080
これで98%のデータを
無視することに成功しました

74
00:04:21,080 --> 00:04:23,460
次に進みましょう

75
00:04:23,460 --> 00:04:25,885
最初に紹介した図を思い出してください

76
00:04:25,885 --> 00:04:29,465
データセットの分割の図です

77
00:04:29,465 --> 00:04:37,170
あのオレンジのボックスに当たる
84万2,000行を取得できました

78
00:04:37,170 --> 00:04:39,970
これがトレーニング用データセットです

79
00:04:39,970 --> 00:04:43,310
ご存じのように トレーニング用、評価用、
さらにテスト用データセットを作成するため

80
00:04:43,310 --> 00:04:47,020
追加でフィルタリングする必要があります

81
00:04:47,020 --> 00:04:49,800
70で割った余りは濫用できないので

82
00:04:49,800 --> 00:04:52,255
7で割った余りなどは使えません

83
00:04:52,255 --> 00:04:53,570
これはすでに0です

84
00:04:53,570 --> 00:04:55,520
すでに一度使っています

85
00:04:55,520 --> 00:04:59,170
そこで2つめのフィルタリング処理を行います

86
00:04:59,170 --> 00:05:02,345
ここでは700を使います
70と700の違いは

87
00:05:02,345 --> 00:05:10,260
データセットを分割するために
作成したいバケットのサイズで決まります

88
00:05:10,260 --> 00:05:18,330
さて 98%減らしたデータセットの
残り80万件のレコードを分割して

89
00:05:18,330 --> 00:05:25,060
評価用、テスト用のデータセットを作成し
最初に作ったトレーニング用と分けます

90
00:05:25,060 --> 00:05:26,400
そのために

91
00:05:26,400 --> 00:05:30,390
WHERE節に別のフィルターを追加して

92
00:05:30,390 --> 00:05:37,680
残りのデータセットの
50%を無視します

93
00:05:37,680 --> 00:05:41,020
これがどんなふうになるか
お見せしましょう

94
00:05:41,020 --> 00:05:45,380
今度はこの列を使います
700で割った余りです

95
00:05:45,380 --> 00:05:52,920
この2番目のモジュロ演算の値は
0と700の間のどれかになります

96
00:05:52,920 --> 00:05:56,590
このどれかを取りましょう

97
00:05:56,590 --> 00:05:59,655
0から700のセットを考えます

98
00:05:59,655 --> 00:06:03,635
0と700の中間点は350です

99
00:06:03,635 --> 00:06:10,005
つまり 0〜350の間 と 350〜700の間に
レコードを分割します

100
00:06:10,005 --> 00:06:15,130
そのために「350以上」と記述します

101
00:06:15,130 --> 00:06:20,785
この630という数字は350より大きいので
ここに含まれているわけです

102
00:06:20,785 --> 00:06:23,455
ここは納得できる瞬間でもあります

103
00:06:23,455 --> 00:06:25,705
この日付を見てください

104
00:06:25,705 --> 00:06:29,840
これらはすべて2005年7月13日に
発生したフライトですが

105
00:06:29,840 --> 00:06:31,980
完全に同じハッシュです

106
00:06:31,980 --> 00:06:36,230
ここは非常に面白い部分ですが
こういうものを扱うときの

107
00:06:36,230 --> 00:06:38,565
難しい部分でもあります

108
00:06:38,565 --> 00:06:40,830
講座の少し前で触れたように

109
00:06:40,830 --> 00:06:44,540
データセットが2日間のデータだとします

110
00:06:44,540 --> 00:06:47,240
たとえば2005年7月13日と
2005年7月14日なら

111
00:06:47,240 --> 00:06:50,135
80-20に分割することはできません

112
00:06:50,135 --> 00:06:53,650
この2つのハッシュしか
ここに存在しないからです

113
00:06:55,100 --> 00:06:57,760
こういうわけで
ノイズの多いデータセットや

114
00:06:57,760 --> 00:07:00,825
分散が十分なデータセットを用意してから
分割を行うよう勧めています

115
00:07:00,825 --> 00:07:06,010
ハッシュはランダム関数と異なり
常に同じ値を返すからです

116
00:07:06,010 --> 00:07:11,135
では 最後にこのサブセットを
さらに分割して

117
00:07:11,135 --> 00:07:13,900
50%だけ含むようにします

118
00:07:13,900 --> 00:07:17,080
これはトレーニング用データ全体の
25%にあたりますが

119
00:07:17,080 --> 00:07:19,340
これをテスト用に取っておきます

120
00:07:19,340 --> 00:07:22,120
そのためにもう一度 中間点で処理します

121
00:07:22,120 --> 00:07:24,905
今回は「525未満」と記述します

122
00:07:24,905 --> 00:07:29,870
350と700の中間点は525だからです

123
00:07:29,870 --> 00:07:33,295
そのため 525未満のレコードを取り出すと

124
00:07:33,295 --> 00:07:36,895
元のトレーニング用データセットの
25%になります

125
00:07:36,895 --> 00:07:40,070
ここで一番大変なのはSQL構文ではなく

126
00:07:40,070 --> 00:07:43,980
心の中でこれらの境界線の
イメージを描くことです

127
00:07:43,980 --> 00:07:47,500
そして使用する中間点やハッシュ関数を
決めることです

128
00:07:47,500 --> 00:07:54,480
では最後に 10件だけのレコードを
お見せしましょう

129
00:07:54,480 --> 00:07:57,430
これらはすべて70で割り切れるので

130
00:07:57,430 --> 00:07:59,340
ここで0を確認できます

131
00:07:59,340 --> 00:08:01,790
そして700で割った余りを使って

132
00:08:01,790 --> 00:08:05,450
最終的なテスト用データセットを作成します

133
00:08:05,450 --> 00:08:09,690
これはトレーニングデータの25%で
テスト用のホールドアウトとして使います

134
00:08:09,690 --> 00:08:16,340
これらの値は
350以上525未満です

135
00:08:16,340 --> 00:08:18,870
ここに420の値がありますね

136
00:08:18,870 --> 00:08:23,230
もう一方のデータセットに
アクセスしたい場合は？

137
00:08:23,230 --> 00:08:27,960
つまり525から700の値にアクセスしたいときは

138
00:08:27,960 --> 00:08:32,980
ここの記号をひっくり返して
「525より大きい」にします

139
00:08:32,980 --> 00:08:38,979
これで トレーニング用、評価用、テスト用の
データができました

140
00:08:38,979 --> 00:08:41,975
ではこれらをインポートして
機械学習モデルに取り込んでみましょう

141
00:08:41,975 --> 00:08:44,170
これはラボでたくさん練習します

142
00:08:44,170 --> 00:08:46,345
また落し穴もいくつかでてきます

143
00:08:46,345 --> 00:08:50,095
ただ ここで学んだことが
基本的な概念です

144
00:08:50,095 --> 00:08:52,000
では次に進みましょう