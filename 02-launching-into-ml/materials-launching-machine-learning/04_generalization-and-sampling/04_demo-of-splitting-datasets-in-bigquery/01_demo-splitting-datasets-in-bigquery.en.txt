As we covered this theory and you've seen a little bit the code in SQL, but honestly, running through a lot of it inside of BigQuery is going to help really solidify that knowledge for how to split these datasets into buckets. But before we do that and you start seeing a lot of code for module operators and splitting the datasets in sort of a SQL where clause, let's cover how exactly we're going to split up this dataset. So, as you see in the image here, we get 70 million flights in total and that can be a very, very large dataset, that could've been 7 billion, but pretty much all you want to do is make that dataset a little bit smaller in a repeatable fashion. So we can't just use a naive random sampling, you want to use a smart where clause filters as you're going to see. So, we're going to say, let's take 1 or 2 percent, as you see there in the orange box and then we want to work our way down into 50 percent of that 1 percent for our associates like the orange box can be your training data, 50 percent of that could be your validation dataset and the remainder of that or half of the validation dataset could then be used for your testing dataset. Remember, those that go or no go dataset. How does that actually work inside of Google BigQuery and you can use this code in your future projects as well. So let's take a look. So inside of Google BigQuery, I have this pre-written query here, but I'm going to walk you through each step of the way. So, this is the Google BigQuery interface, which you might have seen before and the first thing that I like to do, although it says 70 million rows or individual flights are here, I like to get a little bit of information about my source data table. So, inside of show options make sure that you disable legacy SQL, it allows you the feature to actually hold down the command or Windows key and click on the table and that's a fast track actually anywhere inside of your SQL to get access to details about the table. So as you can see here all of our fields, clicking on details will actually take you to the number of records that are in the flight. So here's where you get the 70 million different flights in this dataset, but a gigabytes and you can actually preview this dataset if you want to take a look at it. So here all the different flight dates, you see the departure airports, where it's departing from and a lot of good information that you would expect from an airline dataset. All right. So in addition to some of those basic generic fields that we're pulling from this data that we previewed here, I've added three more for you to see. So what I'm going to do, before we get into the filtering that you see below there on line 17 for that where clause filter. We're going to do is just show you a sample of this and you can execute code in a highlighted block by clicking on the down arrow and running that query there. So what this is going to do is it shows you exactly what is this day, so say take a look at this one. This is June 30th, 2008 and as I mentioned before in this example, this is what a farm fingerprint function does, it pretty much takes this string and turns it into a sequence of numbers. It's a one way hashing function which we can then use to our heart's content, but in all cases June 30th 2008 written just like this will always hash to this particular value which is super useful and then after that we've done the hash as you see here with farm fingerprint, the only thing that I've done differently in lines 5 and 6 as you see, is we want to see whether or not that hash is divisible by 70 or 700 evenly. So, the reason why we're going to use that is basically, we want to pull 1 in 70 records where the remainder is 0 and that's kind form that 1 percent or 2 percent of the 70 million broader flights filter out for us and the sub dataset. So you can see here, we have this field called remain or divide by 70, where that's equal to 0 which is roughly 1 in 70 cases, exactly 1 in 70 cases is we're going to set up our first filter and that's exactly what we're going to do. So as you see, I'm the move this limit down here now so you can just get a little bit more familiar filtering and SQL, filtering records is done in the WHERE clause as you see there on line 15 as the comment is here. We're going to pick 1 in 70 rows, where exactly as you saw this field here remainder divided by 70, that's equal to 0 and I'm going to go ahead and limit 10. So you can see that every value in that column remainder divided by 70 should now be zero and boom. So you've successfully thrown out or ignored is a better way of describing a 98 percent of the data and now what we want to do, we've achieved that, if you remember back in that image that first zoom in or the splitting of that data set that you saw in the image, now of that we have 800 about 842,000 rows in that orange box that you saw a little bit earlier. So that's for my training dateset, as you remember you need to create a training validation and possibly even testing dataset to do some additional filtering. Now we can't abuse the remainder divided by 70, so you couldn't do like remained divided by 7 right? Because this is already 0, you've already used this once, so that's why you are that second filtering operation on them do you live there where we're using the 700 and again the 70 versus 700, that's arbitrary depending upon how the size of your buckets is going to is going to be for your size of your dataset splits that you want to create. So second now, we reduce the dataset by 98 percent and now we need to split that remaining 800,000 records into a wall between our testing and validation datasets and then the training that we started with. So now, what we want to do is add on another filter for the work clause and we want to actually ignore 50 percent of the remaining dataset, I'm going to show you what that actually looks like here. So we're using our this column now the remainder divided by 700, so that could be anywhere between 0 and 700 for that second year law operation there. So we want to take where anything is between. So you think of the sets between 0 and 700, the midpoint between 0 and 700 is 350. So you have records now that exist between 0 and 350 and then 350 and 700. So splitting that down the middle is exactly how we get this greater than or equal to 350 as you see here this 630 figure here is greater. That's why it's included, but it also kind of gotcha moment, right. Is that look at the dates here, these are all flights that happened on July 13th, 2005 they have the exact same hash. So this is one of the really interesting and potentially tricky parts of using something like this, is if you add, as we mentioned a little bit earlier in the lecture if you had a dataset that just had two days like if you just had a July 13th, 2005 and July 14th, 2005 you can't do an 80 20 split, because you're only going to have these two hashes that are that are present here. Okay. So that's why we say you want to have a noisy or a well distributed dataset before you do these splits, because the hashes are always going to return the same value unlike a random function. Alright, and the last thing is we want to further split that subset to only include 50 percent of that, which is going to be 25 percent of your total train data that you want a reserve for testing and then to do that is just again you're working with that midpoint in this particular case it's anything that's less than 525, which is that new midpoint from 350 to 700 is 525. So taking out that chunk of anything that's less than 525, we'll give you your 25 percent of your original training dataset. So honestly, the hardest part about this is not the sequel syntax, but it's mentally drawing that picture of how are you going to actually form these boundaries and then where are those midpoints and those hash functions that you're going to use and at the end of the day, I'm going to show you just the 10 records here just the way. You should see all of these should be divisible by 70, so you should see zeroes for everything there and then end the remainder divide by 700 for ultimate final dataset for testing, say this is the 25 percent that we want to use as hold out for testing. All the values should be less than it should be greater than 350 greater than equal to 350, but less than 525 and you see that confirmed here with a 420 value here and now what happens if you want to access the other way around. If you wanted to access the values that were above 525-700, you would just flip simply flip the sign here to be greater than that 525 and then you would say the results of these in three different locations training, validation and testing and then you're good to go to import those and ingest them into machine learning models. So, you get a lot more practice with this in your labs and then you'll see some of the pitfalls that you can run into, but just understand that this is kind of basic concept that you're going to see. Alright, let's get back to it.