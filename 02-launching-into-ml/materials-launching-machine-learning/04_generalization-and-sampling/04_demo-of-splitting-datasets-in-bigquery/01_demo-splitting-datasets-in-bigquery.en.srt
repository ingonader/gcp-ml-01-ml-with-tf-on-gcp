1
00:00:00,000 --> 00:00:03,950
As we covered this theory and you've seen a little bit the code in SQL, but honestly,

2
00:00:03,950 --> 00:00:06,170
running through a lot of it inside of BigQuery is going to help really

3
00:00:06,170 --> 00:00:09,025
solidify that knowledge for how to split these datasets into buckets.

4
00:00:09,025 --> 00:00:11,890
But before we do that and you start seeing a lot of code for

5
00:00:11,890 --> 00:00:15,075
module operators and splitting the datasets in sort of a SQL where clause,

6
00:00:15,075 --> 00:00:18,190
let's cover how exactly we're going to split up this dataset.

7
00:00:18,190 --> 00:00:19,840
So, as you see in the image here, we get

8
00:00:19,840 --> 00:00:23,440
70 million flights in total and that can be a very, very large dataset,

9
00:00:23,440 --> 00:00:26,090
that could've been 7 billion, but pretty much all you want to

10
00:00:26,090 --> 00:00:28,840
do is make that dataset a little bit smaller in a repeatable fashion.

11
00:00:28,840 --> 00:00:31,535
So we can't just use a naive random sampling,

12
00:00:31,535 --> 00:00:34,130
you want to use a smart where clause filters as you're going to see.

13
00:00:34,130 --> 00:00:35,140
So, we're going to say,

14
00:00:35,140 --> 00:00:36,620
let's take 1 or 2 percent,

15
00:00:36,620 --> 00:00:39,880
as you see there in the orange box and then we want to work our way down into

16
00:00:39,880 --> 00:00:42,135
50 percent of that 1 percent

17
00:00:42,135 --> 00:00:44,770
for our associates like the orange box can be your training data,

18
00:00:44,770 --> 00:00:48,720
50 percent of that could be your validation dataset and the remainder of that or

19
00:00:48,720 --> 00:00:53,120
half of the validation dataset could then be used for your testing dataset.

20
00:00:53,120 --> 00:00:55,225
Remember, those that go or no go dataset.

21
00:00:55,225 --> 00:00:58,150
How does that actually work inside of Google BigQuery and you

22
00:00:58,150 --> 00:01:01,870
can use this code in your future projects as well. So let's take a look.

23
00:01:01,870 --> 00:01:04,040
So inside of Google BigQuery,

24
00:01:04,040 --> 00:01:05,530
I have this pre-written query here,

25
00:01:05,530 --> 00:01:07,420
but I'm going to walk you through each step of the way.

26
00:01:07,420 --> 00:01:09,400
So, this is the Google BigQuery interface,

27
00:01:09,400 --> 00:01:12,315
which you might have seen before and the first thing that I like to do,

28
00:01:12,315 --> 00:01:16,200
although it says 70 million rows or individual flights are here,

29
00:01:16,200 --> 00:01:19,410
I like to get a little bit of information about my source data table.

30
00:01:19,410 --> 00:01:25,140
So, inside of show options make sure that you disable legacy SQL,

31
00:01:25,140 --> 00:01:29,600
it allows you the feature to actually hold down the command or

32
00:01:29,600 --> 00:01:34,160
Windows key and click on the table and

33
00:01:34,160 --> 00:01:36,490
that's a fast track actually anywhere inside of

34
00:01:36,490 --> 00:01:39,165
your SQL to get access to details about the table.

35
00:01:39,165 --> 00:01:40,870
So as you can see here all of our fields,

36
00:01:40,870 --> 00:01:42,540
clicking on details will

37
00:01:42,540 --> 00:01:45,075
actually take you to the number of records that are in the flight.

38
00:01:45,075 --> 00:01:48,290
So here's where you get the 70 million different flights in this dataset,

39
00:01:48,290 --> 00:01:50,260
but a gigabytes and you

40
00:01:50,260 --> 00:01:52,565
can actually preview this dataset if you want to take a look at it.

41
00:01:52,565 --> 00:01:54,750
So here all the different flight dates,

42
00:01:54,750 --> 00:01:56,200
you see the departure airports,

43
00:01:56,200 --> 00:01:58,630
where it's departing from and a lot of good information

44
00:01:58,630 --> 00:02:01,280
that you would expect from an airline dataset. All right.

45
00:02:01,280 --> 00:02:03,810
So in addition to some of those basic generic fields

46
00:02:03,810 --> 00:02:05,925
that we're pulling from this data that we previewed here,

47
00:02:05,925 --> 00:02:08,235
I've added three more for you to see.

48
00:02:08,235 --> 00:02:09,645
So what I'm going to do,

49
00:02:09,645 --> 00:02:11,790
before we get into the filtering that you see below

50
00:02:11,790 --> 00:02:14,345
there on line 17 for that where clause filter.

51
00:02:14,345 --> 00:02:18,910
We're going to do is just show you a sample of this and you can execute code

52
00:02:18,910 --> 00:02:23,795
in a highlighted block by clicking on the down arrow and running that query there.

53
00:02:23,795 --> 00:02:31,320
So what this is going to do is it shows you exactly what is this day,

54
00:02:31,320 --> 00:02:32,420
so say take a look at this one.

55
00:02:32,420 --> 00:02:36,825
This is June 30th, 2008 and as I mentioned before in this example,

56
00:02:36,825 --> 00:02:39,665
this is what a farm fingerprint function does,

57
00:02:39,665 --> 00:02:45,180
it pretty much takes this string and turns it into a sequence of numbers.

58
00:02:45,180 --> 00:02:51,025
It's a one way hashing function which we can then use to our heart's content,

59
00:02:51,025 --> 00:02:56,610
but in all cases June 30th 2008 written just like this will

60
00:02:56,610 --> 00:02:59,430
always hash to this particular value which is super

61
00:02:59,430 --> 00:03:03,040
useful and then after that we've done the hash as you see here with farm fingerprint,

62
00:03:03,040 --> 00:03:06,770
the only thing that I've done differently in lines 5 and 6 as you see,

63
00:03:06,770 --> 00:03:15,660
is we want to see whether or not that hash is divisible by 70 or 700 evenly.

64
00:03:15,660 --> 00:03:18,535
So, the reason why we're going to use that is basically,

65
00:03:18,535 --> 00:03:25,010
we want to pull 1 in 70 records where the remainder is 0 and that's kind form

66
00:03:25,010 --> 00:03:27,650
that 1 percent or 2 percent of

67
00:03:27,650 --> 00:03:33,460
the 70 million broader flights filter out for us and the sub dataset.

68
00:03:33,460 --> 00:03:35,225
So you can see here,

69
00:03:35,225 --> 00:03:37,945
we have this field called remain or divide by 70,

70
00:03:37,945 --> 00:03:41,380
where that's equal to 0 which is roughly 1 in 70 cases,

71
00:03:41,380 --> 00:03:43,860
exactly 1 in 70 cases is

72
00:03:43,860 --> 00:03:46,420
we're going to set up our first filter and that's exactly what we're going to do.

73
00:03:46,420 --> 00:03:49,270
So as you see, I'm the move this limit down here now so you

74
00:03:49,270 --> 00:03:52,100
can just get a little bit more familiar filtering and SQL,

75
00:03:52,100 --> 00:03:54,370
filtering records is done in the WHERE clause as you

76
00:03:54,370 --> 00:03:57,110
see there on line 15 as the comment is here.

77
00:03:57,110 --> 00:03:59,325
We're going to pick 1 in 70 rows,

78
00:03:59,325 --> 00:04:03,370
where exactly as you saw this field here remainder divided by 70,

79
00:04:03,370 --> 00:04:06,820
that's equal to 0 and I'm going to go ahead and limit 10.

80
00:04:06,820 --> 00:04:08,670
So you can see that every value in

81
00:04:08,670 --> 00:04:13,220
that column remainder divided by 70 should now be zero and boom.

82
00:04:13,220 --> 00:04:17,560
So you've successfully thrown out or ignored is a better way of describing

83
00:04:17,560 --> 00:04:22,760
a 98 percent of the data and now what we want to do,

84
00:04:22,760 --> 00:04:25,885
we've achieved that, if you remember back in that image that first

85
00:04:25,885 --> 00:04:29,465
zoom in or the splitting of that data set that you saw in the image,

86
00:04:29,465 --> 00:04:33,440
now of that we have 800 about

87
00:04:33,440 --> 00:04:37,715
842,000 rows in that orange box that you saw a little bit earlier.

88
00:04:37,715 --> 00:04:39,710
So that's for my training dateset,

89
00:04:39,710 --> 00:04:42,960
as you remember you need to create a training validation and possibly

90
00:04:42,960 --> 00:04:46,330
even testing dataset to do some additional filtering.

91
00:04:46,330 --> 00:04:49,580
Now we can't abuse the remainder divided by 70,

92
00:04:49,580 --> 00:04:52,255
so you couldn't do like remained divided by 7 right?

93
00:04:52,255 --> 00:04:53,570
Because this is already 0,

94
00:04:53,570 --> 00:04:54,610
you've already used this once,

95
00:04:54,610 --> 00:04:57,840
so that's why you are that second filtering operation on them do you live

96
00:04:57,840 --> 00:05:02,345
there where we're using the 700 and again the 70 versus 700,

97
00:05:02,345 --> 00:05:06,260
that's arbitrary depending upon how the size of your buckets is going to is going to

98
00:05:06,260 --> 00:05:10,575
be for your size of your dataset splits that you want to create.

99
00:05:10,575 --> 00:05:13,220
So second now, we reduce the dataset by

100
00:05:13,220 --> 00:05:17,590
98 percent and now we need to split that remaining 800,000

101
00:05:17,590 --> 00:05:20,830
records into a wall between

102
00:05:20,830 --> 00:05:25,020
our testing and validation datasets and then the training that we started with.

103
00:05:25,020 --> 00:05:30,280
So now, what we want to do is add on another filter for

104
00:05:30,280 --> 00:05:37,680
the work clause and we want to actually ignore 50 percent of the remaining dataset,

105
00:05:37,680 --> 00:05:41,020
I'm going to show you what that actually looks like here.

106
00:05:41,020 --> 00:05:45,380
So we're using our this column now the remainder divided by 700,

107
00:05:45,380 --> 00:05:52,920
so that could be anywhere between 0 and 700 for that second year law operation there.

108
00:05:52,920 --> 00:05:56,340
So we want to take where anything is between.

109
00:05:56,340 --> 00:05:59,655
So you think of the sets between 0 and 700,

110
00:05:59,655 --> 00:06:03,635
the midpoint between 0 and 700 is 350.

111
00:06:03,635 --> 00:06:10,005
So you have records now that exist between 0 and 350 and then 350 and 700.

112
00:06:10,005 --> 00:06:13,340
So splitting that down the middle is exactly how we get this greater than or equal to

113
00:06:13,340 --> 00:06:19,435
350 as you see here this 630 figure here is greater.

114
00:06:19,435 --> 00:06:23,295
That's why it's included, but it also kind of gotcha moment, right.

115
00:06:23,295 --> 00:06:25,635
Is that look at the dates here,

116
00:06:25,635 --> 00:06:28,830
these are all flights that happened on July 13th,

117
00:06:28,830 --> 00:06:31,980
2005 they have the exact same hash.

118
00:06:31,980 --> 00:06:35,890
So this is one of the really interesting and potentially tricky parts

119
00:06:35,890 --> 00:06:38,225
of using something like this, is if you add,

120
00:06:38,225 --> 00:06:41,600
as we mentioned a little bit earlier in the lecture if you had a dataset that just

121
00:06:41,600 --> 00:06:45,110
had two days like if you just had a July 13th,

122
00:06:45,110 --> 00:06:47,240
2005 and July 14th,

123
00:06:47,240 --> 00:06:50,135
2005 you can't do an 80 20 split,

124
00:06:50,135 --> 00:06:54,340
because you're only going to have these two hashes that are that are present here.

125
00:06:54,340 --> 00:06:57,620
Okay. So that's why we say you want to have a noisy or

126
00:06:57,620 --> 00:07:00,485
a well distributed dataset before you do these splits,

127
00:07:00,485 --> 00:07:05,370
because the hashes are always going to return the same value unlike a random function.

128
00:07:05,370 --> 00:07:10,025
Alright, and the last thing is we want to further split

129
00:07:10,025 --> 00:07:14,850
that subset to only include 50 percent of that,

130
00:07:14,850 --> 00:07:16,320
which is going to be 25 percent of

131
00:07:16,320 --> 00:07:19,340
your total train data that you want a reserve for testing and then to do

132
00:07:19,340 --> 00:07:20,680
that is just again you're working with

133
00:07:20,680 --> 00:07:24,445
that midpoint in this particular case it's anything that's less than 525,

134
00:07:24,445 --> 00:07:30,100
which is that new midpoint from 350 to 700 is 525.

135
00:07:30,100 --> 00:07:33,475
So taking out that chunk of anything that's less than 525,

136
00:07:33,475 --> 00:07:36,795
we'll give you your 25 percent of your original training dataset.

137
00:07:36,795 --> 00:07:39,710
So honestly, the hardest part about this is not the sequel syntax,

138
00:07:39,710 --> 00:07:41,770
but it's mentally drawing that picture of how

139
00:07:41,770 --> 00:07:43,790
are you going to actually form these boundaries and then

140
00:07:43,790 --> 00:07:45,800
where are those midpoints and those hash functions

141
00:07:45,800 --> 00:07:47,900
that you're going to use and at the end of the day,

142
00:07:47,900 --> 00:07:53,170
I'm going to show you just the 10 records here just the way.

143
00:07:53,170 --> 00:07:57,780
You should see all of these should be divisible by 70,

144
00:07:57,780 --> 00:08:01,240
so you should see zeroes for everything there and then end the remainder divide by

145
00:08:01,240 --> 00:08:05,580
700 for ultimate final dataset for testing,

146
00:08:05,580 --> 00:08:09,020
say this is the 25 percent that we want to use as hold out for testing.

147
00:08:09,020 --> 00:08:11,900
All the values should be less than it should be

148
00:08:11,900 --> 00:08:14,955
greater than 350 greater than equal to 350,

149
00:08:14,955 --> 00:08:18,870
but less than 525 and you see that confirmed here with a 420 value here

150
00:08:18,870 --> 00:08:23,230
and now what happens if you want to access the other way around.

151
00:08:23,230 --> 00:08:27,960
If you wanted to access the values that were above 525-700,

152
00:08:27,960 --> 00:08:32,270
you would just flip simply flip the sign here to be greater than

153
00:08:32,270 --> 00:08:34,060
that 525 and then you would say

154
00:08:34,060 --> 00:08:37,065
the results of these in three different locations training,

155
00:08:37,065 --> 00:08:39,460
validation and testing and then you're good to go

156
00:08:39,460 --> 00:08:41,755
to import those and ingest them into machine learning models.

157
00:08:41,755 --> 00:08:43,990
So, you get a lot more practice with this in your labs

158
00:08:43,990 --> 00:08:46,345
and then you'll see some of the pitfalls that you can run into,

159
00:08:46,345 --> 00:08:49,585
but just understand that this is kind of basic concept that you're going to see.

160
00:08:49,585 --> 00:08:52,000
Alright, let's get back to it.