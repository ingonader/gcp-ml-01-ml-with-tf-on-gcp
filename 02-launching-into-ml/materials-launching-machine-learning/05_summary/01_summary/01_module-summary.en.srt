1
00:00:00,000 --> 00:00:03,980
Congratulations. You made it to the end of the Launching into ML course.

2
00:00:03,980 --> 00:00:06,300
Let's recap what we've learned so far.

3
00:00:06,300 --> 00:00:08,040
First up, we looked at how

4
00:00:08,040 --> 00:00:11,535
Google production systems are informed from years of experience.

5
00:00:11,535 --> 00:00:14,110
We then walked through the historical timeline of ML,

6
00:00:14,110 --> 00:00:16,765
and the growth and prominence of Deep Neural Networks,

7
00:00:16,765 --> 00:00:20,155
and why they are the best choice in a large variety of problems.

8
00:00:20,155 --> 00:00:22,520
Finally, we covered how TensorFlow and

9
00:00:22,520 --> 00:00:24,350
Cloud Machine Learning Engine build on

10
00:00:24,350 --> 00:00:27,480
the experience of Google creating all of these systems.

11
00:00:27,480 --> 00:00:32,175
Next, we searched through parameter space to find the optimal ML model,

12
00:00:32,175 --> 00:00:36,060
by using our gradient descent algorithm to walk down our loss surfaces.

13
00:00:36,060 --> 00:00:39,000
Here we Illustrated model training by taking the derivative of

14
00:00:39,000 --> 00:00:41,850
our loss services as our guide towards a minima.

15
00:00:41,850 --> 00:00:45,730
Keeping in mind, you could have more than one minima for complex services like you saw.

16
00:00:45,730 --> 00:00:50,025
This gradient descent process is an intuitive one as you saw on your training loop.

17
00:00:50,025 --> 00:00:54,220
The idea here is change the weights of your model slightly, and re-evaluate it,

18
00:00:54,220 --> 00:00:56,520
and use it as a directional guide walking

19
00:00:56,520 --> 00:00:59,605
down your loss services and changing your weights as you go.

20
00:00:59,605 --> 00:01:02,545
We then introduced multiple loss functions,

21
00:01:02,545 --> 00:01:04,835
like RMSE for regression problems,

22
00:01:04,835 --> 00:01:07,070
and cross entropy for classification.

23
00:01:07,070 --> 00:01:10,300
Then we looked at performance measures like accuracy, precision,

24
00:01:10,300 --> 00:01:14,640
and recall, and discussed the pros and cons for reporting to your boss with each.

25
00:01:14,640 --> 00:01:16,680
We then got to have some fun inside

26
00:01:16,680 --> 00:01:19,860
the TensorFlow playground when you looked at low, moderate,

27
00:01:19,860 --> 00:01:21,660
and high batch sizes,

28
00:01:21,660 --> 00:01:24,860
and then which of those can lead to inconsistent model performance.

29
00:01:24,860 --> 00:01:27,340
We concluded the optimization module by

30
00:01:27,340 --> 00:01:30,340
training neural networks to classify data points in a spiral.

31
00:01:30,340 --> 00:01:34,715
And we ended up with a seemingly complex set of nodes in hidden layers.

32
00:01:34,715 --> 00:01:36,580
And to better understand whether or not that model

33
00:01:36,580 --> 00:01:38,510
would perform well out in the real world,

34
00:01:38,510 --> 00:01:41,925
is where we headed into the world of generalization.

35
00:01:41,925 --> 00:01:46,040
Once we had the perfectly accurate model with an RMSE of zero,

36
00:01:46,040 --> 00:01:51,020
we saw how badly it performed against a set of new data that it had not seen before.

37
00:01:51,020 --> 00:01:53,480
To make our models generalize well and not simply

38
00:01:53,480 --> 00:01:56,215
memorize a train data set that we warned you about before,

39
00:01:56,215 --> 00:01:59,230
we split our original data set into training, evaluation,

40
00:01:59,230 --> 00:02:04,025
and testing, and show them only to the model at predefined milestones.

41
00:02:04,025 --> 00:02:08,240
We then discussed how to create these subsets of data by splitting and sampling

42
00:02:08,240 --> 00:02:12,345
our 70 million flight records data set in a repeatable fashion.

43
00:02:12,345 --> 00:02:15,025
This allowed us to experiment with model improvements,

44
00:02:15,025 --> 00:02:19,090
and keep the underlying data constant during each model training run.

45
00:02:19,090 --> 00:02:21,325
Then in our taxi lab,

46
00:02:21,325 --> 00:02:25,615
we discovered that ML models can make incorrect predictions for a variety of reasons.

47
00:02:25,615 --> 00:02:27,690
Poor representation of our use cases,

48
00:02:27,690 --> 00:02:29,980
overfitting, underfitting, what have you.

49
00:02:29,980 --> 00:02:32,070
We also learned that we can measure the quality

50
00:02:32,070 --> 00:02:34,760
remodel by examining the predictions it made.

51
00:02:34,760 --> 00:02:36,800
So that's it. Keep practicing

52
00:02:36,800 --> 00:02:39,710
your ML skills with these hands on labs if you want another go at them,

53
00:02:39,710 --> 00:02:42,000
and we'll see you in the next course.