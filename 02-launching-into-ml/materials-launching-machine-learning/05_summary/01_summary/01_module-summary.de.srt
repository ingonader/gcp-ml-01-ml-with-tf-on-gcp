1
00:00:00,000 --> 00:00:03,830
Glückwunsch! Sie haben es bis zum Ende
des Kurses "Launching into ML" geschafft!

2
00:00:03,830 --> 00:00:06,300
Fassen wir zusammen,
was Sie bisher gelernt haben.

3
00:00:06,300 --> 00:00:08,039
Zuerst haben wir uns angesehen,

4
00:00:08,039 --> 00:00:11,535
wie Google-Produktionssysteme über Jahre
Informationen sammeln konnten.

5
00:00:11,535 --> 00:00:13,950
Dann behandelten wir die
Geschichte von ML

6
00:00:13,950 --> 00:00:16,955
und das Wachstum und die Bedeutung von
Deep-Learning-Netzwerken

7
00:00:16,955 --> 00:00:20,155
und warum diese die beste Lösung
für zahlreiche Probleme sind.

8
00:00:20,155 --> 00:00:21,890
Abschließend haben wir behandelt,

9
00:00:21,890 --> 00:00:24,150
wie TensorFlow und
Cloud Machine Learning Engine

10
00:00:24,150 --> 00:00:27,640
auf der Erfahrung von Google bei der 
Erstellung dieser Systeme aufbauen.

11
00:00:27,640 --> 00:00:32,174
Wir durchsuchten den Parameterbereich,
um das ideale ML-Modell zu finden.

12
00:00:32,174 --> 00:00:34,240
Wir nutzten
den Gradientenabstiegsalgorithmus

13
00:00:34,240 --> 00:00:36,200
zur Betrachtung der Verlustdienste.

14
00:00:36,200 --> 00:00:39,210
Wir zeigten ein Modelltraining,
indem wir eine Ableitung unserer

15
00:00:39,210 --> 00:00:42,160
Verlustdienste als Orientierung
in Richtung Minima verwenden.

16
00:00:42,160 --> 00:00:45,730
Es könnte sein, dass es mehr als
ein Minimum für komplexe Dienste gibt.

17
00:00:45,730 --> 00:00:50,025
Der Gradientenabstiegsprozess ist, wie in 
der Trainingsschleife gezeigt, intuitiv.

18
00:00:50,025 --> 00:00:54,220
Die Idee ist, die Gewichtung Ihres
Modells zu verändern und neu zu evaluieren

19
00:00:54,220 --> 00:00:56,600
und als direktionale
Orientierung zur Betrachtung

20
00:00:56,600 --> 00:00:59,765
Ihrer Verlustdienste zu nutzen
und die Gewichtung zu ändern.

21
00:00:59,765 --> 00:01:02,545
Wir haben dann 
multiple Verlustfunktionen eingesetzt

22
00:01:02,545 --> 00:01:04,834
wie RMSE bei Regressionsproblemen

23
00:01:04,834 --> 00:01:06,880
und Kreuzentropie für die Klassifizierung.

24
00:01:06,880 --> 00:01:10,680
Wir sahen uns Leistungsmesswerte wie 
Genauigkeit, Präzision und Trefferquote an

25
00:01:10,680 --> 00:01:14,640
und haben die Vor- und Nachteile
für das Reporting an den Chef diskutiert.

26
00:01:14,640 --> 00:01:16,680
Wir haben einige interessante Einblicke

27
00:01:16,680 --> 00:01:18,170
in TensorFlow erhalten und uns

28
00:01:18,170 --> 00:01:20,580
kleinere, mittlere und große
Batch-Größen angesehen

29
00:01:20,580 --> 00:01:24,160
und welche davon zu einer inkonsistenten
Leistung des Modells führen können.

30
00:01:24,160 --> 00:01:27,470
Wir haben das Optimierungsmodul beendet,
indem wir neuronale Netzwerke

31
00:01:27,470 --> 00:01:30,510
trainierten, um Datenpunkte
in einer Spirale zu klassifizieren,

32
00:01:30,510 --> 00:01:34,715
und haben dann einen scheinbar komplexen 
Knotensatz in versteckten Ebenen erhalten.

33
00:01:34,715 --> 00:01:36,580
Um besser zu verstehen, ob ein Modell

34
00:01:36,580 --> 00:01:38,880
in der Realität
gut oder schlecht funktioniert,

35
00:01:38,880 --> 00:01:41,925
haben wir uns das 
Thema "Verallgemeinerung" angesehen.

36
00:01:41,925 --> 00:01:46,040
Als wir das exakte Modell mit einem
RMSE von null hatten, sahen wir,

37
00:01:46,040 --> 00:01:51,020
wie schlecht es bei einem unbekannten
neuen Satz von Daten abgeschnitten hat.

38
00:01:51,020 --> 00:01:53,420
Um unsere Modelle zu
verallgemeinern und nicht nur

39
00:01:53,420 --> 00:01:56,865
einen trainierten Datensatz zu speichern,
wovor wir zuvor gewarnt hatten,

40
00:01:56,865 --> 00:02:00,460
teilen wir unseren Original-Datensatz in
Training, Evaluierung und Test auf

41
00:02:00,460 --> 00:02:03,245
und zeigen sie dem Modell
an festgelegten Meilensteinen.

42
00:02:03,245 --> 00:02:06,420
Wir haben besprochen, wie wir
eine Teilmenge der Daten erstellen,

43
00:02:06,420 --> 00:02:09,205
indem wir unseren Datensatz
mit 70 Millionen Flugeinträgen

44
00:02:09,205 --> 00:02:12,515
auf wiederholbare Weise aufteilen
und Stichproben erfassen.

45
00:02:12,515 --> 00:02:15,695
So konnten wir mit Modellverbesserungen
experimentieren und dabei

46
00:02:15,695 --> 00:02:19,090
die zugrunde liegenden Daten
bei jedem Trainingslauf konstant halten.

47
00:02:19,090 --> 00:02:21,325
In unserem Taxi-Lab haben wir entdeckt,

48
00:02:21,325 --> 00:02:25,615
dass es viele Gründe für
falsche Vorhersagen von ML-Modellen gibt.

49
00:02:25,615 --> 00:02:28,130
Eine schlechte Darstellung
unserer Anwendungsfälle,

50
00:02:28,130 --> 00:02:29,980
Überanpassung, Unteranpassung usw.

51
00:02:29,980 --> 00:02:33,350
Wir haben auch gelernt, wie wir die 
Qualitätsanpassung messen können,

52
00:02:33,350 --> 00:02:35,770
indem wir uns die 
gemachten Vorhersagen ansehen.

53
00:02:35,770 --> 00:02:36,590
Das wars.

54
00:02:36,590 --> 00:02:40,110
Trainieren Sie weiterhin Ihre
ML-Fähigkeiten mit diesen praktischen Labs

55
00:02:40,110 --> 00:02:42,000
und wir sehen uns im nächsten Kurs!