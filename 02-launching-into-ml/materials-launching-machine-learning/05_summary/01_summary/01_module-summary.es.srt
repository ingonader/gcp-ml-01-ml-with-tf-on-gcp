1
00:00:00,000 --> 00:00:03,980
Felicitaciones por terminar
el curso de aprendizaje automático.

2
00:00:03,980 --> 00:00:05,830
Revisemos lo que aprendimos.

3
00:00:06,640 --> 00:00:09,359
Primero, vimos
que los sistemas de producción de Google

4
00:00:09,359 --> 00:00:11,455
se basan en años de experiencia.

5
00:00:11,455 --> 00:00:13,740
Repasamos la
evolución del AA en el tiempo,

6
00:00:13,740 --> 00:00:16,765
el desarrollo y la prominencia
de las redes neuronales profundas

7
00:00:16,765 --> 00:00:20,155
y por qué son la mejor opción
para una amplia gama de problemas.

8
00:00:20,385 --> 00:00:23,860
Por último, hablamos sobre cómo
TensorFlow y Cloud Machine Learning Engine

9
00:00:23,860 --> 00:00:27,370
aprovechan la experiencia
de Google con estos sistemas.

10
00:00:27,890 --> 00:00:32,024
Luego, buscamos el modelo
de AA óptimo en el espacio de parámetros

11
00:00:32,024 --> 00:00:35,700
usando el algoritmo de descenso
de gradientes para reducir las pérdidas.

12
00:00:36,180 --> 00:00:38,210
Ilustramos el entrenamiento de modelos

13
00:00:38,210 --> 00:00:40,470
usando el derivado
de las superficies de pérdida

14
00:00:40,470 --> 00:00:42,300
como guía a los valores mínimos.

15
00:00:42,300 --> 00:00:46,020
Recuerde que puede haber
más de un mínimo en superficies complejas.

16
00:00:46,020 --> 00:00:50,025
El descenso de gradientes es iterativo,
como vio en el bucle de entrenamiento.

17
00:00:50,025 --> 00:00:53,106
La idea es cambiar 
el peso de sus modelos un poco,

18
00:00:53,106 --> 00:00:56,060
reevaluarlos
y usarlos como guía direccional

19
00:00:56,060 --> 00:00:59,655
para reducir las superficies de pérdida
y cambiar los pesos progresivamente.

20
00:01:00,195 --> 00:01:02,545
Luego, presentamos
varias funciones de pérdidas,

21
00:01:02,545 --> 00:01:04,584
como RMSE para los problemas de regresión

22
00:01:04,584 --> 00:01:06,690
y la entropía cruzada
para la clasificación.

23
00:01:07,210 --> 00:01:09,030
Revisamos medidas de rendimiento,

24
00:01:09,030 --> 00:01:11,330
como la exactitud,
la precisión y la recuperación

25
00:01:11,330 --> 00:01:14,580
y analizamos ventajas y desventajas
de usarlas para informar al jefe.

26
00:01:15,180 --> 00:01:18,050
Luego, nos divertimos
en el área de prueba de TensorFlow.

27
00:01:18,050 --> 00:01:21,550
Examinamos los tamaños
de lotes pequeños, moderados y grandes

28
00:01:21,550 --> 00:01:25,000
y cuáles pueden generar
rendimiento poco uniforme en los modelos.

29
00:01:25,000 --> 00:01:27,080
Como cierre del módulo de optimización,

30
00:01:27,080 --> 00:01:30,730
entrenamos redes neuronales
para clasificar datos en espiral.

31
00:01:30,730 --> 00:01:34,765
Terminamos con un conjunto de nodos
y capas ocultas que parecía complejo.

32
00:01:34,765 --> 00:01:38,710
Para saber mejor si el modelo
tendría buen rendimiento en el mundo real,

33
00:01:38,710 --> 00:01:41,605
nos sumergimos
en el mundo de la generalización.

34
00:01:42,635 --> 00:01:46,040
Cuando logramos un modelo exacto
con un valor de RMSE de cero,

35
00:01:46,040 --> 00:01:50,870
vimos que tuvo un pésimo rendimiento
con un conjunto de datos que no conocía.

36
00:01:50,870 --> 00:01:55,170
Para hacer modelos generalizables
que no solo memoricen datos de prueba

37
00:01:55,170 --> 00:01:56,405
como le advertimos antes,

38
00:01:56,405 --> 00:02:00,125
dividimos el conjunto de datos original
en entrenamiento, evaluación y prueba

39
00:02:00,125 --> 00:02:03,590
y solo los presentamos al modelo
en momentos clave predefinidos.

40
00:02:04,835 --> 00:02:07,210
Luego, vimos
cómo crear subconjuntos de datos.

41
00:02:07,210 --> 00:02:09,845
Para ello, dividimos
y muestreamos el conjunto de datos

42
00:02:09,845 --> 00:02:12,765
de 70 millones de registros
de vuelos de manera repetible.

43
00:02:12,865 --> 00:02:16,805
Así, pudimos probar mejoras del modelo
y usar los mismos datos subyacentes

44
00:02:16,805 --> 00:02:19,090
en cada ejecución
de entrenamiento del modelo.

45
00:02:19,630 --> 00:02:22,695
Luego, en el lab sobre taxis,
descubrimos que los modelos de AA

46
00:02:22,695 --> 00:02:25,665
pueden generar
predicciones erradas por diversos motivos,

47
00:02:25,665 --> 00:02:27,680
como mala representación
de casos prácticos

48
00:02:27,680 --> 00:02:29,880
sobreajuste, subajuste, etcétera.

49
00:02:30,300 --> 00:02:31,670
Aprendimos que podemos medir

50
00:02:31,670 --> 00:02:34,780
la calidad del modelo
si examinamos sus predicciones.

51
00:02:35,240 --> 00:02:36,090
Eso fue todo.

52
00:02:36,090 --> 00:02:39,310
Siga practicando sus habilidades
de AA con estos labs prácticos.

53
00:02:39,710 --> 00:02:42,070
Lo veremos en el próximo curso.