1
00:00:00,000 --> 00:00:03,385
Notice anything different about this dataset?

2
00:00:03,385 --> 00:00:07,440
Click on the link and start training the model in the new window.

3
00:00:07,440 --> 00:00:11,080
What do you observe about the loss and the graph of loss over time?

4
00:00:11,080 --> 00:00:15,495
Do you see any convergence towards zero?

5
00:00:15,495 --> 00:00:19,185
Assuming you've clicked the start training button directly,

6
00:00:19,185 --> 00:00:21,985
you should see output like what is shown here.

7
00:00:21,985 --> 00:00:26,875
Note that the decision boundary does a poor job of dividing the data by class.

8
00:00:26,875 --> 00:00:33,375
Why might this be? The reason is that the data have a non-linear relationship,

9
00:00:33,375 --> 00:00:37,450
that is, you can't draw a straight line dividing orange from blue.

10
00:00:37,450 --> 00:00:41,570
What this data calls for is a non-linear decision boundary,

11
00:00:41,570 --> 00:00:46,870
which in this case we intuitively recognize to be a circle around the blue datapoints.

12
00:00:46,870 --> 00:00:50,065
However, all is not lost.

13
00:00:50,065 --> 00:00:52,960
By clicking on some of the boxes in the input column,

14
00:00:52,960 --> 00:00:57,420
see if you can introduce new features that will dramatically improve performance.

15
00:00:57,420 --> 00:01:00,770
Hopefully by now, your output looks like this because you've

16
00:01:00,770 --> 00:01:04,230
selected the X one squared and X two square features.

17
00:01:04,230 --> 00:01:07,595
Note how circular the decision boundary now is.

18
00:01:07,595 --> 00:01:12,510
How is it possible that a linear model can learn a non-linear decision boundary?

19
00:01:12,510 --> 00:01:16,080
Recall that linear models learn a set of weights that they

20
00:01:16,080 --> 00:01:19,355
then multiply by their features to make predictions.

21
00:01:19,355 --> 00:01:22,645
When those features are first degree terms, like x and y,

22
00:01:22,645 --> 00:01:24,880
the result is a first degree polynomial,

23
00:01:24,880 --> 00:01:27,175
like two x or two thirds y.

24
00:01:27,175 --> 00:01:31,615
And then, the model's predictions look like a line or hydroplane but

25
00:01:31,615 --> 00:01:36,140
there's no rule that says that the features in a linear model must be first degree terms,

26
00:01:36,140 --> 00:01:39,385
just as you can take X squared and multiply it by two,

27
00:01:39,385 --> 00:01:45,375
so too can you take a feature of any degree and learn and wait for it in a linear model.

28
00:01:45,375 --> 00:01:50,190
Let's see how far we can take this new idea.

29
00:01:50,190 --> 00:01:53,300
So, what about this curve?

30
00:01:53,300 --> 00:01:55,430
The last time we were able to find

31
00:01:55,430 --> 00:01:58,970
two non-linear features that made the problem linearly solvable.

32
00:01:58,970 --> 00:02:02,925
Will this strategy work here? Try it out.

33
00:02:02,925 --> 00:02:07,820
What you've now figured out is that using the feature options available to us

34
00:02:07,820 --> 00:02:12,555
and this type of model this particular dataset is not nearly solvable.

35
00:02:12,555 --> 00:02:16,810
The best model I was able to train had loss for about point six.

36
00:02:16,810 --> 00:02:22,835
However, the qualifier of feature options available to us is crucial because in fact,

37
00:02:22,835 --> 00:02:26,355
there is a feature that would make learning this relationship trivial.

38
00:02:26,355 --> 00:02:30,825
Imagine for example a feature that somehow unswirled the data,

39
00:02:30,825 --> 00:02:35,025
so that blue and orange appeared simply as two parallel lines.

40
00:02:35,025 --> 00:02:39,970
These parallel lines would then be easily separable with a third line.

41
00:02:39,970 --> 00:02:44,050
Moments when you find powerful features are magical but

42
00:02:44,050 --> 00:02:47,385
they're also very difficult to anticipate, which is problematic.

43
00:02:47,385 --> 00:02:50,540
However, even though we don't often find features that

44
00:02:50,540 --> 00:02:53,505
are as amazing as the ones we've seen in our toy examples,

45
00:02:53,505 --> 00:02:56,560
feature engineering or the systematic improvement of,

46
00:02:56,560 --> 00:03:00,500
or acquisition of new features is an extremely important part of machine learning,

47
00:03:00,500 --> 00:03:03,235
and it's what we'll focus on in course III.

48
00:03:03,235 --> 00:03:08,210
So, what can we do when our attempts to engineer new features for linear models fail?

49
00:03:08,210 --> 00:03:11,725
The answer is to use more complicated models.

50
00:03:11,725 --> 00:03:16,670
There are many types of models that are able to learn non-linear decision boundaries.

51
00:03:16,670 --> 00:03:20,200
In this course, we'll be focusing on neural networks.

52
00:03:20,200 --> 00:03:24,090
Neural networks are in fact no better than any other sort of model.

53
00:03:24,090 --> 00:03:26,720
The reason they've become more popular is because

54
00:03:26,720 --> 00:03:31,420
today's business problems are biased toward those where neural networks excel.