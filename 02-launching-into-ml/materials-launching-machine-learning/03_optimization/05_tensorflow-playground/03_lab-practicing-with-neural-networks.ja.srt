1
00:00:00,000 --> 00:00:04,145
線形モデルによるデータセットの学習の次は

2
00:00:04,145 --> 00:00:07,540
ニューラルネットワーク（NN）です

3
00:00:07,960 --> 00:00:09,582
ですが その前に

4
00:00:09,582 --> 00:00:13,412
TensorFlow Playgroundの機能を
一部追加で有効にしたので確認します

5
00:00:13,412 --> 00:00:15,620
1つ目はActivationです

6
00:00:15,620 --> 00:00:18,825
Activationとは活性化関数のことで

7
00:00:18,825 --> 00:00:22,775
詳細は コース5 MLの理論と実践
で取り上げます

8
00:00:22,775 --> 00:00:25,790
今重要なのは 活性化関数の選択によって

9
00:00:25,790 --> 00:00:29,155
線形モデルなのかNNなのかが決まる点です

10
00:00:29,155 --> 00:00:31,255
先ほどツールを使用した際は

11
00:00:31,255 --> 00:00:34,910
活性化関数はあらかじめ
線形に設定されていました

12
00:00:34,910 --> 00:00:38,935
2つ目の機能はHIDDEN LAYERSです

13
00:00:38,935 --> 00:00:41,970
この機能により 隠れ層の層数と

14
00:00:41,970 --> 00:00:45,550
それぞれの隠れ層のニューロン数を
変更できます

15
00:00:45,550 --> 00:00:49,260
これは ネットワークが
データを変換する回数の変更と

16
00:00:49,260 --> 00:00:51,185
考えることができます

17
00:00:51,185 --> 00:00:53,685
各隠れ層のニューロンはそれぞれ

18
00:00:53,685 --> 00:00:56,430
先行する層からの出力をすべて受け取り

19
00:00:56,430 --> 00:01:01,380
受け取った入力を変換し 出力値を
次の層の全ニューロンに渡します

20
00:01:01,380 --> 00:01:05,560
ニューロン数と ニューロンが
互いに情報をやり取りする仕組みを

21
00:01:05,560 --> 00:01:09,285
簡単に表す方法は 
ネットワークのアーキテクチャです

22
00:01:09,285 --> 00:01:12,480
Batch size機能も有効になっており

23
00:01:12,480 --> 00:01:15,010
この後 実際に使用します

24
00:01:15,010 --> 00:01:20,380
リンクに従って このデータが分類できるよう
モデルにトレーニングさせてください

25
00:01:20,380 --> 00:01:23,680
ただし 非線形の特徴量は使わずに

26
00:01:23,680 --> 00:01:27,995
変更するのは ネットワークの
アーキテクチャのみとします

27
00:01:27,995 --> 00:01:32,550
NNがどのように機能するのか
まだ説明していませんが 問題ありません

28
00:01:32,550 --> 00:01:35,680
ここでは インターフェースを使って

29
00:01:35,680 --> 00:01:39,840
ある程度うまく機能する
ネットワークを作ってみてください

30
00:01:41,680 --> 00:01:45,210
ある程度うまく機能するモデルが作れたら

31
00:01:45,210 --> 00:01:49,905
OUTPUT欄の青い部分が
多角形になっていると思います

32
00:01:49,905 --> 00:01:56,035
詳しく検証して モデルがどのように
この出力を得たのかを理解しましょう

33
00:01:56,035 --> 00:01:59,305
最初の隠れ層のニューロンを見てください

34
00:01:59,305 --> 00:02:01,395
それぞれにカーソルを合わせると

35
00:02:01,395 --> 00:02:05,050
出力ボックスにニューロンの
学習内容が反映されます

36
00:02:05,050 --> 00:02:09,225
ニューロンの見方は
特徴量と出力の場合と同様です

37
00:02:09,225 --> 00:02:14,200
特徴量X1とX2の値がボックス内の
位置にコード化されていて

38
00:02:14,200 --> 00:02:16,680
X1とX2の組み合わせにより

39
00:02:16,680 --> 00:02:20,335
ニューロンが出力する値を色分けしています

40
00:02:20,335 --> 00:02:23,725
各ボックスに順にカーソルを
合わせていきますので

41
00:02:23,725 --> 00:02:27,985
すべて重ね合わせるとどうなるか
頭の中で考えてみてください

42
00:02:27,985 --> 00:02:30,860
青に青が重なると濃い青になり

43
00:02:30,860 --> 00:02:33,465
白に青が重なると薄い青になり

44
00:02:33,465 --> 00:02:36,880
オレンジに青が重なると白になります

45
00:02:37,570 --> 00:02:41,730
各ニューロンによってモデルの
決定境界が形成されていること

46
00:02:41,730 --> 00:02:46,155
出力の形が隠れ層に基づいている
ことがわかってくると思います

47
00:02:46,155 --> 00:02:50,625
例えば このニューロンは
決定境界のこちらの境界線をもたらし

48
00:02:50,625 --> 00:02:54,395
このニューロンは
こちらの境界線をもたらします

49
00:02:55,055 --> 00:02:57,350
幾何学的に
ある程度うまく機能する範囲で

50
00:02:57,350 --> 00:03:02,790
このネットワークをできるだけ小さくする場合
どこまで縮小できるでしょうか

51
00:03:02,790 --> 00:03:05,340
何とか分類できる範囲で

52
00:03:05,340 --> 00:03:09,035
青い点の周りに描ける最も簡単な
形を考えるのがヒントです

53
00:03:09,035 --> 00:03:14,155
実際にTensorFlow Playgroundを使って
直感が正しいか確かめてみましょう

54
00:03:15,285 --> 00:03:18,220
ここまでは ネットワークの最初の隠れ層に
属するニューロンの出力によって

55
00:03:18,220 --> 00:03:21,465
決定境界が構成される仕組みを見てきました

56
00:03:21,465 --> 00:03:23,565
他の層はどうでしょうか

57
00:03:23,565 --> 00:03:28,500
隠れ層が1層のNNと多数あるNNは
どう違うのでしょうか

58
00:03:29,410 --> 00:03:30,860
下のリンクをクリックして

59
00:03:30,860 --> 00:03:34,570
このらせん状のデータセットの
分類をNNに学習させます

60
00:03:34,570 --> 00:03:38,160
ここで バッチサイズが
勾配降下法に与える影響について

61
00:03:38,160 --> 00:03:40,180
理解を深めましょう

62
00:03:40,180 --> 00:03:43,170
バッチサイズパラメータを1に設定して

63
00:03:43,170 --> 00:03:47,805
NNアーキテクチャを変更しながら
うまく機能するものを見つけてください

64
00:03:47,805 --> 00:03:54,130
次に モデルを300エポックトレーニングさせ
このときの損失曲線を覚えておきます

65
00:03:54,130 --> 00:03:59,005
今度は バッチサイズパラメータを
10に設定してトレーニングを再開します

66
00:03:59,005 --> 00:04:05,600
モデルを300エポックトレーニングさせ
このときの損失曲線も覚えておきます

67
00:04:05,600 --> 00:04:11,555
最後に バッチサイズを30に
設定して繰り返します

68
00:04:11,555 --> 00:04:13,760
何がわかりましたか

69
00:04:13,760 --> 00:04:17,649
また これまでに学んだことを基に
どのような解釈ができますか

70
00:04:17,649 --> 00:04:20,170
損失曲線の滑らかさに

71
00:04:20,170 --> 00:04:23,530
顕著な違いが見られたと思います

72
00:04:23,530 --> 00:04:27,420
バッチサイズを増やすと
より滑らかな曲線になりました

73
00:04:27,420 --> 00:04:29,345
なぜでしょうか

74
00:04:29,345 --> 00:04:32,610
バッチサイズが勾配降下法に
与える影響を考えてください

75
00:04:32,610 --> 00:04:34,440
バッチサイズが小さいと

76
00:04:34,440 --> 00:04:39,460
モデルは1つのデータの損失を求めるごとに
パラメータを更新しますが

77
00:04:39,460 --> 00:04:43,465
データは多様なのでそこに問題があります

78
00:04:43,465 --> 00:04:45,760
しかし バッチサイズを上げると

79
00:04:45,760 --> 00:04:51,195
個々のデータポイントによるノイズが収まり
明確なシグナルが現れます

80
00:04:51,195 --> 00:04:53,910
こうした観察内容から

81
00:04:53,910 --> 00:04:59,865
バッチサイズを変更することが 単純に収束率に
影響すると結論付けることはできません

82
00:04:59,865 --> 00:05:03,520
学習率同様 最適なバッチサイズは
問題ごとに異なり

83
00:05:03,520 --> 00:05:07,845
ハイパーパラメータチューニングを
通じて見つけることができます

84
00:05:07,845 --> 00:05:13,695
モデルが学習を終えるとこのようになります

85
00:05:13,695 --> 00:05:17,400
まず注目すべきは 最初の隠れ層と

86
00:05:17,400 --> 00:05:19,875
その後の隠れ層の関係です

87
00:05:19,875 --> 00:05:23,060
最初の隠れ層のニューロンからの出力が

88
00:05:23,060 --> 00:05:25,370
基本的に直線だったのに対し

89
00:05:25,370 --> 00:05:30,075
後続の隠れ層の出力は はるかに
複雑なのは明白だと思います

90
00:05:30,075 --> 00:05:34,050
隠れ層の出力を積み重ねた際と同様に

91
00:05:34,050 --> 00:05:38,125
後続の層は先行する層を基に形成されます

92
00:05:38,125 --> 00:05:43,680
したがって NNは特徴量の
階層と考えることができます

93
00:05:43,680 --> 00:05:47,210
最終的にデータを分類する前に

94
00:05:47,210 --> 00:05:50,810
入力内容を複雑な方法で変換するというのは

95
00:05:50,810 --> 00:05:53,150
NNに特有の考え方で

96
00:05:53,150 --> 00:05:57,255
MLで古くから使用されてきた手法と比べ
大きく発展した点です

97
00:05:57,255 --> 00:06:00,420
NNが誕生する前 データサイエンティストたちは

98
00:06:00,420 --> 00:06:04,100
特徴量エンジニアリングに
はるかに多くの時間を費やしていました

99
00:06:04,100 --> 00:06:07,420
現在では モデル自体が
この役割の一部を担っていて

100
00:06:07,420 --> 00:06:11,985
層はモデルによる特徴量エンジニアリングの
一種と考えられます

101
00:06:11,985 --> 00:06:16,375
次に注目すべきは
モデルの学習内容のおかしな点です

102
00:06:16,375 --> 00:06:20,020
モデルは この2つの領域に
オレンジのポイントがないことを

103
00:06:20,020 --> 00:06:23,545
青であることの証明と解釈したようです

104
00:06:23,545 --> 00:06:26,390
このように モデルがデータセットのノイズを

105
00:06:26,390 --> 00:06:29,725
意味あるものとして解釈してしまう
誤りを過学習と呼びます

106
00:06:29,725 --> 00:06:32,170
過学習は モデルの意思決定権が

107
00:06:32,170 --> 00:06:35,405
厳密に問題に必要な権限を
超えている場合に発生します

108
00:06:35,405 --> 00:06:40,095
モデルの過学習が起こると うまく汎化できず
新しいデータに対応できません

109
00:06:40,095 --> 00:06:43,640
新しいデータでは
基盤となるシグナルは残るものの

110
00:06:43,640 --> 00:06:47,015
まったく同じパターンのノイズが
発生する可能性は低いためです

111
00:06:47,015 --> 00:06:48,585
どう対処すればよいでしょうか

112
00:06:48,585 --> 00:06:53,150
それは 次の汎化と標本抽出の
講義でお話しします