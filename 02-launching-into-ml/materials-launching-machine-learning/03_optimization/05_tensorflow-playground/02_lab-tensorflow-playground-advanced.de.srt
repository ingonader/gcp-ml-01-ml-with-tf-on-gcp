1
00:00:00,000 --> 00:00:03,385
Fällt Ihnen an diesem Dataset etwas auf?

2
00:00:03,385 --> 00:00:07,440
Klicken Sie auf den Link und beginnen Sie 
mit dem Modelltraining im neuen Fenster.

3
00:00:07,440 --> 00:00:11,080
Was fällt Ihnen am Verlust
und an der Verlustkurve auf?

4
00:00:11,080 --> 00:00:15,495
Sehen Sie eine Konvergenz gegen null?

5
00:00:15,495 --> 00:00:19,185
Wenn Sie direkt auf die Schaltfläche
"Training starten" geklickt haben,

6
00:00:19,185 --> 00:00:21,985
sollten Sie eine
Ausgabe wie diese hier sehen.

7
00:00:21,985 --> 00:00:26,875
Beachten Sie, dass die Entscheidungsgrenze
die Daten unzureichend nach Klassen trennt.

8
00:00:26,875 --> 00:00:33,375
Woran liegt das? Der Grund ist
die nicht lineare Beziehung der Daten,

9
00:00:33,375 --> 00:00:37,450
Sie können also keine gerade Linie
ziehen, die Orange von Blau trennt.

10
00:00:37,450 --> 00:00:41,570
Diese Daten erfordern eine
nichtlineare Entscheidungsgrenze,

11
00:00:41,570 --> 00:00:46,870
die wir hier als einen Kreis
um die blauen Datenpunkte erkennen.

12
00:00:46,870 --> 00:00:50,065
Aber es ist nicht alles verloren.

13
00:00:50,065 --> 00:00:52,960
Klicken Sie einige Felder
in der Eingabespalte an und

14
00:00:52,960 --> 00:00:57,420
versuchen Sie neue Merkmale hinzuzufügen,
um die Leistung zu verbessern.

15
00:00:57,420 --> 00:01:00,770
Ihre Ausgabe sollte
nun so aussehen, wenn Sie die Merkmale

16
00:01:00,770 --> 00:01:04,230
"X1 hoch 2" und
"X2 hoch 2" ausgewählt haben.

17
00:01:04,230 --> 00:01:07,595
Beachten Sie die 
nun kreisförmige Entscheidungsgrenze.

18
00:01:07,595 --> 00:01:12,510
Wie kann ein lineares Modell eine 
nicht lineare Entscheidungsgrenze lernen?

19
00:01:12,510 --> 00:01:16,170
Wir erinnern uns: Lineare Modelle 
können eine Reihe von Gewichtungen lernen,

20
00:01:16,170 --> 00:01:19,915
die sie dann mit ihren Eigenschaften
multiplizieren, um Vorhersagen zu treffen.

21
00:01:19,915 --> 00:01:22,645
Wenn diese Merkmale Terme
ersten Grades sind, wie x und y,

22
00:01:22,645 --> 00:01:24,880
ist das Ergebnis
ein Polynom ersten Grades,

23
00:01:24,880 --> 00:01:27,175
wie 2x oder 2/3y.

24
00:01:27,175 --> 00:01:31,745
Dann sehen die Vorhersagen des Modells
wie eine Linie oder eine Hyperebene aus,

25
00:01:31,745 --> 00:01:36,670
aber die Merkmale eines linearen Modells
müssen nicht Terme ersten Grades sein.

26
00:01:36,670 --> 00:01:40,485
Ebenso wie Sie "x hoch 2"
nehmen und mit 2 multiplizieren können,

27
00:01:40,485 --> 00:01:43,105
können Sie ein Merkmal jeden Grades nehmen

28
00:01:43,105 --> 00:01:45,885
und dafür eine Gewichtung
in einem linearen Modell lernen.

29
00:01:45,885 --> 00:01:49,000
Mal sehen, wie weit wir
mit dieser neuen Idee kommen.

30
00:01:50,990 --> 00:01:53,300
Was ist mit dieser Kurve?

31
00:01:53,300 --> 00:01:55,430
Das letzte Mal konnten wir

32
00:01:55,430 --> 00:01:58,970
zwei nichtlineare Merkmale finden,
die das Problem linear lösbar machten.

33
00:01:58,970 --> 00:02:02,925
Wird diese Strategie auch
hier funktionieren? Versuchen Sie es.

34
00:02:02,925 --> 00:02:07,820
Wir wissen jetzt,
dass mit den verfügbaren Merkmalen

35
00:02:07,820 --> 00:02:12,555
und diesem Modelltyp dieses
bestimmte Dataset nicht linear lösbar ist.

36
00:02:12,555 --> 00:02:16,810
Das beste Modell, das ich trainieren 
konnte, hatte einen Verlust von ca. 0,6.

37
00:02:16,810 --> 00:02:22,835
Die verfügbaren Qualifizierer oder
Merkmale sind jedoch entscheidend,

38
00:02:22,835 --> 00:02:26,355
da es ein Merkmal gibt, das das
Lernen dieser Beziehung vereinfacht.

39
00:02:26,355 --> 00:02:30,825
Stellen Sie sich etwa ein
Merkmal vor, das die Daten so entwirrt,

40
00:02:30,825 --> 00:02:35,025
dass Blau und Orange einfach
als zwei parallele Linien erscheinen.

41
00:02:35,025 --> 00:02:39,970
Diese parallelen Linien wären dann 
leicht durch eine dritte Linie trennbar.

42
00:02:39,970 --> 00:02:44,050
Es ist toll, solche Merkmale zu finden,

43
00:02:44,050 --> 00:02:47,385
aber man kann das nicht
erwarten. Und das ist problematisch.

44
00:02:47,385 --> 00:02:53,045
Zwar finden wir nicht oft so tolle 
Merkmale wie in den Spielzeug-Beispielen,

45
00:02:53,045 --> 00:02:56,560
aber Feature Engineering, also
die systematische Verbesserung

46
00:02:56,560 --> 00:03:00,500
oder der Erwerb neuer Merkmale ist ein
wichtiger Teil des maschinellen Lernens.

47
00:03:00,500 --> 00:03:03,235
Darauf konzentrieren wir uns in Kurs 3.

48
00:03:03,235 --> 00:03:05,990
Was können wir tun, wenn das Entwickeln

49
00:03:05,990 --> 00:03:08,210
neuer Merkmale
für lineare Modelle fehlschlägt?

50
00:03:08,210 --> 00:03:11,725
Die Antwort ist,
komplexere Modelle zu verwenden.

51
00:03:11,725 --> 00:03:15,360
Es gibt viele Arten von Modellen,
die nicht lineare Entscheidungsgrenzen

52
00:03:15,360 --> 00:03:16,670
lernen können.

53
00:03:16,670 --> 00:03:20,200
In diesem Kurs konzentrieren
wir uns auf neuronale Netzwerke.

54
00:03:20,200 --> 00:03:24,090
Neuronale Netzwerke sind nicht
besser als jede andere Art von Modell.

55
00:03:24,090 --> 00:03:27,820
Sie sind deshalb populärer geworden,
weil sie ideal sind, um typische Probleme

56
00:03:27,820 --> 00:03:31,420
moderner Unternehmen zu lösen.