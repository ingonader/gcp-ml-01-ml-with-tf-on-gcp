1
00:00:00,300 --> 00:00:03,724
Lorsque vous avez testé différentes
architectures de réseaux de neurones,

2
00:00:03,724 --> 00:00:06,364
certains d'entre vous ont peut-être
entraîné des modèles

3
00:00:06,364 --> 00:00:08,740
qui sont passés à l'état terminal,
comme celui-ci.

4
00:00:08,740 --> 00:00:12,010
Notez l'aspect de la courbe de perte
et celui de la sortie.

5
00:00:12,010 --> 00:00:15,255
Qu'avez-vous fait pour les corriger ?
Et que se passe-t-il ici ?

6
00:00:15,255 --> 00:00:18,205
Vous avez peut-être modifié
l’architecture de votre réseau.

7
00:00:18,205 --> 00:00:20,977
Pourtant, il suffit souvent
d'entraîner à nouveau le modèle

8
00:00:20,977 --> 00:00:22,670
pour corriger ce type de problème.

9
00:00:22,670 --> 00:00:26,197
Souvenez-vous que certaines parties
du processus d'entraînement du modèle

10
00:00:26,197 --> 00:00:30,195
ne sont pas contrôlées, notamment les graines
aléatoires des initialiseurs de poids.

11
00:00:30,195 --> 00:00:33,880
Ici, le problème est que nous semblons
avoir trouvé sur notre surface de perte

12
00:00:33,880 --> 00:00:37,380
une position nettement supérieure
à zéro, bien qu'elle soit plus petite

13
00:00:37,380 --> 00:00:39,415
que ses voisins.

14
00:00:39,415 --> 00:00:42,890
En d'autres termes, nous avons trouvé
un minimum local.

15
00:00:42,890 --> 00:00:45,940
Voyez comme le graphique
d'évolution de la perte au fil du temps

16
00:00:45,940 --> 00:00:51,180
a atteint une valeur de perte inférieure
plus tôt au cours de la recherche.

17
00:00:51,180 --> 00:00:54,510
L'existence et le pouvoir de séduction
des minima locaux sous-optimaux

18
00:00:54,510 --> 00:00:58,265
sont deux exemples des défauts
de notre approche actuelle.

19
00:00:58,265 --> 00:01:01,380
Et il y en a d'autres, notamment
la longue durée des entraînements

20
00:01:01,380 --> 00:01:04,755
et l'existence de minima simples
mais inappropriés.

21
00:01:04,755 --> 00:01:07,535
Ces problèmes n'ont pas une cause unique,

22
00:01:07,535 --> 00:01:10,370
et les méthodes permettant
de les traiter sont donc variées.

23
00:01:10,370 --> 00:01:14,410
Les techniques d'optimisation avancées ont
pour but de réduire le temps d'entraînement

24
00:01:14,410 --> 00:01:16,770
et d'aider les modèles
à ne pas se laisser séduire

25
00:01:16,770 --> 00:01:17,830
par les minima locaux.

26
00:01:17,830 --> 00:01:21,345
Nous en examinerons certaines
plus tard dans ce cours.

27
00:01:21,345 --> 00:01:23,542
L'attente et le suréchantillonnage
des données,

28
00:01:23,542 --> 00:01:25,710
ainsi que la création
de données synthétiques,

29
00:01:25,710 --> 00:01:28,522
ont pour but d'éliminer complètement
les minima inappropriés

30
00:01:28,522 --> 00:01:30,095
de l'espace de recherche.

31
00:01:30,095 --> 00:01:33,570
Les métriques de performances,
dont je parlerai dans la prochaine section,

32
00:01:33,570 --> 00:01:36,390
permettent de s'attaquer au problème
à un niveau plus élevé.

33
00:01:36,390 --> 00:01:39,735
Plutôt que de changer notre mode de recherche
ou l'espace de recherche,

34
00:01:39,735 --> 00:01:42,600
elles changent notre perception
des résultats de la recherche

35
00:01:42,600 --> 00:01:46,295
en les alignant plus étroitement
sur ce qui nous intéresse vraiment.

36
00:01:46,295 --> 00:01:50,377
Elles nous aident ainsi à mieux choisir le
moment où effectuer une nouvelle recherche.