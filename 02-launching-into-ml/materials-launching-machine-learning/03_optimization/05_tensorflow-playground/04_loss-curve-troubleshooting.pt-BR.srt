1
00:00:00,000 --> 00:00:03,594
Durante os testes com
arquiteturas diferentes de rede neural,

2
00:00:03,594 --> 00:00:08,235
alguns de seus modelos treinados podem
ter entrado em status terminal, como este.

3
00:00:08,235 --> 00:00:11,520
Observe a curva de perda e a saída.

4
00:00:11,520 --> 00:00:15,275
O que você fez para consertá-los?
E o que está acontecendo aqui?

5
00:00:15,275 --> 00:00:18,075
Você pode ter alterado
a sua arquitetura de rede,

6
00:00:18,075 --> 00:00:22,400
mas, muitas vezes, podemos corrigir esses
problemas treinando o modelo novamente.

7
00:00:22,400 --> 00:00:26,415
Lembre-se: ainda há elementos que não
controlamos no treinamento de modelos,

8
00:00:26,415 --> 00:00:29,815
como as sugestões aleatórias
dos inicializadores de peso.

9
00:00:30,145 --> 00:00:32,640
O problema neste caso é que aparentemente

10
00:00:32,640 --> 00:00:35,490
encontramos na superfície de perda
uma posição pequena

11
00:00:35,490 --> 00:00:36,750
em relação às adjacentes,

12
00:00:36,750 --> 00:00:39,675
mas ainda muito maior do que zero.

13
00:00:39,675 --> 00:00:42,690
Em outras palavras,
encontramos um mínimo local.

14
00:00:42,690 --> 00:00:45,320
Observe como o gráfico
de perda ao longo do tempo

15
00:00:45,320 --> 00:00:48,502
atingiu um valor de perda
menor antes na pesquisa.

16
00:00:50,820 --> 00:00:52,910
A existência e o poder de sedução

17
00:00:52,910 --> 00:00:57,835
dos mínimos locais não ideais são dois
exemplos de falhas em nossa abordagem atual.

18
00:00:57,835 --> 00:01:00,050
Outras desvantagens incluem problemas como

19
00:01:00,050 --> 00:01:04,755
tempos longos de treinamento e a existência
de mínimos insignificantes, mas inadequados.

20
00:01:04,755 --> 00:01:07,535
Esses problemas não têm uma única causa.

21
00:01:07,535 --> 00:01:10,305
Portanto, há métodos
diversos para lidar com eles.

22
00:01:10,305 --> 00:01:14,190
As técnicas de otimização avançadas
destinam-se a reduzir o tempo de treinamento

23
00:01:14,190 --> 00:01:17,090
e evitar que modelos sejam
seduzidos por um mínimo local.

24
00:01:17,090 --> 00:01:19,885
Vamos voltar a alguns desses
tópicos posteriormente no curso.

25
00:01:20,775 --> 00:01:24,760
A espera e a sobreamostragem de dados,
bem como a criação de dados sintéticos,

26
00:01:24,760 --> 00:01:28,215
destinam-se a remover os mínimos inadequados
do espaço de pesquisa.

27
00:01:30,045 --> 00:01:32,910
As métricas de desempenho,
que discutiremos na próxima seção,

28
00:01:32,910 --> 00:01:34,960
lidam com o problema
em um nível mais elevado.

29
00:01:34,960 --> 00:01:38,645
Em vez de mudar como pesquisamos
ou o próprio espaço de pesquisa,

30
00:01:38,645 --> 00:01:42,270
as métricas de desempenho mudam
como pensamos sobre os resultados da pesquisa,

31
00:01:42,270 --> 00:01:45,875
alinhando-os mais estreitamente
com o que nos interessa.

32
00:01:45,875 --> 00:01:51,000
Isso nos permite tomar decisões embasadas
e melhores sobre futuras pesquisas.