1
00:00:00,000 --> 00:00:03,385
Vê algo de diferente
neste conjunto de dados?

2
00:00:03,385 --> 00:00:07,440
Clique no link e comece
a treinar o modelo na nova janela.

3
00:00:07,440 --> 00:00:11,080
O que você observou quanto à perda e
ao gráfico de perda ao longo do tempo?

4
00:00:11,080 --> 00:00:15,495
Viu alguma convergência
em direção a zero?

5
00:00:15,495 --> 00:00:19,185
Vamos supor que você clicou diretamente
no botão para iniciar o treinamento.

6
00:00:19,185 --> 00:00:21,985
Você deve ter visto algo parecido
com a imagem exibida aqui.

7
00:00:21,985 --> 00:00:26,875
Observe que o limite de decisão
não consegue dividir os dados por classe.

8
00:00:26,875 --> 00:00:29,345
Por quê?

9
00:00:30,180 --> 00:00:33,380
Porque os dados estão
relacionados de maneira não linear,

10
00:00:33,380 --> 00:00:37,450
ou seja, não é possível desenhar uma
linha reta para dividir o laranja do azul.

11
00:00:37,450 --> 00:00:41,570
O que esses dados precisam
é de um limite de decisão não linear,

12
00:00:41,570 --> 00:00:46,870
que aqui reconhecemos intuitivamente como
um círculo em volta dos pontos de dado azuis.

13
00:00:46,870 --> 00:00:50,065
Mas nem tudo está perdido.

14
00:00:50,065 --> 00:00:52,960
Clique em algumas das
caixas da coluna de entradas

15
00:00:52,960 --> 00:00:57,420
e tente introduzir características
novas que melhorarão muito o desempenho.

16
00:00:57,420 --> 00:01:00,770
Com sorte, agora o seu resultado se parece com este

17
00:01:00,770 --> 00:01:04,230
porque você selecionou
as características x1² e x2².

18
00:01:04,230 --> 00:01:07,595
Veja como o limite
de decisão agora é circular.

19
00:01:07,595 --> 00:01:12,510
Como um modelo linear pode
aprender um limite de decisão não linear?

20
00:01:12,510 --> 00:01:16,080
Lembre-se de que os modelos lineares
aprendem um conjunto de pesos

21
00:01:16,080 --> 00:01:19,355
que eles multiplicam pelas
características para fazer previsões.

22
00:01:19,355 --> 00:01:22,645
Quando as características são
termos de primeiro grau, como x e y,

23
00:01:22,645 --> 00:01:24,880
o resultado é um
polinômio de primeiro grau,

24
00:01:24,880 --> 00:01:27,175
como 2x ou 2/3 y.

25
00:01:27,175 --> 00:01:31,085
E as previsões do modelo se parecem
com uma linha ou um hiperplano,

26
00:01:31,085 --> 00:01:36,140
mas não é obrigatório usar termos de primeiro
grau como características no modelo linear.

27
00:01:36,140 --> 00:01:39,385
Assim como podemos multiplicar x² por dois,

28
00:01:39,385 --> 00:01:42,155
podemos também pegar uma
característica de qualquer grau

29
00:01:42,155 --> 00:01:45,375
e aprender um peso para ela no modelo linear.

30
00:01:45,375 --> 00:01:50,190
Vamos ver até onde podemos
ir com essa abordagem nova.

31
00:01:50,190 --> 00:01:53,300
E quanto a esta curva?

32
00:01:53,300 --> 00:01:56,650
Da última vez, encontramos
duas características não lineares

33
00:01:56,650 --> 00:01:58,970
que permitiram resolver
o problema de maneira linear.

34
00:01:58,970 --> 00:02:01,325
Será que essa estratégia funcionará aqui?

35
00:02:01,325 --> 00:02:02,925
Tente.

36
00:02:02,925 --> 00:02:07,820
Você já deve ter percebido que usar
as opções de característica disponíveis

37
00:02:07,820 --> 00:02:12,555
e esse tipo de modelo não resolverá esse
conjunto de dados de maneira linear.

38
00:02:12,555 --> 00:02:16,810
O melhor modelo que consegui
treinar tinha perda de cerca de 0,6.

39
00:02:16,810 --> 00:02:22,225
No entanto, o qualificador das
opções de característica disponíveis é vital

40
00:02:22,225 --> 00:02:26,355
porque há uma característica que simplifica
bastante o aprendizado dessa relação.

41
00:02:26,355 --> 00:02:30,825
Imagine uma característica
que desmanchasse a espiral dos dados,

42
00:02:30,825 --> 00:02:35,025
de modo que azul e laranja
aparecessem como duas linhas paralelas.

43
00:02:35,025 --> 00:02:39,970
Essas linhas paralelas poderiam ser
separadas facilmente por uma terceira linha.

44
00:02:39,970 --> 00:02:44,050
São momentos em que essas
características poderosas fazem mágica,

45
00:02:44,050 --> 00:02:47,385
mas é muito difícil prever isso,
o que é problemático.

46
00:02:47,385 --> 00:02:50,540
No entanto, embora nem sempre
seja possível encontrar características

47
00:02:50,540 --> 00:02:53,505
tão fantásticas quanto as que
vimos nos nossos problemas fictícios,

48
00:02:53,505 --> 00:02:56,560
a engenharia de características
ou a melhoria sistemática

49
00:02:56,560 --> 00:03:00,500
ou aquisição de características novas é
muito importante no aprendizado de máquina.

50
00:03:00,500 --> 00:03:03,235
E é esse assunto que
vamos enfocar no terceiro curso.

51
00:03:03,235 --> 00:03:08,210
O que fazer quando não é possível construir
características novas para modelos lineares?

52
00:03:08,210 --> 00:03:11,725
A resposta é usar
modelos mais complicados.

53
00:03:11,725 --> 00:03:16,670
Há muitos tipos de modelos capazes
de aprender limites de decisão não lineares.

54
00:03:16,670 --> 00:03:20,200
Neste curso, vamos
nos concentrar nas redes neurais.

55
00:03:20,200 --> 00:03:24,090
As redes neurais não são melhores
do que qualquer outro tipo de modelo.

56
00:03:24,090 --> 00:03:26,720
O motivo das redes neurais serem tão usadas

57
00:03:26,720 --> 00:03:31,420
é porque elas tendem a ser perfeitas
para os problemas de negócios atuais.