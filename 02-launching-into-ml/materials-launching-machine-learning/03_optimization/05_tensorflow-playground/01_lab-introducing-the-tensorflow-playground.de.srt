1
00:00:00,320 --> 00:00:03,985
Jetzt haben wir gesehen, 
wie der Gradientenabstieg funktioniert.

2
00:00:03,985 --> 00:00:08,580
Betrachten wir das in Aktion, 
indem wir die Tools benutzen,

3
00:00:08,580 --> 00:00:11,835
die uns viele der besprochenen 
Phänomene in Echtzeit zeigen.

4
00:00:11,835 --> 00:00:15,070
TensorFlow Playground
ist ein leistungsstarkes Tool,

5
00:00:15,070 --> 00:00:17,790
das die Funktionsweise
neuronaler Netzwerke veranschaulicht.

6
00:00:17,790 --> 00:00:21,920
Darüber haben wir 
bisher noch nicht gesprochen,

7
00:00:21,920 --> 00:00:24,385
aber das werden wir bald.

8
00:00:24,385 --> 00:00:28,655
Aus Gründen, die ich noch erkläre,
sind die einfachsten neuronalen Netzwerke

9
00:00:28,655 --> 00:00:30,815
mathematisch äquivalent 
zu linearen Modellen.

10
00:00:30,825 --> 00:00:34,750
Daher ist dieses Tool auch gut geeignet, 
um das bisher Gelernte zu verdeutlichen.

11
00:00:34,750 --> 00:00:37,710
Wir werden es verwenden, 
um den heute besprochenen Theorieteil

12
00:00:37,710 --> 00:00:39,195
experimentell zu überprüfen,

13
00:00:39,195 --> 00:00:41,635
damit Sie Ihre
ML-Kenntnisse vertiefen können.

14
00:00:41,635 --> 00:00:43,720
Sie sehen aus erster Hand die Auswirkungen

15
00:00:43,720 --> 00:00:46,865
der Lernrate und wie ML-Modelle
den Gradientenabstieg durchführen.

16
00:00:46,865 --> 00:00:49,010
Ich nenne auch Verbindungen zu Themen,

17
00:00:49,010 --> 00:00:52,550
die in diesem und in
späteren Kursen vertieft werden.

18
00:00:52,550 --> 00:00:55,645
Sprechen wir zuerst über die Oberfläche.

19
00:00:55,645 --> 00:00:58,095
Ich habe einige der 
Funktionen des Tools entfernt,

20
00:00:58,095 --> 00:01:00,985
weil sie sich auf Themen
beziehen, die wir später behandeln,

21
00:01:00,985 --> 00:01:04,275
aber es gibt noch viele 
interessante Stellschrauben für uns.

22
00:01:04,275 --> 00:01:07,300
Zuerst haben wir die Merkmalspalte.

23
00:01:07,300 --> 00:01:10,190
Das sind die
Eingaben, die Ihr Modell sieht.

24
00:01:10,190 --> 00:01:14,125
Die Färbung in jedem Merkmalfeld 
stellt jeweils den Wert des Merkmals dar.

25
00:01:14,125 --> 00:01:17,550
Orange bedeutet negativ 
und blau bedeutet positiv.

26
00:01:17,550 --> 00:01:22,310
Das ist die Spalte mit den versteckten 
Schichten, in der die Gewichtungen sind.

27
00:01:22,310 --> 00:01:27,100
Bewegen Sie den Mauszeiger über eine 
Gewichtungslinie, um deren Wert zu sehen.

28
00:01:27,100 --> 00:01:29,015
Wenn das Modell trainiert wird,

29
00:01:29,015 --> 00:01:32,110
ändern sich die Breite 
und Deckkraft dieser Linien,

30
00:01:32,110 --> 00:01:35,750
sodass Sie schnell einen 
Überblick über die Werte erhalten.

31
00:01:35,750 --> 00:01:38,720
Dann gibt es die
Ausgabespalte, in der Sie sowohl

32
00:01:38,720 --> 00:01:41,310
die Trainingsdaten als
auch die aktuellen Vorhersagen

33
00:01:41,310 --> 00:01:44,555
des Modells für alle Punkte im 
Merkmalsraum sehen können.

34
00:01:44,555 --> 00:01:47,740
Sie können auch den
aktuellen Trainingsverlust sehen.

35
00:01:47,740 --> 00:01:52,340
Wie bei den Merkmalen werden Farben 
verwendet, um den Wert darzustellen.

36
00:01:52,340 --> 00:01:55,975
Die obere Steuerleiste enthält 
Schaltflächen zum Zurücksetzen

37
00:01:55,975 --> 00:01:59,365
und zum Starten des Trainings 
sowie zum Ausführen einzelner Schritte.

38
00:01:59,365 --> 00:02:02,275
Es gibt auch ein 
Drop-down-Feld für die Lernrate.

39
00:02:02,275 --> 00:02:07,755
In der Datenspalte können Sie aus Datasets
wählen und die Batch-Größe steuern.

40
00:02:07,755 --> 00:02:11,740
Trainieren wir zuerst ein lineares 
Modell, um einige Daten zu klassifizieren.

41
00:02:11,740 --> 00:02:15,750
Mit einem Klick auf diesen Link wird
ein TensorFlow Playground-Fenster mit

42
00:02:15,750 --> 00:02:20,210
nur dem Notwendigsten angezeigt. Die 
versteckten Ebenen sind vorerst unwichtig.

43
00:02:20,210 --> 00:02:22,825
In dieser Konfiguration des Tools

44
00:02:22,825 --> 00:02:24,970
akzeptiert das 
Modell einen Merkmalvektor,

45
00:02:24,970 --> 00:02:27,770
berechnet ein Skalarprodukt 
mit einem Gewichtungsfaktor,

46
00:02:27,770 --> 00:02:31,375
fügt einen Gewichtungsterm hinzu
und verwendet das Vorzeichen einer Summe,

47
00:02:31,375 --> 00:02:33,560
um die
Entscheidungsgrenze zu konstruieren.

48
00:02:33,560 --> 00:02:37,535
Sie können sich diese Konfiguration
also als lineares Modell vorstellen.

49
00:02:37,535 --> 00:02:40,880
Wir beginnen 
mit einem Modell, das versucht,

50
00:02:40,880 --> 00:02:44,725
Daten zu klassifizieren, die zu
zwei verschiedenen Clustern gehören.

51
00:02:44,725 --> 00:02:49,055
Klicken Sie auf die Schritt-Schaltfläche
rechts neben der Wiedergabe-Schaltfläche

52
00:02:49,055 --> 00:02:52,060
und achten Sie auf alle 
Änderungen an der Benutzeroberfläche.

53
00:02:52,060 --> 00:02:54,415
Die Anzahl der Abschnitte steigt um eins,

54
00:02:54,415 --> 00:02:57,620
die Linien für die Gewichtung
ändern ihre Farbe und Größe,

55
00:02:57,620 --> 00:03:00,285
der aktuelle Wert der 
Verlustfunktion ändert sich,

56
00:03:00,285 --> 00:03:02,780
die Verlustkurve zeigt eine Abwärtsneigung

57
00:03:02,780 --> 00:03:07,015
und die Entscheidungsgrenze 
der Ausgabe ändert sich ebenfalls.

58
00:03:07,015 --> 00:03:10,605
Bewegen Sie die Maus über 
die Linie, die Gewichtung 1 darstellt,

59
00:03:10,605 --> 00:03:14,280
und beachten Sie, dass Sie den
Wert dieser Gewichtung sehen können.

60
00:03:14,280 --> 00:03:17,905
Klicken Sie auf Wiedergabe-Schaltfläche,
um mit dem Training fortzufahren.

61
00:03:17,905 --> 00:03:21,595
Pausieren Sie jedoch kurz 
nachdem der Verlust unter 0,002 fällt,

62
00:03:21,595 --> 00:03:24,470
was weniger als 
200 Abschnitte dauern sollte.

63
00:03:24,470 --> 00:03:28,235
Glückwunsch, Sie haben 
gerade Ihr erstes Modell trainiert.

64
00:03:30,285 --> 00:03:32,980
Machen wir das Ganze jetzt komplexer.

65
00:03:32,980 --> 00:03:36,500
Betrachten wir zuerst,
wie drei verschiedene Lernraten

66
00:03:36,500 --> 00:03:38,720
das Modell im Training beeinflussen.

67
00:03:38,720 --> 00:03:41,700
Denken Sie daran, 
dass die Lernrate ein Hyperparameter ist,

68
00:03:41,700 --> 00:03:44,300
der festgelegt wird,
bevor das Modelltraining beginnt,

69
00:03:44,300 --> 00:03:47,200
und der mit der Ableitung
multipliziert wird, um zu bestimmen,

70
00:03:47,200 --> 00:03:50,815
wie stark wir die Gewichtung bei
jeder Iteration unserer Schleife ändern.

71
00:03:50,815 --> 00:03:56,100
Folgen Sie diesem Link, um ein Modell mit 
einer sehr kleinen Lernrate zu trainieren.

72
00:03:56,100 --> 00:03:59,195
Warten Sie, bis der Verlust 
ungefähr 100 Abschnitte erreicht,

73
00:03:59,195 --> 00:04:01,835
was schon nach etwa
zwei Sekunden geschehen sollte,

74
00:04:01,835 --> 00:04:04,495
und pausieren Sie dann das Modell.

75
00:04:05,975 --> 00:04:09,460
Wie entwickelt sich der aktuelle Verlust?

76
00:04:09,460 --> 00:04:13,490
Und welche Gewichtungen wurden gelernt?

77
00:04:15,820 --> 00:04:20,800
Erhöhen Sie jetzt die Lernrate auf
0,001, starten Sie das Training neu

78
00:04:20,800 --> 00:04:26,395
und stoppen Sie wieder
nach 100 Abschnitten. Wie ist der Verlust?

79
00:04:26,395 --> 00:04:29,900
Er sollte diesmal deutlich geringer sein.

80
00:04:29,900 --> 00:04:33,400
Beachten Sie auch
den Wert für Gewichtung 1.

81
00:04:36,150 --> 00:04:38,435
Erhöhen Sie nun die Lernrate auf 0,10,

82
00:04:38,435 --> 00:04:40,090
starten Sie das Modelltraining neu

83
00:04:40,090 --> 00:04:42,375
und trainieren Sie 
wieder 100 Abschnitte lang.

84
00:04:42,375 --> 00:04:45,495
Wie schnell ist die
Verlustkurve diesmal gefallen?

85
00:04:45,495 --> 00:04:48,280
Sie sollte sehr schnell gefallen sein.

86
00:04:48,280 --> 00:04:51,510
Lassen Sie uns diese Beobachtungen 
zusammennehmen und feststellen,

87
00:04:51,510 --> 00:04:55,370
ob wir sie damit erklären können, 
was wir über Optimierung gelernt haben.

88
00:04:55,370 --> 00:04:58,410
Erhöhen Sie nun die Lernrate auf 10,

89
00:04:58,410 --> 00:05:00,150
starten Sie das Modelltraining neu

90
00:05:00,150 --> 00:05:03,765
und machen Sie zuerst einen einzigen 
Schritt mit der Schritt-Schaltfläche.

91
00:05:03,765 --> 00:05:06,285
Beachten Sie den Umfang der Gewichtung.

92
00:05:06,285 --> 00:05:10,185
Trainieren Sie nun 100 Abschnitte lang.

93
00:05:10,185 --> 00:05:13,780
Wie schnell ist 
die Verlustkurve diesmal gefallen?

94
00:05:13,780 --> 00:05:16,920
Sie sollte sehr schnell gefallen sein.

95
00:05:16,920 --> 00:05:20,410
Lassen Sie uns diese Beobachtungen 
zusammennehmen und feststellen,

96
00:05:20,410 --> 00:05:23,965
ob wir sie damit erklären können,
was wir über Optimierung gelernt haben.

97
00:05:23,965 --> 00:05:27,800
Hier habe ich eine Tabelle mit dem 
Ergebnis meines Modelltrainings erstellt.

98
00:05:27,800 --> 00:05:31,230
Ihr Ergebnis sieht vielleicht 
etwas anders aus und das ist in Ordnung.

99
00:05:31,230 --> 00:05:34,150
Der Grund dafür, dass 
es anders aussehen kann, ist derselbe,

100
00:05:34,150 --> 00:05:38,110
aus dem das Experiment andere Ergebnisse 
erbringen würde, wenn Sie es neu starten.

101
00:05:38,110 --> 00:05:41,230
TensorFlow Playground 
initialisiert die Gewichtungen zufällig,

102
00:05:41,230 --> 00:05:45,530
was bedeutet, dass unsere Suche jedes
Mal an einer zufälligen Position beginnt.

103
00:05:45,530 --> 00:05:48,945
Lassen Sie uns über die 
Spalte "Weight1" sprechen.

104
00:05:48,945 --> 00:05:53,545
Beachten Sie, wie die Gewichtung
steigt, wenn die Lernrate steigt.

105
00:05:53,545 --> 00:05:56,265
Was denken Sie, warum das so ist?

106
00:05:56,265 --> 00:06:00,380
Das liegt daran, 
dass das Modell größere Schritte macht.

107
00:06:00,380 --> 00:06:02,790
Wenn die Lernrate 10 ist,

108
00:06:02,790 --> 00:06:06,035
ändert der erste Schritt 
die Gewichtung dramatisch.

109
00:06:06,035 --> 00:06:09,760
Lassen Sie uns über die
Spalte "Loss Over Time" sprechen.

110
00:06:09,760 --> 00:06:11,850
Mit steigender Lernrate

111
00:06:11,850 --> 00:06:13,745
wurde die Verlustkurve steiler.

112
00:06:13,745 --> 00:06:18,780
Das ist derselbe Effekt, den wir zuvor 
beobachtet haben, nur anders wahrgenommen.