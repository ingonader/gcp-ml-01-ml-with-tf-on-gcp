1
00:00:00,780 --> 00:00:03,465
¿Ven algo diferente
en este conjunto de datos?

2
00:00:03,965 --> 00:00:07,740
Hagan clic en el vínculo y comiencen
a entrenar el modelo en la nueva ventana.

3
00:00:08,130 --> 00:00:11,580
¿Qué ven sobre la pérdida
y el gráfico de pérdida con el tiempo?

4
00:00:12,440 --> 00:00:14,995
¿Ven alguna convergencia hacia cero?

5
00:00:16,615 --> 00:00:19,775
Si hicieron clic directamente
en el botón "Comenzar entrenamiento"

6
00:00:19,775 --> 00:00:21,665
deberían ver una salida como esta.

7
00:00:22,385 --> 00:00:26,875
Observen que el límite de decisión
no divide muy bien los datos por clase.

8
00:00:26,875 --> 00:00:28,905
¿Por qué ocurre esto?

9
00:00:29,745 --> 00:00:33,375
La razón es que los datos
tienen una relación no lineal

10
00:00:33,375 --> 00:00:37,320
es decir, no se puede dibujar una línea
recta que divida el naranja del azul.

11
00:00:37,960 --> 00:00:41,570
Lo que requieren estos datos
es un límite de decisión no lineal

12
00:00:41,570 --> 00:00:45,050
que en este caso reconocemos
como el círculo alrededor

13
00:00:45,050 --> 00:00:47,155
de los puntos de datos azules.

14
00:00:47,775 --> 00:00:49,735
Sin embargo, no todo está perdido.

15
00:00:50,165 --> 00:00:53,420
Si hacen clic en algunas
de las casillas de la columna de entrada

16
00:00:53,420 --> 00:00:57,010
verán si pueden introducir nuevos
atributos que mejorarían el rendimiento.

17
00:00:57,810 --> 00:01:00,520
Con suerte,
sus resultados deberían verse así

18
00:01:00,520 --> 00:01:03,830
porque seleccionaron
los atributos x1 y x2 al cuadrado.

19
00:01:04,560 --> 00:01:07,595
Observen lo circular
que es el límite de decisión ahora.

20
00:01:07,595 --> 00:01:12,020
¿Cómo es posible que un modelo lineal
aprenda un límite de decisión no lineal?

21
00:01:13,130 --> 00:01:16,200
Recuerden que los modelos
lineales aprenden un conjunto de pesos

22
00:01:16,200 --> 00:01:19,355
que multiplican por sus atributos
para realizar predicciones.

23
00:01:19,675 --> 00:01:22,645
Cuando esos atributos
son términos de primer grado, como X y Y

24
00:01:22,645 --> 00:01:26,870
el resultado es un polinomio
de primer grado, como 2x o (2/3)y.

25
00:01:27,455 --> 00:01:30,755
Las predicciones del modelo
se ven como una línea o un hiperplano.

26
00:01:31,245 --> 00:01:34,520
Pero no hay una regla que diga
que los atributos de un modelo lineal

27
00:01:34,520 --> 00:01:36,595
deben ser términos de primer grado.

28
00:01:36,595 --> 00:01:39,385
Así como pueden
tomar X al cuadrado y multiplicarlo por 2

29
00:01:39,385 --> 00:01:42,375
también pueden tomar un atributo
de cualquier grado

30
00:01:42,375 --> 00:01:45,585
y aprender un peso
para él en un modelo lineal.

31
00:01:46,405 --> 00:01:49,660
Veamos hasta dónde
podemos llevar esta nueva idea.

32
00:01:51,310 --> 00:01:53,300
¿Qué opinan de esta curva?

33
00:01:53,680 --> 00:01:55,430
La última vez pudimos encontrar

34
00:01:55,430 --> 00:01:58,970
dos atributos no lineales que
le dieron una solución lineal al problema.

35
00:01:59,300 --> 00:02:02,825
¿Funcionará aquí esa estrategia? Probemos.

36
00:02:03,925 --> 00:02:07,820
Ahora saben que, si usamos
los atributos que tenemos disponibles

37
00:02:07,820 --> 00:02:12,305
y este tipo de modelo, el conjunto
de datos no tiene una solución lineal.

38
00:02:12,875 --> 00:02:16,810
El mejor modelo que pude
entrenar tenía una pérdida cercana al .6.

39
00:02:17,190 --> 00:02:22,325
Aun así, el calificador de opciones
de atributos que esté disponible es vital

40
00:02:22,325 --> 00:02:25,955
porque hay un atributo que haría
más fácil el aprendizaje de esta relación.

41
00:02:26,785 --> 00:02:30,825
Por ejemplo, imaginen un
atributo que quite el espiral a los datos

42
00:02:30,825 --> 00:02:35,025
de modo que el azul y el naranja
aparezcan solo como dos líneas paralelas.

43
00:02:35,455 --> 00:02:39,750
Estas líneas paralelas se podrían
separar fácilmente con una tercera línea.

44
00:02:41,100 --> 00:02:44,050
Encontrar atributos importantes es mágico

45
00:02:44,050 --> 00:02:47,225
pero también difícil de anticipar,
lo que puede ser problemático.

46
00:02:47,995 --> 00:02:50,540
Sin embargo,
aunque no siempre encontremos atributos

47
00:02:50,540 --> 00:02:53,505
tan interesantes como
los que vimos en nuestros ejemplos

48
00:02:53,505 --> 00:02:56,560
la ingeniería de atributos
o la mejora sistemática

49
00:02:56,560 --> 00:03:00,500
y la adquisición de nuevos atributos
es una parte muy importante del AA

50
00:03:00,500 --> 00:03:03,235
y en eso nos enfocaremos en el curso Tres.

51
00:03:03,795 --> 00:03:08,270
¿Y si no logramos diseñar
nuevos atributos para modelos lineales?

52
00:03:08,660 --> 00:03:11,415
La respuesta es
usar modelos más complicados.

53
00:03:11,985 --> 00:03:16,670
Hay muchos modelos que pueden
aprender límites de decisión no lineales.

54
00:03:17,060 --> 00:03:20,060
En este curso,
nos enfocaremos en las redes neuronales.

55
00:03:20,520 --> 00:03:24,090
Las redes neuronales
no son mejores que otros tipos de modelos.

56
00:03:24,090 --> 00:03:27,180
Son cada vez más populares
porque los problemas de negocios

57
00:03:27,180 --> 00:03:31,420
de hoy se inclinan hacia aquellos
donde las redes tienen éxito.