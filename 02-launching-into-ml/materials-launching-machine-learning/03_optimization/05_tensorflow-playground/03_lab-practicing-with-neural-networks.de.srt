1
00:00:00,000 --> 00:00:04,145
Wir haben gesehen, wie ein lineares
Modell mit diesem Dataset arbeitet.

2
00:00:04,145 --> 00:00:07,160
Sehen wir uns an, 
wie ein neuronales Netzwerk funktioniert.

3
00:00:07,160 --> 00:00:10,530
Bevor wir das tun, müssen wir
einige zusätzliche Funktionen überprüfen,

4
00:00:10,530 --> 00:00:13,095
die ich in 
TensorFlow Playground freigeschaltet habe.

5
00:00:13,095 --> 00:00:15,620
Die erste ist die Aktivierung

6
00:00:15,620 --> 00:00:18,610
und bezieht sich
auf die Aktivierungsfunktion.

7
00:00:18,610 --> 00:00:20,495
Wir werden sie ausführlicher in Kurs 5,

8
00:00:20,495 --> 00:00:23,085
mit dem Thema "Art and
Science of ML", behandeln.

9
00:00:23,085 --> 00:00:26,480
Momentan ist allein entscheidend,
dass die Wahl der Aktivierungsfunktion

10
00:00:26,480 --> 00:00:29,155
lineare Modelle von 
neuronalen Netzwerken unterscheidet.

11
00:00:29,155 --> 00:00:31,785
Die Aktivierungsfunktion
wurde zuvor, ohne Ihr Wissen,

12
00:00:31,785 --> 00:00:34,370
als lineare Funktion festgelegt.

13
00:00:34,370 --> 00:00:38,655
Die zweite, neu freigeschaltete
Funktion, ist "verborgene Ebenen".

14
00:00:38,655 --> 00:00:42,460
Mit der Funktion "verborgene Ebenen" 
können Sie die Zahl solcher Ebenen

15
00:00:42,460 --> 00:00:45,550
und die Zahl der Neuronen 
pro verborgener Ebene ändern.

16
00:00:45,550 --> 00:00:48,260
Sie ändern praktisch
die Anzahl der Transformationen,

17
00:00:48,260 --> 00:00:51,185
die das Netzwerk für 
Ihre Daten durchführt.

18
00:00:51,185 --> 00:00:53,685
Jedes Neuron in jeder verborgenen Ebene

19
00:00:53,685 --> 00:00:56,570
empfängt die gesamte
Ausgabe von der vorhergehenden Ebene,

20
00:00:56,570 --> 00:01:01,860
transformiert sie und übergibt die Ausgabe
an alle Neuronen der folgenden Ebene.

21
00:01:01,860 --> 00:01:05,310
Die Kurzbeschreibung 
der Anzahl von Neuronen und wie sie

22
00:01:05,310 --> 00:01:09,285
Informationen untereinander 
weitergeben, ist die Netzwerkarchitektur.

23
00:01:09,285 --> 00:01:11,630
Ich habe auch
"Batch-Größe" freigeschaltet,

24
00:01:11,630 --> 00:01:15,010
die wir gleich in einem
Experiment verwenden werden.

25
00:01:15,010 --> 00:01:18,350
Folgen Sie dem Link auf der
Folie und trainieren Sie ein Modell so,

26
00:01:18,350 --> 00:01:20,570
dass es dieses
Dataset klassifizieren kann.

27
00:01:20,570 --> 00:01:23,680
Statt nichtlineare Funktionen
einzuführen, versuchen Sie,

28
00:01:23,680 --> 00:01:27,995
die Leistung nur durch Ändern
der Netzwerkarchitektur zu verbessern.

29
00:01:27,995 --> 00:01:32,550
Es ist OK, dass wir die Funktionsweise
neuronaler Netze noch nicht erklärt haben.

30
00:01:32,550 --> 00:01:35,180
Experimentieren Sie einfach
mit der Benutzeroberfläche,

31
00:01:35,180 --> 00:01:38,320
bis Sie ein Netzwerk haben,
das einigermaßen gut funktioniert.

32
00:01:41,400 --> 00:01:45,530
Sie sollten jetzt ein Modell haben,
das einigermaßen gut funktioniert,

33
00:01:45,530 --> 00:01:49,905
und der blaue Bereich in der
Ausgabespalte sollte ein Polygon sein.

34
00:01:49,905 --> 00:01:54,115
Sehen wir uns das genauer an, um 
ein Gespür für die Funktionsweise

35
00:01:54,115 --> 00:01:55,585
des Modells zu bekommen.

36
00:01:55,585 --> 00:01:59,615
Sehen Sie sich die Neuronen der ersten 
verborgenen Ebene noch einmal an.

37
00:01:59,615 --> 00:02:01,665
Während ich mit der Maus über jedes fahre,

38
00:02:01,665 --> 00:02:05,050
ändert sich das Ausgabefeld 
je nachdem, was das Neuron gelernt hat.

39
00:02:05,050 --> 00:02:09,225
Sie können diese Neuronen genauso
wie Merkmale und die Ausgabe lesen.

40
00:02:09,225 --> 00:02:14,200
Die Werte der Merkmale X1 und X2
sind in der Position im Rechteck codiert.

41
00:02:14,200 --> 00:02:16,820
Die Farbe gibt
den Wert an, den dieses Neuron

42
00:02:16,820 --> 00:02:20,335
für diese Kombination
von X1 und X2 ausgibt.

43
00:02:20,335 --> 00:02:23,725
Während ich mit der Maus 
über jedes der Quadrate fahre,

44
00:02:23,725 --> 00:02:27,985
stellen Sie sich vor,
wie sie übereinander aussehen würden.

45
00:02:27,985 --> 00:02:30,860
Blau auf Blau wird ein dunkleres Blau,

46
00:02:30,860 --> 00:02:33,465
Blau auf Weiß wird Hellblau

47
00:02:33,465 --> 00:02:36,880
und Blau auf Orange wird weiß.

48
00:02:36,880 --> 00:02:40,180
Sie sollten nach und nach 
erkennen können, wie jedes Neuron

49
00:02:40,180 --> 00:02:42,695
an der Entscheidungsgrenze
des Modells beteiligt ist,

50
00:02:42,695 --> 00:02:46,155
wie die Form der Ausgabe eine
Funktion der verborgenen Schichten ist.

51
00:02:46,155 --> 00:02:50,625
Zum Beispiel trägt dieses Neuron 
diese Kante zur Entscheidungsgrenze bei,

52
00:02:50,625 --> 00:02:53,995
und dieses Neuron trägt diese Kante bei.

53
00:02:53,995 --> 00:02:57,580
Was sagen Ihnen Ihre Geometriekenntnisse:

54
00:02:57,580 --> 00:02:59,740
Wie klein könnten Sie
dieses Netzwerk aufbauen

55
00:02:59,740 --> 00:03:03,075
und dennoch eine 
ordentliche Leistung erzielen?

56
00:03:03,075 --> 00:03:05,220
Kleiner Tipp: Welches
ist die einfachste Form,

57
00:03:05,220 --> 00:03:09,035
die Sie um die blauen Punkte zeichnen
könnten, die trotzdem funktionieren würde?

58
00:03:09,035 --> 00:03:13,545
Testen Sie es in TensorFlow Playground
und prüfen Sie, ob Ihre Idee stimmt.

59
00:03:13,545 --> 00:03:17,720
Sie haben gesehen, wie die Ausgabe
der Neuronen in der ersten verborgenen

60
00:03:17,720 --> 00:03:21,465
Netzwerkebene verwendet werden kann,
um die Entscheidungsgrenze zu bilden.

61
00:03:21,465 --> 00:03:23,565
Was ist mit diesen anderen Ebenen?

62
00:03:23,565 --> 00:03:26,270
Wie unterscheidet sich ein 
neuronales Netzwerk mit nur

63
00:03:26,270 --> 00:03:28,680
einer verborgenen
Ebene von einem mit vielen?

64
00:03:28,680 --> 00:03:31,150
Klicken Sie auf den Link,
um mit dem Training eines

65
00:03:31,150 --> 00:03:34,840
neuronalen Netzwerks zu beginnen
und dieses Spiral-Dataset zu klassifizieren.

66
00:03:34,840 --> 00:03:37,450
Nutzen wir diese Gelegenheit,
um mehr darüber zu erfahren,

67
00:03:37,450 --> 00:03:40,320
wie sich die Batch-Größe
auf den Gradientenabstieg auswirkt.

68
00:03:40,320 --> 00:03:43,860
Wählen Sie "1" als Parameter für die 
Batch-Größe und experimentieren Sie

69
00:03:43,860 --> 00:03:47,805
mit neuronalen Netzarchitekturen, bis
Sie eine funktionierende gefunden haben.

70
00:03:47,805 --> 00:03:52,150
Trainieren Sie nun Ihr Modell
für ca. 300 Schritte, pausieren Sie

71
00:03:52,150 --> 00:03:54,130
und achten Sie auf die Verlustkurve.

72
00:03:54,130 --> 00:03:57,425
Wählen Sie nun "10"
als Parameter für die Batch-Größe

73
00:03:57,425 --> 00:03:59,005
und starten Sie das Training neu.

74
00:03:59,005 --> 00:04:02,690
Trainieren Sie nun Ihr Modell
für ca. 300 Schritte, pausieren Sie

75
00:04:02,690 --> 00:04:06,720
und achten Sie wieder 
auf die Verlustkurve.

76
00:04:06,720 --> 00:04:11,555
Wiederholen Sie alles noch
einmal mit der Batch-Größe 30.

77
00:04:11,555 --> 00:04:14,050
Was haben Sie beobachtet?

78
00:04:14,050 --> 00:04:17,399
Wie können wir diese Beobachtungen 
mit dem, was wir wissen, verstehen?

79
00:04:17,399 --> 00:04:20,170
Sie sollten gesehen haben,
dass es deutliche Unterschiede

80
00:04:20,170 --> 00:04:23,530
in der Glätte der Verlustkurven gibt.

81
00:04:23,530 --> 00:04:25,590
Mit zunehmender Batch-Größe

82
00:04:25,590 --> 00:04:29,135
nahm auch die Glätte zu. 
Woran könnte das liegen?

83
00:04:29,135 --> 00:04:32,520
Bedenken Sie, wie sich die Batch-Größe
auf den Gradientenabstieg auswirkt.

84
00:04:32,520 --> 00:04:34,110
Wenn die Batch-Größe gering ist,

85
00:04:34,110 --> 00:04:36,600
führt das Modell eine
Aktualisierung seiner Parameter

86
00:04:36,600 --> 00:04:39,625
auf der Basis des Verlusts
aus einem einzelnen Beispiel durch.

87
00:04:39,625 --> 00:04:43,465
Beispiele variieren jedoch,
und darin liegt das Problem.

88
00:04:43,465 --> 00:04:45,760
Ist die Batch-Größe jedoch höher,

89
00:04:45,760 --> 00:04:48,555
setzt sich das Rauschen 
einzelner Datenpunkte ab

90
00:04:48,555 --> 00:04:51,195
und ein klares Signal wird erkennbar.

91
00:04:51,195 --> 00:04:55,160
Sie sollten aus diesen Beobachtungen
aber nicht schließen, dass Änderungen

92
00:04:55,160 --> 00:04:59,865
der Batch-Größe eine einfache
Wirkung auf die Konvergenzrate haben.

93
00:04:59,865 --> 00:05:03,120
Wie bei der Lernrate
ist die optimale Batch-Größe

94
00:05:03,120 --> 00:05:07,845
problemabhängig und kann durch
Hyperparameter-Tuning gefunden werden.

95
00:05:09,375 --> 00:05:13,695
Jetzt sollte das Modell das Training 
beendet haben und ungefähr so aussehen.

96
00:05:13,695 --> 00:05:16,110
Die erste Sache, die genannt werden muss,

97
00:05:16,110 --> 00:05:19,905
ist die Beziehung zwischen der ersten 
verborgenen Ebene und den folgenden.

98
00:05:19,905 --> 00:05:23,260
Es sollte offensichtlich sein, 
dass die Ausgabe der Neuronen

99
00:05:23,260 --> 00:05:26,180
in der ersten verborgenen 
Ebene im Grunde Linien darstellt.

100
00:05:26,180 --> 00:05:30,075
Nachfolgende verborgene Ebenen
haben eine viel kompliziertere Ausgabe.

101
00:05:30,075 --> 00:05:34,050
Diese nachfolgenden Ebenen
bauen auf den vorhergehenden genauso auf

102
00:05:34,050 --> 00:05:38,125
wie beim Stapeln der 
Ausgabe der verborgenen Ebene.

103
00:05:38,125 --> 00:05:41,170
Also können Sie sich ein
neuronales Netzwerk als eine

104
00:05:41,170 --> 00:05:43,540
Hierarchie von Merkmalen vorstellen.

105
00:05:43,540 --> 00:05:47,960
Diese Idee, Eingaben zu nehmen, die
dann auf komplexe Weise transformiert

106
00:05:47,960 --> 00:05:50,810
und schließlich klassifiziert werden,

107
00:05:50,810 --> 00:05:53,900
ist typisch für neuronale Netze
und stellt eine deutliche Abkehr

108
00:05:53,900 --> 00:05:57,255
vom herkömmlichen Ansatz 
beim maschinellen Lernen dar.

109
00:05:57,255 --> 00:06:02,690
Vor neuronalen Netzwerken verbrachte man
viel mehr Zeit mit Feature Engineering.

110
00:06:02,690 --> 00:06:06,910
Jetzt übernimmt das
Modell selbst einen Teil davon.

111
00:06:06,910 --> 00:06:11,655
Ebenen sind praktisch eine Form
des selbstbezogenen Feature Engineering.

112
00:06:11,655 --> 00:06:16,375
Das nächste Wichtige sind einige seltsame
Dinge, die das Modell gelernt hat.

113
00:06:16,375 --> 00:06:20,020
Das Modell scheint 
das Fehlen oranger Punkte

114
00:06:20,020 --> 00:06:23,545
in diesen beiden Regionen als Beweis
für ihre "Blauheit" zu interpretieren.

115
00:06:23,545 --> 00:06:26,940
Wir nennen Fehler, 
bei denen das Modell Rauschen im Dataset

116
00:06:26,940 --> 00:06:29,725
als signifikant
interpretiert hat, Überanpassung.

117
00:06:29,725 --> 00:06:32,830
Diese kann auftreten, wenn
das Modell mehr Entscheidungskraft hat,

118
00:06:32,830 --> 00:06:35,215
als für das Problem erforderlich ist.

119
00:06:35,215 --> 00:06:38,135
Modelle mit Überanpassungen 
verallgemeinern schlecht,

120
00:06:38,135 --> 00:06:41,370
was eine schlechte Leistung bei
neuen Daten bedeutet, weil diese kaum

121
00:06:41,370 --> 00:06:42,740
dasselbe Rauschmuster haben,

122
00:06:42,740 --> 00:06:46,235
obwohl das zugrunde
liegende Signal gleich bleiben sollte.

123
00:06:46,235 --> 00:06:48,145
Was tun wir dagegen?

124
00:06:48,145 --> 00:06:50,290
Das erfahren Sie im nächsten Kurs

125
00:06:50,290 --> 00:06:53,000
über Generalisierung und Stichproben.