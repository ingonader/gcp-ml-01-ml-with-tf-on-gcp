1
00:00:00,170 --> 00:00:03,050
Nous avons déjà observé les performances
d'un modèle linéaire

2
00:00:03,050 --> 00:00:04,495
pour cet ensemble de données.

3
00:00:04,495 --> 00:00:07,210
Voyons ce qu'il en est
avec un réseau de neurones.

4
00:00:07,400 --> 00:00:09,920
Nous devons d'abord examiner
certaines fonctionnalités

5
00:00:09,920 --> 00:00:13,525
que j'ai ajoutées dans TensorFlow Playground.

6
00:00:13,525 --> 00:00:18,100
La première est l'activation.
J'entends par là la fonction d'activation.

7
00:00:18,100 --> 00:00:20,875
J'en parlerai de manière plus approfondie
dans le cours 5,

8
00:00:20,875 --> 00:00:22,925
qui sera consacré aux techniques de ML.

9
00:00:22,925 --> 00:00:26,940
Pour l'instant, il est crucial de comprendre
que le choix de la fonction d'activation

10
00:00:26,940 --> 00:00:30,085
est ce qui distingue les modèles linéaires
des réseaux de neurones.

11
00:00:30,085 --> 00:00:31,395
Précédemment, à votre insu,

12
00:00:31,395 --> 00:00:35,470
la fonction d'activation était paramétrée
de façon à être linéaire.

13
00:00:35,470 --> 00:00:39,225
La deuxième fonctionnalité que j'ai ajoutée
est celle des couches cachées.

14
00:00:39,225 --> 00:00:42,530
Elle vous permet de modifier
le nombre de couches cachées,

15
00:00:42,530 --> 00:00:46,240
ainsi que le nombre de neurones
de chaque couche cachée.

16
00:00:46,240 --> 00:00:49,020
Voyez cela comme un changement
du nombre de transformations

17
00:00:49,020 --> 00:00:51,785
de vos données effectuées par le réseau.

18
00:00:51,785 --> 00:00:53,655
Chaque neurone de chaque couche cachée

19
00:00:53,655 --> 00:00:57,180
reçoit toutes les sorties de la couche
précédente, transforme ces entrées,

20
00:00:57,180 --> 00:01:00,795
et transmet des sorties
à tous les neurones de la couche suivante.

21
00:01:01,580 --> 00:01:04,850
La méthode la plus rapide permettant
d'indiquer le nombre de neurones

22
00:01:04,850 --> 00:01:07,287
et de décrire comment ils échangent
des informations

23
00:01:07,287 --> 00:01:09,555
consiste à présenter
l'architecture du réseau.

24
00:01:09,555 --> 00:01:13,000
J'ai aussi activé la fonctionnalité
de paramétrage de la taille des lots,

25
00:01:13,000 --> 00:01:16,240
que nous utiliserons dans l'une
de nos prochaines expériences.

26
00:01:16,240 --> 00:01:19,585
Cliquez sur le lien de la diapositive,
et essayez d'entraîner un modèle

27
00:01:19,585 --> 00:01:21,620
qui classifie cet ensemble de données.

28
00:01:21,620 --> 00:01:25,210
Plutôt que d'ajouter des caractéristiques
non linéaires, essayez d'améliorer

29
00:01:25,210 --> 00:01:28,385
les performances en modifiant 
simplement l'architecture du réseau.

30
00:01:28,385 --> 00:01:32,562
Je sais que nous n'avons pas encore étudié 
le fonctionnement d'un réseau de neurones.

31
00:01:32,562 --> 00:01:34,950
Pour le moment, familiarisez-vous
avec l'interface

32
00:01:34,950 --> 00:01:40,530
jusqu'à ce que les performances du réseau
soient satisfaisantes.

33
00:01:42,380 --> 00:01:45,740
Vous devriez alors disposer d'un modèle
qui fonctionne correctement,

34
00:01:45,740 --> 00:01:48,327
et la forme de la zone bleue
de la colonne de la sortie

35
00:01:48,327 --> 00:01:50,545
devrait être un polygone.

36
00:01:50,545 --> 00:01:54,645
Approfondissons un peu pour comprendre
comment le modèle fonctionne.

37
00:01:56,905 --> 00:01:59,935
Examinons à nouveau les neurones
de la première couche cachée.

38
00:01:59,935 --> 00:02:02,385
Lorsque je passe la souris sur
l'un d'entre eux,

39
00:02:02,385 --> 00:02:05,820
le contenu de la colonne de la sortie
indique ce que le neurone a appris.

40
00:02:05,820 --> 00:02:09,647
Vous pouvez lire ces neurones comme
vous lisez les caractéristiques et la sortie.

41
00:02:09,647 --> 00:02:13,142
Les valeurs des caractéristiques X1 et X2
sont représentées par les motifs

42
00:02:13,142 --> 00:02:15,810
visibles dans les carrés,
et la couleur indique la valeur

43
00:02:15,810 --> 00:02:18,970
de la sortie de chaque neurone
pour cette combinaison de X1 et X2.

44
00:02:20,325 --> 00:02:23,835
Pendant que je passe la souris
sur les différents carrés,

45
00:02:23,835 --> 00:02:28,365
commencez à imaginer le résultat
que donnerait leur superposition.

46
00:02:28,365 --> 00:02:31,290
Du bleu sur du bleu donne du bleu plus foncé,

47
00:02:31,290 --> 00:02:33,745
du bleu sur du blanc donne du bleu clair,

48
00:02:33,745 --> 00:02:36,890
et du bleu sur de l'orange donne du blanc.

49
00:02:36,890 --> 00:02:40,190
Vous devriez commencer à voir
comment chaque neurone contribue

50
00:02:40,190 --> 00:02:42,775
à l'élaboration de la frontière
de décision du modèle,

51
00:02:42,775 --> 00:02:46,675
et en quoi la forme de la sortie
est une fonction des couches cachées.

52
00:02:46,675 --> 00:02:50,725
Par exemple, la contribution
de ce neurone est cette limite,

53
00:02:50,725 --> 00:02:54,005
et celle de ce neurone est cette limite.

54
00:02:54,005 --> 00:02:57,060
Compte tenu de vos connaissances
en géométrie, jusqu'à quel point

55
00:02:57,060 --> 00:03:00,790
pensez-vous pouvoir réduire la taille
de ce réseau tout en continuant à obtenir

56
00:03:00,790 --> 00:03:02,655
des performances acceptables ?

57
00:03:02,655 --> 00:03:05,990
Juste un conseil : demandez-vous
quelle serait la forme la plus simple

58
00:03:05,990 --> 00:03:09,637
et la plus efficace que vous pourriez tracer
autour des points bleus ?

59
00:03:09,637 --> 00:03:14,085
Faites un test dans TensorFlow Playground
pour voir si votre intuition est correcte.

60
00:03:14,085 --> 00:03:18,122
Vous avez vu comment les sorties des neurones
de la première couche cachée du réseau

61
00:03:18,122 --> 00:03:21,490
peuvent être utilisées pour composer
la frontière de décision.

62
00:03:21,490 --> 00:03:23,565
Mais qu'en est-il des autres couches ?

63
00:03:23,565 --> 00:03:26,290
En quoi un réseau de neurones
comportant une couche cachée

64
00:03:26,290 --> 00:03:29,630
diffère-t-il d'un réseau
qui en contient beaucoup ?

65
00:03:29,630 --> 00:03:32,790
Cliquez sur le lien ci-dessous pour
entraîner un réseau de neurones

66
00:03:32,790 --> 00:03:35,490
à classifier cet ensemble de données
en forme de spirale.

67
00:03:35,490 --> 00:03:37,425
Profitons-en pour mieux comprendre

68
00:03:37,425 --> 00:03:40,760
comment la taille de lot
affecte la descente de gradient.

69
00:03:40,760 --> 00:03:42,557
Sélectionnez "1" comme taille de lot,

70
00:03:42,557 --> 00:03:45,115
puis testez des architectures
de réseau de neurones

71
00:03:45,115 --> 00:03:47,910
jusqu'à ce que vous en trouviez
une qui semble fonctionner.

72
00:03:47,910 --> 00:03:51,387
Entraînez le modèle pendant 300 itérations,
mettez le traitement en pause,

73
00:03:51,387 --> 00:03:54,125
et regardez la courbe de perte.

74
00:03:54,125 --> 00:03:59,570
Sélectionnez "10" comme taille de lot,
puis relancez l'entraînement.

75
00:03:59,570 --> 00:04:03,064
Entraînez le modèle pendant 300 itérations,
mettez le traitement en pause,

76
00:04:03,064 --> 00:04:05,599
et regardez à nouveau la courbe de perte.

77
00:04:05,599 --> 00:04:11,560
Enfin, répétez encore l'opération
en sélectionnant "30" comme taille de lot.

78
00:04:11,560 --> 00:04:13,710
Qu'avez-vous observé ?

79
00:04:13,710 --> 00:04:17,650
Comment l'expliquez-vous
compte tenu de ce que nous savons ?

80
00:04:17,650 --> 00:04:21,085
Vous devriez avoir vu
des différences marquées

81
00:04:21,085 --> 00:04:24,210
au niveau de la douceur
de la courbe de perte.

82
00:04:24,210 --> 00:04:27,910
Plus la taille de lot augmente,
plus la douceur s'accentue.

83
00:04:27,910 --> 00:04:29,360
Pourquoi ?

84
00:04:29,360 --> 00:04:32,605
Pensez à l'effet de la taille de lot
sur la descente de gradient.

85
00:04:32,605 --> 00:04:34,535
Lorsque cette taille est peu importante,

86
00:04:34,535 --> 00:04:39,465
le modèle met ses paramètres à jour
sur la base de la perte d'un seul exemple.

87
00:04:39,465 --> 00:04:43,480
Cependant, les exemples varient,
et c'est là qu'est le problème.

88
00:04:43,480 --> 00:04:45,965
Mais avec l'augmentation de la taille de lot,

89
00:04:45,965 --> 00:04:48,602
le bruit lié aux différents points
de données s'estompe,

90
00:04:48,602 --> 00:04:52,100
et un signal clair commence à prendre forme.

91
00:04:52,100 --> 00:04:55,905
N'en concluez pas que les modifications
de la taille de lot ont un effet simple

92
00:04:55,905 --> 00:04:59,885
sur le taux de convergence.

93
00:04:59,885 --> 00:05:03,960
Tout comme pour le taux d'apprentissage,
la taille de lot optimale dépend du problème,

94
00:05:03,960 --> 00:05:07,835
et elle peut être trouvée à l'aide
du réglage d'hyperparamètres.

95
00:05:07,835 --> 00:05:11,427
Votre modèle devrait maintenant
avoir fini son entraînement,

96
00:05:11,427 --> 00:05:14,470
et devrait plus ou moins ressembler à ça.

97
00:05:14,470 --> 00:05:17,380
Le premier élément marquant est la relation

98
00:05:17,380 --> 00:05:19,870
entre la première couche cachée
et les suivantes.

99
00:05:19,870 --> 00:05:24,850
Alors que les sorties des neurones
de cette première couche

100
00:05:24,850 --> 00:05:26,415
sont essentiellement des lignes,

101
00:05:26,415 --> 00:05:30,090
les sorties des couches suivantes
sont beaucoup plus complexes.

102
00:05:30,090 --> 00:05:33,640
Ces nouvelles couches se basent
sur celles produites en amont,

103
00:05:33,640 --> 00:05:36,720
de la même manière que lorsque
nous avons superposé les sorties

104
00:05:36,720 --> 00:05:38,160
de la première couche cachée.

105
00:05:38,160 --> 00:05:43,690
Un réseau de neurones peut donc être vu
comme une hiérarchie de caractéristiques.

106
00:05:43,690 --> 00:05:47,755
Cette idée d'exploiter des entrées
en les transformant

107
00:05:47,755 --> 00:05:50,810
à l'aide de traitements complexes
pour finalement les classifier

108
00:05:50,810 --> 00:05:53,220
est vraiment propre
aux réseaux de neurones.

109
00:05:53,220 --> 00:05:57,875
Elle diffère grandement de l'approche
utilisée habituellement en machine learning.

110
00:05:57,875 --> 00:05:59,315
Avant les réseaux de neurones,

111
00:05:59,315 --> 00:06:01,905
les data scientists consacraient
beaucoup plus de temps

112
00:06:01,905 --> 00:06:03,585
à l'extraction de caractéristiques.

113
00:06:03,585 --> 00:06:06,720
Désormais, le modèle se charge
d'une partie de ce travail,

114
00:06:06,720 --> 00:06:10,432
et l'on peut voir les couches comme
une forme d'extraction de caractéristiques

115
00:06:10,432 --> 00:06:12,575
effectuée sur elles-mêmes.

116
00:06:12,575 --> 00:06:17,120
Le deuxième élément marquant est
que le modèle a appris des choses étranges.

117
00:06:17,120 --> 00:06:20,875
Il semble avoir interprété l'absence
de points orange dans ces deux zones

118
00:06:20,875 --> 00:06:23,980
comme la preuve
qu'elles devaient être bleues.

119
00:06:23,980 --> 00:06:26,645
Nous appelons "surapprentissage"
les erreurs de ce type,

120
00:06:26,645 --> 00:06:28,845
qui résultent de l'interprétation
par le modèle

121
00:06:28,845 --> 00:06:30,895
du bruit présent dans l'ensemble
de données.

122
00:06:30,895 --> 00:06:34,040
Elles se produisent lorsque le modèle
a plus de pouvoir décisionnel

123
00:06:34,040 --> 00:06:35,975
qu'il n'est nécessaire pour le problème.

124
00:06:35,975 --> 00:06:39,460
Lorsque des modèles surapprennent,
ils généralisent de manière incorrecte.

125
00:06:42,705 --> 00:06:46,223
qui sont peu susceptibles
d'avoir exactement le même motif de bruit,

126
00:06:46,223 --> 00:06:48,618
même si le signal sous-jacent
est toujours présent.

127
00:06:39,460 --> 00:06:42,705
Leurs performances sont donc
médiocres pour les nouvelles données,

128
00:06:48,618 --> 00:06:49,578
Comment y remédier ?

129
00:06:49,578 --> 00:06:53,508
Nous le verrons dans le prochain cours
sur la généralisation et l’échantillonnage.