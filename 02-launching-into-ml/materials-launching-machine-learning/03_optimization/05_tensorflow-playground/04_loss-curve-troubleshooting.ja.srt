1
00:00:00,000 --> 00:00:03,974
さまざまなNNアーキテクチャで
モデルにトレーニングさせた中で

2
00:00:03,974 --> 00:00:08,235
このような悲惨な状態になった人も
いるのではないでしょうか

3
00:00:08,235 --> 00:00:11,520
損失曲線と出力の両方に注目してください

4
00:00:11,520 --> 00:00:15,275
どのように修正しましたか
また 何が起こっているのでしょうか

5
00:00:15,275 --> 00:00:18,535
ネットワークアーキテクチャを
変更したかもしれませんが

6
00:00:18,535 --> 00:00:22,400
多くの場合 このような問題はモデルに
再トレーニングさせることで修正できます

7
00:00:22,400 --> 00:00:26,725
モデルのトレーニングプロセスには
Weight Initializerの乱数の種など

8
00:00:26,725 --> 00:00:29,935
制御されていない部分があるからです

9
00:00:29,935 --> 00:00:31,750
この場合 問題は

10
00:00:31,750 --> 00:00:33,860
損失曲面のある位置で

11
00:00:33,860 --> 00:00:37,010
隣接する点と比べて小さい値を見つけたものの

12
00:00:37,010 --> 00:00:39,415
それがゼロよりはるかに大きいことです

13
00:00:39,415 --> 00:00:42,350
つまり 見つけたのは極小値です

14
00:00:42,350 --> 00:00:45,320
経時的な損失の変化のグラフを見ると

15
00:00:45,320 --> 00:00:49,640
探索の早い段階で
より低い損失値に到達しています

16
00:00:50,360 --> 00:00:54,350
準最適な極小値が存在し
それに誘因されることは

17
00:00:54,350 --> 00:00:57,835
現在の手法が抱える欠点です

18
00:00:57,835 --> 00:01:00,190
その他の欠点として

19
00:01:00,190 --> 00:01:04,345
長い学習時間と 些末ながら不適切な
最小値の存在などがあります

20
00:01:04,345 --> 00:01:08,335
これらの問題は 1つの原因によって
引き起こされるものではないため

21
00:01:08,335 --> 00:01:10,305
対応方法もさまざまです

22
00:01:10,305 --> 00:01:12,880
高度な最適化手法では

23
00:01:12,880 --> 00:01:16,860
学習時間を短縮し モデルが
極小値に誘因されることを防ぎます

24
00:01:16,860 --> 00:01:20,415
こうした手法の一部は
後ほどこのコースで取り上げます

25
00:01:20,415 --> 00:01:24,930
データの重み付けとオーバーサンプリング
および合成データの作成では

26
00:01:24,930 --> 00:01:29,015
探索領域から不適切な
最小値を完全に除外します

27
00:01:29,015 --> 00:01:32,250
次のセクションで取り上げる
パフォーマンス指標は

28
00:01:32,250 --> 00:01:34,960
より俯瞰的な視点で問題に対処します

29
00:01:34,960 --> 00:01:38,645
探索の方法や探索領域
そのものを変えるのではなく

30
00:01:38,645 --> 00:01:43,240
探索の結果を実際に知りたいことと
より密接にすり合わせることで

31
00:01:43,240 --> 00:01:45,875
結果のとらえ方を変えます

32
00:01:45,875 --> 00:01:51,000
これにより 次の探索のタイミングを
十分な情報を得た上で決めることができます