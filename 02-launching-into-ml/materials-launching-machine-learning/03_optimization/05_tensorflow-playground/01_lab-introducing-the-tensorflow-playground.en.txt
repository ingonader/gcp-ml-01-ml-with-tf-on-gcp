So, now we've seen how gradient descent works. Let's see it in action using the tool that will allow us to see in real time, many of the phenomena that we've discussed. TensorFlow Playground is a powerful tool for visualizing how neural networks work. Now you might be saying, "Hey wait a moment, we haven't actually introduced neural networks yet." Don't worry, that's coming shortly. For reasons that will also explain the simplest neural networks are mathematically equivalent to linear models. So this tool is also well suited to demonstrate what we've learned up until now. We're going to use it to experimentally verify the theoretical stuff we've introduced today, so you can bolster your ML intuitions. You'll see firsthand the impact of setting the learning rate and how ML models descend gradients. I also call our connections to topics that will be explored in greater depth in this course and in later courses. First, let's talk about the interface. I have removed some of the features of the tool because they relate to material we'll be covering later, but there are still plenty of interesting knobs to turn. First, there's the features column. These are the inputs that your model sees. The coloring within each feature box represents the value of each feature. Orange means negative and blue means positive. Then there's the hidden layers column which you can think of as where the weights are. If you hover over a weight line you'll see the value of that weight. As the model trains, the width and opacity of these lines will change to allow you to get a sense of their values quickly in aggregate. Then there's the output column where you can see both the training data and the models current predictions for all the points in the feature space. You can also see the current training loss. As with the features, color is used to represent value. The top control bar includes buttons for resetting training, starting training, and taking a single step. There's also a dropdown for the learning rate. The data column allows you to select different datasets and control the batch size. Let's start by training a linear model to classify some data. When you click on this link, you'll be shown a TensorFlow Playground window with only the bare essentials and don't worry about the hidden layers for now. In this configuration of the tool, the model accepts a feature vector, computes a dot product with a weight factor, and adds a bias term, and uses the sign of a sum to construct the decision boundary. Consequently, you can think of this configuration as a linear model. We'll start with a model that will attempt to classify data that belong to two distinct clusters. Click the step button, which is to the right of the play button, and note all the things that change in the interface. The number of epoch goes up by one, the lines representing weights change color and size, the current value of the loss function changes, the loss graph shows a downward slope, and the output decision boundary also changes. Mouse over the line representing weight one, and note that you can see the value of this weight. Now click the play button to resume training, but pause soon after the loss drops below 0.002, which should occur before 200 epochs. Congrats, you just trained your first model. Now let's start adding some complexity. First, let's see how three different learning rates affect the model during training. Remember that learning rate is our hyper parameter, which is set before model training begins, and which is multiplied by the derivative to determine how much we change the weights at every iteration of our loop. Follow this link to start training a model with a very small learning rate. Wait until the loss reaches about 100 epoch, which should occur only after about two seconds, and then pause the model. What is the current trending loss? And what are the weights that have been learned? Now increase the learning rate to 0.001 and restart training and once again stop around 100 epochs. What is the loss? It should be substantially less this time. Note 2, the value for weight one. Now increase the learning rate to 0.10, restart the model training, and again train for 100 epochs. How fast did the loss curve drop this time? It should have dropped very quickly. Okay, so let's put these observations together and see if we can explain them using what we've learned about optimization. Now increase the learning rate to 10, restart the model training, and first take a single step using the step button. Note the magnitude of the weight. Now continue training up until 100 epochs. How fast did the loss curve drop this time? It should have dropped precipitously. Okay, so let's put these observations together and see if we can explain them using what we've learned about optimization. Here, I've made a table showing the results that I got. Your results may look slightly different, and that's okay. The reason that they may look different from mine is the same reason that they may look different if you rerun the experiment. TensorFlow Playground randomly initializes the weights, and this means that our search starts off in a random position each time we do it. Let's talk about the weight one column. Note how the magnitude of the weights increased as the learning rates increase. Why do you think that is? This is because the model is taking bigger steps. In fact, when the learning rate was 10, the first step changed the weights dramatically. Let's talk about the loss over time column. As the learning rate increased, the loss curve steepened. This is the same effect as we observed earlier just through a different lens.