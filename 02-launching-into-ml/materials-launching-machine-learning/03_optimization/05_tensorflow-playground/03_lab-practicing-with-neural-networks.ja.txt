線形モデルによるデータセットの学習の次は ニューラルネットワーク（NN）です ですが その前に TensorFlow Playgroundの機能を
一部追加で有効にしたので確認します 1つ目はActivationです Activationとは活性化関数のことで 詳細は コース5 MLの理論と実践
で取り上げます 今重要なのは 活性化関数の選択によって 線形モデルなのかNNなのかが決まる点です 先ほどツールを使用した際は 活性化関数はあらかじめ
線形に設定されていました 2つ目の機能はHIDDEN LAYERSです この機能により 隠れ層の層数と それぞれの隠れ層のニューロン数を
変更できます これは ネットワークが
データを変換する回数の変更と 考えることができます 各隠れ層のニューロンはそれぞれ 先行する層からの出力をすべて受け取り 受け取った入力を変換し 出力値を
次の層の全ニューロンに渡します ニューロン数と ニューロンが
互いに情報をやり取りする仕組みを 簡単に表す方法は 
ネットワークのアーキテクチャです Batch size機能も有効になっており この後 実際に使用します リンクに従って このデータが分類できるよう
モデルにトレーニングさせてください ただし 非線形の特徴量は使わずに 変更するのは ネットワークの
アーキテクチャのみとします NNがどのように機能するのか
まだ説明していませんが 問題ありません ここでは インターフェースを使って ある程度うまく機能する
ネットワークを作ってみてください ある程度うまく機能するモデルが作れたら OUTPUT欄の青い部分が
多角形になっていると思います 詳しく検証して モデルがどのように
この出力を得たのかを理解しましょう 最初の隠れ層のニューロンを見てください それぞれにカーソルを合わせると 出力ボックスにニューロンの
学習内容が反映されます ニューロンの見方は
特徴量と出力の場合と同様です 特徴量X1とX2の値がボックス内の
位置にコード化されていて X1とX2の組み合わせにより ニューロンが出力する値を色分けしています 各ボックスに順にカーソルを
合わせていきますので すべて重ね合わせるとどうなるか
頭の中で考えてみてください 青に青が重なると濃い青になり 白に青が重なると薄い青になり オレンジに青が重なると白になります 各ニューロンによってモデルの
決定境界が形成されていること 出力の形が隠れ層に基づいている
ことがわかってくると思います 例えば このニューロンは
決定境界のこちらの境界線をもたらし このニューロンは
こちらの境界線をもたらします 幾何学的に
ある程度うまく機能する範囲で このネットワークをできるだけ小さくする場合
どこまで縮小できるでしょうか 何とか分類できる範囲で 青い点の周りに描ける最も簡単な
形を考えるのがヒントです 実際にTensorFlow Playgroundを使って
直感が正しいか確かめてみましょう ここまでは ネットワークの最初の隠れ層に
属するニューロンの出力によって 決定境界が構成される仕組みを見てきました 他の層はどうでしょうか 隠れ層が1層のNNと多数あるNNは
どう違うのでしょうか 下のリンクをクリックして このらせん状のデータセットの
分類をNNに学習させます ここで バッチサイズが
勾配降下法に与える影響について 理解を深めましょう バッチサイズパラメータを1に設定して NNアーキテクチャを変更しながら
うまく機能するものを見つけてください 次に モデルを300エポックトレーニングさせ
このときの損失曲線を覚えておきます 今度は バッチサイズパラメータを
10に設定してトレーニングを再開します モデルを300エポックトレーニングさせ
このときの損失曲線も覚えておきます 最後に バッチサイズを30に
設定して繰り返します 何がわかりましたか また これまでに学んだことを基に
どのような解釈ができますか 損失曲線の滑らかさに 顕著な違いが見られたと思います バッチサイズを増やすと
より滑らかな曲線になりました なぜでしょうか バッチサイズが勾配降下法に
与える影響を考えてください バッチサイズが小さいと モデルは1つのデータの損失を求めるごとに
パラメータを更新しますが データは多様なのでそこに問題があります しかし バッチサイズを上げると 個々のデータポイントによるノイズが収まり
明確なシグナルが現れます こうした観察内容から バッチサイズを変更することが 単純に収束率に
影響すると結論付けることはできません 学習率同様 最適なバッチサイズは
問題ごとに異なり ハイパーパラメータチューニングを
通じて見つけることができます モデルが学習を終えるとこのようになります まず注目すべきは 最初の隠れ層と その後の隠れ層の関係です 最初の隠れ層のニューロンからの出力が 基本的に直線だったのに対し 後続の隠れ層の出力は はるかに
複雑なのは明白だと思います 隠れ層の出力を積み重ねた際と同様に 後続の層は先行する層を基に形成されます したがって NNは特徴量の
階層と考えることができます 最終的にデータを分類する前に 入力内容を複雑な方法で変換するというのは NNに特有の考え方で MLで古くから使用されてきた手法と比べ
大きく発展した点です NNが誕生する前 データサイエンティストたちは 特徴量エンジニアリングに
はるかに多くの時間を費やしていました 現在では モデル自体が
この役割の一部を担っていて 層はモデルによる特徴量エンジニアリングの
一種と考えられます 次に注目すべきは
モデルの学習内容のおかしな点です モデルは この2つの領域に
オレンジのポイントがないことを 青であることの証明と解釈したようです このように モデルがデータセットのノイズを 意味あるものとして解釈してしまう
誤りを過学習と呼びます 過学習は モデルの意思決定権が 厳密に問題に必要な権限を
超えている場合に発生します モデルの過学習が起こると うまく汎化できず
新しいデータに対応できません 新しいデータでは
基盤となるシグナルは残るものの まったく同じパターンのノイズが
発生する可能性は低いためです どう対処すればよいでしょうか それは 次の汎化と標本抽出の
講義でお話しします