1
00:00:00,000 --> 00:00:03,595
As you were experimenting with different neural network architectures,

2
00:00:03,595 --> 00:00:08,235
some of you may have trained models that entered into terminal status like this one has.

3
00:00:08,235 --> 00:00:11,520
Note both the last curve as well as the output.

4
00:00:11,520 --> 00:00:15,275
What did you do to fix them? And what's going on here?

5
00:00:15,275 --> 00:00:18,075
While you may have changed your network architecture,

6
00:00:18,075 --> 00:00:22,400
oftentimes, you can fix problems like this simply by retraining your model.

7
00:00:22,400 --> 00:00:26,415
Remember, there are still parts of the model trading process that are not controlled,

8
00:00:26,415 --> 00:00:29,935
such as the random seeds of your weight initializers.

9
00:00:29,935 --> 00:00:32,900
The problem in this case is that we seem to have found

10
00:00:32,900 --> 00:00:35,490
a position on our last service that is small,

11
00:00:35,490 --> 00:00:36,750
relative to its neighbors,

12
00:00:36,750 --> 00:00:39,415
but nevertheless, much bigger than zero.

13
00:00:39,415 --> 00:00:42,350
In other words, we found a local minimum.

14
00:00:42,350 --> 00:00:45,320
Note how the last overtime graph actually

15
00:00:45,320 --> 00:00:49,640
reached a lower lost value earlier on in the search.

16
00:00:49,640 --> 00:00:52,910
The existence and seductiveness of

17
00:00:52,910 --> 00:00:57,835
suboptimal local minima are two examples of the shortcomings of our current approach.

18
00:00:57,835 --> 00:01:00,050
Others include problems like

19
00:01:00,050 --> 00:01:04,785
long training times and the existence of trivial but inappropriate minima.

20
00:01:04,785 --> 00:01:07,535
These problems don't have a single cause,

21
00:01:07,535 --> 00:01:10,305
and so our methods for dealing with them are diverse.

22
00:01:10,305 --> 00:01:13,530
Advanced optimization techniques aim to improve

23
00:01:13,530 --> 00:01:17,110
training time and help models not to be seduced by local minima.

24
00:01:17,110 --> 00:01:20,415
We'll review some of these later in the course.

25
00:01:20,415 --> 00:01:24,930
Data waiting and oversampling and synthetic data creation aim

26
00:01:24,930 --> 00:01:29,015
to remove inappropriate minima from the search space altogether.

27
00:01:29,015 --> 00:01:32,910
Performance metrics, which is what we cover in the next section,

28
00:01:32,910 --> 00:01:34,960
tackle the problem at a higher level.

29
00:01:34,960 --> 00:01:38,645
Rather than changing the way we search or the search space itself,

30
00:01:38,645 --> 00:01:41,640
performance metrics change the way we think about the results of

31
00:01:41,640 --> 00:01:45,875
our search by aligning them more closely with what we actually care about.

32
00:01:45,875 --> 00:01:51,000
In so doing, they allow us to make better informed decisions about when to search again.