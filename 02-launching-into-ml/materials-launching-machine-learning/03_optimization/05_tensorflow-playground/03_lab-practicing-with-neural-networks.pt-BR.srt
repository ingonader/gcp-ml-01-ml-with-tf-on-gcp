1
00:00:00,000 --> 00:00:04,145
Já vimos o desempenho de um
modelo linear nesse conjuntos de dados.

2
00:00:04,145 --> 00:00:07,160
Vamos ver como é o
desempenho de uma rede neural.

3
00:00:07,160 --> 00:00:10,760
No entanto, antes precisamos
revisar alguns recursos adicionais

4
00:00:10,760 --> 00:00:13,025
que ativei no TensorFlow Playground.

5
00:00:13,025 --> 00:00:15,620
O primeiro é o Activation.

6
00:00:15,620 --> 00:00:18,610
Activation diz respeito à função de ativação.

7
00:00:18,610 --> 00:00:21,015
Falaremos sobre esse assunto
com mais detalhes no quinto curso,

8
00:00:21,015 --> 00:00:22,775
“The Art and Science of ML”.

9
00:00:22,775 --> 00:00:25,370
Por enquanto, o ponto crucial é que a escolha

10
00:00:25,370 --> 00:00:29,155
da função de ativação é o que
separa os modelos lineares das redes neurais.

11
00:00:29,155 --> 00:00:31,255
Antes, sem que você soubesse,

12
00:00:31,255 --> 00:00:34,370
a função de ativação
estava definida como linear.

13
00:00:34,370 --> 00:00:38,935
O segundo recurso adicional
que ativei foi Hidden Layers.

14
00:00:38,935 --> 00:00:41,660
Esse recurso permite alterar o número

15
00:00:41,660 --> 00:00:45,550
de camadas ocultas e o número
de neurônios em cada camada oculta.

16
00:00:45,550 --> 00:00:47,990
Pense nisso como uma alteração no número

17
00:00:47,990 --> 00:00:51,185
de transformações que
a rede realiza nos dados.

18
00:00:51,185 --> 00:00:55,095
Cada neurônio em cada
camada oculta recebe todas as saídas

19
00:00:55,095 --> 00:00:58,140
da camada antecedente,
transforma essas entradas

20
00:00:58,140 --> 00:01:01,860
e passa as saídas para todos
os neurônios na camada subsequente.

21
00:01:01,860 --> 00:01:05,310
A maneira abreviada
de descrever o número de neurônios

22
00:01:05,310 --> 00:01:09,285
e como eles passam informações
é a arquitetura da rede.

23
00:01:09,285 --> 00:01:11,630
Também ativei Batch size,

24
00:01:11,630 --> 00:01:15,010
que usaremos em um
experimento daqui a pouco.

25
00:01:15,010 --> 00:01:20,380
Acesse o link no slide e treine um modelo
que possa classificar esse conjunto de dados.

26
00:01:20,380 --> 00:01:23,680
Mas, em vez de introduzir
características não lineares,

27
00:01:23,680 --> 00:01:27,995
tente melhorar o desempenho
apenas alterando a arquitetura da rede.

28
00:01:27,995 --> 00:01:32,550
Sei que ainda não expliquei como uma rede
neural funciona, e isso não tem problema.

29
00:01:32,550 --> 00:01:34,950
Por enquanto, apenas brinque
um pouco com a interface

30
00:01:34,950 --> 00:01:38,340
até configurar uma rede que
tenha um desempenho adequado.

31
00:01:42,760 --> 00:01:45,210
Agora, você deve ter um
modelo que funcione bem

32
00:01:45,210 --> 00:01:49,905
e o formato da região azul da
coluna de saídas deve ser um polígono.

33
00:01:49,905 --> 00:01:55,585
Vamos examiná-lo para ter
uma noção de como o modelo faz isso.

34
00:01:57,095 --> 00:01:59,615
Observe novamente os neurônios
na primeira camada oculta.

35
00:01:59,615 --> 00:02:01,395
Quando passo o mouse
sobre cada um deles,

36
00:02:01,395 --> 00:02:05,050
a caixa de saída muda para
mostrar o que o neurônio aprendeu.

37
00:02:05,050 --> 00:02:09,225
Podemos ler esses neurônios do mesmo
modo como lemos as características e a saída.

38
00:02:09,225 --> 00:02:14,200
Os valores das características x1 e x2 são
codificados na posição dentro do quadrado.

39
00:02:14,200 --> 00:02:16,680
A cor indica o valor
que o neurônio resultará

40
00:02:16,680 --> 00:02:20,335
para a combinação de x1 e x2.

41
00:02:21,385 --> 00:02:23,725
Ao passar o mouse sobre
cada quadrados em sequência,

42
00:02:23,725 --> 00:02:27,985
imagine como seria a aparência
se eles estivessem sobrepostos.

43
00:02:27,985 --> 00:02:30,860
Azul sobre azul resulta
em um azul mais forte,

44
00:02:30,860 --> 00:02:33,465
azul sobre branco resulta em azul claro

45
00:02:33,465 --> 00:02:36,880
e azul sobre laranja resulta em branco.

46
00:02:38,170 --> 00:02:39,620
O que você deve observar é como

47
00:02:39,620 --> 00:02:42,615
cada neurônio participa
no limite de decisão do modelo,

48
00:02:42,615 --> 00:02:46,155
como o formato da saída é
uma função das camadas ocultas.

49
00:02:46,155 --> 00:02:50,625
Por exemplo, este neurônio contribui
com esta borda no limite de decisão,

50
00:02:50,625 --> 00:02:53,995
enquanto este neurônio
contribui com esta borda.

51
00:02:55,485 --> 00:02:57,580
Baseado em seu conhecimento em geometria,

52
00:02:57,580 --> 00:02:59,690
o quanto você poderia reduzir essa rede

53
00:02:59,690 --> 00:03:02,115
e ainda conseguir um
desempenho adequado?

54
00:03:02,115 --> 00:03:05,340
Uma dica: qual é o formato mais simples

55
00:03:05,340 --> 00:03:09,035
que podemos desenhar em torno dos
pontos azuis e ainda dar conta do recado?

56
00:03:09,035 --> 00:03:14,105
Teste no TenserFlow Playground
e veja se sua intuição está correta.

57
00:03:14,925 --> 00:03:18,220
Já vimos como a saída dos
neurônios na primeira camada oculta

58
00:03:18,220 --> 00:03:21,465
da rede pode ser usada
para compor o limite de decisão.

59
00:03:21,465 --> 00:03:23,565
E quanto às outras camadas?

60
00:03:23,565 --> 00:03:28,500
Qual é a diferença entre uma rede neural
com uma camada oculta e outra com muitas?

61
00:03:29,480 --> 00:03:31,700
Clique no link abaixo e
comece a treinar uma rede neural

62
00:03:31,710 --> 00:03:34,570
para classificar este
conjunto de dados em espiral.

63
00:03:35,070 --> 00:03:37,720
Vamos aproveitar a
oportunidade para entender mais sobre

64
00:03:37,720 --> 00:03:40,180
como o tamanho do lote
afeta o gradiente descendente.

65
00:03:40,180 --> 00:03:43,080
Defina o parâmetro
de tamanho de lote como 1 e

66
00:03:43,100 --> 00:03:47,805
teste arquiteturas diferentes da
rede neural até encontrar uma que funcione.

67
00:03:47,805 --> 00:03:54,130
Depois, treine o modelo por 300 épocas
e pause para observar a última curva.

68
00:03:54,750 --> 00:03:59,005
Agora, defina o parâmetro de tamanho
de lote como 10 e reinicie o treinamento.

69
00:03:59,005 --> 00:04:05,600
Treine o modelo por 300 épocas e pause
novamente para observar a curva de perda.

70
00:04:06,820 --> 00:04:11,555
Por fim, faça isso de novo, mas
com tamanho de lote definido como 30.

71
00:04:12,685 --> 00:04:15,900
O que você observou e como
podemos interpretar essas observações

72
00:04:15,900 --> 00:04:17,649
de acordo com o que sabemos?

73
00:04:18,479 --> 00:04:21,230
Você deve ter notado que
há diferenças significativas

74
00:04:21,230 --> 00:04:23,530
na suavidade das curvas de perda.

75
00:04:24,410 --> 00:04:27,780
À medida que o tamanho de lote aumenta,
também aumenta a suavidade.

76
00:04:27,780 --> 00:04:29,345
Por que isso ocorre?

77
00:04:29,345 --> 00:04:32,610
Pense em como o tamanho do lote
afeta o gradiente descendente.

78
00:04:32,610 --> 00:04:36,590
Quando o tamanho do lote é pequeno,
o modelo atualiza os parâmetros

79
00:04:36,600 --> 00:04:39,455
de acordo com a perda de um único exemplo.

80
00:04:39,455 --> 00:04:43,465
Entretanto, os exemplos
variam e aí está o problema.

81
00:04:43,465 --> 00:04:45,760
Mas quando o tamanho do lote aumenta,

82
00:04:45,760 --> 00:04:51,195
o ruído de pontos de dados individuais são
resolvidos e um sinal claro toma forma.

83
00:04:51,195 --> 00:04:55,160
O que você não deve concluir
com base nessas observações

84
00:04:55,160 --> 00:04:59,865
é que alterar o tamanho do lote tem
um efeito simples na taxa de convergência.

85
00:04:59,865 --> 00:05:03,660
Assim como a taxa de aprendizado,
o tamanho de lote ideal depende do problema

86
00:05:03,660 --> 00:05:07,845
e pode ser encontrado
por meio do ajuste do parâmetro.

87
00:05:09,525 --> 00:05:13,695
Agora, o treinamento do modelo já deve
ter chegado ao fim e resultado em algo assim.

88
00:05:13,695 --> 00:05:16,720
A primeira coisa a destacar é a relação entre

89
00:05:16,720 --> 00:05:19,875
a primeira camada oculta
e as camadas subsequentes.

90
00:05:19,875 --> 00:05:23,540
O que deve estar claro é que,
ainda que as saídas dos neurônios

91
00:05:23,540 --> 00:05:26,180
na primeira camada oculta
sejam basicamente linhas,

92
00:05:26,180 --> 00:05:30,075
as camadas ocultas subsequentes
têm saídas muito mais complicadas.

93
00:05:30,075 --> 00:05:34,050
Essas camadas subsequentes se
sobrepõem àquelas que vieram antes,

94
00:05:34,050 --> 00:05:38,125
da mesma forma que fizemos quando
empilhamos as saídas da camada oculta.

95
00:05:38,125 --> 00:05:43,680
Portanto, podemos pensar na rede neural
como uma hierarquia de características.

96
00:05:44,250 --> 00:05:47,210
E a ideia de selecionar entradas e

97
00:05:47,210 --> 00:05:50,810
transformá-las de maneiras
complexas antes de classificá-las

98
00:05:50,810 --> 00:05:52,700
é típica das redes neurais.

99
00:05:52,700 --> 00:05:57,255
Isso é um rompimento significativo com a
abordagem clássica do aprendizado de máquina.

100
00:05:57,255 --> 00:06:01,130
Antes das redes neurais, os cientistas de
dados gastavam muito mais tempo

101
00:06:01,130 --> 00:06:02,830
na engenharia de características.

102
00:06:02,830 --> 00:06:06,910
Agora, o próprio modelo assume
um pouco dessa responsabilidade.

103
00:06:06,910 --> 00:06:11,025
Você pode pensar nas camadas como
uma forma de engenharia de características.

104
00:06:12,045 --> 00:06:16,375
A próxima observação a destacar é que
o modelo aprendeu algumas coisas estranhas.

105
00:06:16,375 --> 00:06:20,020
Ele parece ter interpretado
a falta de pontos laranjas

106
00:06:20,020 --> 00:06:23,545
nestas duas regiões como evidência
para embasar a proeminência de azul.

107
00:06:23,545 --> 00:06:26,940
Chamamos esse tipo de erro,
em que o modelo interpreta o ruído

108
00:06:26,940 --> 00:06:29,725
no conjunto de dados como
algo significativo, de sobreajuste.

109
00:06:29,725 --> 00:06:32,970
Isso pode ocorrer quando
o modelo tem um poder de decisão maior

110
00:06:32,970 --> 00:06:35,215
do que o estritamente
necessário para o problema.

111
00:06:35,215 --> 00:06:38,455
Quando os modelos fazem sobreajuste,
eles generalizam incorretamente.

112
00:06:38,455 --> 00:06:40,525
Isso significa que eles não
funcionam com dados novos,

113
00:06:40,525 --> 00:06:43,150
que provavelmente não têm
o mesmo padrão de ruído,

114
00:06:43,150 --> 00:06:46,235
ainda que o sinal
subjacente permaneça o mesmo.

115
00:06:46,235 --> 00:06:48,145
Como evitamos isso?

116
00:06:48,145 --> 00:06:49,780
Para saber, continue conosco

117
00:06:49,780 --> 00:06:53,000
em nossa próxima aula
sobre generalização e amostragem.