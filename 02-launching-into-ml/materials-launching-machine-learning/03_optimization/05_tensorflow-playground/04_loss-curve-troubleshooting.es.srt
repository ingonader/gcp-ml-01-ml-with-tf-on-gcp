1
00:00:00,470 --> 00:00:03,684
Como experimentaron con diferentes
arquitecturas de redes neuronales

2
00:00:03,684 --> 00:00:07,865
tal vez tengan modelos entrenados
en un estado terminal, como este.

3
00:00:08,445 --> 00:00:11,520
Fíjense en la curva de pérdida
y en la salida

4
00:00:11,910 --> 00:00:15,275
¿Qué hicieron para corregirlos?
¿Qué ocurre aquí?

5
00:00:15,595 --> 00:00:18,075
Aún si cambiaron su arquitectura de red

6
00:00:18,075 --> 00:00:22,240
a menudo se pueden solucionar problemas
como este volviendo a entrenar su modelo.

7
00:00:22,560 --> 00:00:25,925
Recuerden, hay partes del proceso
de entrenamiento del modelo

8
00:00:25,925 --> 00:00:28,345
que no se controlan,
como las fuentes aleatorias

9
00:00:28,345 --> 00:00:29,935
de sus iniciadores de peso.

10
00:00:30,405 --> 00:00:32,680
En este caso,
el problema es que al parecer

11
00:00:32,680 --> 00:00:35,870
encontramos una posición
en la superficie de pérdida que es pequeña

12
00:00:35,870 --> 00:00:39,400
en comparación con sus pares,
pero mucho mayor que cero.

13
00:00:40,015 --> 00:00:42,350
En otras palabras,
encontramos un mínimo local.

14
00:00:42,750 --> 00:00:45,200
Observen cómo
el gráfico de pérdida con el tiempo

15
00:00:45,200 --> 00:00:48,840
anteriormente alcanzó un menor
valor de pérdida en la búsqueda.

16
00:00:51,080 --> 00:00:53,650
La existencia
y el atractivo de un mínimo local

17
00:00:53,650 --> 00:00:57,565
subóptimo son dos ejemplos
de las limitaciones de este enfoque.

18
00:00:58,075 --> 00:01:01,210
Otros incluyen problemas
como tiempos extensos de entrenamiento

19
00:01:01,210 --> 00:01:04,785
y la existencia de mínimos triviales
pero inapropiados.

20
00:01:05,305 --> 00:01:07,675
Estos problemas
no se originan de una sola manera

21
00:01:07,675 --> 00:01:10,435
por eso tenemos métodos
variados para lidiar con ellos.

22
00:01:10,435 --> 00:01:13,000
El objetivo de las técnicas
de optimización avanzadas

23
00:01:13,000 --> 00:01:16,730
es mejorar el tiempo de entrenamiento
y que los modelos no se vean seducidos

24
00:01:16,730 --> 00:01:19,835
por el mínimo local.
Revisaremos esto más adelante en el curso.

25
00:01:21,255 --> 00:01:24,850
Los datos en espera y el sobremuestreo,
y la creación de datos sintéticos

26
00:01:24,850 --> 00:01:28,825
pretenden eliminar del todo el mínimo
inapropiado del espacio de búsqueda.

27
00:01:30,035 --> 00:01:33,160
Las métricas de rendimiento,
que veremos en la siguiente sección

28
00:01:33,160 --> 00:01:35,250
enfrentan el problema a un nivel superior.

29
00:01:35,250 --> 00:01:38,755
En lugar de cambiar cómo
buscamos o el espacio de búsqueda

30
00:01:38,755 --> 00:01:41,740
estas métricas cambian
la forma en la que vemos los resultados

31
00:01:41,740 --> 00:01:45,925
de la búsqueda, ya que los alinean
más cerca de lo que nos importa.

32
00:01:46,415 --> 00:01:51,000
Con ello, nos permiten tomar decisiones
mejor fundamentadas sobre nuevas búsquedas