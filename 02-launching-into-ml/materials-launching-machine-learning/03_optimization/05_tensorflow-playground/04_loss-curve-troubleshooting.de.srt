1
00:00:00,000 --> 00:00:03,594
Beim Experimentieren mit verschiedenen 
Architekturen neuronaler Netzwerke

2
00:00:03,594 --> 00:00:08,235
habe Sie vielleicht Modelle trainiert,
die in den Endstatus eingetreten sind.

3
00:00:08,235 --> 00:00:11,520
Beachten Sie sowohl die
letzte Kurve als auch die Ausgabe.

4
00:00:11,520 --> 00:00:15,275
Wie haben Sie sie angepasst? 
Und was passiert hier?

5
00:00:15,275 --> 00:00:18,135
Sie haben vielleicht 
Ihre Netzwerkarchitektur geändert.

6
00:00:18,135 --> 00:00:22,400
Solche Probleme im Modell können Sie
oft beheben, indem Sie es neu trainieren.

7
00:00:22,400 --> 00:00:26,415
Der Ablauf des Modelltrainings hat immer 
noch Teile, die nicht kontrolliert werden,

8
00:00:26,415 --> 00:00:29,935
wie die zufälligen Seeds zur 
Initialisierung der Gewichtung.

9
00:00:29,935 --> 00:00:33,080
Das Problem in diesem Fall ist, 
das wir bei der Verlustoberfläche

10
00:00:33,080 --> 00:00:34,980
anscheinend eine Position gefunden haben,

11
00:00:34,980 --> 00:00:37,220
die im Vergleich zu 
ihren Nachbarn klein ist,

12
00:00:37,220 --> 00:00:39,645
aber trotzdem deutlich größer als null.

13
00:00:39,645 --> 00:00:42,350
Anders gesagt, haben wir
ein lokales Minimum gefunden.

14
00:00:42,350 --> 00:00:45,320
Beachten Sie, 
dass die Verlustverlaufskurve

15
00:00:45,320 --> 00:00:49,640
früher einen niedrigeren 
Wert in der Suche erreicht hat.

16
00:00:49,640 --> 00:00:52,910
Dass suboptimale lokale Minima
existieren und so verlockend sind,

17
00:00:52,910 --> 00:00:57,835
verdeutlicht die Nachteile
unserer Herangehensweise.

18
00:00:57,835 --> 00:01:00,050
Weitere Nachteile umfassen

19
00:01:00,050 --> 00:01:04,785
lange Trainingszeiten und die Existenz 
trivialer, aber ungeeigneter Minima.

20
00:01:04,785 --> 00:01:07,535
Diese Probleme 
haben verschiedene Ursachen,

21
00:01:07,535 --> 00:01:10,305
daher gibt es 
verschiedene Methoden, sie zu beheben.

22
00:01:10,305 --> 00:01:13,560
Erweiterte Optimierungstechniken 
zielen darauf ab, die Trainingszeit

23
00:01:13,560 --> 00:01:17,260
zu verbessern und zu verhindern, dass 
Modelle auf lokale Minima hereinfallen.

24
00:01:17,260 --> 00:01:20,415
Einige davon 
betrachten wir später im Kurs.

25
00:01:20,415 --> 00:01:24,930
Das Warten auf Daten, Oversampling
und synthetische Datenerstellung

26
00:01:24,930 --> 00:01:29,015
zielen darauf ab, ungeeignete Minima
aus dem Suchbereich zu entfernen.

27
00:01:29,015 --> 00:01:32,910
Leistungsmetriken, die wir 
im nächsten Abschnitt behandeln,

28
00:01:32,910 --> 00:01:35,140
gehen das Problem 
auf einer höheren Ebene an.

29
00:01:35,140 --> 00:01:38,645
Statt das Suchverfahren oder 
den Suchbereich selbst zu verändern,

30
00:01:38,645 --> 00:01:42,190
verändern Leistungsmetriken 
die Denkweise in Bezug auf die Ergebnisse

31
00:01:42,190 --> 00:01:46,225
unserer Suche, indem wir sie näher an dem 
ausrichten, was uns wirklich interessiert.

32
00:01:46,225 --> 00:01:51,000
So können wir bessere Entscheidungen
treffen, wann wir eine neue Suche starten.