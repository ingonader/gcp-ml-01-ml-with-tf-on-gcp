さまざまなNNアーキテクチャで
モデルにトレーニングさせた中で このような悲惨な状態になった人も
いるのではないでしょうか 損失曲線と出力の両方に注目してください どのように修正しましたか
また 何が起こっているのでしょうか ネットワークアーキテクチャを
変更したかもしれませんが 多くの場合 このような問題はモデルに
再トレーニングさせることで修正できます モデルのトレーニングプロセスには
Weight Initializerの乱数の種など 制御されていない部分があるからです この場合 問題は 損失曲面のある位置で 隣接する点と比べて小さい値を見つけたものの それがゼロよりはるかに大きいことです つまり 見つけたのは極小値です 経時的な損失の変化のグラフを見ると 探索の早い段階で
より低い損失値に到達しています 準最適な極小値が存在し
それに誘因されることは 現在の手法が抱える欠点です その他の欠点として 長い学習時間と 些末ながら不適切な
最小値の存在などがあります これらの問題は 1つの原因によって
引き起こされるものではないため 対応方法もさまざまです 高度な最適化手法では 学習時間を短縮し モデルが
極小値に誘因されることを防ぎます こうした手法の一部は
後ほどこのコースで取り上げます データの重み付けとオーバーサンプリング
および合成データの作成では 探索領域から不適切な
最小値を完全に除外します 次のセクションで取り上げる
パフォーマンス指標は より俯瞰的な視点で問題に対処します 探索の方法や探索領域
そのものを変えるのではなく 探索の結果を実際に知りたいことと
より密接にすり合わせることで 結果のとらえ方を変えます これにより 次の探索のタイミングを
十分な情報を得た上で決めることができます