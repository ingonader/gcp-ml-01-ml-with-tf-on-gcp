Antes de ver una de las formas en que
los investigadores abordan este problema revisemos lo que aprendimos. Pongámonos en el lugar de nuestro modelo y veamos cómo la pérdida podría
cambiar durante el entrenamiento. Imaginen que
usamos el descenso de gradientes y actualizamos los parámetros del modelo con respecto
al derivado de la función de pérdida y configuramos para ver
cómo la pérdida cambia con el tiempo. Esta es una situación común en el AA en especial cuando
entrenar un modelo requiere de horas o incluso días. Pueden imaginar
lo importante que es no perder tiempo. Con eso en mente, solucionemos
algunos problemas en la curva de pérdida. Esta es la forma común
de una curva de pérdida. La pérdida decrece rápidamente
con pasos grandes hacia el gradiente y luego se mantiene
con el tiempo con pasos más pequeños a medida que llega al mínimo
en la superficie de pérdida. ¿Y una curva de pérdida como esta? Asuman por un momento que
la escala del eje de pérdida es grande. ¿Qué les dice esto sobre su modelo y sobre cómo su búsqueda
pasa por la superficie de pérdida? Significa que nuestra
búsqueda da saltos por todos lados y no como quisiéramos,
con un progreso estable hacia un mínimo en especial. ¿Y qué tal esta? Esta significa que tal vez
seguimos en el mismo valle y tomará mucho
tiempo llegar a la parte inferior. En ambos casos el tamaño del paso
no era el adecuado para el problema. En el primer caso, el tamaño
del paso es demasiado grande y en el segundo es demasiado pequeño. Lo que necesitamos
es un parámetro de escalamiento. En los textos, esto se denomina
tasa de aprendizaje y agregarla a nuestro código nos permite
tener un descenso de gradientes clásico. Cambié la línea donde creamos el bucle
para actualizar los valores del parámetro. Imaginen descubrir a la fuerza el mejor
valor para la tasa de aprendizaje. Recuerden que la tasa de aprendizaje
tal vez tenga un mejor valor específico. Puesto que se configura antes de comenzar la tasa de aprendizaje
es un hiperparámetro. Y para determinar el mejor
valor para los hiperparámetros hay un método mejor
que se llama ajuste de hiperparámetros. Veremos cómo hacer esto
en Cloud ML Engine en un módulo posterior. Sin embargo, la tasa
de aprendizaje es algo menor de 1. Por ahora, recuerden esta formulación
del descenso de gradientes y que la tasa es un hiperparámetro
que se establece durante el entrenamiento.