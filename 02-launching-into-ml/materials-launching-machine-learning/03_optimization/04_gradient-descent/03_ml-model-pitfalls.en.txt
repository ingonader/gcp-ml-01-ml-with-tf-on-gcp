A common situation that practitioners encounter, is that they rerun model code that they've written expecting it to produce the same output, only it doesn't. Programmers are often used to working in deterministic settings. In ML, this is sometimes not the case. For many models, if you retrain the model a second time even when using the same hyperparameter settings, the resulting parameter settings might be very different. This at first seems disconcerting. Aren't we looking for the best set of parameters? Does this mean that gradient descent isn't working, or that I've implemented incorrectly? Not necessarily. What it could mean is that instead of searching a loss surface like on the left hand side, we're actually searching loss surfaces like on the right hand side. Notice that whereas the left hand side loss surface has a single bottom, the right hand side has more than one. The formal name for this property is convexity. The left hand side is a convex surface whereas the right hand side is non-convex. Why might an ML model's loss surface have more than one minimum? Well it means that there are a number of equivalent, or close to equivalent points in parameter space. Meaning settings for our parameters that produce models with the same capacity to make predictions. We'll revisit this later on, when we introduce neural networks, because they're a prime example of where this happens. So it's okay if it's not clear. For now, simply keep in mind that loss services vary with respect to the number of minima that they have. Sometimes fast just isn't fast enough. We all hate waiting for models to finish training. Is there a way to make model training go even faster? Yes. But to understand what our options are, it's best to consider the high level steps of our algorithm and their sources of time complexity. Here I've depicted the three primary steps our algorithm must go through. When we calculate the derivative, the cost of the calculation is proportional to the number of data points we are putting into our loss function, as well as the number of parameters in our model. In practice, models can vary from tens of parameters to hundreds of millions. Similarly, datasets can vary from a few thousand points to hundreds of billions. For the case of updating the models parameters, this happens once per loop and its cost is determined solely by the number of parameters in the model. However, the cost of making the update is typically small relative to the other steps. Finally there's checking the loss. This step time complexity is proportional to the number of data points in the set that we're using for measuring the loss and the complexity of our model. Surprisingly, even though we have represented this process has a loop, the check loss step needing to be done at every pass. And the reason for this, is that most changes in the loss function are incremental. So what can we change to improve training time? Typically, the number of affected parameters in a model is fixed, although we'll return to how this might be varied in a future module on regularisation. Additionally, although it might sound appealing to reduce the number of data points used to check the loss, this is generally not recommended. Instead, we have two main knobs to turn to improve training time. The number of data points we calculate the derivative on, and the frequency with which we check the loss. As we said, one of the knobs we can turn to speed up model training, is the number of data points that we calculate the derivative on. Remember, the derivative comes from our loss function, and our loss function composes the error of a number of predictions together. So, this method essentially reduces the number of data points that we feed into our loss function at each iteration of our algorithm. Take a moment and think about why this might still work. The reason that this might still work, is that it's possible to extract samples from our training data that on average balance each other out. We'll talk more about pitfalls for sampling and how to avoid them in later modules. For now, just keep in mind that our sampling strategy selects from our training set with uniform probability. So, every instance in the training set has an equal chance of being seen by the model. In ML, we refer to this practice of sampling from our training set during training as mini-batching, and this variant of gradient descent as mini-batch gradient descent. The samples themselves are referred to as batches. Mini-batch gradient descent has the added benefit in addition to costing less time, of using less memory and of being easy to parallelize. Now a quick aside. You might hear people using the term batch gradient descent. The batch there refers to batch processing. So, batch gradient descent computes the gradient on the entire dataset. It is definitely not the same as mini-batch gradient descent. Here, we're talking about mini-batch gradient descent. Confusingly, mini-batch size is often just called batch size. This is what TensorFlow calls it. And so, this is what we will call it too. In the rest of the specialization, when we talk about batch size, we're actually talking about the size of the samples in mini-batch gradient descent. So, how big should those mini-batches be? Well like learning rate, batch size is another hyperparameter. And as such, it's optimal value is problem dependent and can be found using hyperparameter tuning, which we'll talk about later. Typically, batch size is between 10 and 100 examples. Well, like learning rate, batch size is another hyperparameter and as such, its optimal value is problem dependent and can be found using hyperparameter tuning, which we'll talk about later. Typically, batch size is between 10 and 1,000 examples. The other knob we can turn to speed up model training, is the frequency with which we check the loss. Recall that although it will be great to simply check the loss on a subset of the data, this isn't a good idea. The implementation is quite simple. We introduce some logic, such that our expensive compute loss function evaluates reduced frequency. Some popular strategies for the ready to update loss function, are time-based and step-based. For example, once every 1,000 steps, or once every 30 minutes. With the reduction of the frequency that we check the loss and the introduction of mini-batching, we've now begun to decouple the two fundamental parts of model training. Changing our model's parameters, and checking to see when we've made the right changes.