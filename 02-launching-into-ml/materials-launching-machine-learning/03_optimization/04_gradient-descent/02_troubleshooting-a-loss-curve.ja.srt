1
00:00:00,000 --> 00:00:04,325
研究者が元々この問題に
どう対処してきたかを説明する前に

2
00:00:04,325 --> 00:00:07,670
これまでに学んだことをまとめましょう

3
00:00:07,670 --> 00:00:10,810
モデルの立場になって

4
00:00:10,810 --> 00:00:15,470
トレーニングの経過とともに
損失がどう変化するかを考えます

5
00:00:15,470 --> 00:00:17,935
勾配降下法を使って

6
00:00:17,935 --> 00:00:21,985
損失関数の微分係数について
モデルのパラメータを更新し

7
00:00:21,985 --> 00:00:27,415
時間の経過とともに損失がどう変化するかが
わかるアルゴリズムを構築したとします

8
00:00:27,415 --> 00:00:30,280
これは 機械学習において
よくあるシナリオですが

9
00:00:30,280 --> 00:00:34,295
モデルのトレーニングに数時間または
数日かかる場合もあります

10
00:00:34,300 --> 00:00:38,715
時間を無駄にしないことがいかに重要かは
言うまでもありません

11
00:00:38,715 --> 00:00:43,165
これを念頭に損失曲線を修正します

12
00:00:43,165 --> 00:00:46,105
こちらが一般的な損失曲線です

13
00:00:46,105 --> 00:00:49,030
勾配を大きなステップで降下すると
損失も急激に低下し

14
00:00:49,030 --> 00:00:53,630
時間の経過とともに最小値に近付いて
ステップが小さくなるにつれ

15
00:00:53,630 --> 00:00:56,670
損失の低下は緩やかになります

16
00:00:57,100 --> 00:01:00,605
損失曲線がこのような場合はどうでしょうか

17
00:01:00,605 --> 00:01:04,875
損失軸の範囲が広いと仮定すると

18
00:01:04,875 --> 00:01:08,430
モデルと 損失曲面における
探索プロセスの進行状況について

19
00:01:08,430 --> 00:01:11,040
何がわかりますか

20
00:01:11,660 --> 00:01:14,340
この損失曲線から探索があちこちに飛んでいて

21
00:01:14,340 --> 00:01:18,460
最小値へと着実に近付いて
いないことがわかります

22
00:01:20,720 --> 00:01:23,000
では こちらはどうでしょう

23
00:01:23,000 --> 00:01:25,780
この場合は 同じ谷を探索しているけれども

24
00:01:25,780 --> 00:01:27,860
最小値に達するまでには

25
00:01:27,860 --> 00:01:30,630
長い時間がかかることがわかります

26
00:01:31,580 --> 00:01:33,080
どちらのケースも

27
00:01:33,080 --> 00:01:35,990
問題に適したステップサイズが
使用されていません

28
00:01:35,990 --> 00:01:38,715
最初のケースではステップサイズが大きすぎ

29
00:01:38,715 --> 00:01:41,285
2つ目のケースでは小さすぎます

30
00:01:41,285 --> 00:01:44,375
そこで必要になるのが
スケーリングパラメータです

31
00:01:44,375 --> 00:01:47,735
各種文献ではこれを学習率と呼んでいます

32
00:01:47,735 --> 00:01:52,390
これをコードに組み込むことで
標準的な勾配降下アルゴリズムが完成します

33
00:01:52,390 --> 00:01:56,605
パラメータ値を更新する
forループの行を変更しました

34
00:01:57,155 --> 00:02:01,640
学習率の最適値を求めるのに
総当たりを使用すると思うかもしれませんが

35
00:02:01,640 --> 00:02:06,045
学習率の最適値は 問題によって異なります

36
00:02:06,045 --> 00:02:09,145
学習率は 
学習が始まる前に設定するものなので

37
00:02:09,145 --> 00:02:11,185
ハイパーパラメータです

38
00:02:11,185 --> 00:02:13,850
ハイパーパラメータの最適値を求める場合

39
00:02:13,850 --> 00:02:17,980
ハイパーパラメータチューニングと呼ばれる
より適した方法が存在します

40
00:02:17,980 --> 00:02:22,470
Cloud ML Engineでの使用方法は
後のモジュールで確認します

41
00:02:22,470 --> 00:02:26,265
一般に 学習率は1よりはるかに小さい小数です

42
00:02:26,265 --> 00:02:29,175
ここでは 勾配降下法の記述と

43
00:02:29,175 --> 00:02:33,045
学習率が学習を通じて決定される
ハイパーパラメータであることを

44
00:02:33,045 --> 00:02:35,210
押さえておきましょう