前のセクションでは 最適化をパラメータ領域の探索として位置付け ポイントを比較する方法である
損失関数について説明しました では 損失関数からどのように
探索ストラテジーを構築するのでしょうか 勾配下降法を使います 勾配降下法とは
パラメータ領域のすべての点について 損失関数によって形成される損失曲面を
降下していくプロセスを指します 実際の曲面はこのようなものです もちろんこれは情報が完全な場合 つまり グラフを
すべて把握できている場合です 実際には パラメータ領域内の 損失関数で評価した点の
損失値しかわかりません この場合は 赤で囲った部分にある
2つの点のみです しかし どうにかして最小値を見つける方法を
決めなければなりません 最小値を見つけるという問題は 2つの異なる重要な問いに
分けることができます 探索すべき方向は？ ステップの大きさは？ ここでは わかりやすくするため ステップサイズは固定とします すると 非常にシンプルな
アルゴリズムができます 損失が微小な定数より大きいとき
探索方向を計算して モデルの各パラメータの値を 古い値にステップサイズと
探索方向の積を足した値に設定し 最後に 損失を再計算します 損失曲面は 地形図や等高線と考えられます それぞれの線が特定の深さを表します 線が密集している部分は
曲面の勾配がより急になっています アルゴリズムは ここに点で示した
ステップで進みます この場合 アルゴリズムは上端から始まり 中央にある最小値まで降下しています アルゴリズムは一定のステップサイズで
最小値の方向に進んでいます 探索方向はさておき ステップサイズが小さすぎると
学習に途方もない時間がかかります しかも 最小値が必ず見つかるとは限りません ここでは 最小値は1つであると
仮定して話を進めますが 今後 最小値が複数存在する場合が出てきます その際の対処方法は 後ほど説明します ステップサイズが大きすぎると 損失曲面の一方の端から
もう一方の端まで飛んでしまう または 現在の谷から損失曲面のまったく
別の部分に飛んでしまう可能性があります このため ステップサイズが大きすぎると プロセスは必ずしも収束しません ステップサイズが適切であれば
言うことはありません ただし 適切なステップサイズの値は 問題ごとに異なります 左側の曲線には適しているステップサイズも 右側の曲線ではまったく機能していません あらゆるモデルに適した
ステップサイズはありません では ステップサイズは
どう決めるのでしょうか 幸いにも 傾き つまり
曲線の変化の割合によって ステップの大きさと探索方向を
同時に十分把握できます 下のサブプロットを見てください 重み・損失曲線の各ポイントの
傾きの値を示しています 通常 傾きが大きいほど 
傾きが小さいポイントよりも 最小値からは遠くなります また 上のグラフで 傾きが負の場合
最小値は右側にあり 傾きが正の場合は左側にあります 別の例を挙げます ポイントBの傾きは正と負のどちらでしょうか 傾きは正なので 最小値を見つけるには
左に進む必要があります また 傾きが急なので ステップサイズは
大きくする必要があります 次に ポイントCです 傾きは 正と負のどちらでしょうか 傾き度合いはどうでしょうか ポイントCも傾きは正なので
左に進む必要があります 傾きは はるかに緩やかなので 最小値を越えてしまわないように ステップサイズを小さくします 一定だったステップサイズと
探索方向の計算の呼び出しを computeDerivativeという
新たな関数の呼び出しに置き換え モデルのパラメータを更新するforループは 各パラメータを設定する際 古い値からそのパラメータの
損失についての偏微分係数を引いた値に 設定するよう変更しました 正しい方向に 適切なステップサイズで 探索を進める方法がわかりましたが 何か問題があるでしょうか 実地パフォーマンスに問題があります ML研究者が取り組んできた問題 つまり このプロシージャを
適用した損失曲面に関して 多くの場合 基本のアルゴリズムは
時間がかかりすぎる 準最適な最小値しか見つからない
終了しないのいずれかです しかし アルゴリズムが
機能しないということではなく 単純に適した種類の問題が
少ないということです