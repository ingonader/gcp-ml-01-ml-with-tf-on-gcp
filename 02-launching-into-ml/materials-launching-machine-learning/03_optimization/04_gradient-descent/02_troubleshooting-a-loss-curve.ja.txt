研究者が元々この問題に
どう対処してきたかを説明する前に これまでに学んだことをまとめましょう モデルの立場になって トレーニングの経過とともに
損失がどう変化するかを考えます 勾配降下法を使って 損失関数の微分係数について
モデルのパラメータを更新し 時間の経過とともに損失がどう変化するかが
わかるアルゴリズムを構築したとします これは 機械学習において
よくあるシナリオですが モデルのトレーニングに数時間または
数日かかる場合もあります 時間を無駄にしないことがいかに重要かは
言うまでもありません これを念頭に損失曲線を修正します こちらが一般的な損失曲線です 勾配を大きなステップで降下すると
損失も急激に低下し 時間の経過とともに最小値に近付いて
ステップが小さくなるにつれ 損失の低下は緩やかになります 損失曲線がこのような場合はどうでしょうか 損失軸の範囲が広いと仮定すると モデルと 損失曲面における
探索プロセスの進行状況について 何がわかりますか この損失曲線から探索があちこちに飛んでいて 最小値へと着実に近付いて
いないことがわかります では こちらはどうでしょう この場合は 同じ谷を探索しているけれども 最小値に達するまでには 長い時間がかかることがわかります どちらのケースも 問題に適したステップサイズが
使用されていません 最初のケースではステップサイズが大きすぎ 2つ目のケースでは小さすぎます そこで必要になるのが
スケーリングパラメータです 各種文献ではこれを学習率と呼んでいます これをコードに組み込むことで
標準的な勾配降下アルゴリズムが完成します パラメータ値を更新する
forループの行を変更しました 学習率の最適値を求めるのに
総当たりを使用すると思うかもしれませんが 学習率の最適値は 問題によって異なります 学習率は 
学習が始まる前に設定するものなので ハイパーパラメータです ハイパーパラメータの最適値を求める場合 ハイパーパラメータチューニングと呼ばれる
より適した方法が存在します Cloud ML Engineでの使用方法は
後のモジュールで確認します 一般に 学習率は1よりはるかに小さい小数です ここでは 勾配降下法の記述と 学習率が学習を通じて決定される
ハイパーパラメータであることを 押さえておきましょう