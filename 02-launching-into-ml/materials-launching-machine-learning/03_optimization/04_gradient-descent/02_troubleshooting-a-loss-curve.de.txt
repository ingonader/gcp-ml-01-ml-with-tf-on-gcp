Bevor wir eine der 
ersten Möglichkeiten betrachten, mit denen dieses Problem angegangen wurde, gehen wir einiges vom Gelernten durch. Versetzen wir uns in das Modell und sehen uns die
Verlustentwicklung im Trainnigsverlauf an. Angenommen, wir machen
einen Gradientenabstieg und aktualisieren die Parameter
anhand der Ableitung der Verlustfunktion. Wir haben das so konfiguriert, 
dass wir sehen können, wie sich der Verlust 
im Laufe der Zeit verändert. Das kommt beim 
Machine Learning häufig vor, vor allem wenn das Modelltraining Stunden oder gar Tage dauert. Sie können sich vorstellen, wie wichtig
es ist, nicht mehrere Tage zu verlieren. Also untersuchen wir eine Verlustkurve. Das hier ist eine typische Verlustkurve. Der Verlust fällt stark,
mit diesen großen Schritten am Gradienten, und flacht im Laufe der Zeit ab. Mit kleinen Schritten erreicht er 
das Minimum an der Verlustoberfläche. Was, wenn Sie eine 
Verlustkurve wie diese sehen? Angenommen, der
Maßstab der Verlustachse ist groß. Was sagt Ihnen das über Ihr Modell und den Verlauf Ihrer
Suche an der Verlustoberfläche? Es bedeutet, dass die Suche
von Punkt zu Punkt springt und sich nicht stetig in
Richtung eines bestimmten Minimums bewegt. Was ist mit dieser hier? Wir sind wohl noch im selben Tal, aber es wird sehr, sehr lange dauern, 
die Untergrenze zu erreichen. In diesen beiden Fällen war jedoch die Schrittgröße nicht 
für das spezielle Problem geeignet. Im ersten Fall waren die Schritte zu groß, im zweiten zu klein. Wir brauchen also 
einen Parameter für den Maßstab. In der Literatur 
wird das Lernrate genannt. Übernehmen wir diese in den Code, haben
wir einen klassischen Gradientenabstieg. Beachten Sie die Änderungen der Zeile 
mit den aktualisierten Parameterwerten. Sicher könnte man mit Brute Force den 
besten Wert für die Lernrate ermitteln. Bedenken Sie aber, dass der beste Wert 
für die Lernrate problemspezifisch ist. Weil sie feststeht, 
bevor das Lernen beginnt, ist die Lernrate ein Hyperparameter. Die bessere Methode, um den besten
Wert eines Hyperparameters zu ermitteln, ist das Hyperparameter-Tuning. Wie das in der Cloud ML Engine 
funktioniert, wird in einem späteren Modul erklärt. Allgemein gesagt ist die
Lernrate aber deutlich kleiner als eins. Merken Sie sich vorerst 
diese Formel des Gradientenabstiegs, und dass die Lernrate ein Hyperparameter 
ist, der im Training festgelegt wird.