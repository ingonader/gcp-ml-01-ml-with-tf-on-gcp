1
00:00:00,300 --> 00:00:04,325
Antes de ver una de las formas en que
los investigadores abordan este problema

2
00:00:04,325 --> 00:00:06,910
revisemos lo que aprendimos.

3
00:00:07,870 --> 00:00:10,600
Pongámonos en el lugar de nuestro modelo

4
00:00:10,600 --> 00:00:14,390
y veamos cómo la pérdida podría
cambiar durante el entrenamiento.

5
00:00:15,640 --> 00:00:17,805
Imaginen que
usamos el descenso de gradientes

6
00:00:17,805 --> 00:00:19,755
y actualizamos los parámetros del modelo

7
00:00:19,755 --> 00:00:22,105
con respecto
al derivado de la función de pérdida

8
00:00:22,105 --> 00:00:26,465
y configuramos para ver
cómo la pérdida cambia con el tiempo.

9
00:00:27,435 --> 00:00:29,850
Esta es una situación común en el AA

10
00:00:29,850 --> 00:00:32,545
en especial cuando
entrenar un modelo requiere de horas

11
00:00:32,545 --> 00:00:34,300
o incluso días.

12
00:00:34,300 --> 00:00:37,875
Pueden imaginar
lo importante que es no perder tiempo.

13
00:00:38,775 --> 00:00:42,265
Con eso en mente, solucionemos
algunos problemas en la curva de pérdida.

14
00:00:43,395 --> 00:00:45,715
Esta es la forma común
de una curva de pérdida.

15
00:00:45,715 --> 00:00:49,210
La pérdida decrece rápidamente
con pasos grandes hacia el gradiente

16
00:00:49,210 --> 00:00:52,410
y luego se mantiene
con el tiempo con pasos más pequeños

17
00:00:52,410 --> 00:00:55,470
a medida que llega al mínimo
en la superficie de pérdida.

18
00:00:57,450 --> 00:01:00,005
¿Y una curva de pérdida como esta?

19
00:01:01,025 --> 00:01:04,775
Asuman por un momento que
la escala del eje de pérdida es grande.

20
00:01:04,775 --> 00:01:06,850
¿Qué les dice esto sobre su modelo

21
00:01:06,850 --> 00:01:10,070
y sobre cómo su búsqueda
pasa por la superficie de pérdida?

22
00:01:11,850 --> 00:01:14,470
Significa que nuestra
búsqueda da saltos por todos lados

23
00:01:14,470 --> 00:01:17,050
y no como quisiéramos,
con un progreso estable

24
00:01:17,050 --> 00:01:19,150
hacia un mínimo en especial.

25
00:01:20,560 --> 00:01:21,800
¿Y qué tal esta?

26
00:01:22,760 --> 00:01:25,910
Esta significa que tal vez
seguimos en el mismo valle

27
00:01:25,910 --> 00:01:28,760
y tomará mucho
tiempo llegar a la parte inferior.

28
00:01:31,590 --> 00:01:33,080
En ambos casos

29
00:01:33,080 --> 00:01:36,290
el tamaño del paso
no era el adecuado para el problema.

30
00:01:36,290 --> 00:01:39,035
En el primer caso, el tamaño
del paso es demasiado grande

31
00:01:39,035 --> 00:01:41,275
y en el segundo es demasiado pequeño.

32
00:01:41,575 --> 00:01:44,375
Lo que necesitamos
es un parámetro de escalamiento.

33
00:01:44,715 --> 00:01:47,835
En los textos, esto se denomina
tasa de aprendizaje

34
00:01:47,835 --> 00:01:51,880
y agregarla a nuestro código nos permite
tener un descenso de gradientes clásico.

35
00:01:51,880 --> 00:01:56,905
Cambié la línea donde creamos el bucle
para actualizar los valores del parámetro.

36
00:01:57,495 --> 00:02:01,810
Imaginen descubrir a la fuerza el mejor
valor para la tasa de aprendizaje.

37
00:02:01,810 --> 00:02:05,765
Recuerden que la tasa de aprendizaje
tal vez tenga un mejor valor específico.

38
00:02:06,685 --> 00:02:08,955
Puesto que se configura antes de comenzar

39
00:02:09,385 --> 00:02:11,615
la tasa de aprendizaje
es un hiperparámetro.

40
00:02:11,615 --> 00:02:14,280
Y para determinar el mejor
valor para los hiperparámetros

41
00:02:14,280 --> 00:02:17,980
hay un método mejor
que se llama ajuste de hiperparámetros.

42
00:02:18,480 --> 00:02:21,970
Veremos cómo hacer esto
en Cloud ML Engine en un módulo posterior.

43
00:02:22,390 --> 00:02:26,195
Sin embargo, la tasa
de aprendizaje es algo menor de 1.

44
00:02:26,735 --> 00:02:30,515
Por ahora, recuerden esta formulación
del descenso de gradientes

45
00:02:30,515 --> 00:02:34,874
y que la tasa es un hiperparámetro
que se establece durante el entrenamiento.