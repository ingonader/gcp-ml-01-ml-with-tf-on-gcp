1
00:00:01,616 --> 00:00:04,111
Uma situação comum para os profissionais

2
00:00:04,115 --> 00:00:06,685
é executar novamente
o código que eles escreveram

3
00:00:06,685 --> 00:00:09,244
esperando que o mesmo resultado seja gerado,

4
00:00:09,244 --> 00:00:10,652
mas isso não acontece.

5
00:00:11,667 --> 00:00:15,028
Os programadores costumam trabalhar 
com cenários determinísticos.

6
00:00:15,028 --> 00:00:17,427
No ML, nem sempre é assim.

7
00:00:18,473 --> 00:00:21,485
Há muitos modelos que,
quando treinados pela segunda vez,

8
00:00:21,486 --> 00:00:24,226
mesmo usando as mesmas
configurações de hiperparâmetro,

9
00:00:24,226 --> 00:00:26,901
retornam configurações de parâmetro
muito diferentes.

10
00:00:27,793 --> 00:00:30,038
Em um primeiro momento,
isso parece preocupante.

11
00:00:30,049 --> 00:00:32,629
Não estamos procurando
o melhor conjunto de parâmetros?

12
00:00:32,629 --> 00:00:35,153
Será que o gradiente
descendente não está funcionando?

13
00:00:35,153 --> 00:00:37,620
Ou que não foi implementado corretamente?

14
00:00:37,620 --> 00:00:38,970
Não necessariamente.

15
00:00:39,930 --> 00:00:44,004
Talvez, ao invés de pesquisar uma superfície
de perda como a do lado esquerdo,

16
00:00:44,010 --> 00:00:47,585
estamos pesquisando uma superfície de perda
como a do lado direito.

17
00:00:47,585 --> 00:00:51,465
Observe que, enquanto a superfície
à esquerda tem uma única base,

18
00:00:51,465 --> 00:00:53,920
a superfície da direita tem mais de uma.

19
00:00:54,580 --> 00:00:57,475
O nome formal dessa propriedade 
é convexidade.

20
00:00:57,475 --> 00:01:02,270
Temos uma superfície convexa à esquerda
e uma não convexa à direita.

21
00:01:03,642 --> 00:01:08,180
Por que a superfície de perda de um modelo
de ML teria mais de um mínimo?

22
00:01:08,180 --> 00:01:10,350
Quer dizer que há vários pontos equivalentes,

23
00:01:10,350 --> 00:01:13,340
ou quase equivalentes, em parâmetro-espaço.

24
00:01:13,340 --> 00:01:15,370
Ou seja, configurações de parâmetros

25
00:01:15,370 --> 00:01:18,505
que produzem modelos com
a mesma capacidade de previsão.

26
00:01:18,865 --> 00:01:20,250
Voltaremos ao assunto depois,

27
00:01:20,250 --> 00:01:21,965
quando falarmos sobre redes neurais,

28
00:01:21,965 --> 00:01:24,485
porque elas apresentam muitas
ocorrências desse tipo.

29
00:01:24,485 --> 00:01:27,020
Tudo bem se não ficou muito claro.

30
00:01:27,020 --> 00:01:29,180
Por enquanto, apenas lembre-se de que

31
00:01:29,180 --> 00:01:33,025
as superfícies de perda variam
com relação ao número de mínimos.

32
00:01:33,641 --> 00:01:36,840
Às vezes, não é tão rápido como gostaríamos.

33
00:01:36,840 --> 00:01:39,660
Odiamos ter que esperar
até que o modelo termine de treinar.

34
00:01:39,660 --> 00:01:43,600
Há alguma maneira de acelerar
ainda mais o treinamento de modelos?

35
00:01:43,600 --> 00:01:46,045
Sim, mas para entender as nossas opções,

36
00:01:46,045 --> 00:01:48,950
o melhor é pensar
sobre as etapas gerais do algoritmo

37
00:01:48,950 --> 00:01:51,155
e o que causa a complexidade temporal.

38
00:01:51,155 --> 00:01:55,253
Temos aqui as três etapas principais
a que o algoritmo é submetido.

39
00:01:56,562 --> 00:01:58,245
Quando calculamos a derivada,

40
00:01:58,245 --> 00:02:00,500
o custo do cálculo é proporcional

41
00:02:00,500 --> 00:02:03,235
ao número de pontos de dados
incluídos na função de perda,

42
00:02:03,235 --> 00:02:06,209
bem como ao número
de parâmetros no modelo.

43
00:02:07,106 --> 00:02:12,135
Na prática, os modelos podem ter
de dezenas a milhões de parâmetros.

44
00:02:12,135 --> 00:02:17,145
Da mesma maneira, os conjuntos de dados
podem ter de milhares a bilhões de pontos.

45
00:02:18,099 --> 00:02:20,670
No caso da atualização
dos parâmetros do modelo,

46
00:02:20,670 --> 00:02:22,497
isso ocorre uma vez por loop

47
00:02:22,497 --> 00:02:26,545
e o custo é determinado apenas
pelo número de parâmetros no modelo.

48
00:02:26,545 --> 00:02:31,516
No entanto, o custo da atualização
é normalmente menor do que em outras etapas.

49
00:02:32,467 --> 00:02:34,540
Por fim, há a verificação da perda.

50
00:02:35,431 --> 00:02:39,220
Ela tem complexidade temporal proporcional
ao número de pontos de dados

51
00:02:39,220 --> 00:02:43,445
no conjunto que estamos usando para medir
a perda e à complexidade do modelo.

52
00:02:44,013 --> 00:02:47,415
É surpreendente que mesmo tendo
representado esse processo como um loop,

53
00:02:47,415 --> 00:02:50,695
a verificação da perda
não precisa ser executada todas as vezes.

54
00:02:50,695 --> 00:02:54,886
O motivo é que a maioria das alterações
na função de perda é incrementada.

55
00:02:57,494 --> 00:03:00,570
Então, o que podemos mudar
para reduzir o tempo de treinamento?

56
00:03:00,570 --> 00:03:04,280
Geralmente, o número de parâmetros afetados
no modelo é fixo,

57
00:03:04,280 --> 00:03:09,360
mas falaremos como ele pode variar
em um módulo sobre regularização.

58
00:03:10,051 --> 00:03:12,620
Além disso, pode parecer uma boa ideia

59
00:03:12,620 --> 00:03:15,835
reduzir o número de pontos de dados
usados para verificar a perda,

60
00:03:15,835 --> 00:03:17,688
mas isso não é recomendável.

61
00:03:19,406 --> 00:03:22,820
Podemos mudar dois aspectos 
para reduzir o tempo de treinamento:

62
00:03:22,820 --> 00:03:25,860
o número de pontos de dados
que usamos para calcular a derivada

63
00:03:25,867 --> 00:03:28,436
e a frequência de verificação da perda.

64
00:03:29,149 --> 00:03:32,030
Como dissemos, um desses aspectos

65
00:03:32,030 --> 00:03:35,290
é o número de pontos de dados
que usamos para calcular a derivada.

66
00:03:36,255 --> 00:03:38,850
A derivada é resultante da função de perda,

67
00:03:38,850 --> 00:03:43,110
que, por sua vez, compõe
o erro de várias previsões juntas.

68
00:03:43,110 --> 00:03:46,380
Portanto, esse método basicamente
reduz o número de pontos de dados

69
00:03:46,380 --> 00:03:50,078
que inserimos na função de perda
a cada iteração do algoritmo.

70
00:03:51,065 --> 00:03:53,816
Pare e pense por que isso pode funcionar.

71
00:03:56,894 --> 00:03:58,680
Isso pode funcionar

72
00:03:58,680 --> 00:04:02,210
porque é possível extrair amostras
que costumam se equilibrar mutuamente

73
00:04:02,210 --> 00:04:04,395
dos dados de treinamento .

74
00:04:05,073 --> 00:04:09,480
Falaremos sobre as armadilhas da amostragem
e como evitá-las nos próximos módulos.

75
00:04:09,480 --> 00:04:12,570
Por enquanto, lembre-se de que
nossa amostragem seleciona os dados

76
00:04:12,570 --> 00:04:15,330
no conjunto de treinamento
com distribuição uniforme.

77
00:04:15,330 --> 00:04:20,200
Cada instância do conjunto tem a mesma chance
de ser analisada pelo modelo.

78
00:04:21,808 --> 00:04:25,410
Em ML, chamamos a amostragem
do conjunto de treinamento,

79
00:04:25,410 --> 00:04:28,025
extraída durante o treinamento,
de minilote,

80
00:04:28,025 --> 00:04:32,175
e essa variante do gradiente descendente,
de gradiente descendente de minilote.

81
00:04:32,798 --> 00:04:35,281
As amostras são chamadas de lotes.

82
00:04:36,550 --> 00:04:41,230
Além de poupar tempo, o gradiente descendente
de minilote tem os benefícios adicionais

83
00:04:41,230 --> 00:04:45,030
de usar menos memória e
de ser fácil de reproduzir em paralelo.

84
00:04:46,210 --> 00:04:51,215
Talvez você escute o termo
gradiente descendente em lote.

85
00:04:51,215 --> 00:04:54,585
Nesse caso,
ele se refere a processamento em lote.

86
00:04:54,585 --> 00:04:58,850
Logo, o gradiente descendente em lote calcula
o gradiente em todo o conjunto de dados.

87
00:04:58,850 --> 00:05:02,440
Definitivamente, não é o mesmo
que gradiente descendente de minilote.

88
00:05:02,440 --> 00:05:06,100
Agora, estamos falando de
gradiente descendente de minilote.

89
00:05:06,100 --> 00:05:10,845
Pode ser confuso, mas tamanho de minilote
é muitas vezes chamado de tamanho de lote.

90
00:05:10,845 --> 00:05:12,700
É o termo que o TensorFlow usa.

91
00:05:12,700 --> 00:05:15,435
Portanto, é o termo que usaremos também.

92
00:05:15,435 --> 00:05:17,415
No restante da especialização,

93
00:05:17,415 --> 00:05:18,940
ao mencionarmos tamanho de lote,

94
00:05:18,940 --> 00:05:23,171
estaremos falando do tamanho das amostras
no gradiente descendente de minilote.

95
00:05:24,275 --> 00:05:26,835
Qual tamanho esses minilotes devem ter?

96
00:05:26,835 --> 00:05:28,620
Assim como a taxa de aprendizado,

97
00:05:28,620 --> 00:05:30,880
o tamanho do lote é outro hiperparâmetro.

98
00:05:30,880 --> 00:05:33,550
Assim, o valor ideal depende do problema

99
00:05:33,550 --> 00:05:36,310
e pode ser encontrado
por meio do ajuste do hiperparâmetro,

100
00:05:36,310 --> 00:05:38,125
que abordaremos mais tarde.

101
00:05:38,125 --> 00:05:42,260
Normalmente, o tamanho do lote
é de 10 a 100 exemplos.

102
00:05:42,260 --> 00:05:44,470
Assim como a taxa de aprendizado,

103
00:05:44,470 --> 00:05:47,405
o tamanho do lote é outro hiperparâmetro
e, assim,

104
00:05:47,405 --> 00:05:52,080
o valor ideal depende do problema e pode ser
encontrado com o ajuste do hiperparâmetro,

105
00:05:52,080 --> 00:05:54,260
que abordaremos mais tarde.

106
00:05:54,260 --> 00:05:58,295
Normalmente, o tamanho do lote
é de 10 a 1.000 exemplos.

107
00:05:58,295 --> 00:06:00,455
O outro aspecto que podemos ajustar

108
00:06:00,455 --> 00:06:04,625
para acelerar o modelo de treinamento
é a frequência de verificação da perda.

109
00:06:04,625 --> 00:06:09,255
Lembre-se de que apenas checar a perda 
em um subconjunto de dados

110
00:06:09,255 --> 00:06:11,960
não é uma boa ideia.

111
00:06:11,960 --> 00:06:13,740
A implementação é bastante simples.

112
00:06:13,740 --> 00:06:15,215
Introduzimos um pouco de lógica

113
00:06:15,215 --> 00:06:19,735
para que a nossa função de perda
faça avaliações com frequência reduzida.

114
00:06:19,735 --> 00:06:23,305
Algumas estratégias muito utilizadas
para a função readyToUpdateLoss

115
00:06:23,305 --> 00:06:25,435
são baseadas em tempo e passos.

116
00:06:25,435 --> 00:06:28,295
Por exemplo, uma vez a cada mil passos

117
00:06:28,295 --> 00:06:30,215
ou uma vez a cada 30 minutos.

118
00:06:30,215 --> 00:06:33,430
Com a redução da frequência
de verificação da perda

119
00:06:33,430 --> 00:06:35,385
e a introdução de minilotes,

120
00:06:35,385 --> 00:06:39,260
começamos a separar os dois elementos
fundamentais do treinamento de modelos:

121
00:06:39,260 --> 00:06:41,100
a alteração de parâmetros do modelo e

122
00:06:41,100 --> 00:06:44,410
a verificação para constatar
quando as alterações certas foram feitas.