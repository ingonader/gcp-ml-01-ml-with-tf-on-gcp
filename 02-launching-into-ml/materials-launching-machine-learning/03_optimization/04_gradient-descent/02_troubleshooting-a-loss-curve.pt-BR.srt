1
00:00:00,550 --> 00:00:02,357
Antes de ver
uma das primeiras maneiras

2
00:00:02,357 --> 00:00:04,455
como os pesquisadores
trataram esse problema,

3
00:00:04,455 --> 00:00:07,370
vejamos
alguns pontos que aprendemos.

4
00:00:08,240 --> 00:00:10,355
Vamos nos colocar no
lugar do nosso modelo

5
00:00:10,355 --> 00:00:13,890
e ver como a perda pode mudar
com o tempo durante o treinamento.

6
00:00:15,330 --> 00:00:17,995
Imagine que estejamos realizando
um gradiente descendente

7
00:00:17,995 --> 00:00:21,865
e atualizando nossos parâmetros de modelo
em relação à derivada da função de perda

8
00:00:21,865 --> 00:00:26,165
e que configuramos itens que nos permitem
acompanhar a perda ao longo do tempo.

9
00:00:27,495 --> 00:00:29,950
Esse é um cenário comum
no aprendizado de máquina,

10
00:00:29,950 --> 00:00:32,675
principalmente quando o
treinamento de modelo inclui horas

11
00:00:32,675 --> 00:00:34,300
ou possivelmente até dias.

12
00:00:34,300 --> 00:00:37,565
Imagine o quanto é importante
não perder dias de trabalho.

13
00:00:38,685 --> 00:00:41,585
Com isso em mente, vamos
resolver uma curva de perda.

14
00:00:43,345 --> 00:00:45,715
Veja um formato comum
de curva de perda.

15
00:00:45,715 --> 00:00:49,210
A perda cai rápido com
grandes passos abaixo do gradiente

16
00:00:49,210 --> 00:00:52,370
e, depois, suaviza ao longo do tempo
com passos menores,

17
00:00:52,370 --> 00:00:54,760
pois atinge
um mínimo na superfície de perda.

18
00:00:57,030 --> 00:00:59,675
E se você vir uma
curva de perda como esta?

19
00:01:01,135 --> 00:01:04,775
Por um momento, pressuponha
que a escala do eixo de perda é grande.

20
00:01:04,775 --> 00:01:06,770
O que isso lhe diz 
sobre seu modelo

21
00:01:06,770 --> 00:01:09,960
e a maneira como sua pesquisa
está ocorrendo na superfície da perda?

22
00:01:11,390 --> 00:01:14,340
Mostra que nossa pesquisa está oscilando

23
00:01:14,340 --> 00:01:16,790
e não está fazendo um progresso contínuo

24
00:01:16,790 --> 00:01:18,440
em direção a um mínimo específico.

25
00:01:20,140 --> 00:01:21,590
E esta curva de perda?

26
00:01:22,760 --> 00:01:25,630
Essa significa que provavelmente
ainda estamos no mesmo vale

27
00:01:25,630 --> 00:01:27,770
e que levaremos um bom tempo

28
00:01:27,770 --> 00:01:29,270
para alcançar a parte inferior.

29
00:01:31,190 --> 00:01:33,160
De qualquer maneira,
nos dois casos

30
00:01:33,160 --> 00:01:36,120
o tamanho do passo não estava
certo para o problema específico.

31
00:01:36,120 --> 00:01:38,715
No primeiro caso,
o passo era muito grande.

32
00:01:38,715 --> 00:01:40,715
No segundo, muito pequeno.

33
00:01:41,465 --> 00:01:44,375
Precisamos de um parâmetro de escala.

34
00:01:44,375 --> 00:01:47,775
Na literatura, isso é chamado
de taxa de aprendizado.

35
00:01:47,775 --> 00:01:52,310
E, com sua introdução ao nosso código,
temos um gradiente descendente clássico.

36
00:01:52,310 --> 00:01:57,155
Veja como mudei a linha onde fiz o loop
para atualizar os valores dos parâmetros.

37
00:01:57,155 --> 00:02:01,640
Imagine usar a força bruta para descobrir
o melhor valor da taxa de aprendizado.

38
00:02:01,640 --> 00:02:05,625
Mas, lembre-se de que a taxa pode ter um
valor específico melhor para o problema.

39
00:02:06,485 --> 00:02:08,955
Como ela é informada antes do
início do aprendizado,

40
00:02:08,955 --> 00:02:11,185
aprender a taxa é um hiperparâmetro.

41
00:02:11,185 --> 00:02:13,850
E, para determinar o melhor
valor para hiperparâmetros,

42
00:02:13,850 --> 00:02:17,980
há um método melhor disponível
chamado de ajuste de hiperparâmetro.

43
00:02:17,980 --> 00:02:21,970
Veremos como fazer isso no Cloud ML
Engine em um próximo módulo.

44
00:02:21,970 --> 00:02:26,415
Porém, em geral, a taxa de aprendizado é
uma fração bem menor que um.

45
00:02:26,415 --> 00:02:30,565
Por enquanto, basta saber esta
fórmula de gradiente descendente

46
00:02:30,565 --> 00:02:34,140
e que a taxa de aprendizado é um 
hiperparâmetro fixado no treinamento.