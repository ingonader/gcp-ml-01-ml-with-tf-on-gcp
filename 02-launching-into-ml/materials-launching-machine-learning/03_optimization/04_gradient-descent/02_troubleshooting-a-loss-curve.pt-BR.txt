Antes de ver
uma das primeiras maneiras como os pesquisadores
trataram esse problema, vejamos
alguns pontos que aprendemos. Vamos nos colocar no
lugar do nosso modelo e ver como a perda pode mudar
com o tempo durante o treinamento. Imagine que estejamos realizando
um gradiente descendente e atualizando nossos parâmetros de modelo
em relação à derivada da função de perda e que configuramos itens que nos permitem
acompanhar a perda ao longo do tempo. Esse é um cenário comum
no aprendizado de máquina, principalmente quando o
treinamento de modelo inclui horas ou possivelmente até dias. Imagine o quanto é importante
não perder dias de trabalho. Com isso em mente, vamos
resolver uma curva de perda. Veja um formato comum
de curva de perda. A perda cai rápido com
grandes passos abaixo do gradiente e, depois, suaviza ao longo do tempo
com passos menores, pois atinge
um mínimo na superfície de perda. E se você vir uma
curva de perda como esta? Por um momento, pressuponha
que a escala do eixo de perda é grande. O que isso lhe diz 
sobre seu modelo e a maneira como sua pesquisa
está ocorrendo na superfície da perda? Mostra que nossa pesquisa está oscilando e não está fazendo um progresso contínuo em direção a um mínimo específico. E esta curva de perda? Essa significa que provavelmente
ainda estamos no mesmo vale e que levaremos um bom tempo para alcançar a parte inferior. De qualquer maneira,
nos dois casos o tamanho do passo não estava
certo para o problema específico. No primeiro caso,
o passo era muito grande. No segundo, muito pequeno. Precisamos de um parâmetro de escala. Na literatura, isso é chamado
de taxa de aprendizado. E, com sua introdução ao nosso código,
temos um gradiente descendente clássico. Veja como mudei a linha onde fiz o loop
para atualizar os valores dos parâmetros. Imagine usar a força bruta para descobrir
o melhor valor da taxa de aprendizado. Mas, lembre-se de que a taxa pode ter um
valor específico melhor para o problema. Como ela é informada antes do
início do aprendizado, aprender a taxa é um hiperparâmetro. E, para determinar o melhor
valor para hiperparâmetros, há um método melhor disponível
chamado de ajuste de hiperparâmetro. Veremos como fazer isso no Cloud ML
Engine em um próximo módulo. Porém, em geral, a taxa de aprendizado é
uma fração bem menor que um. Por enquanto, basta saber esta
fórmula de gradiente descendente e que a taxa de aprendizado é um 
hiperparâmetro fixado no treinamento.