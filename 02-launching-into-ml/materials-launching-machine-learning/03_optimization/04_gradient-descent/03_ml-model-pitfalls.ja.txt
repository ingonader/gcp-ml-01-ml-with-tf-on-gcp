機械学習を実践した時に
よく起こる状況として 同様の出力を期待して
モデルコードを再実行したのに 出力が異なるという状況があります プログラマーは 決定論的な環境での
作業に慣れていることが多いですが MLにはこれが当てはまらないことがあります 多くのモデルで 2回目の再トレーニングを行うと ハイパーパラメータの設定は同じでも 結果として得られるパラメータ設定が
大きく異なる場合があります これには当惑してしまうかもしれません 最適なパラメータセットが探索できていない？ 勾配降下法が機能していない？ または 実施方法が間違っているのでしょうか？ いいえ そうとは限りません 探索しているのが左側のような損失曲面でなく 右側のような損失曲面であることが
考えられます 左側の損失曲面には最小値が1つなのに対し 右側には複数の最小値が存在します この特性は正式には凸性と呼ばれます 左側は凸曲面 右側は非凸曲面です なぜMLモデルの損失曲面に
複数の最小値が存在するのでしょうか それはつまり パラメータ領域内に
等しいまたは等しいに近い点 つまり 同等の予測力を備えたモデルを作り出す
パラメータ設定が多数存在しているからです これについては 後ほど 際たる例として取り上げる
ニューラルネットワークの箇所で再度説明します 今はまだわからなくても問題ありません 今は 損失曲面によって最小値の数が
異なることだけ押さえておきましょう 誰もがモデルがトレーニングを終えるまでの
待ち時間を短くしたいと考えます モデル トレーニングをさらに高速化する
方法はあるのでしょうか 答えはイエスです ただし 選択肢を理解するため アルゴリズムの大まかなステップと
時間計算量の要因を見ていきます こちらは アルゴリズムの
3つの重要なステップです 微分係数を計算するとき 計算コストは 損失関数に入力するデータポイントの数と モデルのパラメータの数に比例します 実際 モデルのパラメータ数は
数十から数億までさまざまです 同様にデータセットの数も
数千から数千億までさまざまです モデルのパラメータを更新する場合 更新は各ループで1回行われ コストはモデルのパラメータ数
のみによって決まります ただし 一般に 更新のコストは
その他のステップに比べて小さくなります 最後に 損失の確認です このステップの時間計算量は 損失の測定に使用するデータポイントの数と
モデルの複雑さに比例します このプロセスをループとして説明しましたが 損失の確認は毎回行う必要はありません 理由は 損失関数における変更の大半が
インクリメンタルだからです トレーニング時間を短縮するには
何を変えればよいのでしょうか 正則化に関するモジュールでは モデル内で
影響を受けるパラメータ数の違いに触れますが 通常 このパラメータ数は決まっています また 損失を確認するのに
使用するデータポイントの数を 減らせばよいと思うかもしれませんが 通常は推奨されません 学習時間を短縮するために
調整すべき項目は大きく2つあります 微分係数を計算するデータポイントの数と 損失を確認する頻度です 1つ目の項目は 微分係数を計算するデータポイントの数です 微分係数は損失関数から求められるもので 損失関数はいくつもの予測の誤差を
まとめて求めるものです したがって この方法では基本的に
アルゴリズムが反復されるたびに 損失関数に入力される
データポイントの数が減少します 数を減らしても機能する
理由を考えてみましょう それは 学習データの中から 平均して相殺し合う標本を
抽出することが可能だからです 標本抽出の落とし穴とその回避方法については
後のモジュールで詳しく説明します 今は 標本抽出ストラテジーによって
一様な確率で学習セットが選択されるため トレーニングセットのどのインスタンスも
モデルに選ばれる確率が等しいことを 押さえておきましょう MLでは このようにトレーニング中に トレーニングセットから標本を抽出することを
ミニバッチと呼び ミニバッチを使った勾配降下法を
ミニバッチ勾配降下法と呼びます 標本自体はバッチと呼ばれます ミニバッチ勾配降下法には
時間コストの削減の他にも メモリ使用量が少なく
並列処理が容易という利点があります 余談ですが バッチ勾配降下法という
言葉を聞いたことがあるかもしれません ここでのバッチは バッチ処理を指します したがって バッチ勾配降下法では
データセット全体の勾配を計算します ミニバッチ勾配降下法とはまったく別物です ここで取り上げているのは
ミニバッチ勾配降下法です まぎらわしいことに ミニバッチのサイズは
多くの場合 バッチサイズと呼ばれています TensorFlowでもそう呼んでいますので ここでもバッチサイズと呼ぶことにします 実際のところ MLでバッチサイズと言うときは ミニバッチ勾配降下法の標本サイズを指します ミニバッチのサイズはどれくらいに
設定すればよいでしょうか 学習率と同様に バッチサイズもハイパーパラメータです このため 最適値は問題ごとに異なり 後ほど説明するハイパーパラメータチューニング
によって求めることができます 通常 バッチサイズは10～100の間です 学習率と同様に バッチサイズもハイパーパラメータです このため 最適値は問題ごとに異なり 後ほど説明する ハイパーパラメータチューニングによって
求めることができます 通常 バッチサイズは10～1000の間です モデルの学習を高速化するために
調整できる2つ目の項目は 損失を確認する頻度です 一部のデータの損失を確認すればよいと
思われるかもしれませんが それは好ましくありません 実施方法はごくシンプルで 時間コストの大きい
computeLoss関数の実行頻度を下げる ロジックを組み込みます readyToUpdateLoss関数で
よく使用されるストラテジーは 時間ベースとステップベースです たとえば 1,000ステップごと または 30分ごとなどです 損失を確認する頻度を減らし ミニバッチを使用することで モデルの学習に欠かせない モデルのパラメータの変更と 適切な変更が行われたかの確認という 2つの要素を切り離して考えました