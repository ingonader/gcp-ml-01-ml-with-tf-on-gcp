1
00:00:00,750 --> 00:00:04,155
機械学習を実践した時に
よく起こる状況として

2
00:00:04,155 --> 00:00:09,065
同様の出力を期待して
モデルコードを再実行したのに

3
00:00:09,065 --> 00:00:11,550
出力が異なるという状況があります

4
00:00:11,550 --> 00:00:15,265
プログラマーは 決定論的な環境での
作業に慣れていることが多いですが

5
00:00:15,265 --> 00:00:18,015
MLにはこれが当てはまらないことがあります

6
00:00:18,015 --> 00:00:21,070
多くのモデルで 2回目の再トレーニングを行うと

7
00:00:21,070 --> 00:00:23,755
ハイパーパラメータの設定は同じでも

8
00:00:23,755 --> 00:00:27,475
結果として得られるパラメータ設定が
大きく異なる場合があります

9
00:00:27,475 --> 00:00:30,120
これには当惑してしまうかもしれません

10
00:00:30,120 --> 00:00:33,060
最適なパラメータセットが探索できていない？

11
00:00:33,060 --> 00:00:35,110
勾配降下法が機能していない？

12
00:00:35,110 --> 00:00:38,090
または 実施方法が間違っているのでしょうか？

13
00:00:38,090 --> 00:00:40,360
いいえ そうとは限りません

14
00:00:40,360 --> 00:00:44,010
探索しているのが左側のような損失曲面でなく

15
00:00:44,010 --> 00:00:47,585
右側のような損失曲面であることが
考えられます

16
00:00:47,585 --> 00:00:51,465
左側の損失曲面には最小値が1つなのに対し

17
00:00:51,465 --> 00:00:54,300
右側には複数の最小値が存在します

18
00:00:54,300 --> 00:00:58,015
この特性は正式には凸性と呼ばれます

19
00:00:58,015 --> 00:01:03,795
左側は凸曲面 右側は非凸曲面です

20
00:01:03,795 --> 00:01:07,490
なぜMLモデルの損失曲面に
複数の最小値が存在するのでしょうか

21
00:01:07,490 --> 00:01:13,300
それはつまり パラメータ領域内に
等しいまたは等しいに近い点

22
00:01:13,340 --> 00:01:18,350
つまり 同等の予測力を備えたモデルを作り出す
パラメータ設定が多数存在しているからです

23
00:01:18,395 --> 00:01:20,660
これについては

24
00:01:20,660 --> 00:01:24,995
後ほど 際たる例として取り上げる
ニューラルネットワークの箇所で再度説明します

25
00:01:24,995 --> 00:01:27,480
今はまだわからなくても問題ありません

26
00:01:27,480 --> 00:01:33,900
今は 損失曲面によって最小値の数が
異なることだけ押さえておきましょう

27
00:01:33,910 --> 00:01:39,390
誰もがモデルがトレーニングを終えるまでの
待ち時間を短くしたいと考えます

28
00:01:39,390 --> 00:01:43,120
モデル トレーニングをさらに高速化する
方法はあるのでしょうか

29
00:01:43,120 --> 00:01:44,885
答えはイエスです

30
00:01:44,885 --> 00:01:47,000
ただし 選択肢を理解するため

31
00:01:47,000 --> 00:01:51,755
アルゴリズムの大まかなステップと
時間計算量の要因を見ていきます

32
00:01:51,755 --> 00:01:55,675
こちらは アルゴリズムの
3つの重要なステップです

33
00:01:56,395 --> 00:01:58,245
微分係数を計算するとき

34
00:01:58,245 --> 00:02:00,500
計算コストは

35
00:02:00,500 --> 00:02:03,235
損失関数に入力するデータポイントの数と

36
00:02:03,235 --> 00:02:06,315
モデルのパラメータの数に比例します

37
00:02:06,315 --> 00:02:11,855
実際 モデルのパラメータ数は
数十から数億までさまざまです

38
00:02:11,855 --> 00:02:17,640
同様にデータセットの数も
数千から数千億までさまざまです

39
00:02:17,640 --> 00:02:20,670
モデルのパラメータを更新する場合

40
00:02:20,670 --> 00:02:23,270
更新は各ループで1回行われ

41
00:02:23,270 --> 00:02:27,025
コストはモデルのパラメータ数
のみによって決まります

42
00:02:27,025 --> 00:02:31,890
ただし 一般に 更新のコストは
その他のステップに比べて小さくなります

43
00:02:31,890 --> 00:02:34,810
最後に 損失の確認です

44
00:02:34,810 --> 00:02:37,540
このステップの時間計算量は

45
00:02:37,540 --> 00:02:43,445
損失の測定に使用するデータポイントの数と
モデルの複雑さに比例します

46
00:02:43,445 --> 00:02:46,935
このプロセスをループとして説明しましたが

47
00:02:46,935 --> 00:02:50,695
損失の確認は毎回行う必要はありません

48
00:02:50,695 --> 00:02:56,155
理由は 損失関数における変更の大半が
インクリメンタルだからです

49
00:02:57,395 --> 00:03:00,370
トレーニング時間を短縮するには
何を変えればよいのでしょうか

50
00:03:00,370 --> 00:03:05,910
正則化に関するモジュールでは モデル内で
影響を受けるパラメータ数の違いに触れますが

51
00:03:05,910 --> 00:03:09,455
通常 このパラメータ数は決まっています

52
00:03:09,455 --> 00:03:13,520
また 損失を確認するのに
使用するデータポイントの数を

53
00:03:13,520 --> 00:03:16,165
減らせばよいと思うかもしれませんが

54
00:03:16,165 --> 00:03:18,310
通常は推奨されません

55
00:03:18,750 --> 00:03:22,830
学習時間を短縮するために
調整すべき項目は大きく2つあります

56
00:03:22,830 --> 00:03:26,020
微分係数を計算するデータポイントの数と

57
00:03:26,020 --> 00:03:28,600
損失を確認する頻度です

58
00:03:28,600 --> 00:03:30,970
1つ目の項目は

59
00:03:30,970 --> 00:03:35,495
微分係数を計算するデータポイントの数です

60
00:03:35,495 --> 00:03:38,750
微分係数は損失関数から求められるもので

61
00:03:38,750 --> 00:03:42,800
損失関数はいくつもの予測の誤差を
まとめて求めるものです

62
00:03:42,800 --> 00:03:47,070
したがって この方法では基本的に
アルゴリズムが反復されるたびに

63
00:03:47,070 --> 00:03:51,110
損失関数に入力される
データポイントの数が減少します

64
00:03:51,110 --> 00:03:55,125
数を減らしても機能する
理由を考えてみましょう

65
00:03:57,045 --> 00:03:58,410
それは 学習データの中から

66
00:03:58,410 --> 00:04:03,320
平均して相殺し合う標本を
抽出することが可能だからです

67
00:04:05,110 --> 00:04:09,405
標本抽出の落とし穴とその回避方法については
後のモジュールで詳しく説明します

68
00:04:09,410 --> 00:04:15,000
今は 標本抽出ストラテジーによって
一様な確率で学習セットが選択されるため

69
00:04:15,000 --> 00:04:19,030
トレーニングセットのどのインスタンスも
モデルに選ばれる確率が等しいことを

70
00:04:19,030 --> 00:04:21,660
押さえておきましょう

71
00:04:21,660 --> 00:04:24,150
MLでは このようにトレーニング中に

72
00:04:24,150 --> 00:04:27,725
トレーニングセットから標本を抽出することを
ミニバッチと呼び

73
00:04:27,725 --> 00:04:32,615
ミニバッチを使った勾配降下法を
ミニバッチ勾配降下法と呼びます

74
00:04:32,615 --> 00:04:36,100
標本自体はバッチと呼ばれます

75
00:04:36,670 --> 00:04:40,530
ミニバッチ勾配降下法には
時間コストの削減の他にも

76
00:04:40,530 --> 00:04:45,660
メモリ使用量が少なく
並列処理が容易という利点があります

77
00:04:46,070 --> 00:04:51,315
余談ですが バッチ勾配降下法という
言葉を聞いたことがあるかもしれません

78
00:04:51,315 --> 00:04:54,585
ここでのバッチは バッチ処理を指します

79
00:04:54,585 --> 00:04:59,340
したがって バッチ勾配降下法では
データセット全体の勾配を計算します

80
00:04:59,340 --> 00:05:02,440
ミニバッチ勾配降下法とはまったく別物です

81
00:05:02,440 --> 00:05:06,100
ここで取り上げているのは
ミニバッチ勾配降下法です

82
00:05:06,100 --> 00:05:10,845
まぎらわしいことに ミニバッチのサイズは
多くの場合 バッチサイズと呼ばれています

83
00:05:10,845 --> 00:05:13,680
TensorFlowでもそう呼んでいますので

84
00:05:13,680 --> 00:05:16,195
ここでもバッチサイズと呼ぶことにします

85
00:05:16,195 --> 00:05:20,205
実際のところ MLでバッチサイズと言うときは

86
00:05:20,205 --> 00:05:23,740
ミニバッチ勾配降下法の標本サイズを指します

87
00:05:23,740 --> 00:05:26,835
ミニバッチのサイズはどれくらいに
設定すればよいでしょうか

88
00:05:26,835 --> 00:05:28,620
学習率と同様に

89
00:05:28,620 --> 00:05:31,450
バッチサイズもハイパーパラメータです

90
00:05:31,450 --> 00:05:34,240
このため 最適値は問題ごとに異なり

91
00:05:34,240 --> 00:05:38,940
後ほど説明するハイパーパラメータチューニング
によって求めることができます

92
00:05:39,055 --> 00:05:42,320
通常 バッチサイズは10～100の間です

93
00:05:42,320 --> 00:05:44,470
学習率と同様に

94
00:05:44,470 --> 00:05:47,405
バッチサイズもハイパーパラメータです

95
00:05:47,405 --> 00:05:50,180
このため 最適値は問題ごとに異なり

96
00:05:50,180 --> 00:05:52,130
後ほど説明する

97
00:05:52,130 --> 00:05:55,575
ハイパーパラメータチューニングによって
求めることができます

98
00:05:55,575 --> 00:05:58,685
通常 バッチサイズは10～1000の間です

99
00:05:58,685 --> 00:06:02,185
モデルの学習を高速化するために
調整できる2つ目の項目は

100
00:06:02,185 --> 00:06:04,325
損失を確認する頻度です

101
00:06:04,325 --> 00:06:09,065
一部のデータの損失を確認すればよいと
思われるかもしれませんが

102
00:06:09,065 --> 00:06:11,240
それは好ましくありません

103
00:06:11,980 --> 00:06:13,740
実施方法はごくシンプルで

104
00:06:13,740 --> 00:06:17,415
時間コストの大きい
computeLoss関数の実行頻度を下げる

105
00:06:17,415 --> 00:06:19,675
ロジックを組み込みます

106
00:06:19,675 --> 00:06:23,005
readyToUpdateLoss関数で
よく使用されるストラテジーは

107
00:06:23,005 --> 00:06:25,435
時間ベースとステップベースです

108
00:06:25,435 --> 00:06:28,295
たとえば 1,000ステップごと

109
00:06:28,295 --> 00:06:30,525
または 30分ごとなどです

110
00:06:30,525 --> 00:06:33,190
損失を確認する頻度を減らし

111
00:06:33,190 --> 00:06:35,385
ミニバッチを使用することで

112
00:06:35,385 --> 00:06:36,920
モデルの学習に欠かせない

113
00:06:36,920 --> 00:06:38,760
モデルのパラメータの変更と

114
00:06:38,760 --> 00:06:41,330
適切な変更が行われたかの確認という

115
00:06:41,330 --> 00:06:44,000
2つの要素を切り離して考えました