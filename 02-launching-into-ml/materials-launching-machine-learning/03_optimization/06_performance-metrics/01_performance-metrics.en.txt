In the previous section, we train models in our browsers using grading descent. And the models that we created, were able to learn complex non-linear relationships using a learned hierarchy of features. However, we discovered at the end of the section that our current approach suffers from problems. The consequences of which include things like long training times, suboptimal minima, and inappropriate minima. In this section, we'll review what exactly an inappropriate minimum is, why they exist, and how performance metrics help us get better results. So, what is an inappropriate minimum? You can think of them as points in parameter space that reflect strategies, either that won't generalize well, that don't reflect the true relationship being modeled, or both. For example, say we're training a model to predict whether a parking spot is vacant from an image of the parking lot. One inappropriate strategy will be to simply predict that every space is occupied. With a dataset composed of an equal number of positive and negative examples, such a strategy would never survive the optimization process. However, when datasets are skewed and contain far more of one class than another, then suddenly strategies like this can become much more seductive. Such a strategy doesn't make an effort to understand the true relationship between the features and the label, which we would expect would have something to do with the visual characteristics of an empty space. Consequently, it won't generalize well to new parking lots where the underlying relationship will be the same, but the proportion of vacant spots may not be. It's tempting to think of the existence of inappropriate minima as a problem with our loss function. If only we had the perfect loss function, one that rewarded the truly best strategies and penalize the bad ones, then life would be grand. Sadly, this just is not possible. There will always be a gap between the metrics we care about, and the metrics that work well with gradient descent. For example, let's assume we're still classifying parking spaces. A seemingly perfect loss function would minimize the number of incorrect predictions. However, such a loss function would be piecewise. That is, the range of values it could take would be integers and not real numbers. And surprisingly, this is problematic. The issue boils down to differentiability. Gradient descent makes incremental changes to our weights. This in turn requires that we can differentiate the weights with respect to the loss. Piecewise functions however, have gaps in their ranges. And while TensorFlow can differentiate them, the resulting loss surface will have discontinuities that will make it much more challenging to traverse. So, we need to reframe the problem. Instead of searching for the perfect loss function during training, we're instead going to use a new sort of metric after training is complete. And this new sort of metric will allow us to reject models that have settled into inappropriate minima. Such metrics are called performance metrics. Performance metrics have two benefits over loss functions. Firstly, they're easier to understand. This is because they're often simple combinations of countable statistics. Secondly, performance metrics are directly connected to business goals. This is a sadder point, but it boils down to the fact that while loss and the business goal that is being sought will often agree, they won't always agree. Sometimes, it will be possible to lower loss, or making little progress toward the business goal. We'll review three performance metrics; confusion matrices, precision, and recall, and when you want to use them.