Probablemente ya vieron
esta matriz cuando analizamos el AA inclusivo
y el reconocimiento facial. En ese ejemplo, analizamos un modelo de AA de detección de rostros que predecía
incorrectamente una estatua como un rostro lo que se denomina como falso positivo. Y no reconoció un rostro en el conjunto
de datos porque estaba oculto por la ropa a esto se le llama falso negativo. Una matriz de confusión
como esta nos permitirá evaluar de forma cuantificable el rendimiento
de nuestro modelo de clasificación. Ahora tenemos cuatro números uno para cada cuadrante y quienes toman
decisiones comerciales solo quieren uno. ¿Cuál presentamos? Para explorar más esto veamos otro ejemplo
de clasificación de fotos. Si sabemos que hay
un estacionamiento disponible que está etiquetado como positivo y el modelo también
predice que está disponible lo llamamos un verdadero positivo. Si sabemos que
el estacionamiento no está disponible pero el modelo predice que sí lo llamamos un falso
positivo o error de tipo 1. Para comparar cómo le fue a nuestro
modelo con sus predicciones positivas usaremos una métrica llamada precisión. Con una alta precisión, puedo decir
que hay un estacionamiento disponible estoy seguro de ello. Una precisión de 1.0 significa que
los espacios disponibles que identifiqué están todos disponibles. Pero es posible que haya omitido otros
espacios disponibles, o falsos negativos. La precisión se define
formalmente como la cantidad de verdaderos positivos dividida
por la cantidad clasificada como positiva. Si vemos la matriz, ¿el aumento
de qué factor disminuiría la precisión? Un aumento en los falsos positivos. En nuestro ejemplo de estacionamientos mientras más espacios considere disponibles el modelo
sin estarlo, menor es la precisión. La recuperación tiene
una relación inversa con la precisión. Con una alta recuperación, puedo
encontrar muchos espacios disponibles. Con una recuperación de 1.0 encontraría
los estacionamientos disponibles, 10 de 10 pero también podría tener muchos espacios
que pensé que estaban disponible, pero no. Estos se llaman falsos positivos. ¿Cuál fue la recuperación
de nuestro ejemplo de estacionamiento? Recuerden, teníamos
10 estacionamientos disponibles y nuestro modelo
identificó uno solo disponible. La respuesta es 1 de 10 o 0.1. Aquí se les presenta
un conjunto de imágenes. En las imágenes hay un gato o no lo hay. Tomen un momento
para identificar cuál es cuál. Con suerte, encontrarán todos
los gatos domésticos que se muestran aquí. Observen que el gato
oculto en el cuadrado rojo y el tigre no califican
como gatos para nuestros fines. Ahora, veamos cómo clasifica el modelo. Y este es el resultado del modelo. Comparemos los resultados
con lo que sabemos que es verdad. Tenemos nuestros puntos de datos
junto a las predicciones del modelo. En total, tenemos ocho ejemplos o instancias que le mostramos al modelo. ¿Cuántas veces acertó el modelo? Predijo correctamente
tres de un total de ocho. Esto le da una exactitud de 0.375. ¿Es la exactitud la mejor métrica
para describir el rendimiento del modelo? Antes de seguir avanzando analicemos un obstáculo común. Ahora repasaremos
nuestro ejemplo de gato y no gato ¿cuál es la precisión del modelo? Las cinco imágenes
estaban en la clase positiva. ¿Cuántos son gatos domésticos? Dos de cinco
o una tasa de precisión de 0.4. La recuperación es como alguien que
no quiere quedarse fuera de la decisión. Aquí pueden ver todos los ejemplos
correctamente etiquetados de gatos y el rendimiento del modelo
para ellos. ¿Cuál fue la recuperación? Dicho de otra forma, ¿cuántos
verdaderos positivos acertó el modelo? El modelo solo obtuvo 2 de 4 gatos
correctos para una recuperación de 0.5. Resumamos lo que aprendieron
sobre optimización hasta ahora. Primero, definimos modelos de AA como conjuntos
de parámetros e hiperparámetros y tratamos de enmarcar la optimización
como búsqueda en el espacio de parámetros. Luego, presentamos
las funciones de pérdida que es cómo medimos y evaluamos el rendimiento de nuestro
modelo en cada paso del entrenamiento. Dos ejemplos de funciones
de pérdidas que analizamos fueron RMSE para la regresión lineal y entropía
cruzada para la tarea de clasificación. Aprendimos cómo diversificar
nuestras superficies de pérdida con el análisis de pendientes
de nuestras funciones de pérdida que nos proporcionaron
la dirección y la magnitud del paso. Este proceso
se llama descenso de gradientes. Experimentamos con diferentes modelos
de AA en TensorFlow Playground y vimos y vimos cómo los modelos lineales pueden aprender relaciones no lineales
cuando se les asignan atributos no lineales y cómo las redes neuronales
aprenden las jerarquías de atributos También vimos cómo los hiperparámetros tasa de aprendizaje y tamaño del lote
influyen en el descenso de gradientes. Luego, hablamos sobre cómo elegir
entre exactitud, precisión y recuperación con el rendimiento
de un modelo de clasificación según el problema que intentan solucionar. Y como vimos en este módulo nuestro conjunto de datos etiquetados
es el motor donde aprende nuestro modelo. En el siguiente módulo cubriremos cómo dividir el conjunto
de datos en entrenamiento y evaluación y los obstáculos que debemos evitar.