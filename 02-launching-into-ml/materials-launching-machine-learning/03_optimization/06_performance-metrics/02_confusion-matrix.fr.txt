Vous avez peut-être déjà vu cette matrice
lorsque nous avons abordé le ML inclusif et la reconnaissance faciale
dans un précédent cours. Dans cet exemple, un modèle de ML
de reconnaissance faciale faisait une prédiction incorrecte en
confondant une statue avec un visage humain. Ce cas de figure est un faux positif. Dans le même ensemble de données,
ce modèle n'avait pas non plus détecté un visage réel, assombri
par des vêtements d'hiver. Ce cas de figure est un faux négatif. Une matrice de confusion comme celle-ci, permet de quantifier les performances
de notre modèle de classification. Mais nous avons maintenant quatre chiffres,
un par case, et les décisionnaires de l'entreprise
n'en veulent qu'un. Lequel choisir ? Pour approfondir, regardons un autre exemple
de classification de photos. Si nous savons qu'une place
de stationnement est libre, donc si son étiquette est positive, et que le modèle prédit
également qu'elle est libre, nous parlons de vrai positif. Si nous savons qu'une place
de stationnement est occupée, mais que le modèle prédit qu'elle est libre, nous parlons de faux positif,
ou d'erreur de type 1. Afin d'évaluer le niveau d'exactitude
des prédictions positives du modèle, nous utilisons une métrique
qui s'appelle la précision. Avec une précision élevée, si je dis
qu'une place de stationnement est libre, je suis vraiment sûr qu'elle l'est. Une précision de 1 signifie que les places
libres que j'ai identifiées sont toutes réellement disponibles. Mais je pourrais ne pas avoir
identifié d'autres places libres : on les appelle "faux négatifs". Par définition, la précision correspond
au nombre de vrais positifs divisé par le nombre total d'éléments
classifiés comme positifs. Dans la matrice, quel facteur devrait
augmenter pour que la précision baisse ? Le nombre de faux positifs. Dans notre exemple du parking, plus le modèle prédit que des places
sont libres alors qu'elles ne le sont pas, plus la précision baisse. Le rappel est souvent
inversement lié à la précision. Un rappel élevé indique
que j'ai identifié un grand nombre de places
effectivement libres. Un rappel de 1 signifie
que j'ai trouvé 10 des 10 places libres, mais également que j'ai pu
identifié comme libres de nombreuses places ne l'étant pas. C'est ce que l'on appelle
des faux positifs. À combien s'élevait le rappel
dans notre exemple du parking ? Souvenez-vous que 10 places étaient libres, et que le modèle n'en avait identifié qu'une. La réponse est 1 sur 10, soit 0,1. Voici un ensemble de données
constitué d'images. Chaque image comporte ou
ne comporte pas un chat. Prenez un instant, et voyez si vous pouvez
les identifier. J'espère que vous avez trouvé
tous les chats domestiques indiqués ici. Notez bien qu'il y a un chat caché
encadré en rouge et que le tigre n'est pas classifié
comme un chat. Voyons maintenant quelle est
la classification établie par le modèle. Voici les résultats. Comparons-les à ce que nous savons être vrai. Voici nos points de données. Ils sont correctement étiquetés, côte à côte
avec les prédictions du modèle. Au total, nous avons montré huit exemples
(ou instances) au modèle. Dans combien de cas ses prédictions
sont-elles correctes ? Dans trois cas sur un total de huit. Cela donne une justesse de 0,375. La justesse est-elle la meilleure métrique
pour décrire les performances du modèle ? Avant de nous intéresser
à d'autres possibilités, commençons par examiner un piège courant. Revenons à notre exemple des chats. Quelle est la précision du modèle ? Ces cinq images étaient
dans la classe positive. Combien contiennent réellement
des chats domestiques ? Deux sur cinq, soit un taux
de précision de 0,4. Le rappel est comme une personne qui ne veut jamais rester
à l'écart d'une décision positive. Voici toutes les images étiquetées
comme contenant des chats que nous pouvons utiliser
pour évaluer les performances du modèle. À combien s'élevait le rappel ?
Ou, en d'autres termes, combien de vrais positifs
le modèle a-t-il pu identifier ? Seulement 2 sur 4, soit un rappel de 0,5. Récapitulons rapidement ce que vous avez
appris jusqu'à présent sur l'optimisation. Nous avons tout d'abord défini
les modèles de ML comme des ensembles
de paramètres et d'hyperparamètres, et nous avons tenté
d'envisager l'optimisation comme une recherche
dans l'espace des paramètres. Nous avons ensuite présenté
les fonctions de perte, qui permettent de quantifier et d'évaluer
les performances du modèle pour chaque pas de l'entraînement. Nous avons vu deux exemples
de fonctions de perte spécifiques : la RMSE pour la régression linéaire,
et l'entropie croisée pour la classification. Nous avons appris à diversifier
efficacement les surfaces de perte en analysant les pentes
des fonctions de perte, lesquelles indiquent la direction
et la magnitude des pas. Ce processus est appelé descente de gradient. Nous avons fait des tests
avec différents modèles de ML dans TensorFlow Playground,
et nous avons vu comment les modèles linéaires peuvent
apprendre des relations non linéaires en se basant sur des caractéristiques
non linéaires. Nous avons vu que les réseaux de neurones apprennent des hiérarchies
de caractéristiques, et comment les hyperparamètres
de taux d'apprentissage et de taille de lot affectent
la descente de gradient. Nous avons ensuite vu comment choisir
entre la justesse, la précision et le rappel pour évaluer les performances
d'un modèle de classification en fonction de la nature
du problème à résoudre. Vous avez pu constater que notre ensemble
de données d’entraînement étiqueté était déterminant pour l'entraînement
du modèle. Dans le prochain module, nous apprendrons
à répartir l'ensemble de données entre l'entraînement et l'évaluation, et nous verrons les pièges à éviter.