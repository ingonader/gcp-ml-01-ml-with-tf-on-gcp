Diese Matrix 
haben Sie vielleicht schon einmal gesehen, als wir in einem früheren Kurs 
ML und Gesichtserkennung besprochen haben. In diesem Beispiel haben wir ein ML-Modell
zur Gesichtserkennung betrachtet, das eine Statue falsch
als menschliches Gesicht einstufte, was als "falsch positiv" bezeichnet wird. Es übersah auch ein Gesicht im Datensatz, 
das durch Winterkleidung verdeckt wurde, dieser Fehler 
wird als "falsch negativ" bezeichnet. Mit einer solchen 
Wahrheitsmatrix können wir die Leistung eines Klassifizierungsmodells
quantifizierbar bewerten. Doch jetzt haben wir vier Zahlen, eine für jeden Quadranten, aber die
Entscheidungsträger wollen nur eine sehen. Welche präsentieren wir ihnen? Untersuchen wir dies ein wenig genauer und werfen einen Blick auf ein 
weiteres Beispiel der Fotoklassifizierung. Wenn wir wissen, 
dass ein Parkplatz frei ist, sein Label also "positiv" ist, und das Modell den
Platz auch als frei voraussagt, nennen wir das "richtig positiv". Wenn wir wissen,
dass ein Parkplatz besetzt ist, aber das Modell 
ihn als frei voraussagt, bezeichnen wir das als
"falsch positiv" oder als Typ-I-Fehler. Um zu vergleichen, wie gut unser
Modell positive Vorhersagen getroffen hat, verwenden wir den Messwert "Präzision". Hohe Präzision heißt: Ich bin
mir sicher, dass ein freier Parkplatz auch wirklich frei ist. Eine Präzision von 1,0
bedeutet, dass von den freien Plätzen, die ich identifiziert
habe, alle wirklich frei sind. Aber ich könnte freie Plätze übersehen
haben, sogenannte falsch Negative. Die formale Definition von Präzision ist:
die Anzahl der richtig Positiven geteilt durch die Gesamtzahl 
der als positiv Eingestuften. Betrachten Sie die Matrix. Wo würde 
ein höherer Wert die Präzision verringern? Eine Zunahme von 
falsch positiven Ergebnissen. Je mehr freie Plätze das Modell 
im Parkplatzbeispiel voraussagt, die in Wahrheit nicht frei sind, desto geringer ist die Präzision. Recall ist oft 
umgekehrt proportional zur Präzision. Der Recall ist höher, je mehr 
tatsächlich freie Parkplätze ich finde. Ein Recall von 1,0 bedeutet, dass ich
10 von 10 freien Plätzen gefunden habe. Ich könnte aber auch viele "freie" Plätze
gefunden haben, die gar nicht frei sind. Diese werden als 
falsch Positive bezeichnet. Wie war der Recall im Parkplatzbeispiel? Wir hatten 10 tatsächlich freie Plätze und unser Modell hat
nur nur einen als frei erkannt. Die Antwort ist 1 von 10 oder 0,1. Hier sehen Sie ein Dataset mit Bildern. Auf jedem Bild ist entweder
eine Katze zu sehen oder nicht. Sehen Sie kurz selbst, 
ob Sie alle richtig zuordnen können. Hoffentlich haben Sie alle 
hier gezeigten Hauskatzen gefunden. Beachten Sie die versteckte
Katze in Rot und dass der Tiger für unsere Zwecke nicht als Katze gilt. Betrachten wir, wie das
Modell die Klassifizierung durchführt. Hier sehen wir, 
was unser Modell ergeben hat. Vergleichen wir die Ergebnisse 
mit der uns bekannten Wahrheit. Da sind die Datenpunkte mit dem richtigen
Label neben den Modellvorhersagen. Insgesamt haben wir acht Beispiele oder Instanzen, 
die wir dem Modell gezeigt haben. Wie oft lag das Modell richtig? Drei von insgesamt acht 
wurden richtig vorhergesagt. Somit hat das Modell
eine Präzision von 0,375. Ist die Präzision das beste Maß
für die Beschreibung der Modellleistung? Bevor wir uns anderen Methoden zuwenden, wollen wir zuerst 
eine typische Falle besprechen. Zurück zu unserem Beispiel mit den Katzen: Wie ist die Präzision des Modells? Diese fünf Bilder hier 
wurden als positiv klassifiziert. Wie viele sind wirklich Hauskatzen? Zwei der fünf oder 
eine Präzision von 0,4. Recall ist wie jemand, der bei positiven 
Entscheidungen immer dabei sein möchte. Hier sehen Sie alle 
Katzenbilder mit richtigem Label und die entsprechende Leistung 
des Modells. Wie war der Recall? Oder anders gesagt: Wie viele 
richtige Positive hat das Modell gefunden? Das Modell hat nur 2 der 4 Katzen 
gefunden, was einen Recall von 0,5 ergibt. Fassen wir kurz zusammen, was wir 
bisher über Optimierung gelernt haben. Zuerst haben wir ML-Modelle als Sets von
Parametern und Hyperparametern definiert und versucht, Optimierung als
Suche im Parameterraum zu definieren. Als Nächstes haben wir 
Verlustfunktionen eingeführt, mit denen wir die Leistung unseres Modells bei jedem Trainingsschritt 
quantifizierbar messen und bewerten. Wir haben zwei Beispiele für spezifische 
Verlustfunktionen besprochen, RMSE für lineare Regression und Kreuzentropie 
bei unserer Klassifikationsaufgabe. Wir haben gelernt, unsere
Verlustoberflächen zu diversifizieren, indem wir das Gefälle der 
Verlustfunktionen analysieren, das uns Richtung und Schrittgröße liefert. Dieser Vorgang wird 
Gradientenabstieg genannt. In TensorFlow Playground haben
wir verschiedene ML-Modelle getestest und gesehen, wie lineare Modelle 
nicht lineare Beziehungen lernen können, wenn sie nicht lineare Merkmale erhalten, und wie neuronale Netzwerke
Hierarchien von Merkmalen lernen. Wir haben auch gesehen,
wie Hyperparameter wie die Lernrate und die Batch-Größe
den Gradientenabstieg beeinflussen. Wir haben dann anhand der Modellleistung 
bei der Klassifizierung besprochen, wie Sie zwischen Genauigkeit,
Präzision und Recall wählen, je nachdem,
welches Problem Sie lösen möchten. Wie Sie in diesem Modul gesehen haben, war unser Trainings-Dataset
mit Labels das wesentliche Element, mit dem das Modell gelernt hat. Im nächsten Modul behandeln wir, wie Sie Ihr Dataset effektiv
für Training und Evaluierung aufteilen und welche Fallstricke 
Sie umgehen sollten.