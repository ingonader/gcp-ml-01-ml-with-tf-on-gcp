This is a matrix you might have seen before when we discussed inclusive ML and facial recognition in an earlier course. In that example, we looked at a face detection ML model which incorrectly predicted a statue as a human face, which is called a false positive. And also missed an actual face in the dataset when it was obscured with winter clothing, and this miss is called a false negative. A confusion matrix like this one will allow us to quantifiably assess the performance of our classification model. But now we have four numbers, one for each quadrant, and business decision makers want to see only one. Which one do we present? To explore this a bit more, let's take a look at another photo classification example. If we know a parking spot is available, that is its label is positive, and the model also predicts that it is available, we call that a true positive. If we know that a parking spot is not available, but the model predicts that it is, we call this a false positive or type I error. To compare how well our model did with its positive predictions, we use a metric called precision. With high precision, if I say a parking space is available, I'm really sure it is. A precision of 1.0 means that of the available spaces I've identified, all of them are actually available. But, I could have missed other available spaces which are called false negatives. Precision is formally defined, as the number of true positives divided by the total number classified as positive. Looking at the matrix, an increase in what factor would drive down precision? An increase in false positives. In our parking lot example, the more the model predicts spaces is available which really aren't, the lower the precision. Recall is often inversely related to precision. With high recall, I'm rewarded for finding lots of actually available spots. A recall at 1.0 would mean I found all 10 out of 10 available parking spots, but I also could have had many spots I thought were available but weren't. These are called false positives. What was the recall of our parking lot example? Remember, we had 10 actually available spaces, and our model identified only one as available. The answer is 1 out of 10 or 0.1. Here, you're presented with a dataset of images. Each image either has a cat or it doesn't. Take a moment and see if you can spot which is which. Hopefully, you found all the domestic cats as correctly shown here. Note the hidden cat in red and that the tiger for our purposes is not classified as a cat. Now let's see how the model does the classifying. And here's what our model came up with. Let's compare the results against what we know to be true. Now we have our properly labeled data points side by side with our model predictions. In total, we have eight examples, or instances that we have showed the model. How many times did the model get it right? Three out of a total of eight were accurately predicted. This gives our model an accuracy 0.375. Is accuracy the best metric for describing model performance? Before we get into other ways, let's first discuss a common pitfall. Now revisiting our cat and no cat example, what's the precision of the model? The five images here were in the positive class. How many are actually domestic cats? Two out of the five, or a precision rate of 0.4. Recall is like a person who never wants to be left out of a positive decision. Here, you see all the true labeled examples of cats and the model's performance against them. What was the recall? Or say it another way, how many actual true positives did the model get right? The model only got 2 out of the 4 actual cats correct for a recall of 0.5. Let's do a quick wrap up of what you've learned so far about optimization. First, we defined ML models as sets of parameters and hyper-parameters, and tried to frame optimization as a search in parameter space. Next, we introduce loss functions, which is how we quantifiably measure and evaluate the performance of our model with each training step. Two examples of specific loss functions we discussed were RMSE for linear regression and cross-entropy for our classification task. We learned how to diverse our loss surfaces efficiently, by analyzing the slopes of our loss functions, which provide us direction and step magnitude. This process is called gradient descent. We experimented with different ML models inside of TensorFlow playground, and saw how the linear models can learn non-linear relationships when given non-linear features, and how neural networks learned hierarchies of features. We also saw how hyper-parameters like learning rate and batch size affect gradient descent. We then walked through how to choose between accuracy, precision, and recall, through a classification model performance depending on which problem you're trying to solve. As you saw throughout this module, our labeled training dataset was the driving force that the model learned from. In the next module, we'll cover how to effectively split your full dataset into training and evaluation, and the pitfalls to avoid along the way.