1
00:00:00,370 --> 00:00:03,050
このモジュールのトピックは主に５つです

2
00:00:03,050 --> 00:00:06,980
まず モデルの
実用的かつ正式な定義を作成します

3
00:00:06,980 --> 00:00:11,330
次に 最適化には常に改善の基準が
必要であることから

4
00:00:11,330 --> 00:00:14,370
損失関数についてお話します

5
00:00:14,370 --> 00:00:17,920
それから 勾配下降法について
損失関数で定義した勾配の最小値を

6
00:00:17,920 --> 00:00:20,320
見つける方法を説明します

7
00:00:21,150 --> 00:00:25,315
次に サンドボックスを使って
モデルが損失曲面を降下するのを

8
00:00:25,315 --> 00:00:27,790
リアルタイムで見ていきます

9
00:00:27,790 --> 00:00:30,180
最後に トレーニング以外の状況で

10
00:00:30,180 --> 00:00:33,770
モデルのパフォーマンスを測定する
方法について説明します

11
00:00:33,770 --> 00:00:36,267
まず 正確なMLモデルの定義と

12
00:00:36,267 --> 00:00:39,550
パラメータがMLモデルのどこに
組み込まれるのかを確認します

13
00:00:40,040 --> 00:00:45,277
MLモデルはパラメータと
ハイパーパラメータを使った数学的関数です

14
00:00:45,277 --> 00:00:50,210
パラメータは実数値の変数で
モデルのトレーニング中に変化します

15
00:00:50,210 --> 00:00:53,776
ハイパーパラメータは
トレーニングの前に設定する値で

16
00:00:53,776 --> 00:00:56,054
設定後は変化しません

17
00:00:56,672 --> 00:00:59,556
前のモジュールでお話したとおり

18
00:00:59,556 --> 00:01:03,210
線形モデルは 初期のMLモデルタイプの1つです

19
00:01:03,210 --> 00:01:08,360
現在でも重要かつ広く使用されている
タイプのモデルです

20
00:01:08,360 --> 00:01:14,730
線形モデルでは 機械学習で特徴量と呼ぶ
独立変数がわずかに変化すると

21
00:01:14,730 --> 00:01:18,120
ラベルと呼ばれる従属変数も
同量だけ変化します

22
00:01:18,120 --> 00:01:21,720
その変化が生じる入力領域は
関係ありません

23
00:01:21,720 --> 00:01:25,031
視覚化すると 2次元領域の
直線のように見えます

24
00:01:25,031 --> 00:01:29,287
関係をモデル化するために使用する式は
単純にy = mx + bです

25
00:01:29,287 --> 00:01:35,291
このときmは 特徴量のわずかな変化に対して
ラベルで生じた変化量を示します

26
00:01:36,430 --> 00:01:42,010
ラベルと特徴量の間の一定比率の変化で
定義される関係と同じ考え方を

27
00:01:42,010 --> 00:01:47,710
入力と出力の両方に関して
任意の高次元まで拡張できます

28
00:01:47,710 --> 00:01:51,750
つまり 多くの特徴量を入力として
受け入れるモデルを構築するか

29
00:01:51,750 --> 00:01:56,410
複数のラベルを同時にモデル化するかの
いずれかまたは両方が可能です

30
00:01:56,410 --> 00:02:02,150
入力の次元を増やすと傾斜項mは
n次元になります

31
00:02:02,180 --> 00:02:04,860
この新たな項を重みと呼びます

32
00:02:05,530 --> 00:02:09,530
このプロセスを視覚化すると
直線をn次元に一般化した

33
00:02:09,530 --> 00:02:11,400
超平面となります

34
00:02:11,400 --> 00:02:14,240
それは右側に示されています

35
00:02:14,240 --> 00:02:18,830
ここでは詳細は省きますが
出力の次元を増やすと

36
00:02:18,830 --> 00:02:24,190
y項とc項は次元と2のベクトルとなります

37
00:02:24,760 --> 00:02:29,460
b項は スカラーでも ベクトルでも
バイアス項と呼ばれます

38
00:02:29,460 --> 00:02:33,580
どうすれば線形モデルを使って回帰を
求めることができるのかは やや直感的です

39
00:02:33,580 --> 00:02:37,960
単純にb + mxという式で 予測値yを求めますが

40
00:02:37,960 --> 00:02:42,275
線形モデルでクラス分類をするには
どうすればよいのでしょうか

41
00:02:42,275 --> 00:02:47,540
連続する数を1つのクラスと見なすには
どうすればよいでしょうか

42
00:02:47,540 --> 00:02:50,820
モデルの出力値をクラス分けするには

43
00:02:50,830 --> 00:02:55,109
まず クラスメンバーシップを
コード化する方法を考える必要があります

44
00:02:55,109 --> 00:02:58,380
最も簡単な方法はバイナリです

45
00:02:58,380 --> 00:03:01,300
クラスのメンバーか
そうでないかのどちらかです

46
00:03:01,310 --> 00:03:05,790
カテゴリ変数には3つ以上の値が
必要とされる場合が多いですが

47
00:03:05,790 --> 00:03:08,020
それでもこの方法は有効です

48
00:03:08,020 --> 00:03:11,690
各値がそれぞれ独立した
クラスだと考えればよいのです

49
00:03:11,700 --> 00:03:15,600
しかし 今は単一の
バイナリクラスについて考えます

50
00:03:15,600 --> 00:03:20,170
特徴量の表現については
コース3で取り上げます

51
00:03:20,230 --> 00:03:24,850
このラベルの表現を採用すれば
タスクがより扱いやすくなります

52
00:03:24,880 --> 00:03:30,310
次に バイナリ式クラス分類ルールに
合った線引きの方法が必要です

53
00:03:30,360 --> 00:03:34,960
簡単なやり方として出力値の
符号に従って分類する方法があります

54
00:03:34,960 --> 00:03:39,540
グラフでは 2つの領域に
分かれているように見えます

55
00:03:39,540 --> 00:03:42,150
線より上の値と線より下の値です

56
00:03:42,150 --> 00:03:44,500
この線を決定境界と呼びます

57
00:03:44,500 --> 00:03:48,630
クラスの始まりと終わりに関する
決定を反映しているためです

58
00:03:48,630 --> 00:03:53,730
そして 極めて重要なのは 決定境界は
現在のデータを分類するだけでなく

59
00:03:53,730 --> 00:03:57,020
未知のデータを予測する
ためのものでもある点です

60
00:03:57,030 --> 00:04:01,240
この 未知のデータに拡張する特性は
汎化と呼ばれ

61
00:04:01,240 --> 00:04:03,830
MLモデルには必要不可欠です

62
00:04:03,830 --> 00:04:07,135
汎化については次のモジュールで
詳しく説明します

63
00:04:07,820 --> 00:04:11,400
MLの理論を学ぶだけでは
面白みに欠けるかもしれないので

64
00:04:11,400 --> 00:04:14,720
MLで起こりうる重要な問題と

65
00:04:14,720 --> 00:04:17,070
その枠組みについてお話しましょう