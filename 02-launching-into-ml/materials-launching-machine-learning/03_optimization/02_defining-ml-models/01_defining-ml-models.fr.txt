Ce module aborde cinq thèmes principaux. On va d'abord définir formellement
ce qu'est un modèle. Comme il est nécessaire de se baser
sur un critère d'optimisation, on utilisera les fonctions de perte
pour évaluer notre progression. On verra que la descente de gradient
consiste à trouver le bas d'une pente, définie par la fonction de perte. Dans un bac à sable, nous vous montrerons
la descente des modèles sur les surfaces perdues en temps réel. Nous verrons enfin comment mesurer
les performances d'un modèle en dehors de l'entraînement. Commençons par définir
ce que sont les modèles de ML et les paramètres à définir. Ces modèles sont des fonctions mathématiques
avec des paramètres et des hyperparamètres. Un paramètre est une variable à valeur réelle
qui change pendant l'entraînement. Un hyperparamètre se définit avant
le début de l'entraînement et est définitif. Comme indiqué dans le module précédent, les modèles linéaires ont été les premiers
types de modèles de ML utilisés. Ils sont encore couramment
utilisés aujourd'hui. Dans ces modèles, les changements
apportés aux variables indépendantes, appelées caractéristiques dans
le domaine du ML, sont répercutés en même quantité dans
les variables ou étiquettes dépendantes, peu importe où ils surviennent
dans l'espace d'entrée. Le modèle prend la forme
d'une droite dans un plan 2D. La formule décrivant cette relation est :
y = mx + b. "m" indique la quantité
de changements observés dans l'étiquette suite aux petites modifications
de la caractéristique. Ce concept de relation, défini par
une proportion fixe de changements entre les étiquettes et les caractéristiques, peut
s'étendre à une grande dimensionnalité arbitraire, en termes d'entrées
comme de sorties. On peut donc créer des modèles acceptant
davantage de caractéristiques en entrée, modéliser plusieurs étiquettes
simultanément, ou les deux. Si l'on augmente la dimensionnalité
de l'entrée, notre valeur "m" doit adopter un espace de dimension n. "m" est appellé "poids". On obtient alors une vue de la droite
dans un espace de dimension n généralisé, appelé hyperplan, qui s'affiche à droite
sur l'écran. Je n'entrerai pas dans les détails, mais
si l'on augmente la dimensionnalité des sorties, "y" et "c" doivent aussi
devenir des vecteurs de dimensions n. La valeur "b", qu'il s'agisse d'un vecteur
ou d'un scalaire, est un biais. L'utilisation d'un modèle linéaire pour
la régression est assez intuitive. Il suffit d'appliquer la formule
b + m * x pour obtenir la prédiction y. Mais comment utiliser un modèle linéaire
pour la classification ? Comment interpréter une valeur continue
en tant que classe ? Pour transformer la sortie numérique
de notre modèle en classe, on doit d'abord savoir comment encoder
l'appartenance à une classe. Le plus simple est d'utiliser
un système binaire : L'entrée appartient
à la classe ou pas. Dans la plupart des cas, les variables
catégorielles ont plusieurs valeurs. Cette approche reste valable. Faisons comme si chaque valeur
correspondait à sa propre classe. Pour l'instant, restons sur
une classe binaire unique. Nous reviendrons sur la représentation
des caractéristiques dans le 3e cours. Ce mode de représentation de l'étiquette
facilite la tâche. Il faut maintenant mapper notre droite sur
une règle de classification binaire. On peut simplement se baser
sur le signe de la sortie. Visuellement, cela revient à diviser
notre graphique en deux zones, avec des points au-dessus
et en dessous de la droite. Cette droite s'appelle
la frontière de décision, car elle délimite le début
et la fin des classes. Cette frontière de décision ne sert pas uniquement à décrire
les données actuelles. Elle est utilisée pour prédire
des données non observées. Cette capacité d'adaptation à des exemples
non observés s'appelle la généralisation. Elle est fondamentale
pour les modèles de ML. Nous approfondirons la généralisation
dans le prochain module. Il est difficile de comprendre le ML
sans pratique. Prenons donc un problème concret
auquel appliquer le ML, et voyons comment procéder.