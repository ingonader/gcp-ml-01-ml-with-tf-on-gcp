Dieses Modul
umfasst fünf Hauptthemen. Zuerst erstellen wir eine
formelle Definition eines Modells. Da Optimierungen einen Standard
zum Messen der Verbesserung erfordern,
besprechen wir dann Verlustfunktionen. Wir vergleichen das Gradientenverfahren
mit einem durch die Verlustfunktion definierten Bergabstieg. Dann beobachten wir in einer
Sandbox Modelle in Echtzeit beim Verlustdienst-Abstieg. Zum Schluss widmen wir uns
dem Messen der Modellleistung außerhalb des Trainingskontexts. Sehen wir uns zuerst an,
was ein ML-Modell ist und wo sich Parameter einreihen. ML-Modelle sind mathematische Funktionen
mit Parametern und Hyperparametern. Parameter sind reellwertige Variablen,
die sich während des Trainings ändern. Hyperparameter sind vor dem
Training festgelegte und danach unveränderliche Einstellungen. Wie im vorherigen Modul erwähnt, waren lineare Modelle eine der
ersten Arten von ML-Modellen. Auch heute sind sie noch eine
wichtige und viel genutzte Modellklasse. In linearen Modellen führen kleine
Änderungen an unabhängigen Variablen oder Features, wie wir sie
im ML-Kontext nennen, zum gleichen Änderungsausmaß in
der abhängigen Variable oder dem Label. Das ist unabhängig vom Ort
der Änderung in der Eingabe. Visuell sieht das wie
eine Linie im 2D-Raum aus. Die für die Modellierung der Beziehung
verwendete Formel ist einfach y = mx + b. "m" steht dabei für das
Änderungsausmaß in unserem Label aufgrund einer kleinen Änderung in unserem Feature. Dieses Konzept einer Beziehung, die durch
eine feste Verhältnisänderung zwischen Labels und Features definiert ist, ist
auf eine beliebig hohe Dimensionalität erweiterbar, und zwar im
Hinblick auf Ein- und Ausgaben. Somit können wir Modelle erstellen, die
mehr Features als Eingabe unterstützen, mehrere Labels
gleichzeitig modellieren oder beides. Ist die Eingabedimensionalität
höher, muss der Steigungsterm "m" n-dimensional werden. Dieser neue Term ist die Gewichtung. Visuell ergibt der Prozess
eine n-dimensionale Verallgemeinerung einer Linie, genannt
Hyperebene, wie rechts abgebildet. Ich gehe hier nicht ins Detail,
aber bei höherer Dimensionalität der Ausgaben müssen y und c
zweidimensionale Vektoren werden. Der b-Term wird unabhängig davon, ob
Skalar oder Vektor, Bias-Term genannt. Die Verwendung eines linearen Modells
für die Regression sollte intuitiv sein. Anhand der Formel b plus m
mal x erhält man die Vorhersage y. Doch wie kann ein lineares Modell
für die Klassifizierung verwendet werden? Wie wird eine fortlaufende
Nummer als Klasse interpretiert? Um aus der numerischen Ausgabe
unseres Modells eine Klasse zu machen, müssen wir uns der Codierung
der Klassenmitgliedschaft widmen. Eine Binärmethode ist hier
die einfachste Codierungsoption. Man ist Mitglied oder nicht. Kategorievariablen können natürlich
oftmals mehr als zwei Werte haben. Der Ansatz funktioniert trotzdem. Jeder Wert wird als eigene,
unabhängige Klasse interpretiert. Bleiben wir jetzt aber bei
einer einzigen Binärklasse. Mit der Feature-Darstellung
befassen wir uns im dritten Kurs. Bei Annahme dieser Label-Darstellung
wird die Aufgabe leichter zu verwalten. Nun müssen wir unsere Linie auf
eine Binärklassifikationsregel abbilden. Eine einfache Option hierfür ist die
Berücksichtigung des Zeichens der Ausgabe. Grafisch sieht das aus, als hätten
wir unsere Grafik in zwei Bereiche unterteilt: die Punkte über
der Linie und die Punkte darunter. Die Linie ist die Entscheidungsgrenze, die unsere Entscheidung über
Klassenanfang und -ende widerspiegelt. Wichtig ist, dass die Entscheidungslinie nicht nur für
die aktuellen Daten aussagekräftig sein soll, sondern auch
prädiktiv für unbekannte Daten. Diese Eigenschaft der Ausdehnung auf
unbekannte Daten wird Generalisierung genannt und ist essenziell für ML-Modelle. Mit der Generalisierung befassen wir
uns im nächsten Modul eingehender. ML nur in der Theorie näher zu
erkunden, kann ziemlich trocken sein. Sprechen wir also über ein wichtiges
Problem, das ein Kandidat für ML wäre, und darüber,
wie es formuliert werden kann.