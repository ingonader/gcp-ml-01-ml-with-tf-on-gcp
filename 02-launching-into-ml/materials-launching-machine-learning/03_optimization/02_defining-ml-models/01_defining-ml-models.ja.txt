このモジュールのトピックは主に５つです まず モデルの
実用的かつ正式な定義を作成します 次に 最適化には常に改善の基準が
必要であることから 損失関数についてお話します それから 勾配下降法について
損失関数で定義した勾配の最小値を 見つける方法を説明します 次に サンドボックスを使って
モデルが損失曲面を降下するのを リアルタイムで見ていきます 最後に トレーニング以外の状況で モデルのパフォーマンスを測定する
方法について説明します まず 正確なMLモデルの定義と パラメータがMLモデルのどこに
組み込まれるのかを確認します MLモデルはパラメータと
ハイパーパラメータを使った数学的関数です パラメータは実数値の変数で
モデルのトレーニング中に変化します ハイパーパラメータは
トレーニングの前に設定する値で 設定後は変化しません 前のモジュールでお話したとおり 線形モデルは 初期のMLモデルタイプの1つです 現在でも重要かつ広く使用されている
タイプのモデルです 線形モデルでは 機械学習で特徴量と呼ぶ
独立変数がわずかに変化すると ラベルと呼ばれる従属変数も
同量だけ変化します その変化が生じる入力領域は
関係ありません 視覚化すると 2次元領域の
直線のように見えます 関係をモデル化するために使用する式は
単純にy = mx + bです このときmは 特徴量のわずかな変化に対して
ラベルで生じた変化量を示します ラベルと特徴量の間の一定比率の変化で
定義される関係と同じ考え方を 入力と出力の両方に関して
任意の高次元まで拡張できます つまり 多くの特徴量を入力として
受け入れるモデルを構築するか 複数のラベルを同時にモデル化するかの
いずれかまたは両方が可能です 入力の次元を増やすと傾斜項mは
n次元になります この新たな項を重みと呼びます このプロセスを視覚化すると
直線をn次元に一般化した 超平面となります それは右側に示されています ここでは詳細は省きますが
出力の次元を増やすと y項とc項は次元と2のベクトルとなります b項は スカラーでも ベクトルでも
バイアス項と呼ばれます どうすれば線形モデルを使って回帰を
求めることができるのかは やや直感的です 単純にb + mxという式で 予測値yを求めますが 線形モデルでクラス分類をするには
どうすればよいのでしょうか 連続する数を1つのクラスと見なすには
どうすればよいでしょうか モデルの出力値をクラス分けするには まず クラスメンバーシップを
コード化する方法を考える必要があります 最も簡単な方法はバイナリです クラスのメンバーか
そうでないかのどちらかです カテゴリ変数には3つ以上の値が
必要とされる場合が多いですが それでもこの方法は有効です 各値がそれぞれ独立した
クラスだと考えればよいのです しかし 今は単一の
バイナリクラスについて考えます 特徴量の表現については
コース3で取り上げます このラベルの表現を採用すれば
タスクがより扱いやすくなります 次に バイナリ式クラス分類ルールに
合った線引きの方法が必要です 簡単なやり方として出力値の
符号に従って分類する方法があります グラフでは 2つの領域に
分かれているように見えます 線より上の値と線より下の値です この線を決定境界と呼びます クラスの始まりと終わりに関する
決定を反映しているためです そして 極めて重要なのは 決定境界は
現在のデータを分類するだけでなく 未知のデータを予測する
ためのものでもある点です この 未知のデータに拡張する特性は
汎化と呼ばれ MLモデルには必要不可欠です 汎化については次のモジュールで
詳しく説明します MLの理論を学ぶだけでは
面白みに欠けるかもしれないので MLで起こりうる重要な問題と その枠組みについてお話しましょう