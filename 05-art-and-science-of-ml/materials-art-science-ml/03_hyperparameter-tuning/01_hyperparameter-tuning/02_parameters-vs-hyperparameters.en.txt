This should look familiar to you. You should have seen it in course
number two, launching into ML. Remember we said ML models are
mathematical functions with parameters and hyper-parameters. A parameter is a real valued variable
that changes during model training, like all those base and
biases that we've come to know so well. A hyper-parameter, on the other hand, is
a setting that we set before training, and it doesn't change afterwards. Examples of hyper-parameters are learning
rate, regularization rate, batch size, number of hidden layers in the neural net,
and number of neurons in each layer. Now that you're clear about
the differences between parameters and hyper-parameters, let's shift our
attention to hyper-parameters. Since we know parameters are going to
be adjusted by the training algorithm, our job is to set the hyper-parameters,
right? In the previous module, we manually played
with some of those hyper-parameters. For instance, we learned that batch
size and and learning rate matter. Here I have some graphs from
Andrej Karpathy's great article, that I recommend you
review at your leisure. He visualizes the problem so well. As you see on the left,
at the lower learning rate, like the blue graph here,
improvement is linear. But you often don't get
the best possible performance. At a high learning rate, like the green
graph here, you get exponential improvement at first, but you often don't
find the best possible performance. At the very high learning rate, like this
yellow graph, you can get completely lost. There's often a Goldilocks learning rate,
like this red one here. But good luck finding it. Let's see what these graphs
tell us about batch size. On the right you see a very
noisy loss curve, and that's due to the small batch size. From previous module, you should remember
that setting the batch size too large can dramatically slow things down. One thing to note though
these graphs are by epoch but unfortunately tensor flow
doesn't know much about epochs. You'll have to figure out the epoch
by calculating how many steps of a given batch size
will equate one epoch. In other words you need to find out
how many steps of given batch size you will be required to traverse
your data set once.