1
00:00:01,050 --> 00:00:02,750
Isso é familiar para você.

2
00:00:02,750 --> 00:00:06,240
Você já viu no segundo curso,
sobre os primeiros passos em ML.

3
00:00:06,640 --> 00:00:10,005
Nós dissemos que os modelos
de ML são funções matemáticas

4
00:00:10,005 --> 00:00:11,940
com parâmetros
e hiperparâmetros.

5
00:00:11,940 --> 00:00:16,810
Um parâmetro é um número real variável
que muda durante o treinamento,

6
00:00:16,810 --> 00:00:20,390
como as bases e os vieses
que conhecemos bem.

7
00:00:21,340 --> 00:00:25,500
Mas o hiperparâmetro é uma configuração
definida antes do treinamento

8
00:00:25,500 --> 00:00:27,556
que não muda.

9
00:00:28,236 --> 00:00:33,200
Alguns exemplos são a taxa de aprendizado
e de regularização, o tamanho do lote,

10
00:00:33,200 --> 00:00:37,739
o número de camadas ocultas na rede neural
e o número de neurônios em cada camada.

11
00:00:38,610 --> 00:00:42,945
Agora que você sabe
a diferença entre os dois,

12
00:00:42,945 --> 00:00:45,638
vamos ver os hiperparâmetros.

13
00:00:45,638 --> 00:00:49,670
Como sabemos que os parâmetros
serão ajustados pelo algoritmo,

14
00:00:49,670 --> 00:00:52,080
nosso trabalho é definir
os hiperparâmetros, certo?

15
00:00:52,080 --> 00:00:56,330
No módulo anterior,
ajustamos alguns manualmente.

16
00:00:56,330 --> 00:01:00,520
Por exemplo, vimos que o tamanho
do lote e a taxa de aprendizado importam.

17
00:01:00,520 --> 00:01:03,680
Tenho alguns gráficos
do artigo de Andrej Karpathy,

18
00:01:03,680 --> 00:01:05,970
que eu recomendo
que você leia.

19
00:01:05,970 --> 00:01:08,170
Ele visualiza o
problema muito bem.

20
00:01:08,840 --> 00:01:12,630
Como você vê na esquerda,
na taxa de aprendizado menor,

21
00:01:12,630 --> 00:01:16,260
na linha azul,
a melhora é linear.

22
00:01:16,260 --> 00:01:19,560
Mas, muitas vezes, não
conseguimos o melhor desempenho.

23
00:01:20,520 --> 00:01:25,830
Com uma taxa de aprendizado alta,
na linha verde, há uma melhora

24
00:01:25,830 --> 00:01:30,560
exponencial no começo, mas muitas vezes
não conseguimos o melhor desempenho.

25
00:01:31,620 --> 00:01:38,030
Com uma taxa muito alta,
a linha amarela, você se perde.

26
00:01:38,030 --> 00:01:42,940
Pode haver uma taxa melhor,
como a linha vermelha,

27
00:01:42,940 --> 00:01:44,696
mas é difícil encontrar.

28
00:01:46,184 --> 00:01:50,375
Vamos ver o que esses gráficos
dizem sobre o tamanho do lote.

29
00:01:50,375 --> 00:01:53,825
À direita, há uma curva
de perda com muito ruído,

30
00:01:53,825 --> 00:01:56,185
devido a um tamanho
de lote pequeno.

31
00:01:56,185 --> 00:02:00,795
Você lembra do módulo anterior
que definir um lote muito grande

32
00:02:00,795 --> 00:02:03,025
pode deixar o processo lento.

33
00:02:03,640 --> 00:02:06,650
É bom notar que estes
gráficos são por período,

34
00:02:06,650 --> 00:02:10,210
mas o TensorFlow não
entende muito de períodos.

35
00:02:10,210 --> 00:02:14,320
Para calcular o período,
calcule quantas etapas

36
00:02:14,320 --> 00:02:17,800
com um lote de tamanho definido
serão iguais a um período.

37
00:02:18,370 --> 00:02:22,550
Ou seja, quantas etapas em
um lote de tamanho definido

38
00:02:22,550 --> 00:02:25,760
você precisará percorrer
no conjunto de dados.