Isso é familiar para você. Você já viu no segundo curso,
sobre os primeiros passos em ML. Nós dissemos que os modelos
de ML são funções matemáticas com parâmetros
e hiperparâmetros. Um parâmetro é um número real variável
que muda durante o treinamento, como as bases e os vieses
que conhecemos bem. Mas o hiperparâmetro é uma configuração
definida antes do treinamento que não muda. Alguns exemplos são a taxa de aprendizado
e de regularização, o tamanho do lote, o número de camadas ocultas na rede neural
e o número de neurônios em cada camada. Agora que você sabe
a diferença entre os dois, vamos ver os hiperparâmetros. Como sabemos que os parâmetros
serão ajustados pelo algoritmo, nosso trabalho é definir
os hiperparâmetros, certo? No módulo anterior,
ajustamos alguns manualmente. Por exemplo, vimos que o tamanho
do lote e a taxa de aprendizado importam. Tenho alguns gráficos
do artigo de Andrej Karpathy, que eu recomendo
que você leia. Ele visualiza o
problema muito bem. Como você vê na esquerda,
na taxa de aprendizado menor, na linha azul,
a melhora é linear. Mas, muitas vezes, não
conseguimos o melhor desempenho. Com uma taxa de aprendizado alta,
na linha verde, há uma melhora exponencial no começo, mas muitas vezes
não conseguimos o melhor desempenho. Com uma taxa muito alta,
a linha amarela, você se perde. Pode haver uma taxa melhor,
como a linha vermelha, mas é difícil encontrar. Vamos ver o que esses gráficos
dizem sobre o tamanho do lote. À direita, há uma curva
de perda com muito ruído, devido a um tamanho
de lote pequeno. Você lembra do módulo anterior
que definir um lote muito grande pode deixar o processo lento. É bom notar que estes
gráficos são por período, mas o TensorFlow não
entende muito de períodos. Para calcular o período,
calcule quantas etapas com um lote de tamanho definido
serão iguais a um período. Ou seja, quantas etapas em
um lote de tamanho definido você precisará percorrer
no conjunto de dados.