1
00:00:00,000 --> 00:00:02,670
Das kennen Sie wahrscheinlich schon.

2
00:00:02,670 --> 00:00:06,810
Sie sollten es im zweiten Kurs
mit der Einführung in ML gesehen haben.

3
00:00:06,810 --> 00:00:10,655
Wir haben gelernt, dass ML-Modelle
mathematische Funktionen mit Parametern

4
00:00:10,655 --> 00:00:12,130
und Hyperparametern sind.

5
00:00:12,130 --> 00:00:16,810
Ein Parameter ist eine reelle Variable
und ändert sich beim Modelltraining

6
00:00:16,810 --> 00:00:21,350
wie auch die Gewichtungen und
Verzerrungen, die wir schon kennen.

7
00:00:21,350 --> 00:00:25,760
Ein Hyperparameter hingegen ist eine
vor dem Training festgelegte Einstellung,

8
00:00:25,760 --> 00:00:27,256
die sich danach nicht mehr ändert.

9
00:00:27,256 --> 00:00:33,200
Beispiele für Hyperparameter sind
Lernrate, Regulierungsrate, Batchgröße,

10
00:00:33,200 --> 00:00:37,820
Anzahl versteckter Ebenen im neuronalen
Netz und Anzahl Neuronen auf jeder Ebene.

11
00:00:37,820 --> 00:00:42,755
Da Sie nun die Unterschiede zwischen
Parametern und Hyperparametern kennen,

12
00:00:42,755 --> 00:00:45,638
wollen wir uns den
Hyperparametern zuwenden.

13
00:00:45,638 --> 00:00:49,210
Parameter werden durch den
Trainingsalgorithmus angepasst.

14
00:00:49,210 --> 00:00:51,930
Die Hyperparameter
müssen wir richtig festlegen.

15
00:00:51,930 --> 00:00:56,330
Im vorherigen Modul haben wir
einige Hyperparameter manuell geändert.

16
00:00:56,330 --> 00:01:00,520
Zum Beispiel haben wir gelernt, dass
Batchgröße und Lernrate wichtig sind.

17
00:01:00,520 --> 00:01:03,860
Hier habe ich einige Diagramme aus
Andrej Karpathys großartigem Artikel,

18
00:01:03,860 --> 00:01:05,970
den ich sehr empfehlen kann.

19
00:01:05,970 --> 00:01:08,870
Er visualisiert das Problem sehr gut.

20
00:01:08,870 --> 00:01:12,630
Sie sehen auf der linken Seite,
dass bei geringer Lernrate,

21
00:01:12,630 --> 00:01:16,260
wie bei dem blauen Graphen hier, 
Verbesserungen linear sind.

22
00:01:16,260 --> 00:01:20,510
Aber Sie erzielen oft nicht
die bestmögliche Leistung.

23
00:01:20,520 --> 00:01:24,650
Bei einer hohen Lernrate,
wie im grünen Graphen hier,

24
00:01:24,650 --> 00:01:27,240
erzielen Sie zunächst
eine exponentielle Verbesserung,

25
00:01:27,240 --> 00:01:31,610
aber oft nicht die bestmögliche Leistung.

26
00:01:31,620 --> 00:01:38,030
Einer sehr hohe Lernrate, der gelbe
Graph, führt manchmal nirgendwohin.

27
00:01:38,030 --> 00:01:40,429
Oft gibt es eine perfekte Lernrate,

28
00:01:40,429 --> 00:01:42,829
wie z. B. die rote hier.

29
00:01:42,829 --> 00:01:46,206
Aber sie kommt nur selten vor.

30
00:01:46,206 --> 00:01:50,375
Was sagen diese
Graphen über die Batchgröße aus?

31
00:01:50,375 --> 00:01:53,825
Auf der rechten Seite sehen Sie
eine Verlustkurve mit hohem Rauschen.

32
00:01:53,825 --> 00:01:56,185
Das liegt an der geringen Batchgröße.

33
00:01:56,185 --> 00:02:00,535
Sie wissen inzwischen,
dass eine zu hohe Batchgröße

34
00:02:00,535 --> 00:02:03,655
die Vorgänge deutlich verlangsamen kann.

35
00:02:03,655 --> 00:02:06,500
Diese Graphen sind
im Verhältnis zu Epochen dargestellt,

36
00:02:06,500 --> 00:02:08,355
doch TensorFlow kann

37
00:02:08,355 --> 00:02:10,210
mit Epochen nicht viel anfangen.

38
00:02:10,210 --> 00:02:14,230
Sie müssen die Epochen herausfinden,
indem Sie berechnen, wie viele Schritte

39
00:02:14,230 --> 00:02:18,410
mit einer bestimmten
Batchgröße einer Epoche entsprechen.

40
00:02:18,410 --> 00:02:20,170
Das heißt, Sie müssen herausfinden,

41
00:02:20,170 --> 00:02:23,430
wie viele Schritte Sie
mit einer bestimmten Batchgröße benötigen,

42
00:02:23,430 --> 00:02:25,830
um Ihr Dataset einmal zu durchlaufen.