Think about all these knobs and leavers and finding the Goldi logs combination that's data dependent sounds like a daunting task. Just think about the permutations, you could automate it using any number of Gritz search algorithms, but the search for the right combination can take forever and burn many hours of computational resources. Wouldn't it be nice to have a training loop, do meta-training and all these hyper parameters and find the setting that's just right. Fear not, Google Vizier is at your service. For the most part you'll enjoy auto magic hyperparameter tuning that's powered by Google Vizier algorithm, without needing to know about the details. If you're curious to know what's going on inside the black box, I'd recommend reviewing the research paper via the link that's on the screen. All you need to know is that Cloud ML Engine takes the burden away, you just need to configure your job properly and let the ML Engine do the heavy lifting. Let's see what it takes to get some hyperparameters tuned for us the auto magic way. By now you should have played with Cloud ML Engine, the Serverless platform for training and hosting ML models. ML Engine nicely abstracts the way the process of hyperparameter tuning. All you need to do to use this service is as follows. One, you need to express the hyperparameters in need of tuning as a command-line argument. Then, you need to ensure different iterations of training don't clobber each other. Finally, you'll need to supply those hyperparameters to the training job. Before we jump into a lab and do some hyperparameter tuning, let's quickly highlight how those three steps look like in code. The first step is to define any hyperparameter that you intend to get tuned as a command-line argument. For instance, here I have two hyperparameters: the number of packets to discretize latitude and longitude, and the number of hidden units in my deep neural net. The second step is to ensure the outputs of different trials don't clobber each other, and that's done by employing good naming convention for output folders. Here for instance, I use a trial value as a suffix that makes the output name unique. The last step is to supply hyperparameters when submitting a training job, and here is how. First you create the yaml file like this one, then you supply the path to the yaml file via command-line parameters to the G Cloud ML Engine command, like this. Now let's get a closer look at the contents of the yaml file. Notice in this example we want to minimize the rmse on the evaluation data set. We run the ML Engine to try to find the optimal batch size between 64 and 512. Remind you this is not Gritz search, it's much smarter. Notice the max trial here, ML Engine will algorithmically search into promising areas, it randomly starts a number of parallel trials as specified by max parallel trials, and starts exploring. Here, we are asking ML Engine to systematically try various neural network architectures. Ready to put this into a test? Let's do it.