1
00:00:01,050 --> 00:00:02,750
この図を見てください

2
00:00:02,750 --> 00:00:06,810
コース2の
MLの開始で使用した図です

3
00:00:06,810 --> 00:00:12,125
MLモデルはパラメータと
ハイパーパラメータを持つ数学関数です

4
00:00:12,125 --> 00:00:16,810
パタメータは変数で
値はモデルトレーニング中に変わります

5
00:00:16,810 --> 00:00:21,330
これまでに説明した
ベースとバイアスと同様です

6
00:00:21,340 --> 00:00:25,760
ハイパーパラメータの値は
トレーニング前に設定します

7
00:00:25,760 --> 00:00:27,256
値は変わりません

8
00:00:27,256 --> 00:00:33,200
ハイパーパラメータの例としては
学習率、正規化率、バッチサイズ

9
00:00:33,200 --> 00:00:38,609
ニューラルネットの隠れ層数、
各層のニューロン数などがあります

10
00:00:38,609 --> 00:00:41,585
2つの違いが明らかになったところで

11
00:00:41,585 --> 00:00:45,638
ハイパーパラメータを見ていきましょう

12
00:00:45,638 --> 00:00:49,670
パラメータはトレーニングアルゴリズムで
調整するので

13
00:00:49,670 --> 00:00:52,080
ハイパーパラメータを設定します

14
00:00:52,080 --> 00:00:56,330
前のモジュールでは
ハイパーパラメータを手動で設定し

15
00:00:56,330 --> 00:01:00,520
バッチサイズと学習率が
重要であることを学びました

16
00:01:00,520 --> 00:01:03,680
Andrej Karpathyの記事から
グラフを引用します

17
00:01:03,680 --> 00:01:05,970
ぜひ記事を参照してください

18
00:01:05,970 --> 00:01:08,840
問題がよく可視化されています

19
00:01:08,840 --> 00:01:12,630
左側のグラフでは
学習率を低くすると

20
00:01:12,630 --> 00:01:16,260
青曲線のように
パフォーマンスが線形で向上します

21
00:01:16,260 --> 00:01:20,520
ただし最大パフォーマンスは
通常得られません

22
00:01:20,520 --> 00:01:25,830
学習率を高くすると
緑曲線のように最初は大幅に向上しますが

23
00:01:25,830 --> 00:01:31,620
最大パフォーマンスは
通常得られません

24
00:01:31,620 --> 00:01:38,030
学習率を非常に高くすると
黄曲線のように完全に逸脱します

25
00:01:38,030 --> 00:01:42,940
通常は赤曲線のように
適度な学習率がありますが

26
00:01:42,940 --> 00:01:46,186
見つけるのは困難です

27
00:01:46,186 --> 00:01:50,375
グラフからバッチサイズの影響を確認します

28
00:01:50,375 --> 00:01:53,825
右側の損失曲線には
かなりのノイズがあります

29
00:01:53,825 --> 00:01:56,185
バッチサイズが小さいためです

30
00:01:56,185 --> 00:02:03,645
バッチサイズが大きすぎると
処理速度が大幅に低下します

31
00:02:03,645 --> 00:02:06,650
これらのグラフはエポック基準ですが

32
00:02:06,650 --> 00:02:10,210
TensorFlowには
エポック情報が足りません

33
00:02:10,210 --> 00:02:14,320
エポックを明らかにするには
1エポックに相当する

34
00:02:14,320 --> 00:02:18,370
バッチサイズのステップ数を計算します

35
00:02:18,370 --> 00:02:22,550
つまり データセットを1回
走査するのに必要な

36
00:02:22,550 --> 00:02:25,860
バッチサイズのステップ数を計算します