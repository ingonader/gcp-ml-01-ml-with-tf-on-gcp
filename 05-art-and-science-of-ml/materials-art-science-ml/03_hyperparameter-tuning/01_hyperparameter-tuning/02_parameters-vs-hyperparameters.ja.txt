この図を見てください コース2の
MLの開始で使用した図です MLモデルはパラメータと
ハイパーパラメータを持つ数学関数です パタメータは変数で
値はモデルトレーニング中に変わります これまでに説明した
ベースとバイアスと同様です ハイパーパラメータの値は
トレーニング前に設定します 値は変わりません ハイパーパラメータの例としては
学習率、正規化率、バッチサイズ ニューラルネットの隠れ層数、
各層のニューロン数などがあります 2つの違いが明らかになったところで ハイパーパラメータを見ていきましょう パラメータはトレーニングアルゴリズムで
調整するので ハイパーパラメータを設定します 前のモジュールでは
ハイパーパラメータを手動で設定し バッチサイズと学習率が
重要であることを学びました Andrej Karpathyの記事から
グラフを引用します ぜひ記事を参照してください 問題がよく可視化されています 左側のグラフでは
学習率を低くすると 青曲線のように
パフォーマンスが線形で向上します ただし最大パフォーマンスは
通常得られません 学習率を高くすると
緑曲線のように最初は大幅に向上しますが 最大パフォーマンスは
通常得られません 学習率を非常に高くすると
黄曲線のように完全に逸脱します 通常は赤曲線のように
適度な学習率がありますが 見つけるのは困難です グラフからバッチサイズの影響を確認します 右側の損失曲線には
かなりのノイズがあります バッチサイズが小さいためです バッチサイズが大きすぎると
処理速度が大幅に低下します これらのグラフはエポック基準ですが TensorFlowには
エポック情報が足りません エポックを明らかにするには
1エポックに相当する バッチサイズのステップ数を計算します つまり データセットを1回
走査するのに必要な バッチサイズのステップ数を計算します