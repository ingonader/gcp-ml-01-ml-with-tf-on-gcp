1
00:00:01,050 --> 00:00:02,750
This should look familiar to you.

2
00:00:02,750 --> 00:00:05,810
You should have seen it in course
number two, launching into ML.

3
00:00:06,810 --> 00:00:10,895
Remember we said ML models are
mathematical functions with parameters and

4
00:00:10,895 --> 00:00:12,130
hyper-parameters.

5
00:00:12,130 --> 00:00:16,810
A parameter is a real valued variable
that changes during model training,

6
00:00:16,810 --> 00:00:20,320
like all those base and
biases that we've come to know so well.

7
00:00:21,340 --> 00:00:25,760
A hyper-parameter, on the other hand, is
a setting that we set before training, and

8
00:00:25,760 --> 00:00:27,256
it doesn't change afterwards.

9
00:00:27,256 --> 00:00:33,200
Examples of hyper-parameters are learning
rate, regularization rate, batch size,

10
00:00:33,200 --> 00:00:37,119
number of hidden layers in the neural net,
and number of neurons in each layer.

11
00:00:38,610 --> 00:00:41,585
Now that you're clear about
the differences between parameters and

12
00:00:41,585 --> 00:00:45,638
hyper-parameters, let's shift our
attention to hyper-parameters.

13
00:00:45,638 --> 00:00:49,670
Since we know parameters are going to
be adjusted by the training algorithm,

14
00:00:49,670 --> 00:00:52,080
our job is to set the hyper-parameters,
right?

15
00:00:52,080 --> 00:00:56,330
In the previous module, we manually played
with some of those hyper-parameters.

16
00:00:56,330 --> 00:01:00,520
For instance, we learned that batch
size and and learning rate matter.

17
00:01:00,520 --> 00:01:03,680
Here I have some graphs from
Andrej Karpathy's great article,

18
00:01:03,680 --> 00:01:05,970
that I recommend you
review at your leisure.

19
00:01:05,970 --> 00:01:07,820
He visualizes the problem so well.

20
00:01:08,840 --> 00:01:12,630
As you see on the left,
at the lower learning rate,

21
00:01:12,630 --> 00:01:16,260
like the blue graph here,
improvement is linear.

22
00:01:16,260 --> 00:01:18,970
But you often don't get
the best possible performance.

23
00:01:20,520 --> 00:01:25,830
At a high learning rate, like the green
graph here, you get exponential

24
00:01:25,830 --> 00:01:30,200
improvement at first, but you often don't
find the best possible performance.

25
00:01:31,620 --> 00:01:38,030
At the very high learning rate, like this
yellow graph, you can get completely lost.

26
00:01:38,030 --> 00:01:42,940
There's often a Goldilocks learning rate,
like this red one here.

27
00:01:42,940 --> 00:01:44,696
But good luck finding it.

28
00:01:46,184 --> 00:01:50,375
Let's see what these graphs
tell us about batch size.

29
00:01:50,375 --> 00:01:53,825
On the right you see a very
noisy loss curve, and

30
00:01:53,825 --> 00:01:56,185
that's due to the small batch size.

31
00:01:56,185 --> 00:02:00,795
From previous module, you should remember
that setting the batch size too large

32
00:02:00,795 --> 00:02:02,485
can dramatically slow things down.

33
00:02:03,640 --> 00:02:06,650
One thing to note though
these graphs are by epoch but

34
00:02:06,650 --> 00:02:10,210
unfortunately tensor flow
doesn't know much about epochs.

35
00:02:10,210 --> 00:02:14,320
You'll have to figure out the epoch
by calculating how many steps

36
00:02:14,320 --> 00:02:17,230
of a given batch size
will equate one epoch.

37
00:02:18,370 --> 00:02:22,550
In other words you need to find out
how many steps of given batch size you

38
00:02:22,550 --> 00:02:25,510
will be required to traverse
your data set once.