1
00:00:00,000 --> 00:00:04,780
Cet atelier vous a permis d'expérimenter
les régularisations L1 et L2

2
00:00:04,780 --> 00:00:06,965
et d'observer visuellement leurs effets.

3
00:00:06,965 --> 00:00:09,475
Examinons ensemble les résultats.

4
00:00:09,475 --> 00:00:13,325
J'ai lancé Tensorflow Playground
à l'aide du lien indiqué à l'écran.

5
00:00:13,325 --> 00:00:15,340
Avant de démarrer
la boucle d'entraînement,

6
00:00:15,340 --> 00:00:18,760
j'ai ajouté du bruit dans l'ensemble
de données (niveau réglé sur 30).

7
00:00:18,760 --> 00:00:21,945
Au lieu d'utiliser seulement
les caractéristiques X1 et X2,

8
00:00:21,945 --> 00:00:24,730
j'ai également intégré
des croisements de caractéristiques.

9
00:00:24,730 --> 00:00:27,385
J'ai d'abord essayé
un entraînement sans régularisation

10
00:00:27,385 --> 00:00:29,295
pour observer le comportement du modèle.

11
00:00:29,295 --> 00:00:32,585
Comme attendu, la "perte d'entraînement"
a correctement convergé,

12
00:00:32,585 --> 00:00:35,080
mais la "perte de test" est restée
à un niveau élevé.

13
00:00:35,080 --> 00:00:37,585
Observez la forme
du modèle d'entraînement !

14
00:00:37,585 --> 00:00:40,200
Avez-vous remarqué
la forme étrange de la région bleue ?

15
00:00:40,690 --> 00:00:43,745
Cela indique clairement
que le modèle s'est surajusté de lui-même

16
00:00:43,745 --> 00:00:47,010
pour apprendre tout le bruit
contenu dans les données d'entraînement.

17
00:00:47,150 --> 00:00:49,855
Eh bien, on peut dire
que j'ai élaboré un mauvais modèle.

18
00:00:49,855 --> 00:00:51,655
Il est impossible de le généraliser.

19
00:00:52,205 --> 00:00:55,080
Ensuite, j'ai forcé mon modèle
à appliquer le rasoir d'Ockham

20
00:00:55,080 --> 00:00:57,090
et à "rester simple".

21
00:00:57,090 --> 00:01:00,110
Souvenez-vous qu'un moyen
de pénaliser la complexité est

22
00:01:00,110 --> 00:01:02,150
d'appliquer la régularisation L1.

23
00:01:02,150 --> 00:01:03,420
Après avoir procédé ainsi,

24
00:01:03,420 --> 00:01:06,185
j'ai pu constater que le modèle
était bien plus performant.

25
00:01:06,185 --> 00:01:09,340
La forme bleue était beaucoup plus lisse
et le bruit avait disparu.

26
00:01:09,340 --> 00:01:12,130
En outre, la "perte de test"
convergeait correctement.

27
00:01:12,130 --> 00:01:14,115
Ce modèle est de bien meilleure qualité.

28
00:01:14,115 --> 00:01:16,190
Je souhaite aussi
que vous prêtiez attention

29
00:01:16,190 --> 00:01:18,320
aux caractéristiques ignorées
par mon modèle.

30
00:01:18,320 --> 00:01:26,040
Notez qu'aucune ligne n'émane de X1,
de X2 ou de X1X2.

31
00:01:26,470 --> 00:01:29,070
Rappelez-vous,
la régularisation L1 peut être utilisée

32
00:01:29,070 --> 00:01:31,420
comme mécanisme de
sélection de caractéristiques.

33
00:01:31,420 --> 00:01:33,875
Ensuite, j'ai essayé la régularisation L2.

34
00:01:33,875 --> 00:01:37,400
Ici, il n'y avait aucun mécanisme 
de sélection de caractéristiques.

35
00:01:37,400 --> 00:01:40,895
J'ai attribué un poids important
aux caractéristiques essentielles,

36
00:01:40,895 --> 00:01:44,945
et un poids plus faible
aux autres caractéristiques.

37
00:01:44,945 --> 00:01:47,655
Cela n'est pas visible
sur la capture d'écran,

38
00:01:47,655 --> 00:01:49,475
mais lors de l'exécution,

39
00:01:49,475 --> 00:01:54,925
un mouvement était perceptible sur
les lignes provenant de X1, X2 et X1X2.

40
00:01:54,925 --> 00:01:58,150
Souvenez-vous que le poids
d'une caractéristique est représenté

41
00:01:58,150 --> 00:02:02,275
par l'épaisseur de la ligne
qui émane de cette caractéristique.

42
00:02:02,275 --> 00:02:05,150
Je n'ai pas constaté de courbure extrême,

43
00:02:05,150 --> 00:02:07,250
la perte de test
était normale, régulière...

44
00:02:07,250 --> 00:02:08,505
Un bon modèle en somme.

45
00:02:08,505 --> 00:02:12,240
Ensuite, j'ai essayé de mettre un peu plus
l'accent sur la simplicité du modèle

46
00:02:12,240 --> 00:02:14,160
en augmentant le taux de régularisation.

47
00:02:14,160 --> 00:02:17,040
Je suis passée de 0,1 à 0,3.

48
00:02:17,040 --> 00:02:22,150
La performance du modèle s'est améliorée,
elle est passée de 0,179 à 0,160.

49
00:02:22,150 --> 00:02:25,245
Puis j'ai décidé d'aller encore plus loin

50
00:02:25,245 --> 00:02:27,425
et de régler
le taux de régularisation sur 1.

51
00:02:27,425 --> 00:02:28,925
C'était beaucoup trop.

52
00:02:28,925 --> 00:02:30,960
Mon modèle ne pouvait plus rien apprendre.

53
00:02:30,960 --> 00:02:32,900
Comme pour les autres hyperparamètres,

54
00:02:32,900 --> 00:02:36,560
le réglage du taux de régularisation
nécessite du temps et de la patience.

55
00:02:38,390 --> 00:02:41,140
En résumé, les modèles complexes
sont de mauvais modèles.

56
00:02:41,140 --> 00:02:46,205
Pour conserver un modèle simple,
vous pouvez appliquer la régularisation

57
00:02:46,205 --> 00:02:48,570
et ajuster progressivement le taux

58
00:02:48,570 --> 00:02:51,570
jusqu'à ce que vous obteniez
une performance acceptable.

59
00:02:52,540 --> 00:02:54,200
J'espère que ceci vous aidera

60
00:02:54,200 --> 00:02:56,900
à mieux comprendre
le concept de régularisation.