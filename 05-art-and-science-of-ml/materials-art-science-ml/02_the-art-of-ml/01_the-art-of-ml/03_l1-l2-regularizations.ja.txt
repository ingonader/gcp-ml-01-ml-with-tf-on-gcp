モデルの複雑さにペナルティを課す
正則化を行っていくわけですが ここでモデルの複雑さの
測定方法が問題になります L1とL2の正則化法はともにモデルの複雑さを
重みベクトルの大きさで表し それを抑制しようとします 線形代数では ベクトルの大きさはノルム関数で
表されることを覚えておきましょう L1とL2のノルム関数を簡単に見てみましょう 重みベクトルの次元数は何でもかまいませんが
2次元空間で可視化すると簡単です w0=a、w1=bのベクトルは
緑色の矢印のようになります このベクトルの大きさはどれくらいでしょうか すぐに cだと考えるかもしれません 原点からのユークリッド距離という
高校で学習する最も一般的な方法だからです cは(aの平方 + bの平方)の平方根です これは線形代数でL2ノルムと呼ばれ
式では二重縦線と添字2で示しますが 2は既知のデフォルトなので
添字は省略可能です L2ノルムはすべてのベクトル成分の
平方値の和の平方根として計算されます しかし これがベクトルの大きさを計算する
唯一の方法ではありません 別の一般的な計算方法はL1ノルムです L1はaの絶対値とbの絶対値の和を測定します ここで黄色にハイライトされている経路です モデルの複雑さの定義方法を探していることを
思い出してください 正則化法としてL1とL2を使用しました ここではモデルの複雑さを
重みベクトルの大きさの形で測定しました つまり 重みベクトルの大きさを特定の値より
小さくできれば目標を達成したと言えます 重みベクトルのL2ノルムが特定の値よりも
小さいという意味を可視化しましょう 特定の値を1とします L2は原点からのユークリッド距離です そのため目的のベクトルは
この原点を中心に半径1の円内にあるべきです L1ノルムを特定の値以下にする場合
どうなりますか 重みベクトルが存在できる領域は
この黄色いダイヤ形になります 一番重要な点は L1正則化の適用時に特定の重みの最適値が
0になる可能性があることです これは注目している最適領域が
極端なダイヤ形だからです L2正則化で見られる
きれいな円形とは対照的です ベクトルノルムを使用してモデルを
正則化するという問題に戻りましょう これはweight decayとしても知られる
L2の正則化適用方法です 重み値を原点近くに
維持しようとしていましたが 2D空間では重みベクトルは円内に限定されます この考えは簡単に3D空間に拡大できますが 3Dを超えると可視化が
難しくなるので試さないでください 機械学習で100%正直であるため
数学面で少しうそをつきます L2ノルムの平方を使って
導関数の計算を単純化します ここに新しいパラメータλがあります この単純なスカラー値を使って トレーニング誤差の最小化に対して
モデルの単純さをどの程度重視するかを制御します これは明示的に設定する必要がある
もう1つの調整パラメータです 残念ながら特定の問題に対する
最適値はデータに依存します そのため 手動であれ自動であれ
調整が必要です ハイパーパラメータ調整などの
ツールを使用しますが この点は次のモジュールで扱います L1正則化を適用するには
L2ノルムをL1ノルムと入れ替えるだけですが 結果は大きく異なる可能性があります L1正則化は
スパースなソリューションになります ここでのスパースとは 一部の重みで
最適値が0になるという事実を意味します 最適領域のダイヤ形を覚えているでしょうか L1正則化のこの特性は
特徴選択メカニズムとして広く使用されます 特徴選択は重みのサブセットを
0にすることでMLの問題を単純化します そして重み0は支障なく切り捨てられない
本質的特徴を示しています