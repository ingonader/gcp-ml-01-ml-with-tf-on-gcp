Utilizaremos métodos de regularización
que penalizan la complejidad del modelo. La pregunta es cómo podemos
medir la complejidad de un modelo. Los métodos de regularización L1 y L2
representan la complejidad de un modelo como la magnitud del vector de peso
y tratan de mantenerla bajo control. Recordemos que el álgebra lineal nos dice que la magnitud de un vector
está representada por la función norma. Veamos rápidamente
las funciones norma L1 y L2. El vector de peso puede tener
cualquier cantidad de dimensiones pero es más fácil visualizarlo
en un espacio bidimensional. Un vector con w0=a y w1=b se verá como esta flecha verde. ¿Cuál es la magnitud de este vector? Podríamos decir que es c
si aplicamos el método más común que aprendimos en la secundaria,
la distancia euclidiana desde el origen. c sería la raíz cuadrada de la suma
de a al cuadrado más b al cuadrado. En álgebra lineal, esto se llama norma L2,
simbolizada por las barras dobles y el subíndice 2 o sin subíndice,
porque el 2 se sobreentiende. La norma L2 se calcula
como la raíz cuadrada de la suma de los valores al cuadrado
de todos los componentes del vector. No es la única manera como se puede
calcular la magnitud de un vector. Otro método común es la norma L1. L1 mide el valor absoluto de a
más el valor absoluto de b básicamente, la línea amarilla
que aparece destacada. Recordemos que queremos un método
para definir la complejidad de un modelo. Utilizamos L1 y L2
como métodos de regularización en los que la complejidad del modelo
se mide como la magnitud del vector de peso. En otras palabras, si mantenemos
la magnitud de nuestro vector de peso menor que cierto valor,
conseguiremos nuestro objetivo. Visualicemos qué pasa si la norma L2 de nuestro vector de peso
es menor que cierto valor, digamos 1. Como L2 es la distancia euclidiana
desde el origen nuestro vector deseado debe estar
dentro de este círculo con un radio de 1 centrado en el origen. Si intentamos mantener la norma L1
menor a cierto valor el área en la que puede residir
nuestro vector de peso tendrá la forma de este diamante amarillo. Lo más importante de esto es que
si aplicamos la regularización L1 el valor óptimo de ciertos pesos
puede ser cero al final. Esto se debe a la forma de diamante
que tiene esta región óptima que es la que nos interesa. Es diferente de la forma circular
de la regularización L2. Volvamos a cómo podemos regularizar
nuestro modelo usando norma vectorial. Así se aplica una regularización L2,
conocida como decaimiento de peso. Recuerde que tratamos de mantener
el valor del peso cerca del origen. En un espacio 2D, el vector de peso
se ubicará dentro de un círculo. Este concepto se puede expandir
fácilmente a un espacio 3D pero más allá de 3D
es difícil de visualizar, no lo intente. Para ser honesta,
en el aprendizaje automático hacemos un poco de trampa
en las matemáticas. Usamos el cuadrado de la norma L2
para simplificar el cálculo de derivadas. Aquí tenemos un nuevo parámetro:
lambda. Es un valor escalar simple
que nos permite controlar el énfasis que queremos darle
a la simplicidad del modelo con respecto a minimizar
errores de entrenamiento. Es otro parámetro de ajuste
que se debe definir explícitamente. Lamentablemente, el mejor valor
para un problema depende de los datos. Tendremos que hacer ajustes,
sean manuales o automáticos con herramientas
como ajuste de hiperparámetros que veremos en el siguiente módulo. Para aplicar una regularización L1 simplemente reemplazamos la norma L2
con la norma L1. El resultado podría ser muy diferente. La regularización L1
ofrece una solución más dispersa. Dispersión, en este contexto,
se refiere a que algunos pesos terminan con el valor óptimo,
que es cero. ¿Recuerda la forma de diamante
del área óptima? Esta propiedad de la regularización L1 se usa mucho como un mecanismo
de selección de atributos. La selección de atributos
simplifica el problema del AA al hacer que un subconjunto de pesos
se transforme en cero. Cuando los pesos son cero,
se destaca el subconjunto de atributos que se pueden descartar sin riesgos.