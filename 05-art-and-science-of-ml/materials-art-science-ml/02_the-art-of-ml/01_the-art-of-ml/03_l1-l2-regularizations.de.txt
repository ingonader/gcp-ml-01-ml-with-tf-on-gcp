Modellkomplexität bestrafen wir
mithilfe von Regularisierungsmethoden. Die Frage dabei ist,
wie wir die Modellkomplexität messen. Die Regularisierungsmethoden L1
und L2 stellen die Modellkomplexität als Größe des Gewichtsvektors dar
und versuchen, diese im Rahmen zu halten. Aus der linearen Algebra wissen wir, dass die Größe eines Vektors
über die Normfunktion ermittelt wird. Wiederholen wir schnell
die L1- und L2-Normfunktionen. Der Gewichtsvektor
kann jede Anzahl Dimensionen haben, doch die Visualisierung
fällt uns bei zwei Dimensionen leichter. Der grüne Pfeil stellt
einen Vektor mit w0 = a und w1 = b dar. Wie ist nun die Größe dieses Vektors? Ihre erste Antwort könnte "c" sein. Dann haben Sie die Theorie angewendet,
die wir meist schon aus der Schule kennen: die euklidische Entfernung vom Ursprung. C wäre die Wurzel
aus der Summe der Quadrate von a und b. Dies ist die L2-Norm
aus der linearen Algebra. Sie wird angezeigt
durch zwei senkrechte Striche und eine tiefgestellte "2"
oder ohne "2", da dies der Standard ist. Die L2-Norm ist die Wurzel der Summe
der Quadrate aller Vektorkomponenten. Es gibt einen weiteren Weg,
die Vektorgröße zu berechnen. Noch eine übliche Methode ist die L1-Norm. L1 ist der Absolutwert von a
plus dem Absolutwert von b. Das ist hier der gelbe Pfad. Wir suchen immer noch
eine Definition für Modellkomplexität. Wir haben die Regularisierungsmethoden
L1 und L2 verwendet. Dort wird die Modellkomplexität
über die Gewichtsvektorgröße gemessen. Wenn wir also
die Größe unseres Gewichtsvektors unter einem bestimmten Wert halten,
haben wir das Ziel erreicht. Visualisieren wir, was es bedeutet, wenn die L2-Norm des Gewichtsvektors
unter einem Wert wie 1 bleibt. Da L2 die euklidische Entfernung
vom Ursprung ist, sollte unser Vektor vom Ursprung ausgehend
in diesem Kreis mit dem Radius 1 bleiben. Wenn L1
unter einem bestimmten Wert bleiben soll, nimmt der maximale Bereich
für unseren Gewichtsvektor die Form der gelben Raute an. Wichtig hierbei ist,
dass bei Anwendung der L1-Regularisierung der optimale Wert
für bestimmte Gewichtungen null sein kann. Das liegt an der extremen Rautenform
des für uns interessanten Optimalbereichs gegenüber der gleichmäßigen Kreisform
bei der L2-Regularisierung. Wenden wir uns dem Modell zu, das wir
mit der Vektornorm regularisieren möchten. So wenden Sie die L2-Regularisierung
oder auch Gewichtsdämpfung an. Wir möchten die Gewichtungswerte
weiterhin nahe am Ursprung halten. Im 2D-Raum sollte der Gewichtungsfaktor
innerhalb eines Kreises bleiben. Sie können das Konzept
einfach auf den 3D-Raum ausweiten, doch darüber hinaus 
ist das sehr schwierig. Beim maschinellen Lernen
mogeln wir ein wenig bei der Mathematik. Wir verwenden das Quadrat der L2-Norm, was
Berechnungen von Ableitungen vereinfacht. Wir haben hier
einen neuen Parameter eingeführt, Lambda. Das ist ein Skalarwert, mit dem wir
die Gewichtung der Modelleinfachheit steuern können gegenüber
der Minimierung von Trainingsfehlern. Dieser Optimierungsparameter
muss explizit festgelegt werden. Leider hängt der beste Wert für ein
vorhandenes Problem von den Daten ab. Wir müssen daher
manuell oder automatisch optimieren. Dies ist über Tools
wie Hyperparameter-Abstimmung möglich, das wir im nächsten Modul behandeln. Zur Anwendung der L1-Regularisierung
tauschen wir nur die L2-Norm gegen L1 aus. Das Ergebnis
kann sich aber sehr unterscheiden. Die L1-Regularisierung führt
zu einer weniger dichten Lösung. Dichte bezieht sich
in diesem Kontext darauf, dass einige Gewichtungen
am Ende einen Optimalwert von null haben. Erinnern Sie sich
an die Rautenform des Optimalbereichs? Diese Eigenschaft der L1-Regularisierung verwendet ausgiebig
einen Mechanismus zur Merkmalauswahl. Die Merkmalauswahl
vereinfacht das ML-Problem, indem eine Teilmenge
der Gewichtungen null annimmt. Eine Gewichtung von null markiert dann
die Merkmale, die Sie verwerfen können.