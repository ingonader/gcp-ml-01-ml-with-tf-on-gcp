1
00:00:00,520 --> 00:00:05,310
Utilizaremos métodos de regularización
que penalizan la complejidad del modelo.

2
00:00:05,520 --> 00:00:08,430
La pregunta es cómo podemos
medir la complejidad de un modelo.

3
00:00:09,120 --> 00:00:13,460
Los métodos de regularización L1 y L2
representan la complejidad de un modelo

4
00:00:13,530 --> 00:00:17,450
como la magnitud del vector de peso
y tratan de mantenerla bajo control.

5
00:00:17,950 --> 00:00:19,900
Recordemos que el álgebra lineal

6
00:00:20,480 --> 00:00:24,960
nos dice que la magnitud de un vector
está representada por la función norma.

7
00:00:25,380 --> 00:00:28,670
Veamos rápidamente
las funciones norma L1 y L2.

8
00:00:29,140 --> 00:00:31,950
El vector de peso puede tener
cualquier cantidad de dimensiones

9
00:00:32,030 --> 00:00:34,900
pero es más fácil visualizarlo
en un espacio bidimensional.

10
00:00:35,010 --> 00:00:40,110
Un vector con w0=a y w1=b

11
00:00:40,350 --> 00:00:42,170
se verá como esta flecha verde.

12
00:00:42,570 --> 00:00:44,960
¿Cuál es la magnitud de este vector?

13
00:00:46,320 --> 00:00:50,790
Podríamos decir que es c
si aplicamos el método más común

14
00:00:50,790 --> 00:00:54,430
que aprendimos en la secundaria,
la distancia euclidiana desde el origen.

15
00:00:55,010 --> 00:00:59,199
c sería la raíz cuadrada de la suma
de a al cuadrado más b al cuadrado.

16
00:01:00,440 --> 00:01:05,640
En álgebra lineal, esto se llama norma L2,
simbolizada por las barras dobles

17
00:01:05,780 --> 00:01:10,420
y el subíndice 2 o sin subíndice,
porque el 2 se sobreentiende.

18
00:01:11,160 --> 00:01:13,790
La norma L2 se calcula
como la raíz cuadrada

19
00:01:13,790 --> 00:01:17,170
de la suma de los valores al cuadrado
de todos los componentes del vector.

20
00:01:17,710 --> 00:01:21,690
No es la única manera como se puede
calcular la magnitud de un vector.

21
00:01:23,030 --> 00:01:25,570
Otro método común es la norma L1.

22
00:01:25,870 --> 00:01:30,150
L1 mide el valor absoluto de a
más el valor absoluto de b

23
00:01:30,490 --> 00:01:33,120
básicamente, la línea amarilla
que aparece destacada.

24
00:01:33,830 --> 00:01:38,130
Recordemos que queremos un método
para definir la complejidad de un modelo.

25
00:01:38,390 --> 00:01:41,240
Utilizamos L1 y L2
como métodos de regularización

26
00:01:41,430 --> 00:01:44,870
en los que la complejidad del modelo
se mide como la magnitud

27
00:01:44,870 --> 00:01:46,050
del vector de peso.

28
00:01:46,760 --> 00:01:50,520
En otras palabras, si mantenemos
la magnitud de nuestro vector de peso

29
00:01:50,580 --> 00:01:53,770
menor que cierto valor,
conseguiremos nuestro objetivo.

30
00:01:54,730 --> 00:01:57,410
Visualicemos qué pasa

31
00:01:57,410 --> 00:02:01,650
si la norma L2 de nuestro vector de peso
es menor que cierto valor, digamos 1.

32
00:02:02,310 --> 00:02:05,520
Como L2 es la distancia euclidiana
desde el origen

33
00:02:05,680 --> 00:02:09,970
nuestro vector deseado debe estar
dentro de este círculo con un radio de 1

34
00:02:10,140 --> 00:02:11,500
centrado en el origen.

35
00:02:13,020 --> 00:02:15,900
Si intentamos mantener la norma L1
menor a cierto valor

36
00:02:16,110 --> 00:02:18,850
el área en la que puede residir
nuestro vector de peso

37
00:02:18,850 --> 00:02:21,270
tendrá la forma de este diamante amarillo.

38
00:02:22,060 --> 00:02:26,450
Lo más importante de esto es que
si aplicamos la regularización L1

39
00:02:26,660 --> 00:02:29,930
el valor óptimo de ciertos pesos
puede ser cero al final.

40
00:02:30,470 --> 00:02:34,520
Esto se debe a la forma de diamante
que tiene esta región óptima

41
00:02:34,560 --> 00:02:36,135
que es la que nos interesa.

42
00:02:36,495 --> 00:02:40,726
Es diferente de la forma circular
de la regularización L2.

43
00:02:42,831 --> 00:02:48,650
Volvamos a cómo podemos regularizar
nuestro modelo usando norma vectorial.

44
00:02:49,300 --> 00:02:53,990
Así se aplica una regularización L2,
conocida como decaimiento de peso.

45
00:02:55,260 --> 00:02:58,690
Recuerde que tratamos de mantener
el valor del peso cerca del origen.

46
00:02:58,810 --> 00:03:02,920
En un espacio 2D, el vector de peso
se ubicará dentro de un círculo.

47
00:03:03,370 --> 00:03:06,300
Este concepto se puede expandir
fácilmente a un espacio 3D

48
00:03:06,300 --> 00:03:09,110
pero más allá de 3D
es difícil de visualizar, no lo intente.

49
00:03:10,050 --> 00:03:12,345
Para ser honesta,
en el aprendizaje automático

50
00:03:12,345 --> 00:03:14,580
hacemos un poco de trampa
en las matemáticas.

51
00:03:14,770 --> 00:03:19,200
Usamos el cuadrado de la norma L2
para simplificar el cálculo de derivadas.

52
00:03:20,090 --> 00:03:22,777
Aquí tenemos un nuevo parámetro:
lambda.

53
00:03:22,987 --> 00:03:26,365
Es un valor escalar simple
que nos permite controlar

54
00:03:26,365 --> 00:03:29,401
el énfasis que queremos darle
a la simplicidad del modelo

55
00:03:29,401 --> 00:03:31,631
con respecto a minimizar
errores de entrenamiento.

56
00:03:33,604 --> 00:03:37,650
Es otro parámetro de ajuste
que se debe definir explícitamente.

57
00:03:37,730 --> 00:03:42,420
Lamentablemente, el mejor valor
para un problema depende de los datos.

58
00:03:42,830 --> 00:03:47,035
Tendremos que hacer ajustes,
sean manuales o automáticos

59
00:03:47,035 --> 00:03:49,530
con herramientas
como ajuste de hiperparámetros

60
00:03:50,010 --> 00:03:51,890
que veremos en el siguiente módulo.

61
00:03:53,840 --> 00:03:56,085
Para aplicar una regularización L1

62
00:03:56,405 --> 00:04:00,300
simplemente reemplazamos la norma L2
con la norma L1.

63
00:04:00,450 --> 00:04:03,040
El resultado podría ser muy diferente.

64
00:04:04,420 --> 00:04:08,520
La regularización L1
ofrece una solución más dispersa.

65
00:04:09,130 --> 00:04:12,710
Dispersión, en este contexto,
se refiere a que algunos pesos

66
00:04:12,710 --> 00:04:15,150
terminan con el valor óptimo,
que es cero.

67
00:04:15,680 --> 00:04:18,120
¿Recuerda la forma de diamante
del área óptima?

68
00:04:18,380 --> 00:04:20,519
Esta propiedad de la regularización L1

69
00:04:20,559 --> 00:04:23,510
se usa mucho como un mecanismo
de selección de atributos.

70
00:04:23,540 --> 00:04:26,518
La selección de atributos
simplifica el problema del AA

71
00:04:26,578 --> 00:04:29,640
al hacer que un subconjunto de pesos
se transforme en cero.

72
00:04:30,300 --> 00:04:33,182
Cuando los pesos son cero,
se destaca el subconjunto de atributos

73
00:04:33,182 --> 00:04:35,242
que se pueden descartar sin riesgos.