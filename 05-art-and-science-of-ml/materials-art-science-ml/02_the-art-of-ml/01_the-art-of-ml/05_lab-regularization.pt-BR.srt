1
00:00:00,000 --> 00:00:04,780
O objetivo deste laboratório é
testar a regularização L1 e L2

2
00:00:04,780 --> 00:00:06,965
e observar visualmente
os efeitos.

3
00:00:06,965 --> 00:00:09,475
Vamos analisar
os resultados juntos.

4
00:00:09,475 --> 00:00:13,425
Iniciei o TensorFlow
Playground com este link.

5
00:00:13,425 --> 00:00:15,260
Antes de começar
o loop de treinamento,

6
00:00:15,260 --> 00:00:18,570
eu adicionei ruído ao
conjunto de dados. Escolhi 30.

7
00:00:18,570 --> 00:00:22,305
Em vez de usar
X1 e X2 como recursos,

8
00:00:22,305 --> 00:00:24,730
eu usei cruzamentos também.

9
00:00:24,730 --> 00:00:29,305
Primeiro, eu testei o
modelo sem a regularização.

10
00:00:29,305 --> 00:00:32,905
Como esperado, a perda
do treinamento convergiu,

11
00:00:32,905 --> 00:00:35,080
mas a do teste
permaneceu alta.

12
00:00:35,080 --> 00:00:37,585
Veja o formato do
modelo de treinamento.

13
00:00:37,585 --> 00:00:40,350
Percebeu a forma estranha
na região em azul?

14
00:00:40,350 --> 00:00:43,745
O modelo estava
se sobreajustando

15
00:00:43,745 --> 00:00:47,150
para aprender o ruído
nos dados de treinamento.

16
00:00:47,150 --> 00:00:49,855
Eu criei um modelo ruim,

17
00:00:49,855 --> 00:00:51,955
mas isso não pode
ser generalizado.

18
00:00:51,955 --> 00:00:57,080
Em seguida, fiz o modelo
aplicar a navalha de Occam.

19
00:00:57,080 --> 00:01:02,150
Podemos penalizar a complexidade
se aplicarmos a regularização L1.

20
00:01:02,150 --> 00:01:03,420
Depois disso,

21
00:01:03,420 --> 00:01:05,715
observei um desempenho
muito melhor.

22
00:01:05,715 --> 00:01:09,340
A forma em azul
cancelou melhor o ruído.

23
00:01:09,340 --> 00:01:12,130
E a perda do teste
convergiu bem.

24
00:01:12,130 --> 00:01:14,115
Esse modelo é
claramente melhor.

25
00:01:14,115 --> 00:01:18,320
Também quero chamar a atenção para
os recursos ignorados pelo modelo.

26
00:01:18,320 --> 00:01:22,930
Não há linhas
que emanam de X1, X2

27
00:01:22,930 --> 00:01:26,010
ou X1 multiplicado por X2.

28
00:01:26,470 --> 00:01:31,240
Lembre-se, a regularização L1 pode
ser usada para selecionar recursos.

29
00:01:31,240 --> 00:01:33,875
Em seguida, eu testei
a regularização L2.

30
00:01:33,875 --> 00:01:37,250
Em comparação com L1,
não houve seleção de recursos.

31
00:01:37,250 --> 00:01:40,895
Os recursos mais relevantes
tiveram maior peso,

32
00:01:40,895 --> 00:01:44,945
mas os outros ainda
foram usados com peso menor.

33
00:01:44,945 --> 00:01:49,015
Isso pode não ser visível
na captura de tela, mas, ao vivo,

34
00:01:49,015 --> 00:01:51,855
as linhas que saíam de X1, X2

35
00:01:51,855 --> 00:01:55,080
e X1 multiplicado por X2
mostraram movimento.

36
00:01:56,170 --> 00:01:58,570
O peso de um recurso
é visualizado

37
00:01:58,570 --> 00:02:02,275
pela grossura da linha
que emana dele.

38
00:02:02,275 --> 00:02:04,880
Também não há
nenhuma curvatura absurda.

39
00:02:04,880 --> 00:02:07,080
A perda do teste foi saudável.

40
00:02:07,080 --> 00:02:08,535
Parece um bom modelo.

41
00:02:08,535 --> 00:02:11,170
Em seguida, eu enfatizei
a simplicidade do modelo

42
00:02:11,170 --> 00:02:14,020
um pouco mais aumentando
a taxa de regularização.

43
00:02:14,020 --> 00:02:17,040
Eu mudei de 0,1 para 0,3.

44
00:02:17,040 --> 00:02:22,150
O desempenho do modelo
melhorou de 0,179 para 0,160.

45
00:02:22,150 --> 00:02:27,425
Decidi aumentar ainda mais e definir
a taxa de regularização como 1.

46
00:02:27,425 --> 00:02:28,925
Foi demais.

47
00:02:28,925 --> 00:02:30,960
Meu modelo não conseguia
aprender nada.

48
00:02:30,960 --> 00:02:32,900
Assim como outros
hiperparâmetros,

49
00:02:32,900 --> 00:02:36,620
ajustar a taxa de regularização
exige tempo e paciência.

50
00:02:38,280 --> 00:02:41,140
Para recapitular,
modelos complexos são ruins.

51
00:02:41,540 --> 00:02:44,055
Uma das maneiras de
manter a simplicidade

52
00:02:44,055 --> 00:02:51,560
é usar a regularização e ajustar a
taxa até alcançar um desempenho bom.

53
00:02:53,160 --> 00:02:56,910
Espero que isso tenha ajudado
a entender melhor a regularização.