1
00:00:00,000 --> 00:00:04,780
The goal of this lab was for you to experiment with L1 and L2 regularization,

2
00:00:04,780 --> 00:00:06,965
and visually observe their effects.

3
00:00:06,965 --> 00:00:09,475
Let's review the findings together.

4
00:00:09,475 --> 00:00:13,425
I launched Tensorflow playground using the link shown here.

5
00:00:13,425 --> 00:00:15,200
Before I started the training loop,

6
00:00:15,200 --> 00:00:18,570
I added some noise to the dataset. I went with 30.

7
00:00:18,570 --> 00:00:22,305
Instead of just using X1 and X2 as features,

8
00:00:22,305 --> 00:00:24,730
I used some feature crosses as well.

9
00:00:24,730 --> 00:00:29,305
First I tried training without regularization to see how my model did.

10
00:00:29,305 --> 00:00:32,905
As expected, the training loss converge nicely,

11
00:00:32,905 --> 00:00:35,080
but the test loss stayed high.

12
00:00:35,080 --> 00:00:37,585
Pay attention to the shape of the training model.

13
00:00:37,585 --> 00:00:40,350
Notice the odd shape of a blue region?

14
00:00:40,350 --> 00:00:43,745
Clearly the model was overfitting itself,

15
00:00:43,745 --> 00:00:47,150
to learn all the noise in the training data.

16
00:00:47,150 --> 00:00:49,855
Well, I ended up cooking a bad model,

17
00:00:49,855 --> 00:00:51,955
it can't be generalized.

18
00:00:51,955 --> 00:00:57,080
Next I forced my model to apply Occam's Razor and keep it simple.

19
00:00:57,080 --> 00:01:02,150
Remember one of the ways to penalize complexity was to apply L1 regularization.

20
00:01:02,150 --> 00:01:03,420
After I did that,

21
00:01:03,420 --> 00:01:05,715
I observed a much better performance.

22
00:01:05,715 --> 00:01:09,340
The blue shape was much smoother cancelling the noise.

23
00:01:09,340 --> 00:01:12,130
Also the test loss converged nicely,

24
00:01:12,130 --> 00:01:14,115
this is clearly a better model.

25
00:01:14,115 --> 00:01:18,320
I also want you to pay attention to the features which were ignored by my model.

26
00:01:18,320 --> 00:01:22,130
Notice there were no lines emanating from X1,

27
00:01:22,130 --> 00:01:25,520
X2, or 1 multiplied by X2.

28
00:01:25,520 --> 00:01:31,240
To remind you, L1 regularization can be used as a feature selection mechanism.

29
00:01:31,240 --> 00:01:33,875
Next, I tried L2 regularization.

30
00:01:33,875 --> 00:01:37,250
Compared to L1 there was no feature selection going on.

31
00:01:37,250 --> 00:01:40,895
The most relevant features had strong weights associated with them,

32
00:01:40,895 --> 00:01:44,945
but the rest and they're still in play with weaker weights.

33
00:01:44,945 --> 00:01:49,015
This may not be visible in the screen capture but while running live,

34
00:01:49,015 --> 00:01:51,855
the lines coming out of X1, X2,

35
00:01:51,855 --> 00:01:54,890
and X1 times X2 showed movement.

36
00:01:54,890 --> 00:01:58,150
To remind you the weight of a feature is

37
00:01:58,150 --> 00:02:02,275
visualized by the thickness of the line emanating from that feature.

38
00:02:02,275 --> 00:02:04,880
There was no crazy curvature either.

39
00:02:04,880 --> 00:02:07,080
Test loss was nice and healthy.

40
00:02:07,080 --> 00:02:08,535
It looked like a good model.

41
00:02:08,535 --> 00:02:11,170
Next I tried emphasizing model simplicity

42
00:02:11,170 --> 00:02:14,020
a bit more by increasing the regularization rate.

43
00:02:14,020 --> 00:02:17,040
I changed it from 0.1 to 0.3.

44
00:02:17,040 --> 00:02:22,150
Model performance improved from 0.179 To point 0.160.

45
00:02:22,150 --> 00:02:27,425
Then I decided to crank it up even further and set regularization rate to one.

46
00:02:27,425 --> 00:02:28,925
That was too much.

47
00:02:28,925 --> 00:02:30,960
My model couldn't learn anything.

48
00:02:30,960 --> 00:02:32,900
Just like other hyperparameters,

49
00:02:32,900 --> 00:02:37,330
adjusting regularization rate takes a bit of time and patience.

50
00:02:37,390 --> 00:02:41,140
To recap, complex models are bad.

51
00:02:41,140 --> 00:02:44,955
One of the ways to keep our model simple is by applying

52
00:02:44,955 --> 00:02:52,160
regularization and adjust the rate until we achieve an acceptable performance.

53
00:02:52,300 --> 00:02:56,910
I hope this helps you get comfortable with the concept of regularization.