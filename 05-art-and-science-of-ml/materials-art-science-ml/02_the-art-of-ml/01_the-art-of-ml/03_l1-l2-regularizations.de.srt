1
00:00:00,520 --> 00:00:05,520
Modellkomplexität bestrafen wir
mithilfe von Regularisierungsmethoden.

2
00:00:05,520 --> 00:00:09,180
Die Frage dabei ist,
wie wir die Modellkomplexität messen.

3
00:00:09,180 --> 00:00:13,630
Die Regularisierungsmethoden L1
und L2 stellen die Modellkomplexität

4
00:00:13,630 --> 00:00:17,950
als Größe des Gewichtsvektors dar
und versuchen, diese im Rahmen zu halten.

5
00:00:17,950 --> 00:00:20,620
Aus der linearen Algebra wissen wir,

6
00:00:20,620 --> 00:00:25,380
dass die Größe eines Vektors
über die Normfunktion ermittelt wird.

7
00:00:25,380 --> 00:00:29,200
Wiederholen wir schnell
die L1- und L2-Normfunktionen.

8
00:00:29,200 --> 00:00:31,880
Der Gewichtsvektor
kann jede Anzahl Dimensionen haben,

9
00:00:31,880 --> 00:00:35,010
doch die Visualisierung
fällt uns bei zwei Dimensionen leichter.

10
00:00:35,010 --> 00:00:42,570
Der grüne Pfeil stellt
einen Vektor mit w0 = a und w1 = b dar.

11
00:00:42,570 --> 00:00:45,210
Wie ist nun die Größe dieses Vektors?

12
00:00:46,320 --> 00:00:48,540
Ihre erste Antwort könnte "c" sein.

13
00:00:48,540 --> 00:00:52,370
Dann haben Sie die Theorie angewendet,
die wir meist schon aus der Schule kennen:

14
00:00:52,370 --> 00:00:54,790
die euklidische Entfernung vom Ursprung.

15
00:00:54,790 --> 00:00:59,209
C wäre die Wurzel
aus der Summe der Quadrate von a und b.

16
00:01:00,440 --> 00:01:03,750
Dies ist die L2-Norm
aus der linearen Algebra.

17
00:01:03,750 --> 00:01:06,030
Sie wird angezeigt
durch zwei senkrechte Striche

18
00:01:06,030 --> 00:01:11,160
und eine tiefgestellte "2"
oder ohne "2", da dies der Standard ist.

19
00:01:11,160 --> 00:01:17,680
Die L2-Norm ist die Wurzel der Summe
der Quadrate aller Vektorkomponenten.

20
00:01:17,680 --> 00:01:21,900
Es gibt einen weiteren Weg,
die Vektorgröße zu berechnen.

21
00:01:23,030 --> 00:01:26,010
Noch eine übliche Methode ist die L1-Norm.

22
00:01:26,010 --> 00:01:30,490
L1 ist der Absolutwert von a
plus dem Absolutwert von b.

23
00:01:30,490 --> 00:01:33,830
Das ist hier der gelbe Pfad.

24
00:01:33,830 --> 00:01:38,420
Wir suchen immer noch
eine Definition für Modellkomplexität.

25
00:01:38,420 --> 00:01:41,470
Wir haben die Regularisierungsmethoden
L1 und L2 verwendet.

26
00:01:41,470 --> 00:01:46,880
Dort wird die Modellkomplexität
über die Gewichtsvektorgröße gemessen.

27
00:01:46,880 --> 00:01:50,460
Wenn wir also
die Größe unseres Gewichtsvektors

28
00:01:50,460 --> 00:01:54,720
unter einem bestimmten Wert halten,
haben wir das Ziel erreicht.

29
00:01:54,730 --> 00:01:57,410
Visualisieren wir, was es bedeutet,

30
00:01:57,410 --> 00:02:01,930
wenn die L2-Norm des Gewichtsvektors
unter einem Wert wie 1 bleibt.

31
00:02:01,930 --> 00:02:05,670
Da L2 die euklidische Entfernung
vom Ursprung ist,

32
00:02:05,670 --> 00:02:11,630
sollte unser Vektor vom Ursprung ausgehend
in diesem Kreis mit dem Radius 1 bleiben.

33
00:02:13,020 --> 00:02:15,910
Wenn L1
unter einem bestimmten Wert bleiben soll,

34
00:02:15,910 --> 00:02:18,810
nimmt der maximale Bereich
für unseren Gewichtsvektor

35
00:02:18,810 --> 00:02:21,330
die Form der gelben Raute an.

36
00:02:22,060 --> 00:02:26,660
Wichtig hierbei ist,
dass bei Anwendung der L1-Regularisierung

37
00:02:26,660 --> 00:02:30,550
der optimale Wert
für bestimmte Gewichtungen null sein kann.

38
00:02:30,550 --> 00:02:36,490
Das liegt an der extremen Rautenform
des für uns interessanten Optimalbereichs

39
00:02:36,495 --> 00:02:40,666
gegenüber der gleichmäßigen Kreisform
bei der L2-Regularisierung.

40
00:02:42,831 --> 00:02:49,300
Wenden wir uns dem Modell zu, das wir
mit der Vektornorm regularisieren möchten.

41
00:02:49,300 --> 00:02:54,330
So wenden Sie die L2-Regularisierung
oder auch Gewichtsdämpfung an.

42
00:02:55,260 --> 00:02:58,810
Wir möchten die Gewichtungswerte
weiterhin nahe am Ursprung halten.

43
00:02:58,810 --> 00:03:03,430
Im 2D-Raum sollte der Gewichtungsfaktor
innerhalb eines Kreises bleiben.

44
00:03:03,430 --> 00:03:06,360
Sie können das Konzept
einfach auf den 3D-Raum ausweiten,

45
00:03:06,360 --> 00:03:09,200
doch darüber hinaus 
ist das sehr schwierig.

46
00:03:10,050 --> 00:03:14,730
Beim maschinellen Lernen
mogeln wir ein wenig bei der Mathematik.

47
00:03:14,730 --> 00:03:20,080
Wir verwenden das Quadrat der L2-Norm, was
Berechnungen von Ableitungen vereinfacht.

48
00:03:20,090 --> 00:03:22,807
Wir haben hier
einen neuen Parameter eingeführt, Lambda.

49
00:03:22,807 --> 00:03:26,525
Das ist ein Skalarwert, mit dem wir
die Gewichtung der Modelleinfachheit

50
00:03:26,525 --> 00:03:31,701
steuern können gegenüber
der Minimierung von Trainingsfehlern.

51
00:03:33,514 --> 00:03:37,730
Dieser Optimierungsparameter
muss explizit festgelegt werden.

52
00:03:37,730 --> 00:03:42,830
Leider hängt der beste Wert für ein
vorhandenes Problem von den Daten ab.

53
00:03:42,830 --> 00:03:47,220
Wir müssen daher
manuell oder automatisch optimieren.

54
00:03:47,220 --> 00:03:50,075
Dies ist über Tools
wie Hyperparameter-Abstimmung möglich,

55
00:03:50,075 --> 00:03:52,390
das wir im nächsten Modul behandeln.

56
00:03:53,840 --> 00:04:00,430
Zur Anwendung der L1-Regularisierung
tauschen wir nur die L2-Norm gegen L1 aus.

57
00:04:00,430 --> 00:04:03,220
Das Ergebnis
kann sich aber sehr unterscheiden.

58
00:04:04,560 --> 00:04:09,190
Die L1-Regularisierung führt
zu einer weniger dichten Lösung.

59
00:04:09,190 --> 00:04:11,490
Dichte bezieht sich
in diesem Kontext darauf,

60
00:04:11,490 --> 00:04:15,680
dass einige Gewichtungen
am Ende einen Optimalwert von null haben.

61
00:04:15,680 --> 00:04:18,380
Erinnern Sie sich
an die Rautenform des Optimalbereichs?

62
00:04:18,380 --> 00:04:20,519
Diese Eigenschaft der L1-Regularisierung

63
00:04:20,519 --> 00:04:23,510
verwendet ausgiebig
einen Mechanismus zur Merkmalauswahl.

64
00:04:23,510 --> 00:04:26,528
Die Merkmalauswahl
vereinfacht das ML-Problem,

65
00:04:26,528 --> 00:04:29,920
indem eine Teilmenge
der Gewichtungen null annimmt.

66
00:04:29,920 --> 00:04:35,220
Eine Gewichtung von null markiert dann
die Merkmale, die Sie verwerfen können.