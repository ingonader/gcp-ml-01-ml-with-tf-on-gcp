1
00:00:00,000 --> 00:00:04,780
In diesem Lab sollten Sie mit L1-
und L2-Regularisierung experimentieren

2
00:00:04,780 --> 00:00:06,965
und die Auswirkungen beobachten.

3
00:00:06,965 --> 00:00:09,475
Schauen wir uns die Ergebnisse
zusammen an.

4
00:00:09,475 --> 00:00:13,425
Ich habe TensorFlow Playground
über den hier angezeigten Link gestartet.

5
00:00:13,425 --> 00:00:15,200
Vor dem Start der Trainingsschleife

6
00:00:15,200 --> 00:00:18,570
habe ich dem Dataset Rauschen
mit einem Wert von 30 hinzugefügt.

7
00:00:18,570 --> 00:00:22,305
Anstatt nur x1 und x2
als Merkmale zu verwenden,

8
00:00:22,305 --> 00:00:24,730
habe ich auch Merkmalkreuzungen verwendet.

9
00:00:24,730 --> 00:00:27,170
Zuerst habe ich
ohne Regularisierung trainiert,

10
00:00:27,170 --> 00:00:29,305
um mein Modell einzuordnen.

11
00:00:29,305 --> 00:00:32,905
Das Trainingsverlust
konvergierte wie erwartet gut,

12
00:00:32,905 --> 00:00:35,080
doch blieb der Testverlust hoch.

13
00:00:35,080 --> 00:00:37,585
Sehen Sie sich
die Form des Trainingsmodells an.

14
00:00:37,585 --> 00:00:40,350
Bemerken Sie
die seltsame Form des blauen Bereichs?

15
00:00:40,350 --> 00:00:43,745
Das Modell hat sich klar überangepasst,

16
00:00:43,745 --> 00:00:47,150
um das Rauschen
in den Trainingsdaten zu erlernen.

17
00:00:47,150 --> 00:00:49,855
Ich habe das Modell am Ende ruiniert.

18
00:00:49,855 --> 00:00:51,955
Es kann nicht generalisiert werden.

19
00:00:51,955 --> 00:00:57,080
Danach habe ich mein Modell angewiesen,
das Sparsamkeitsprinzip anzuwenden.

20
00:00:57,080 --> 00:01:02,150
Ein Weg, Komplexität zu bestrafen,
war das Anwenden der L1-Regularisierung.

21
00:01:02,150 --> 00:01:05,750
Danach ergab sich
eine deutlich bessere Leistung.

22
00:01:05,750 --> 00:01:09,340
Die blaue Form war glatter
und konnte das Rauschen unterdrücken.

23
00:01:09,340 --> 00:01:12,130
Auch der Testverlust konvergierte gut.

24
00:01:12,130 --> 00:01:14,115
Dies ist eindeutig ein besseres Modell.

25
00:01:14,115 --> 00:01:18,320
Achten Sie auf die Merkmale,
die von meinem Modell ignoriert wurden.

26
00:01:18,320 --> 00:01:26,170
Von x1, x2
und x1 mal x2 gehen keine Linien aus.

27
00:01:26,170 --> 00:01:31,240
Ich kann die L1-Regularisierung nämlich
zur Merkmalauswahl verwenden.

28
00:01:31,240 --> 00:01:33,875
Danach habe ich
die L2-Regularisierung probiert.

29
00:01:33,875 --> 00:01:37,250
Im Gegensatz zu L1
fand keine Merkmalauswahl statt.

30
00:01:37,250 --> 00:01:40,895
Die relevantesten Merkmale
hatten eine starke Gewichtung,

31
00:01:40,895 --> 00:01:44,945
der Rest wurde aber weiterhin
mit geringerer Gewichtung berücksichtigt.

32
00:01:44,945 --> 00:01:47,595
Dies ist im Screenshot
vielleicht nicht ersichtlich,

33
00:01:47,595 --> 00:01:54,875
doch beim Ausführen zeigten die Linien
aus x1, x2 und x1 mal x2 Bewegung.

34
00:01:54,890 --> 00:01:58,150
Die Gewichtung eines Merkmals

35
00:01:58,150 --> 00:02:02,275
wird durch die Dicke
der von ihr ausgehenden Linie dargestellt.

36
00:02:02,275 --> 00:02:04,880
Auch der Graph war nicht ungewöhnlich.

37
00:02:04,880 --> 00:02:07,080
Der Testverlust sah gut aus.

38
00:02:07,080 --> 00:02:08,535
Ergebnis war ein gutes Modell.

39
00:02:08,535 --> 00:02:11,170
Dann habe ich
mehr Wert auf Modelleinfachheit gelegt,

40
00:02:11,170 --> 00:02:14,020
indem ich
die Regularisierungsrate erhöht habe.

41
00:02:14,020 --> 00:02:17,040
Ich änderte sie von 0,1 auf 0,3.

42
00:02:17,040 --> 00:02:22,150
Die Modellleistung
verbesserte sich von 0,179 zu 0,160.

43
00:02:22,150 --> 00:02:27,425
Mit einer Regularisierungsrate von 1
wollte ich dies dann weiter verstärken.

44
00:02:27,425 --> 00:02:28,925
Das war aber zu viel.

45
00:02:28,925 --> 00:02:30,960
Mein Modell konnte gar nichts lernen.

46
00:02:30,960 --> 00:02:32,900
Wie bei den anderen Hyperparametern

47
00:02:32,900 --> 00:02:37,530
erfordert die Anpassung
der Regularisierungsrate Zeit und Geduld.

48
00:02:37,530 --> 00:02:41,140
Zusammengefasst
sind komplexe Modelle schlecht.

49
00:02:41,140 --> 00:02:46,235
Wir können unser Modell einfach halten,
indem wir Regularisierung anwenden

50
00:02:46,235 --> 00:02:52,280
und die Rate anpassen, bis wir
eine annehmbare Leistung erreichen.

51
00:02:52,280 --> 00:02:56,910
Ich hoffe, ich konnte Ihnen das Konzept
der Regularisierung vermitteln.