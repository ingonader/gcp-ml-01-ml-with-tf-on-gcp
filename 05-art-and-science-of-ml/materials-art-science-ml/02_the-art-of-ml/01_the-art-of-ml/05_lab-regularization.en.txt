The goal of this lab was for you to experiment with L1 and L2 regularization, and visually observe their effects. Let's review the findings together. I launched Tensorflow playground using the link shown here. Before I started the training loop, I added some noise to the dataset. I went with 30. Instead of just using X1 and X2 as features, I used some feature crosses as well. First I tried training without regularization to see how my model did. As expected, the training loss converge nicely, but the test loss stayed high. Pay attention to the shape of the training model. Notice the odd shape of a blue region? Clearly the model was overfitting itself, to learn all the noise in the training data. Well, I ended up cooking a bad model, it can't be generalized. Next I forced my model to apply Occam's Razor and keep it simple. Remember one of the ways to penalize complexity was to apply L1 regularization. After I did that, I observed a much better performance. The blue shape was much smoother cancelling the noise. Also the test loss converged nicely, this is clearly a better model. I also want you to pay attention to the features which were ignored by my model. Notice there were no lines emanating from X1, X2, or 1 multiplied by X2. To remind you, L1 regularization can be used as a feature selection mechanism. Next, I tried L2 regularization. Compared to L1 there was no feature selection going on. The most relevant features had strong weights associated with them, but the rest and they're still in play with weaker weights. This may not be visible in the screen capture but while running live, the lines coming out of X1, X2, and X1 times X2 showed movement. To remind you the weight of a feature is visualized by the thickness of the line emanating from that feature. There was no crazy curvature either. Test loss was nice and healthy. It looked like a good model. Next I tried emphasizing model simplicity a bit more by increasing the regularization rate. I changed it from 0.1 to 0.3. Model performance improved from 0.179 To point 0.160. Then I decided to crank it up even further and set regularization rate to one. That was too much. My model couldn't learn anything. Just like other hyperparameters, adjusting regularization rate takes a bit of time and patience. To recap, complex models are bad. One of the ways to keep our model simple is by applying regularization and adjust the rate until we achieve an acceptable performance. I hope this helps you get comfortable with the concept of regularization.