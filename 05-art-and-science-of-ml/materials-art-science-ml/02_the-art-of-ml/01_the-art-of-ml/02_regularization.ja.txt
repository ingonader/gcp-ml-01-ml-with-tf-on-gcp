モジュールの最初のセクションは
「正則化」です モデルのトレーニングの目的は
誤差（損失値）の最小化です トレーニングデータとテストデータ両方で
誤差曲線を作成するとこんなふうになります グラフのY軸が誤差、X軸が時間です ここで欠陥にお気付きですか トレーニングデータでは
誤差がきれいな下降線ですが テストデータでは数か所で上昇しています これはいけません 明らかに 一定量の過剰適合が発生しており トレーニングの反復回数と相関があるようです どう対処しますか トレーニングの反復回数を減らして
早期停止することが考えられます 早期停止は間違いなく1つの選択肢ですが
もっとよい方法があるはずです ここが正則化の出番です TensorFlow Playgroundを使用して
直感を働かせましょう 過去のコースで
このPlaygroundを使用しましたね 手短に復習しましょう TF Playgroundはニューラル
ネットワーク学習の可視化ツールです このコースでは
直感的な概念を把握するのに使用します 画面に注目してください 奇妙なことが生じています 左下に青を示す部分がありますが 青を示唆するデータはありません モデルの決定は気が狂っているかのようです なぜだと思いますか 入力から出力に走る5本の線の
相対的太さに注目してください これらの線は 5個の特徴の
相対的重みを示します X1とX2から放射する線は
特徴クロスからの線よりずっと太いです クロスしない通常の特徴に比べ
クロスはモデルへの貢献度が低いので クロスをすべてを取り除くと
まっとうなモデルになります ぜひご自分で試してください どのように 過剰適合を示唆する曲線が消え
テスト誤差が収束するかを確認します 1,000回反復したら テスト誤差は特徴クロスより
値が若干低くなるはずです ただし データセットによって結果は多少異なります この演習のデータは
基本的に線形データ＋ノイズです クロスの多すぎるモデルなど
複雑すぎるモデルを使用すると トレーニングデータに
ノイズが入り込む余地が生じます そしてテストデータで
モデルがうまく動作しません その場合 明らかに早期停止は役に立ちません 必要なのはモデルの複雑さを制御することです どうすれば モデルの複雑さを測定して
避けられるでしょうか 一般的にシンプルなモデルのほうが
良いというのが結論です どんな料理でも 棚にある調味料を
全部使うことはありませんよね これについては 一般化理論と
呼ばれる分野があり 統計フレームワークを定義しています ただ これを考える一番簡単な方法は直感です 14世紀にウィリアム オッカムは
「オッカムの剃刀」を提唱しました この原理を 経験則的な指針として
モデルのトレーニングに適用すると 「トレーニングに関する仮定が少ない
シンプルなモデルが良い」ということになります では 最も一般的な正則化法を見てみましょう
原理を実際に適用する助けとなります モデルの複雑さに
ペナルティを与える考え方です これまで トレーニングプロセスではモデルの
データ誤差を最小化しようとしてきました しかしモデルの複雑さとの
バランスをとる必要があります モデルの複雑さの測定方法の前に 複雑さと誤差とのバランスが
なぜ大切なのか考えましょう シンプルが良いといっても
単純化しすぎたモデルは役に立ちません 極端になりすぎると
結局モデルが得られないからです トレーニングデータへの正確な適合と
単純さの適切なバランスが鍵となります ここまでで 正則化が早期停止よりも
原則に合う理由がわかったと思います 正則化は機械学習の主要な研究分野のひとつです すでに多くの手法が公表されており
さらに登場するでしょう 早期停止はすでに言及しました パラメータノルムペナルティの
手法にも少し触れました データセット水増し法、耐ノイズ性、
スパース表現など さらに多くの方法があります このモジュールでは
L1正則化とL2正則化を詳しく見ていきます これらはパラメータノルムペナルティに
属する手法です しかしその前に正則化がどんな問題を
解決するのか復習しておきましょう 正則化とは
モデルを一般化する助けになる手法です 一般化されたモデルは
トレーニングデータだけでなく 未知のテストデータでもうまく動作します