1
00:00:00,000 --> 00:00:04,290
モジュールの最初のセクションは
「正則化」です

2
00:00:04,290 --> 00:00:08,320
モデルのトレーニングの目的は
誤差（損失値）の最小化です

3
00:00:08,320 --> 00:00:14,295
トレーニングデータとテストデータ両方で
誤差曲線を作成するとこんなふうになります

4
00:00:14,295 --> 00:00:20,065
グラフのY軸が誤差、X軸が時間です

5
00:00:20,065 --> 00:00:22,085
ここで欠陥にお気付きですか

6
00:00:22,085 --> 00:00:26,175
トレーニングデータでは
誤差がきれいな下降線ですが

7
00:00:26,175 --> 00:00:29,680
テストデータでは数か所で上昇しています

8
00:00:29,680 --> 00:00:31,600
これはいけません

9
00:00:31,600 --> 00:00:35,125
明らかに 一定量の過剰適合が発生しており

10
00:00:35,125 --> 00:00:38,630
トレーニングの反復回数と相関があるようです

11
00:00:38,630 --> 00:00:40,345
どう対処しますか

12
00:00:40,345 --> 00:00:43,700
トレーニングの反復回数を減らして
早期停止することが考えられます

13
00:00:43,700 --> 00:00:48,450
早期停止は間違いなく1つの選択肢ですが
もっとよい方法があるはずです

14
00:00:48,450 --> 00:00:51,205
ここが正則化の出番です

15
00:00:51,645 --> 00:00:55,755
TensorFlow Playgroundを使用して
直感を働かせましょう

16
00:00:55,755 --> 00:00:59,180
過去のコースで
このPlaygroundを使用しましたね

17
00:00:59,180 --> 00:01:01,120
手短に復習しましょう

18
00:01:01,120 --> 00:01:07,305
TF Playgroundはニューラル
ネットワーク学習の可視化ツールです

19
00:01:07,305 --> 00:01:12,925
このコースでは
直感的な概念を把握するのに使用します

20
00:01:12,925 --> 00:01:16,205
画面に注目してください

21
00:01:16,205 --> 00:01:18,630
奇妙なことが生じています

22
00:01:18,630 --> 00:01:22,830
左下に青を示す部分がありますが

23
00:01:22,830 --> 00:01:25,650
青を示唆するデータはありません

24
00:01:25,650 --> 00:01:29,610
モデルの決定は気が狂っているかのようです

25
00:01:29,610 --> 00:01:31,600
なぜだと思いますか

26
00:01:31,600 --> 00:01:36,690
入力から出力に走る5本の線の
相対的太さに注目してください

27
00:01:36,690 --> 00:01:40,670
これらの線は 5個の特徴の
相対的重みを示します

28
00:01:40,670 --> 00:01:47,730
X1とX2から放射する線は
特徴クロスからの線よりずっと太いです

29
00:01:47,730 --> 00:01:53,780
クロスしない通常の特徴に比べ
クロスはモデルへの貢献度が低いので

30
00:01:53,780 --> 00:01:57,620
クロスをすべてを取り除くと
まっとうなモデルになります

31
00:01:57,620 --> 00:01:59,615
ぜひご自分で試してください

32
00:01:59,615 --> 00:02:06,300
どのように 過剰適合を示唆する曲線が消え
テスト誤差が収束するかを確認します

33
00:02:06,300 --> 00:02:08,330
1,000回反復したら

34
00:02:08,330 --> 00:02:13,975
テスト誤差は特徴クロスより
値が若干低くなるはずです

35
00:02:13,975 --> 00:02:19,040
ただし データセットによって結果は多少異なります

36
00:02:19,040 --> 00:02:23,730
この演習のデータは
基本的に線形データ＋ノイズです

37
00:02:23,730 --> 00:02:28,340
クロスの多すぎるモデルなど
複雑すぎるモデルを使用すると

38
00:02:28,340 --> 00:02:31,870
トレーニングデータに
ノイズが入り込む余地が生じます

39
00:02:31,870 --> 00:02:36,310
そしてテストデータで
モデルがうまく動作しません

40
00:02:36,310 --> 00:02:39,640
その場合 明らかに早期停止は役に立ちません

41
00:02:39,640 --> 00:02:43,050
必要なのはモデルの複雑さを制御することです

42
00:02:43,050 --> 00:02:46,515
どうすれば モデルの複雑さを測定して
避けられるでしょうか

43
00:02:46,515 --> 00:02:50,060
一般的にシンプルなモデルのほうが
良いというのが結論です

44
00:02:50,060 --> 00:02:53,400
どんな料理でも 棚にある調味料を
全部使うことはありませんよね

45
00:02:53,400 --> 00:02:57,740
これについては 一般化理論と
呼ばれる分野があり

46
00:02:57,740 --> 00:03:01,960
統計フレームワークを定義しています

47
00:03:01,960 --> 00:03:05,499
ただ これを考える一番簡単な方法は直感です

48
00:03:05,499 --> 00:03:09,680
14世紀にウィリアム オッカムは
「オッカムの剃刀」を提唱しました

49
00:03:09,680 --> 00:03:13,680
この原理を 経験則的な指針として
モデルのトレーニングに適用すると

50
00:03:13,680 --> 00:03:18,870
「トレーニングに関する仮定が少ない
シンプルなモデルが良い」ということになります

51
00:03:18,870 --> 00:03:25,060
では 最も一般的な正則化法を見てみましょう
原理を実際に適用する助けとなります

52
00:03:25,060 --> 00:03:28,365
モデルの複雑さに
ペナルティを与える考え方です

53
00:03:28,365 --> 00:03:34,690
これまで トレーニングプロセスではモデルの
データ誤差を最小化しようとしてきました

54
00:03:34,690 --> 00:03:38,165
しかしモデルの複雑さとの
バランスをとる必要があります

55
00:03:38,165 --> 00:03:41,380
モデルの複雑さの測定方法の前に

56
00:03:41,380 --> 00:03:45,900
複雑さと誤差とのバランスが
なぜ大切なのか考えましょう

57
00:03:45,900 --> 00:03:50,985
シンプルが良いといっても
単純化しすぎたモデルは役に立ちません

58
00:03:50,985 --> 00:03:54,505
極端になりすぎると
結局モデルが得られないからです

59
00:03:54,505 --> 00:04:00,220
トレーニングデータへの正確な適合と
単純さの適切なバランスが鍵となります

60
00:04:00,220 --> 00:04:05,920
ここまでで 正則化が早期停止よりも
原則に合う理由がわかったと思います

61
00:04:05,920 --> 00:04:10,445
正則化は機械学習の主要な研究分野のひとつです

62
00:04:10,445 --> 00:04:13,200
すでに多くの手法が公表されており
さらに登場するでしょう

63
00:04:13,200 --> 00:04:15,700
早期停止はすでに言及しました

64
00:04:15,700 --> 00:04:21,164
パラメータノルムペナルティの
手法にも少し触れました

65
00:04:21,164 --> 00:04:28,865
データセット水増し法、耐ノイズ性、
スパース表現など さらに多くの方法があります

66
00:04:28,870 --> 00:04:33,570
このモジュールでは
L1正則化とL2正則化を詳しく見ていきます

67
00:04:33,570 --> 00:04:37,105
これらはパラメータノルムペナルティに
属する手法です

68
00:04:37,105 --> 00:04:43,980
しかしその前に正則化がどんな問題を
解決するのか復習しておきましょう

69
00:04:43,980 --> 00:04:47,810
正則化とは
モデルを一般化する助けになる手法です

70
00:04:47,810 --> 00:04:51,010
一般化されたモデルは
トレーニングデータだけでなく

71
00:04:51,010 --> 00:04:53,800
未知のテストデータでもうまく動作します