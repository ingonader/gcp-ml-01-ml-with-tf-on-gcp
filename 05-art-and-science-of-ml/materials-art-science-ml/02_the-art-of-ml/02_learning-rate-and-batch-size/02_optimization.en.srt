1
00:00:00,000 --> 00:00:06,495
-Now let's conclude this module by a brief look into the concept of optimization.

2
00:00:06,495 --> 00:00:11,370
Optimization is another evolving field of research within machine learning.

3
00:00:11,370 --> 00:00:14,390
There are many published techniques and more to come.

4
00:00:14,390 --> 00:00:18,030
Optimization refers to the task of either minimizing or

5
00:00:18,030 --> 00:00:21,945
maximizing some function of effects by altering X.

6
00:00:21,945 --> 00:00:25,485
Some of the algorithms are listed here.

7
00:00:25,485 --> 00:00:29,370
You're already familiar with the gradient descent which tries to

8
00:00:29,370 --> 00:00:33,450
find the minimum of last function by altering vait values.

9
00:00:33,450 --> 00:00:39,555
Momentum reduces learning rate when gradient values are small.

10
00:00:39,555 --> 00:00:45,790
AdaGrad gives frequently occurring features lower learning rates.

11
00:00:45,920 --> 00:00:53,055
AdaDelta improves AdaGrad by avoiding reducing learning rate to zero.

12
00:00:53,055 --> 00:00:58,210
Adam is basically AdaGrad with a bunch of fixes.

13
00:00:59,090 --> 00:01:05,885
Ftrl or follow the regularized leader works well on white models.

14
00:01:05,885 --> 00:01:09,180
At this time, Adam and ftrl make

15
00:01:09,180 --> 00:01:14,015
good defaults for deep neural networks as well as linear models.

16
00:01:14,015 --> 00:01:18,480
Who knows what new optimization techniques will become mainstream tomorrow.