In this lab, we've experimented the manual hyperparameter tuning. Let's review the results. We're doing simple linear regression to predict housing prices based on the number of rooms in a house. We will judge the accuracy of our model by keeping an eye on RMSE, and we adjust the learning rate and batch size to improve the RMSE. The data that we used for this lab is based on 1990 census from California. So, if you pay attention to the Python notebook here, at the beginning I'm just loading some libraries, nothing exciting. I am getting a warning here that can be ignored. You may not get it, depending on the versions that you're running. Then, I will load my data set from this public URL into a pandas DataFrame, and then we will examine the data by looking at some of the records and some statistics from the data. One problem that this data set has is that, it's at the granularity level of the city blocks, not at the household level, and we need to fix that before we start using it. And in order to do that, we'll just make up a new column of data based on the total number of rooms that we have at the block level divided by households, that again is at the block level, and that would give us roughly the total number of rooms per household. If I get to describe that, it would give me these statistics. Before we get into training the model, let's quickly look at our data set, and remind ourselves what we are doing. This is the column that we made up just now by dividing the two existing columns, number of rooms. This is going to be our feature. This is basically the input to our model, and what we are going to do with our model is to predict the housing median age. So, this column basically makes up the label for us. In this cell, I'm defining what it takes to actually start training. The training function, the input function is from the data frame. Remember that the number of rooms is the feature, and the median house value is the label. Here, I'm defining the output directory, and I'm making sure that every time I start from scratch by removing the content of that output directory. We did mention that this is going to be a linear regressor, so that's what I'm doing. I'm using the linear regressor from the Tensorflow library, and I'm passing the feature and the output directory to that estimator, and then I start the training here. And as I mentioned, I'm going to judge the performance of our model by looking at the RMSE, and that's what's happening here. If I run the cell, I will see there is a very large number that's reported for the RMSE, and that's kind of insane because RMSE is supposed to be in the hundredths, not such a giant number. What's happening here is that, there is some scaling going on. This is reported at the 100,000 scale, and we need to apply that scale, so that we look at the RMSE at the proper scale, and that's what I'm doing in an Excel. I'm just dividing the Y value based on that scale that I mentioned, and the rest remains the same. If I run that now, it's giving me sort of a 7.4 percent error rate, which is for the beginning, not too bad but we can do better. This is what's happening in the next cell. I'm going to be changing the learning rate and batch size to see how it improves the error rate. The same way, the scale remains. Now, I am defining a batch size here, batch size of 10. Same way, we will start from scratch. Everytime we remove the output directory, we are introducing a learning rate here of 0.01. Again, it's a linear regressor. The rest of the code remains as is, and the only thing that's different here is because, now we have a small batch size. We need to do more number of steps, and that's what's happening here. And then, we'll print the RMSE and see what's happening. So remember, before having the learning rate and batch size, we were at 7.4. And with this minor change, now we are down to 3.6. You should experiment with these hyperparameters to get the best performance possible. The last time I played with this, I got 2.528. So, this is pretty much what I wanted to cover in this lab. So, one of the commonly asked questions is, if there is a standard method for tuning these parameters. The short answer is that, the effects of different hyperparameters is data-dependent, so there is no hard and fast rules. You need to run the test on your data. There are a few rule of thumbs that may help guide you. When you monitor your training error, it should steadily decrease and typically, it's steeply at first, and then it should eventually plateau as the training converges. If the training has not converged, try running it for longer. If the training error decreases too slowly, increasing the learning rate may help it decrease faster. But sometimes, the exact opposite may happen if the learning rate is too high. If the training error varies widely, try decreasing the learning rate. Lowering learning rate, plus larger number of steps or larger batch size is often a good combination. Very small batch sizes can also cause instability. First, try larger values, like hundred or thousand and decrease until you see degradation. Again, never go strictly by these rules of thumbs because the effects are data-dependent. Always experiment and verify. As a bonus to this lab, you should add some more features and see the results. It should not take you too long. Between five to 10 minutes, you should be able to add some more features and see how your model performs.