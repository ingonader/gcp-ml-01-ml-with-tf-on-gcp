1
00:00:00,560 --> 00:00:04,090
Dies bringt uns
zum zweiten Abschnitt dieses Moduls,

2
00:00:04,090 --> 00:00:06,840
in dem wir
zwei wichtige Hyperparameter diskutieren,

3
00:00:06,840 --> 00:00:08,760
Lernrate und Batchgröße.

4
00:00:08,760 --> 00:00:13,720
Spielen wir mit den Parametern
erst einmal in Playground.

5
00:00:15,420 --> 00:00:18,270
Beginnen wir mit der Lernrate.

6
00:00:18,270 --> 00:00:22,960
Die Lernrate steuert
die Schrittgröße im Gewichtungsbereich.

7
00:00:22,960 --> 00:00:25,510
Wir lassen die Batchgröße bei 30

8
00:00:25,510 --> 00:00:27,830
und auch die anderen Parameter konstant.

9
00:00:27,830 --> 00:00:31,585
Als Erstes setze ich
die Lernrate auf 0,01.

10
00:00:31,585 --> 00:00:34,720
Tensorflow Playground
verwendet zufällige Startpunkte.

11
00:00:34,720 --> 00:00:37,645
Ihr Ergebnis kann daher
von meinem abweichen.

12
00:00:37,645 --> 00:00:41,260
Möglicherweise erhalten Sie
in der Verlustkurve seltsame Sprünge,

13
00:00:41,260 --> 00:00:43,210
doch sie konvergiert ziemlich schnell.

14
00:00:43,210 --> 00:00:48,760
Ich habe für die Testdaten
einen Verlustwert von 0,139 erhalten

15
00:00:48,760 --> 00:00:51,210
und bin unter 300 Epochen geblieben.

16
00:00:52,770 --> 00:00:56,230
Nach Änderung der Lernrate auf 0,001

17
00:00:56,230 --> 00:00:58,260
hat die Leistung deutlich nachgelassen.

18
00:00:58,260 --> 00:01:01,110
In meinem Fall
waren fast 3.000 Epochen erforderlich,

19
00:01:01,110 --> 00:01:04,810
um einen Testverlust
wie im vorherigen Experiment zu erreichen.

20
00:01:04,810 --> 00:01:09,660
Immerhin sollten Sie in der Verlustkurve
keine seltsamen Sprünge sehen.

21
00:01:09,660 --> 00:01:13,080
Sie sollte langsam,
aber gleichmäßig konvergieren.

22
00:01:14,840 --> 00:01:18,100
Experimentieren wir nun
mit den Auswirkungen der Batchgröße.

23
00:01:18,100 --> 00:01:24,150
Die Batchgröße steuert die Beispielanzahl,
für die der Gradient berechnet wird.

24
00:01:24,150 --> 00:01:27,390
Wir lassen die Lernrate bei 0,01

25
00:01:27,390 --> 00:01:29,740
und alle anderen Parameter konstant.

26
00:01:29,740 --> 00:01:32,775
Als Erstes habe ich
eine Batchgröße von 100 ausprobiert.

27
00:01:32,775 --> 00:01:36,965
Sie stoßen beim Experimentieren
vielleicht auf das Problem,

28
00:01:36,965 --> 00:01:39,630
wie Sie die Batchgröße
über 30 hinaus erhöhen können.

29
00:01:39,630 --> 00:01:41,210
Keine Sorge, das ist in Ordnung.

30
00:01:41,210 --> 00:01:42,480
Das soll so sein.

31
00:01:42,480 --> 00:01:46,170
Die UI lässt keine Werte über 30 zu,

32
00:01:46,170 --> 00:01:49,080
Sie können
die Batchgröße jedoch in der URL ändern.

33
00:01:49,080 --> 00:01:51,690
Mit einer Batchgröße von 100

34
00:01:51,690 --> 00:01:54,185
erhielt ich eine eher langsame Konvergenz.

35
00:01:54,185 --> 00:01:56,510
Über 1.000 Epochen waren erforderlich,

36
00:01:56,510 --> 00:02:00,395
um einen ähnlichen Verlustwert wie
in vorherigen Experimenten zu erreichen.

37
00:02:00,395 --> 00:02:03,130
Ich konnte dafür
aber kein Rauschen beobachten.

38
00:02:04,840 --> 00:02:07,630
Nach Verringerung der Batchgröße auf 5

39
00:02:07,630 --> 00:02:09,610
erhielt ich sehr schnell Ergebnisse.

40
00:02:09,610 --> 00:02:11,780
Im Grunde habe ich in nur 65 Epochen

41
00:02:11,780 --> 00:02:15,670
einen ähnlichen Testverlust erreicht
wie in den vorherigen Experimenten.

42
00:02:15,670 --> 00:02:19,745
In der Verlustkurve waren allerdings
einige ungleichmäßige Schritte zu sehen.

43
00:02:19,745 --> 00:02:24,710
Offensichtlich reagiert die Modellleistung
sehr sensibel auf Lernrate und Batchgröße.

44
00:02:24,710 --> 00:02:27,750
Ist das nicht
wie das Stimmen eines Musikinstruments?

45
00:02:27,750 --> 00:02:30,250
Ich sagte ja,
dass auch Kunst eine Rolle spielt.

46
00:02:32,270 --> 00:02:34,460
Fassen wir unsere Ergebnisse zusammen.

47
00:02:34,460 --> 00:02:39,575
Die Lernrate steuert
die Schrittgröße im Gewichtungsbereich.

48
00:02:39,575 --> 00:02:42,095
Wenn die Schritte zu klein sind,

49
00:02:42,095 --> 00:02:44,165
dauert das Training sehr lange.

50
00:02:44,165 --> 00:02:47,235
Wenn die Schritte aber zu groß sind,

51
00:02:47,235 --> 00:02:51,125
ist das Training unkontrollierbar
und könnte den optimalen Punkt verfehlen.

52
00:02:51,125 --> 00:02:54,200
Eine Lernrate von 0,001 bedeutet,

53
00:02:54,200 --> 00:02:58,850
dass die Schrittgröße
1/1.000 des Eingabebereichs beträgt.

54
00:02:58,850 --> 00:03:04,410
Dies könnte zu wenig für eine große
Optimierungsoberfläche sein.

55
00:03:04,420 --> 00:03:10,545
Zum Beispiel beträgt der Standardwert
für LinearRegressor im TensorFlow-Estimator

56
00:03:10,545 --> 00:03:15,430
0,2 oder den Kehrwert der Wurzel
aus der Anzahl der Merkmale.

57
00:03:15,430 --> 00:03:19,530
Das setzt voraus, dass Merkmal-
und Labelwerte kleine Zahlen sind.

58
00:03:21,690 --> 00:03:25,220
Die andere Option, die Batchgröße,
steuert die Anzahl der Beispiele,

59
00:03:25,220 --> 00:03:27,510
für die der Gradient berechnet wird.

60
00:03:27,510 --> 00:03:29,460
Wenn die Batchgröße zu klein ist,

61
00:03:29,460 --> 00:03:30,790
erhalten wir Sprünge,

62
00:03:30,790 --> 00:03:34,845
da der Batch möglicherweise
die Eingabe nicht gut genug repräsentiert.

63
00:03:34,845 --> 00:03:37,815
Wenn aber die Batchgröße zu hoch ist,

64
00:03:37,815 --> 00:03:40,650
dauert das Training sehr lange.

65
00:03:40,650 --> 00:03:46,820
Als Faustregel ist ein Wert
zwischen 40 und 100 eine gute Batchgröße.

66
00:03:46,830 --> 00:03:49,950
Sie kann auch bis zu 500 betragen.

67
00:03:53,320 --> 00:03:55,490
Lassen Sie uns beim Thema Batchvorgänge

68
00:03:55,490 --> 00:03:58,355
nicht die Batchverarbeitung
nach Zufallsprinzip vergessen.

69
00:03:58,355 --> 00:04:01,807
Sie haben sicher gehört, dass
Mischen von Beispielen eine gute Idee ist.

70
00:04:01,807 --> 00:04:02,570
Aber warum?

71
00:04:02,570 --> 00:04:05,590
Nehmen wir Buchtitel wie hier.

72
00:04:05,590 --> 00:04:08,530
Nehmen wir an,
Sie möchten einem Nutzer vorschlagen,

73
00:04:08,530 --> 00:04:10,870
was er als Nächstes lesen könnte.

74
00:04:10,870 --> 00:04:13,975
Wie Sie sehen,
sind die Titel alphabetisch sortiert.

75
00:04:13,975 --> 00:04:17,120
Wenn Sie das Dataset so verwenden,

76
00:04:17,120 --> 00:04:20,950
enthält jeder Trainingsbatch
eine Teilmenge der Titel

77
00:04:20,950 --> 00:04:24,265
auf Basis aufeinanderfolgender Buchstaben.

78
00:04:24,265 --> 00:04:26,450
Sie gewähren Ihrem armen Modell

79
00:04:26,450 --> 00:04:29,170
eine sehr eingeschränkte Sicht
auf den Problembereich

80
00:04:29,170 --> 00:04:32,585
und verwehren ihm die Chance,
die ganze Wahrheit zu entdecken.

81
00:04:32,585 --> 00:04:35,065
Sie möchten doch wohl
kein schlechter Lehrer sein.

82
00:04:35,065 --> 00:04:40,225
Stattdessen möchten Sie, dass jeder Batch
repräsentativ für das gesamte Dataset ist.

83
00:04:40,225 --> 00:04:43,570
Die meisten Datasets
weisen irgendeine Reihenfolge auf.

84
00:04:43,570 --> 00:04:46,420
Bücher
sind alphabetisch nach Titel sortiert,

85
00:04:46,420 --> 00:04:48,880
Kundendaten nach Postleitzahl,

86
00:04:48,880 --> 00:04:52,085
Einkäufe nach Jahreszeit, Jahr usw.

87
00:04:52,085 --> 00:04:54,740
Durch gründliches Mischen des Datasets

88
00:04:54,740 --> 00:04:58,940
sorgen Sie dafür, dass jeder Batch
für das gesamte Dataset repräsentativ ist.

89
00:04:58,940 --> 00:05:02,790
Erinnern wir uns,
der Gradient wird im Batch berechnet.

90
00:05:02,790 --> 00:05:04,985
Wenn der Batch nicht repräsentativ ist,

91
00:05:04,985 --> 00:05:09,690
variiert der Verlust
von Batch zu Batch zu stark.