モジュールのセクション2に入ります ここでは2つの重要な
ハイパーパラメータを見ていきます 学習率とバッチサイズです まずこれらのパラメータを
いつものPlaygroundで使ってみましょう 学習率から始めましょう 学習率は重み空間での
ステップサイズを制御します バッチサイズを30にし
他のすべてのパラメータを一定に維持します 最初に学習率を0.01に設定しました TF Playgroundの開始点は無作為なので
皆さんの結果は異なる可能性があります 誤差曲線におかしな跳ね返りがありますが
急速に収束しています 私の場合 テストデータで
誤差値は0.139でした エポック数は300未満でした 学習率を0.001に変更すると
ずっと遅くなりました 前回の実験に匹敵するテスト誤差に
達するのに約3,000エポック必要でした ただ 誤差曲線に
奇妙な跳ね返りはありません ゆっくり なだらかに収束しています ここからバッチサイズの効果を実験しましょう バッチサイズは
勾配計算に使うサンプル数を制御します 学習率を0.01にし
他のすべてのパラメータを一定に維持します 最初はバッチサイズを100で試しました 皆様が試すとき どうやってバッチサイズを
30以上に上げるか悩むかもしれません 心配不要です 壊れてはいません
設計どおりです UIは30を超えるバッチサイズを
受け入れません しかしURLから変更できます バッチサイズを100とします 収束はかなりゆっくりとしています 前回の実験に近い誤差値に達するまで
1,000以上のエポックが必要でした しかしノイズのあるステップは
ありませんでした バッチサイズを5に下げると
非常に早く結果が得られました たったの 65エポックだけで
前回の実験に近いテスト誤差に達しました しかし誤差曲線には
ノイズのあるステップがありました モデルのパフォーマンスはバッチサイズと
学習率の影響を強く受けることがわかりました まるで 技巧が求められる
楽器の調律のようだと思いませんか では結果を要約します 学習率は重み空間での
ステップのサイズを制御します ステップが小さすぎると
トレーニングに長時間かかります 一方ステップが大きすぎると
跳ね返りがあります そして最適点を失うこともあり得ます 学習率が0.001とは
ステップサイズ=入力空間の1/1000です 最適化表面が大きければ
この学習率は小さすぎるかもしれません たとえばTensorFlowライブラリでの
線形回帰estimatorのデフォルト値は0.2 または 
1/(特徴数の平方根) に設定されます これは特徴とラベルが
小さい数であることを想定しています もう1つの設定はバッチサイズです 勾配が計算されるサンプル数を制御します バッチサイズが小さすぎると
跳ね返ってしまいます これはバッチがデータ全体を象徴するのに
不十分である可能性があるからです 一方 バッチサイズが大きすぎると
トレーニングに非常に長時間かかります 経験則として
バッチサイズの適切な範囲は40～100です 最高では500まで可能です バッチの話題を続けます バッチシャッフリングを忘れてはいけません サンプルのシャッフリングは良いことだと
聞いたことがあるはずです ここに示す書名を取り上げましょう 次に読む本をユーザーに提案する機能を
トレーニングしているとします 書名はアルファベット順です このデータセットをそのまま使用すると 各トレーニングバッチには 連続するアルファベットに基づいた
書名のサブセットが含まれます 一部のアルファベットで始まる
非常に限られた書名しか見ていないモデルは 書名の全容を解明することができません これではモデルがかわいそうですね そこで すべてのバッチがデータセット全体を
象徴するようにしたいわけです 多くのデータセットは
一定の順番を持つ傾向があります アルファベット順の書名、
郵便番号順の顧客記録、 季節や年度ごとに保管された購入記録などです データセットを適切にシャッフルすると 各バッチは
データセット全体を象徴するものになります 勾配はバッチ内で計算されることを
覚えておきましょう バッチが全体を象徴していない場合
バッチごとに誤差が大きく変動します