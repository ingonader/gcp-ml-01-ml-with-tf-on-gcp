1
00:00:00,000 --> 00:00:06,495
Zum Abschluss des Moduls blicken wir
auf das Konzept der Optimierung.

2
00:00:06,495 --> 00:00:11,382
Optimierung ist noch ein Forschungsbereich
in ML, der sich weiterentwickelt.

3
00:00:11,382 --> 00:00:14,390
Viele Techniken
wurden veröffentlicht und es werden mehr.

4
00:00:14,390 --> 00:00:18,380
Optimierung ist die Aufgabe,
einige sich auswirkende Funktionen

5
00:00:18,380 --> 00:00:21,945
durch Verändern von X
zu minimieren oder zu maximieren.

6
00:00:21,945 --> 00:00:25,485
Einige der Algorithmen
sind hier aufgelistet.

7
00:00:25,485 --> 00:00:29,370
Sie kennen bereits
"GradientDescent", das versucht,

8
00:00:29,370 --> 00:00:33,450
durch Änderung der Gewichtungswerte
das Minimum der Funktion "Loss" zu finden.

9
00:00:33,450 --> 00:00:39,555
"Momentum" verringert
bei kleinen Gradientwerten die Lernrate.

10
00:00:39,555 --> 00:00:45,940
"AdaGrad" legt niedrigere Lernraten
für oft auftretende Merkmale fest.

11
00:00:45,940 --> 00:00:53,055
"AdaDelta" verbessert "AdaGrad",
indem es eine Lernrate von null verhindert.

12
00:00:53,055 --> 00:00:58,210
"Adam" ist eigentlich
"AdaGrad" mit einigen Fehlerkorrekturen.

13
00:00:59,090 --> 00:01:05,885
Ftrl, "folge der regularisierten Führung",
funktioniert gut bei breiten Modellen.

14
00:01:05,885 --> 00:01:09,180
Derzeit sind "Adam" und "Ftrl"

15
00:01:09,180 --> 00:01:14,015
bei neuralen Deep-Learning-Netzwerken
und linearen Modellen ein guter Standard.

16
00:01:14,015 --> 00:01:18,480
Welche neuen Optimierungsmethoden
zukünftig alltäglich sind, ist unbekannt.