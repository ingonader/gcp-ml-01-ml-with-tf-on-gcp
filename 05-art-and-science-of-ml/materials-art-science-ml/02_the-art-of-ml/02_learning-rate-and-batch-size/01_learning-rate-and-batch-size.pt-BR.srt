1
00:00:01,430 --> 00:00:04,090
Isso nos leva à segunda
seção deste módulo,

2
00:00:04,090 --> 00:00:06,840
em que discutimos dois
hiperparâmetros importantes:

3
00:00:06,840 --> 00:00:08,760
taxa de aprendizado
e tamanho do lote.

4
00:00:09,750 --> 00:00:13,830
Para começar, vamos
testá-los no Playground.

5
00:00:15,340 --> 00:00:17,610
Começando com a
taxa de aprendizado.

6
00:00:18,270 --> 00:00:22,960
Lembre-se de que essa taxa controla
o tamanho da parada no espaço do peso.

7
00:00:22,960 --> 00:00:25,510
Mantendo o tamanho
do lote igual a 30

8
00:00:25,510 --> 00:00:27,830
e os outros
parâmetros constantes,

9
00:00:27,830 --> 00:00:31,585
eu defino uma taxa
de aprendizado de 0,01.

10
00:00:31,585 --> 00:00:34,720
O TensorFlow Playground usa
pontos de partida aleatórios.

11
00:00:34,720 --> 00:00:37,645
Por isso, seus resultados
podem ser diferentes dos meus.

12
00:00:38,135 --> 00:00:41,410
Você pode ver saltos
na curva de perda,

13
00:00:41,410 --> 00:00:43,210
mas ela converge rapidamente.

14
00:00:43,210 --> 00:00:48,760
No meu caso, consegui
0,139 de perda na taxa de teste

15
00:00:48,760 --> 00:00:51,240
e menos de 300 épocas.

16
00:00:52,610 --> 00:00:56,230
Ao mudar a taxa de
aprendizado para 0,001,

17
00:00:56,230 --> 00:00:58,260
vi um desempenho
muito mais lento.

18
00:00:58,260 --> 00:01:01,110
No meu caso, levou
quase 3 mil épocas

19
00:01:01,110 --> 00:01:04,560
para alcançar uma perda de teste
comparável ao último teste.

20
00:01:05,420 --> 00:01:09,660
O ponto positivo é que não há
saltos na curva de perda.

21
00:01:09,660 --> 00:01:12,690
Ela converge rapidamente,
mas de maneira suave.

22
00:01:14,990 --> 00:01:17,870
Vamos testar os efeitos
do tamanho do lote.

23
00:01:18,370 --> 00:01:24,150
O tamanho do lote controla o número de
amostras em que o gradiente é calculado.

24
00:01:24,150 --> 00:01:27,390
Mantendo a taxa de
aprendizado em 0,01

25
00:01:27,390 --> 00:01:29,740
e todos os outros
parâmetros contantes,

26
00:01:29,740 --> 00:01:32,775
primeiro eu testo um
tamanho de lote igual a 100.

27
00:01:33,215 --> 00:01:34,965
Se você estiver
fazendo junto comigo,

28
00:01:34,965 --> 00:01:39,630
pode estar pensando como aumentar
o tamanho do lote acima de 30.

29
00:01:39,630 --> 00:01:41,130
Não se preocupe,
nada quebrou.

30
00:01:41,130 --> 00:01:42,480
É proposital.

31
00:01:42,480 --> 00:01:46,170
A interface não
permite ir além de 30,

32
00:01:46,170 --> 00:01:48,630
mas você pode alterar no URO.

33
00:01:49,730 --> 00:01:51,690
Com um tamanho de lote 100,

34
00:01:51,690 --> 00:01:54,185
eu vejo uma
convergência mais lenta.

35
00:01:54,185 --> 00:01:57,290
Levou mais de mil épocas
para alcançar um valor de perda

36
00:01:57,290 --> 00:02:00,395
parecido com o
dos testes anteriores.

37
00:02:00,395 --> 00:02:03,040
Mas não há ruído nas etapas.

38
00:02:04,800 --> 00:02:07,630
Ao mudar o tamanho
do lote para cinco,

39
00:02:07,630 --> 00:02:09,610
eu consigo resultados
muito rápidos.

40
00:02:09,610 --> 00:02:11,780
Basicamente,
em apenas 65 épocas

41
00:02:11,780 --> 00:02:15,670
eu consegui uma perda parecida
com a dos últimos testes.

42
00:02:16,150 --> 00:02:19,745
Mas há ruídos visíveis
na curva de perda.

43
00:02:19,745 --> 00:02:22,557
O desempenho do modelo
é muito sensível

44
00:02:22,557 --> 00:02:24,970
à taxa de aprendizado
e ao tamanho do lote.

45
00:02:24,970 --> 00:02:27,860
É como afinar um
instrumento musical, não?

46
00:02:27,860 --> 00:02:29,830
Eu disse que era uma arte.

47
00:02:32,270 --> 00:02:34,460
Vamos recapitular
as descobertas.

48
00:02:34,460 --> 00:02:39,575
A taxa de aprendizado controla o
tamanho da etapa no espaço do peso.

49
00:02:39,575 --> 00:02:42,095
Se as etapas forem
pequenas demais,

50
00:02:42,095 --> 00:02:44,165
o treinamento levará
muito tempo.

51
00:02:44,165 --> 00:02:47,235
Por outro lado, se as etapas
forem grandes demais,

52
00:02:47,235 --> 00:02:50,795
ele ficará instável e pode
perder o ponto ideal.

53
00:02:51,535 --> 00:02:54,200
Uma taxa de
aprendizado de 0,001

54
00:02:54,200 --> 00:02:58,850
significa uma etapa igual a
1/1.000 do espaço da entrada.

55
00:02:58,850 --> 00:03:01,150
Pode ser uma etapa
muito pequena

56
00:03:01,150 --> 00:03:04,150
quando você tem uma
superfície de otimização grande.

57
00:03:05,020 --> 00:03:08,935
Por exemplo, o padrão
do estimador de regressão linear

58
00:03:08,935 --> 00:03:10,545
na biblioteca do TensorFlow

59
00:03:10,545 --> 00:03:15,430
é definido como 0,2 ou 1 sobre a raiz
quadrada do número de recursos.

60
00:03:15,430 --> 00:03:19,560
Isso assume que seus valores de
recursos e marcadores sejam pequenos.

61
00:03:21,840 --> 00:03:23,610
O outro ajuste,
o tamanho do lote.

62
00:03:23,610 --> 00:03:27,510
controla o número de amostras
em que o gradiente é calculado.

63
00:03:27,510 --> 00:03:29,460
Se o tamanho do lote
for muito pequeno,

64
00:03:29,460 --> 00:03:31,980
ele pode ficar instável
porque o lote

65
00:03:31,980 --> 00:03:34,845
não é uma boa
representação da entrada.

66
00:03:34,845 --> 00:03:37,815
Por outro lado, se o
tamanho for muito grande,

67
00:03:37,815 --> 00:03:40,430
o treinamento
levará muito tempo.

68
00:03:41,210 --> 00:03:42,460
Em geral,

69
00:03:42,460 --> 00:03:46,830
40 a 100 tende a ser um bom
intervalo para o tamanho do lote.

70
00:03:46,830 --> 00:03:49,620
Ele pode ir até 500.

71
00:03:53,500 --> 00:03:55,710
Como estamos
falando de lotes,

72
00:03:55,710 --> 00:03:58,125
não podemos esquecer
da aleatorização.

73
00:03:58,795 --> 00:04:02,640
Você deve ter ouvido que é bom
aleatorizar os exemplos. Mas por quê?

74
00:04:03,620 --> 00:04:05,590
Veja estes livros.

75
00:04:05,590 --> 00:04:08,110
Digamos que você que você
está treinando um comando

76
00:04:08,110 --> 00:04:10,880
que sugere um
novo título para o leitor.

77
00:04:11,320 --> 00:04:13,975
Observe que os títulos
estão em ordem alfabética.

78
00:04:14,345 --> 00:04:17,120
Se você usar o conjunto
de dados como está,

79
00:04:17,120 --> 00:04:20,950
cada lote de treinamento conterá
um subconjunto de títulos

80
00:04:20,950 --> 00:04:23,995
com base nas letras
consecutivas do alfabeto.

81
00:04:24,775 --> 00:04:27,670
Você daria ao modelo
uma visão muito limitada

82
00:04:27,670 --> 00:04:32,585
do domínio do problema e tiraria as
chances de ele descobrir a verdade.

83
00:04:32,585 --> 00:04:34,825
Você não quer ser
um professor ruim.

84
00:04:35,435 --> 00:04:40,225
Cada lote precisa ser representativo
de todo o conjunto de dados.

85
00:04:40,535 --> 00:04:43,570
A maior parte dos conjuntos
tende a ter uma ordem.

86
00:04:43,570 --> 00:04:46,420
Como esses títulos
em ordem alfabética,

87
00:04:46,420 --> 00:04:48,880
registros de cliente
classificados por CEP,

88
00:04:48,880 --> 00:04:52,085
compras arquivadas
por temporada, ano, etc.

89
00:04:52,085 --> 00:04:54,740
Ao aleatorizar o
conjunto de dados,

90
00:04:54,740 --> 00:04:58,940
você garante que cada lote
represente todo o conjunto de dados.

91
00:04:59,290 --> 00:05:02,790
O gradiente é computado
dentro do lote.

92
00:05:02,790 --> 00:05:04,985
Se o lote não
for representativo,

93
00:05:04,985 --> 00:05:09,690
a perda será muito irregular
por causa de um lote ruim.