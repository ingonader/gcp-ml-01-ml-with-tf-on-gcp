1
00:00:00,940 --> 00:00:05,975
Para finalizar el módulo,
veamos el concepto de optimización.

2
00:00:07,005 --> 00:00:10,107
La optimización
es otro campo de investigación en desarrollo

3
00:00:10,107 --> 00:00:11,400
del aprendizaje automático.

4
00:00:11,510 --> 00:00:14,120
Se han publicado muchas técnicas
y aparecerán otras más.

5
00:00:15,030 --> 00:00:18,890
La optimización se refiere
a la tarea de minimizar o maximizar

6
00:00:18,890 --> 00:00:21,625
alguna función f(x) al alterar x.

7
00:00:22,355 --> 00:00:24,805
Estos son algunos de los algoritmos.

8
00:00:26,505 --> 00:00:28,637
Ya conoce GradientDescent

9
00:00:28,637 --> 00:00:31,220
que intenta encontrar
la función de pérdida mínima

10
00:00:31,220 --> 00:00:32,930
al alterar los valores de peso.

11
00:00:33,960 --> 00:00:38,815
Momentum reduce la tasa de aprendizaje
cuando los valores de gradiente son bajos.

12
00:00:40,525 --> 00:00:44,950
AdaGrad asigna a los atributos frecuentes
tasas de aprendizaje más bajas.

13
00:00:46,880 --> 00:00:49,150
AdaDelta mejora AdaGrad

14
00:00:49,350 --> 00:00:52,370
ya que evita que la tasa de aprendizaje
se reduzca a cero.

15
00:00:53,995 --> 00:00:57,350
Adam es AdaGrad con varias mejoras.

16
00:00:59,330 --> 00:01:05,705
Ftrl o "seguir al líder regularizado"
funciona bien en los modelos amplios.

17
00:01:06,035 --> 00:01:10,110
Actualmente, Adam y Ftrl son buenas
opciones predeterminadas

18
00:01:10,110 --> 00:01:13,555
para las redes neuronales profundas
y los modelos lineales.

19
00:01:14,575 --> 00:01:16,658
Quién sabe qué nuevas técnicas
de optimización

20
00:01:16,658 --> 00:01:18,318
serán las más usadas en el futuro.