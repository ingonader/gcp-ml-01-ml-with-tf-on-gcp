1
00:00:00,000 --> 00:00:06,785
最適化という概念を簡単に見て
このモジュールを終了しましょう

2
00:00:06,785 --> 00:00:11,370
最適化とは MLで進化し続けている
もうひとつの研究領域です

3
00:00:11,370 --> 00:00:14,790
すでに多くの手法が公表されており
さらに登場するでしょう

4
00:00:14,790 --> 00:00:22,640
最適化とはXを変えることで機能の効果を
最小化または最大化するタスクです

5
00:00:22,640 --> 00:00:26,365
いくつかのアルゴリズムをここに列挙しました

6
00:00:26,365 --> 00:00:29,320
GradientDescent（勾配降下法）は
すでにご存じですね

7
00:00:29,320 --> 00:00:33,970
この手法は 重み値を変更することで
損失関数の最低値を見つけようとします

8
00:00:33,970 --> 00:00:40,355
Momentumは勾配値が小さいときに
学習率を低下させます

9
00:00:40,355 --> 00:00:46,740
AdaGradは発生頻度の高い特徴の学習率を
低下させます

10
00:00:46,740 --> 00:00:53,885
AdaDeltaは学習率が0にならないように
改善したAdaGradです

11
00:00:53,885 --> 00:00:59,160
Adamは基本的に
多くの修正を加えたAdaGradです

12
00:00:59,160 --> 00:01:06,105
FtrlつまりFollow the regularized leaderは
広範なモデルで良好に動作します

13
00:01:06,105 --> 00:01:07,715
現時点では

14
00:01:07,715 --> 00:01:14,775
AdamとFtrlが深層ニューラルネットワークと
線形モデルに対する良好なデフォルトです

15
00:01:14,775 --> 00:01:18,480
明日どの新しい最適化手法が主流になるかは
誰にもわかりません