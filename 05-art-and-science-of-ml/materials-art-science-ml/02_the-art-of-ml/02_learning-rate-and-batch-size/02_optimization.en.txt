-Now let's conclude this module by a brief look into the concept of optimization. Optimization is another evolving field of research within machine learning. There are many published techniques and more to come. Optimization refers to the task of either minimizing or maximizing some function of effects by altering X. Some of the algorithms are listed here. You're already familiar with the gradient descent which tries to find the minimum of last function by altering vait values. Momentum reduces learning rate when gradient values are small. AdaGrad gives frequently occurring features lower learning rates. AdaDelta improves AdaGrad by avoiding reducing learning rate to zero. Adam is basically AdaGrad with a bunch of fixes. Ftrl or follow the regularized leader works well on white models. At this time, Adam and ftrl make good defaults for deep neural networks as well as linear models. Who knows what new optimization techniques will become mainstream tomorrow.