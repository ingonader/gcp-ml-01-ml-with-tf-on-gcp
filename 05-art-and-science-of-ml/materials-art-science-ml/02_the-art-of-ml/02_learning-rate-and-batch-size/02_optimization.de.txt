Zum Abschluss des Moduls blicken wir
auf das Konzept der Optimierung. Optimierung ist noch ein Forschungsbereich
in ML, der sich weiterentwickelt. Viele Techniken
wurden veröffentlicht und es werden mehr. Optimierung ist die Aufgabe,
einige sich auswirkende Funktionen durch Verändern von X
zu minimieren oder zu maximieren. Einige der Algorithmen
sind hier aufgelistet. Sie kennen bereits
"GradientDescent", das versucht, durch Änderung der Gewichtungswerte
das Minimum der Funktion "Loss" zu finden. "Momentum" verringert
bei kleinen Gradientwerten die Lernrate. "AdaGrad" legt niedrigere Lernraten
für oft auftretende Merkmale fest. "AdaDelta" verbessert "AdaGrad",
indem es eine Lernrate von null verhindert. "Adam" ist eigentlich
"AdaGrad" mit einigen Fehlerkorrekturen. Ftrl, "folge der regularisierten Führung",
funktioniert gut bei breiten Modellen. Derzeit sind "Adam" und "Ftrl" bei neuralen Deep-Learning-Netzwerken
und linearen Modellen ein guter Standard. Welche neuen Optimierungsmethoden
zukünftig alltäglich sind, ist unbekannt.