This brings us to the second section of this module, where we discuss two important hyperparameters, Learning Rate and Batch Size. Let's just start by playing with these parameters in our favorite playground. Starting with Learning Rate. Remember learning rate controls the size of the stop in the weight space. Keeping batch size equal to 30, and all other parameters constant, I first set the learning rate to 0.01. Tensorflow playground uses random starting points. So your results might be different than mine. You may notice funny bounces on the last curve, but it converges pretty fast. In my case, I got to 0.139 loss value on the test rate, and less than 300 epochs. By changing the learning rate to 0.001, I saw much slower performance. In my case, it took almost 3,000 epochs to reach a test loss comparable to the previous experiment. On the bright side, you should not see any crazy bounces on the loss curve. It should converge slowly, but smoothly. Now, let's experiment with the effects of batch size. Remember, batch size controls the number of samples that the gradient is calculated on. Keeping learning rate as 0.01, and all the other parameters constant, I first tried batch size equal 100. If you're playing along, you may be scratching your head at this point as how to increase batch size beyond 30. Don't worry, it's not broken. It's by design. The UI doesn't allow you to go beyond 30, but you can change it into URO. With batch size equal 100, I noticed a rather slow convergence. It took more than 1,000 epochs to reach a similar loss value as previous experiments. But there were no noisy steps. When reducing the batch size to five, I got very fast results. Basically, in only 65 epochs, I reached similar test loss as previous experiments. But there were some noisy steps visible on the loss curve. Turns out model performance is very sensitive to learning rate and batch size. Doesn't it feel like tuning a musical instrument? Told you there's some art involved. So, let's recap our findings. Once again, learning rate controls the size of the step in the weight space. If the steps are too small, training will take a long time. On the other hand, if the steps are too large, it will bounce around and could even miss the optimal point. A learning rate of point 0.001, means a step size equal to one over 1,000 of the input space. This could be too small of a learning rate, when you have a large optimization surface. For instance, the default value for linear regressor estimator in Tensorflow library, is set to 0.2 or one over a square root of the number of features. This assumes your feature and label values are small numbers. The other knob being batch size, controls the number of samples that gradient is calculated on. If batch size is too small, we could be bouncing around because the batch may not be a good enough representation of the input. On the other hand, if batch size is too large, training will take a very long time. As a rule of thumb, 40 to 100 tends to be a good range for batch size. It can go up to as high as 500. While on the topic of batching, let's not forget the batch shuffling. You must have heard that shuffling examples is a good idea. But why? Take book titles like these. Let's say you're training every commander that's suggest the next title to read to a user. Notice the titles are alphabetically sorted. If you use your dataset as is, each training batch will contain a subset of the titles, based on the consecutive letters of the alphabet. You'll be giving your poor model a very narrow view of the problem domain and taking away its chance of discovering the whole truth. You wouldn't want to be a bad teacher. Instead, you want every batch to be representative of the entire dataset. Most datasets tend to have some in hand order. Like these alphabetically sorted book titles, customer records sorted by zip code, purchases archived by season, year etc. By properly shuffling the dataset, you'll ensure each batch is representative of the entire dataset. Remember, the gradient are computed within the batch. If the batch is not representative, the loss will jump around too much from botched batch.