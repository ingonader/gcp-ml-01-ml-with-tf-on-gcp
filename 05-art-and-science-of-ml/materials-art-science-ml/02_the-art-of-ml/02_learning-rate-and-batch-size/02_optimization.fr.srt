1
00:00:01,040 --> 00:00:04,275
Concluons ce module
par une brève présentation

2
00:00:04,275 --> 00:00:06,265
du concept d'optimisation.

3
00:00:06,455 --> 00:00:09,590
L'optimisation est un autre champ
de recherche en pleine évolution

4
00:00:09,590 --> 00:00:11,440
dans le domaine du machine learning.

5
00:00:11,440 --> 00:00:14,990
De nombreuses techniques existent déjà,
et d'autres sont en développement.

6
00:00:14,990 --> 00:00:20,345
L'optimisation fait référence au fait de
minimiser ou maximiser une fonction "ffx"

7
00:00:20,345 --> 00:00:21,945
en modifiant "x".

8
00:00:21,945 --> 00:00:24,705
Certains algorithmes sont présentés ici.

9
00:00:25,995 --> 00:00:28,840
Vous connaissez déjà GradientDescent,

10
00:00:28,840 --> 00:00:31,430
qui tente de trouver
la fonction de perte minimale

11
00:00:31,430 --> 00:00:33,450
en modifiant les valeurs de poids.

12
00:00:33,820 --> 00:00:39,185
Momentum réduit le taux d'apprentissage
quand les valeurs de gradient sont basses.

13
00:00:40,455 --> 00:00:42,620
AdaGrad abaisse le taux d'apprentissage

14
00:00:42,620 --> 00:00:45,340
pour les caractéristiques
qui se manifestent fréquemment.

15
00:00:46,460 --> 00:00:48,985
AdaDelta améliore AdaGrad

16
00:00:48,985 --> 00:00:52,675
en empêchant la réduction à zéro
du taux d'apprentissage.

17
00:00:53,875 --> 00:00:57,690
Adam est une version améliorée d'AdaGrad.

18
00:00:59,090 --> 00:01:05,955
Ftrl (Follow the regularized leader)
fonctionne bien sur les modèles étendus.

19
00:01:05,955 --> 00:01:09,790
À l'heure actuelle, Adam et Ftrl sont
des solutions par défaut idéales

20
00:01:09,790 --> 00:01:13,805
pour les réseaux de neurones profonds
et les modèles linéaires.

21
00:01:14,105 --> 00:01:17,288
Qui peut dire quelles nouvelles
techniques d'optimisation domineront

22
00:01:17,288 --> 00:01:18,258
le secteur demain ?