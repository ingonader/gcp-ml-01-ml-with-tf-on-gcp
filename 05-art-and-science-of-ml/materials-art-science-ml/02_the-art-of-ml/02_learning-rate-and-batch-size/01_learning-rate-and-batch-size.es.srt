1
00:00:01,510 --> 00:00:04,000
Con esto llegamos
a la segunda sección de este módulo

2
00:00:04,170 --> 00:00:06,780
en la que hablaremos de dos
hiperparámetros importantes

3
00:00:06,890 --> 00:00:08,900
la tasa de aprendizaje
y el tamaño del lote.

4
00:00:09,900 --> 00:00:13,510
Para comenzar, experimentemos 
con ellos en Playground.

5
00:00:15,490 --> 00:00:17,270
Comencemos con la tasa de aprendizaje.

6
00:00:18,270 --> 00:00:22,590
La tasa de aprendizaje controla el tamaño 
del paso en el espacio del peso.

7
00:00:23,220 --> 00:00:25,340
Con un tamaño del lote igual a 30

8
00:00:25,690 --> 00:00:27,770
y todos los demás parámetros constantes

9
00:00:27,950 --> 00:00:31,225
definí mi primera
tasa de aprendizaje en 0.01.

10
00:00:32,025 --> 00:00:34,810
TensorFlow Playground
usa puntos de inicio aleatorios.

11
00:00:34,890 --> 00:00:37,405
Es posible que su resultado
sea distinto del mío.

12
00:00:38,135 --> 00:00:41,220
Note que hay algunas variaciones
en la curva de pérdida

13
00:00:41,410 --> 00:00:43,190
pero converge muy rápidamente.

14
00:00:43,340 --> 00:00:48,810
En mi caso, obtuve un valor de pérdida
de 0.139 en los datos de prueba

15
00:00:48,810 --> 00:00:50,750
con menos de 300 ciclos de entrenamiento.

16
00:00:52,820 --> 00:00:56,160
Cuando cambié
la tasa de aprendizaje a 0.001

17
00:00:56,230 --> 00:00:58,210
observé un rendimiento mucho más lento.

18
00:00:58,280 --> 00:01:01,110
En mi caso, tomó casi 3,000 ciclos

19
00:01:01,110 --> 00:01:04,440
para llegar a una pérdida de prueba
comparable al experimento anterior.

20
00:01:05,480 --> 00:01:09,660
Lo bueno es que no habrá
mucha variación en la curva de pérdida.

21
00:01:10,070 --> 00:01:12,900
Debería converger de manera
lenta pero constante.

22
00:01:15,070 --> 00:01:17,840
Veamos los efectos del tamaño del lote.

23
00:01:18,460 --> 00:01:23,940
El tamaño del lote controla la cantidad
de muestras para calcular el gradiente.

24
00:01:24,590 --> 00:01:27,130
Con la tasa de aprendizaje en 0.01

25
00:01:27,600 --> 00:01:29,700
y todos los otros parámetros constantes

26
00:01:29,900 --> 00:01:32,335
primero probé
un tamaño del lote igual a 100.

27
00:01:33,365 --> 00:01:34,995
Si está haciendo lo mismo que yo

28
00:01:35,075 --> 00:01:39,620
tal vez se pregunte cómo aumentar
el tamaño del lote más allá de 30.

29
00:01:39,770 --> 00:01:41,200
No se preocupe, no es un error.

30
00:01:41,410 --> 00:01:42,480
Está diseñado así.

31
00:01:42,710 --> 00:01:45,950
La IU no permite sobrepasar 30

32
00:01:46,470 --> 00:01:48,590
pero se puede cambiar en la URL.

33
00:01:49,850 --> 00:01:51,620
Con un tamaño de lote igual a 100

34
00:01:51,690 --> 00:01:54,015
observé una convergencia bastante lenta.

35
00:01:54,395 --> 00:01:56,435
Tomó más de 1,000 ciclos

36
00:01:56,435 --> 00:02:00,185
llegar a un valor de pérdida similar
al de experimentos anteriores.

37
00:02:00,815 --> 00:02:02,820
Pero no hubo pasos con mucho ruido.

38
00:02:05,020 --> 00:02:07,250
Cuando reduje el tamaño de lote a 5

39
00:02:07,970 --> 00:02:09,540
obtuve resultados muy rápidos.

40
00:02:09,610 --> 00:02:11,730
Básicamente, solo tardó 65 ciclos

41
00:02:11,810 --> 00:02:15,300
con una pérdida de prueba similar
a la de experimentos anteriores.

42
00:02:16,220 --> 00:02:19,355
Pero hubo algunos pasos ruidosos
visibles en la curva de pérdida.

43
00:02:20,205 --> 00:02:23,450
El rendimiento del modelo
es muy sensible a la tasa de aprendizaje

44
00:02:23,450 --> 00:02:24,620
y el tamaño del lote.

45
00:02:25,010 --> 00:02:27,710
¿No se siente
como afinar un instrumento musical?

46
00:02:28,070 --> 00:02:29,810
Le dije que esto tiene algo de arte.

47
00:02:32,460 --> 00:02:34,320
Recordemos nuestros hallazgos.

48
00:02:35,800 --> 00:02:39,355
La tasa de aprendizaje controla el tamaño
del paso en el espacio del peso.

49
00:02:39,925 --> 00:02:41,855
Si los pasos son muy pequeños

50
00:02:42,215 --> 00:02:44,145
el entrenamiento tardará mucho.

51
00:02:44,405 --> 00:02:47,185
Por otro lado,
si los pasos son muy grandes

52
00:02:47,245 --> 00:02:50,745
rebotará por todos lados e incluso
podría ignorar el punto óptimo.

53
00:02:51,635 --> 00:02:54,030
Una tasa de aprendizaje de 0.001

54
00:02:54,330 --> 00:02:58,750
equivale a un tamaño de paso
de 1/1,000 del espacio de entrada.

55
00:02:59,060 --> 00:03:01,150
Podría ser una tasa
de aprendizaje muy baja

56
00:03:01,360 --> 00:03:03,850
si tiene una superficie
de optimización muy grande.

57
00:03:05,150 --> 00:03:09,000
El valor predeterminado
para el Estimator LinearRegressor

58
00:03:09,050 --> 00:03:12,230
en la biblioteca de TensorFlow
está definido en 0.2

59
00:03:12,360 --> 00:03:15,450
o 1/raíz cuadrada
de la cantidad de atributos.

60
00:03:15,540 --> 00:03:18,000
Esto supone que los valores
de atributos y etiquetas

61
00:03:18,000 --> 00:03:19,130
son números pequeños.

62
00:03:21,870 --> 00:03:23,610
La otra opción es el tamaño del lote

63
00:03:23,610 --> 00:03:27,330
que controla la cantidad de muestras
con la que calculamos el gradiente.

64
00:03:27,740 --> 00:03:29,410
Si el tamaño de lote es muy pequeño

65
00:03:29,550 --> 00:03:32,640
podríamos terminar rebotando,
porque el lote puede no ser

66
00:03:32,640 --> 00:03:34,645
una buena representación
de la entrada.

67
00:03:35,115 --> 00:03:37,775
Por otro lado, si el tamaño
del lote es muy grande

68
00:03:37,815 --> 00:03:40,160
el entrenamiento tardará mucho.

69
00:03:41,230 --> 00:03:42,460
Como regla general

70
00:03:42,630 --> 00:03:46,830
entre 40 y 100 suele ser un buen rango
para el tamaño del lote.

71
00:03:46,920 --> 00:03:49,370
Puede llegar hasta 500.

72
00:03:53,670 --> 00:03:55,460
Ya que estamos hablando de lotes

73
00:03:55,880 --> 00:03:58,035
no olvidemos
la redistribución del lote.

74
00:03:58,795 --> 00:04:01,837
Tal vez haya escuchado
que es bueno redistribuir los ejemplos.

75
00:04:01,837 --> 00:04:02,610
Pero ¿por qué?

76
00:04:03,640 --> 00:04:05,450
Piense en títulos de libros, como estos.

77
00:04:05,990 --> 00:04:07,930
Supongamos que quiere
entrenar un modelo

78
00:04:07,930 --> 00:04:10,660
para sugerirle un libro a un usuario.

79
00:04:11,310 --> 00:04:13,905
Observe que los libros
están ordenados alfabéticamente.

80
00:04:14,495 --> 00:04:16,790
Si usa la base de datos como está

81
00:04:17,610 --> 00:04:20,950
cada lote de entrenamiento
contendrá un subconjunto de títulos

82
00:04:20,950 --> 00:04:23,785
basado en el orden alfabético.

83
00:04:24,995 --> 00:04:29,020
Le daría a su pobre modelo una visión
muy reducida del dominio del problema

84
00:04:29,150 --> 00:04:32,415
y le impediría descubrir toda la verdad.

85
00:04:32,915 --> 00:04:34,895
No queremos ser malos profesores.

86
00:04:35,535 --> 00:04:39,965
Queremos que cada lote sea representativo
de todo el conjunto de datos.

87
00:04:40,675 --> 00:04:43,620
La mayoría de los conjuntos de datos
tienen un orden inherente.

88
00:04:43,620 --> 00:04:46,150
Por ejemplo,
estos títulos ordenados alfabéticamente

89
00:04:46,630 --> 00:04:48,970
registros de clientes
ordenados por código postal

90
00:04:48,970 --> 00:04:51,795
compras archivadas
por temporadas o año, etc.

91
00:04:52,445 --> 00:04:54,850
Si redistribuimos correctamente
el conjunto de datos

92
00:04:54,930 --> 00:04:58,770
nos aseguramos de que cada lote
sea representativo del conjunto de datos.

93
00:04:59,430 --> 00:05:02,620
Recuerde que el gradiente
se calcula dentro del lote.

94
00:05:02,980 --> 00:05:04,955
Si el lote no es representativo

95
00:05:05,025 --> 00:05:08,910
la pérdida variará mucho de lote a lote.