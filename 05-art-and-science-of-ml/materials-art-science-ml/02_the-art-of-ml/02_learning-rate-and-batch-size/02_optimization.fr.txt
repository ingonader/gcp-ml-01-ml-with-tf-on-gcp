Concluons ce module
par une brève présentation du concept d'optimisation. L'optimisation est un autre champ
de recherche en pleine évolution dans le domaine du machine learning. De nombreuses techniques existent déjà,
et d'autres sont en développement. L'optimisation fait référence au fait de
minimiser ou maximiser une fonction "ffx" en modifiant "x". Certains algorithmes sont présentés ici. Vous connaissez déjà GradientDescent, qui tente de trouver
la fonction de perte minimale en modifiant les valeurs de poids. Momentum réduit le taux d'apprentissage
quand les valeurs de gradient sont basses. AdaGrad abaisse le taux d'apprentissage pour les caractéristiques
qui se manifestent fréquemment. AdaDelta améliore AdaGrad en empêchant la réduction à zéro
du taux d'apprentissage. Adam est une version améliorée d'AdaGrad. Ftrl (Follow the regularized leader)
fonctionne bien sur les modèles étendus. À l'heure actuelle, Adam et Ftrl sont
des solutions par défaut idéales pour les réseaux de neurones profonds
et les modèles linéaires. Qui peut dire quelles nouvelles
techniques d'optimisation domineront le secteur demain ?