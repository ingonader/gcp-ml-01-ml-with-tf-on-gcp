1
00:00:00,740 --> 00:00:06,305
Vamos concluir este módulo
com o conceito de otimização.

2
00:00:06,835 --> 00:00:11,370
A otimização é outro campo de pesquisa
em evolução no aprendizado de máquina.

3
00:00:11,370 --> 00:00:14,230
Há várias técnicas publicadas
e muitas mais no futuro.

4
00:00:14,720 --> 00:00:18,030
O objetivo da otimização
é minimizar ou

5
00:00:18,030 --> 00:00:21,945
maximizar alguma função
ou efeito com a alteração de X.

6
00:00:21,945 --> 00:00:24,795
Alguns dos algoritmos
estão listados aqui.

7
00:00:26,335 --> 00:00:28,730
Você já conhece
o gradiente descendente,

8
00:00:28,730 --> 00:00:33,080
que altera os valores para
encontrar o mínimo da função.

9
00:00:33,910 --> 00:00:36,835
O momentum reduz
a taxa de aprendizado

10
00:00:36,835 --> 00:00:39,285
quando os valores
do gradiente são pequenos.

11
00:00:40,255 --> 00:00:45,320
AdaGrad confere taxas menores
a recursos mais frequentes.

12
00:00:46,520 --> 00:00:52,755
AdaDelta impede que
AdaGrad reduza a taxa para 0.

13
00:00:53,735 --> 00:00:57,780
Adam é basicamente AdaGrad
com várias correções.

14
00:00:59,090 --> 00:01:05,885
Ftrl, ou siga o mestre regularizado,
funciona bem em modelos brancos.

15
00:01:05,885 --> 00:01:08,800
Por hora, Adam e Ftrl

16
00:01:08,800 --> 00:01:13,845
são bons padrões para redes
neurais profundas e modelos lineares.

17
00:01:14,535 --> 00:01:18,480
Quem sabe quais serão
as melhores técnicas amanhã?