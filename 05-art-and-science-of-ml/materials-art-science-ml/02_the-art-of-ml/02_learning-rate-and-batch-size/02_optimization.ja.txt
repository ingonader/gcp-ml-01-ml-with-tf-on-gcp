最適化という概念を簡単に見て
このモジュールを終了しましょう 最適化とは MLで進化し続けている
もうひとつの研究領域です すでに多くの手法が公表されており
さらに登場するでしょう 最適化とはXを変えることで機能の効果を
最小化または最大化するタスクです いくつかのアルゴリズムをここに列挙しました GradientDescent（勾配降下法）は
すでにご存じですね この手法は 重み値を変更することで
損失関数の最低値を見つけようとします Momentumは勾配値が小さいときに
学習率を低下させます AdaGradは発生頻度の高い特徴の学習率を
低下させます AdaDeltaは学習率が0にならないように
改善したAdaGradです Adamは基本的に
多くの修正を加えたAdaGradです FtrlつまりFollow the regularized leaderは
広範なモデルで良好に動作します 現時点では AdamとFtrlが深層ニューラルネットワークと
線形モデルに対する良好なデフォルトです 明日どの新しい最適化手法が主流になるかは
誰にもわかりません