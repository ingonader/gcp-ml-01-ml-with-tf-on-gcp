1
00:00:01,510 --> 00:00:03,870
Ceci nous amène
à la deuxième partie de ce module,

2
00:00:03,870 --> 00:00:06,880
dans laquelle nous allons voir
deux hyperparamètres importants :

3
00:00:06,880 --> 00:00:09,260
le taux d'apprentissage
et la taille de lot.

4
00:00:09,670 --> 00:00:13,980
Amusons-nous un peu avec ces paramètres
dans notre terrain de jeu préféré.

5
00:00:15,430 --> 00:00:17,360
Commençons par le taux d'apprentissage.

6
00:00:18,270 --> 00:00:21,580
Rappelez-vous, le taux d'apprentissage
contrôle la taille des "pas"

7
00:00:21,580 --> 00:00:22,960
dans l'espace de poids.

8
00:00:22,960 --> 00:00:25,050
J'ai utilisé une taille de lot égale à 30

9
00:00:25,050 --> 00:00:27,830
et une valeur constante pour
tous les autres paramètres.

10
00:00:27,830 --> 00:00:31,585
Pour le premier essai, j'ai réglé
le taux d'apprentissage sur 0,01.

11
00:00:31,585 --> 00:00:34,720
TensorFlow Playground utilise
des points de départ aléatoires.

12
00:00:34,720 --> 00:00:37,645
Il se peut donc que vos résultats
soient différents des miens.

13
00:00:37,645 --> 00:00:41,300
Vous remarquerez peut-être de drôles
de rebonds sur la courbe de perte,

14
00:00:41,300 --> 00:00:43,210
mais elle converge assez vite.

15
00:00:43,210 --> 00:00:48,900
Pour ma part, j'ai une valeur de perte
de 0,139 sur les données de test,

16
00:00:48,900 --> 00:00:50,720
et moins de 300 itérations.

17
00:00:52,880 --> 00:00:55,970
Lorsque je suis passée à
un taux d'apprentissage de 0,001,

18
00:00:55,970 --> 00:00:58,260
j'ai constaté
un ralentissement des performances.

19
00:00:58,260 --> 00:01:00,980
Dans mon cas,
il a fallu près de 3 000 itérations

20
00:01:00,980 --> 00:01:02,540
pour atteindre une perte de test

21
00:01:02,540 --> 00:01:04,930
comparable à celle
de l'expérimentation précédente.

22
00:01:05,300 --> 00:01:07,320
Le côté positif est
que vous ne devriez pas

23
00:01:07,320 --> 00:01:09,860
observer de rebonds importants
sur la courbe de perte.

24
00:01:09,860 --> 00:01:12,950
Elle devrait converger lentement,
mais en douceur.

25
00:01:15,180 --> 00:01:17,970
Intéressons-nous maintenant
aux effets de la taille de lot.

26
00:01:18,300 --> 00:01:21,700
Rappelez-vous que la taille de lot
contrôle le nombre d'échantillons

27
00:01:21,700 --> 00:01:23,940
sur lesquels le gradient est calculé.

28
00:01:24,460 --> 00:01:27,110
J'ai réutilisé
le taux d'apprentissage de 0,01

29
00:01:27,110 --> 00:01:29,740
et une valeur constante pour
tous les autres paramètres.

30
00:01:29,740 --> 00:01:32,385
J'ai d'abord essayé
avec une taille de lot de 100.

31
00:01:33,385 --> 00:01:36,925
Si vous participez activement, vous êtes
sûrement en train de vous demander

32
00:01:36,925 --> 00:01:39,450
comment augmenter
la taille de lot au-delà de 30.

33
00:01:39,450 --> 00:01:41,380
Ne vous inquiétez pas, rien n'est cassé.

34
00:01:41,380 --> 00:01:42,520
C'est conçu ainsi.

35
00:01:42,740 --> 00:01:45,970
L'interface utilisateur ne permet pas
d'aller au-delà de 30,

36
00:01:46,380 --> 00:01:48,430
mais vous pouvez modifier cela dans l'URL.

37
00:01:49,810 --> 00:01:51,690
Avec une taille de lot de 100,

38
00:01:51,690 --> 00:01:54,185
j'ai constaté que la convergence
était assez lente.

39
00:01:54,185 --> 00:01:56,295
Il a fallu près de 1 000 itérations

40
00:01:56,295 --> 00:01:58,005
pour atteindre une valeur de perte

41
00:01:58,005 --> 00:02:00,395
semblable à celle
des expérimentations précédentes,

42
00:02:00,395 --> 00:02:02,930
mais cela s'est fait en douceur.

43
00:02:04,920 --> 00:02:07,590
Lorsque j'ai réduit la taille de lot à 5,

44
00:02:07,590 --> 00:02:09,610
j'ai obtenu des résultats très rapidement.

45
00:02:09,610 --> 00:02:11,780
Pour tout dire, en à peine 65 itérations,

46
00:02:11,780 --> 00:02:15,450
la perte de test était comparable
à celle des expérimentations précédentes.

47
00:02:16,100 --> 00:02:19,425
Mais certaines étapes généraient
du bruit sur la courbe de perte.

48
00:02:19,725 --> 00:02:22,790
Tout ceci montre que la performance
du modèle est très dépendante

49
00:02:22,790 --> 00:02:25,100
du taux d'apprentissage
et de la taille de lot.

50
00:02:25,100 --> 00:02:28,370
Ça ne vous fait pas penser à l'accordage
d'un instrument de musique ?

51
00:02:28,370 --> 00:02:30,300
Je vous ai dit que c'était tout un art.

52
00:02:32,240 --> 00:02:34,460
Récapitulons nos observations.

53
00:02:34,460 --> 00:02:38,195
N'oubliez pas, le taux d'apprentissage
contrôle la taille des "pas"

54
00:02:38,195 --> 00:02:39,725
dans l'espace de poids.

55
00:02:39,725 --> 00:02:42,095
Si les pas sont trop petits,

56
00:02:42,095 --> 00:02:44,165
l'entraînement durera longtemps.

57
00:02:44,165 --> 00:02:48,615
En revanche, s'ils sont trop grands,
des rebonds peuvent se produire.

58
00:02:48,615 --> 00:02:51,365
Il se peut même
que le point optimal ne soit pas atteint.

59
00:02:51,365 --> 00:02:55,290
Un taux d'apprentissage de 0,001
correspond à une taille de pas

60
00:02:55,290 --> 00:02:58,850
égale à 1 sur 1 000 de l'espace d'entrée.

61
00:02:58,850 --> 00:03:01,560
Ce taux pourrait se révéler trop faible

62
00:03:01,560 --> 00:03:04,060
si vous disposez
d'une grande surface d'optimisation.

63
00:03:05,050 --> 00:03:08,525
Par exemple, sachez que
pour l'estimateur de régression linéaire

64
00:03:08,525 --> 00:03:12,195
de la bibliothèque TensorFlow,
la valeur par défaut est définie sur 0,2

65
00:03:12,195 --> 00:03:15,430
(ou sur 1 divisé par la racine carrée
du nombre de caractéristiques).

66
00:03:15,430 --> 00:03:19,380
Cela implique que les valeurs des libellés
et des caractéristiques soient faibles.

67
00:03:21,680 --> 00:03:23,720
L'autre facteur concerne la taille de lot.

68
00:03:23,720 --> 00:03:25,920
Il permet de contrôler
le nombre d'échantillons

69
00:03:25,920 --> 00:03:27,690
sur lesquels le gradient est calculé.

70
00:03:27,690 --> 00:03:30,820
Si la taille de lot est trop petite,
des rebonds sont à craindre,

71
00:03:30,820 --> 00:03:34,865
car il se peut que le lot ne représente
pas l'entrée assez fidèlement.

72
00:03:34,865 --> 00:03:37,815
D'un autre côté,
si la taille de lot est trop importante,

73
00:03:37,815 --> 00:03:40,370
l'entraînement prendra beaucoup de temps.

74
00:03:41,140 --> 00:03:42,590
En règle générale,

75
00:03:42,590 --> 00:03:46,860
il est recommandé d'utiliser une taille
de lot comprise entre 40 et 100.

76
00:03:46,860 --> 00:03:49,530
Sachez toutefois qu'il est possible
d'aller jusqu'à 500.

77
00:03:53,740 --> 00:03:55,710
Tant que nous parlons des lots,

78
00:03:55,710 --> 00:03:58,475
n'oubliez pas que vous pouvez
utiliser le brassage de lots.

79
00:03:58,475 --> 00:03:59,970
Vous avez sûrement entendu dire

80
00:03:59,970 --> 00:04:02,900
que le brassage d'exemples est
une bonne idée, mais pourquoi ?

81
00:04:03,440 --> 00:04:05,610
Prenons des titres de livres
tels que ceux-ci.

82
00:04:05,610 --> 00:04:08,300
Imaginons que vous entraînez
un outil de recommandation

83
00:04:08,300 --> 00:04:11,280
chargé de suggérer le prochain livre
à lire aux utilisateurs.

84
00:04:11,280 --> 00:04:13,975
Notez que les titres sont classés
par ordre alphabétique.

85
00:04:14,385 --> 00:04:16,700
Si vous utilisez
l'ensemble de données en l'état,

86
00:04:17,530 --> 00:04:20,950
chaque lot d'entraînement contiendra
un sous-ensemble des titres,

87
00:04:20,950 --> 00:04:24,015
basé sur les lettres consécutives
de l'alphabet.

88
00:04:24,905 --> 00:04:29,030
Vous donnerez à votre pauvre modèle
une vision étriquée du domaine du problème

89
00:04:29,030 --> 00:04:32,485
et lui enlèverez toute chance
de découvrir toute la vérité.

90
00:04:32,795 --> 00:04:35,185
Vous ne souhaitez pas être
un mauvais professeur ?

91
00:04:35,185 --> 00:04:38,315
Vous préférez certainement que
chaque lot soit représentatif

92
00:04:38,315 --> 00:04:40,465
de l'ensemble de données
dans son intégralité.

93
00:04:40,465 --> 00:04:43,570
La plupart des ensembles de données
ont un ordre intrinsèque,

94
00:04:43,570 --> 00:04:46,420
comme ces titres de livres
classés alphabétiquement,

95
00:04:46,420 --> 00:04:48,880
des dossiers clients triés
selon le code postal,

96
00:04:48,880 --> 00:04:52,085
des achats archivés par saison,
par année, etc.

97
00:04:52,085 --> 00:04:54,740
Si vous brassez correctement
votre ensemble de données,

98
00:04:54,740 --> 00:04:55,800
vous avez l'assurance

99
00:04:55,800 --> 00:04:58,830
que chaque lot sera
représentatif de tout l'ensemble.

100
00:04:59,380 --> 00:05:02,790
Souvenez-vous que les gradients
sont calculés au sein des lots.

101
00:05:02,790 --> 00:05:05,175
Si les lots ne sont pas représentatifs,

102
00:05:05,175 --> 00:05:08,940
la perte sera bien trop importante
lorsque vous passerez d'un lot à l'autre.