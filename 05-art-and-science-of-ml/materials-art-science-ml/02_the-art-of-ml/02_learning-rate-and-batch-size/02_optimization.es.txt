Para finalizar el módulo,
veamos el concepto de optimización. La optimización
es otro campo de investigación en desarrollo del aprendizaje automático. Se han publicado muchas técnicas
y aparecerán otras más. La optimización se refiere
a la tarea de minimizar o maximizar alguna función f(x) al alterar x. Estos son algunos de los algoritmos. Ya conoce GradientDescent que intenta encontrar
la función de pérdida mínima al alterar los valores de peso. Momentum reduce la tasa de aprendizaje
cuando los valores de gradiente son bajos. AdaGrad asigna a los atributos frecuentes
tasas de aprendizaje más bajas. AdaDelta mejora AdaGrad ya que evita que la tasa de aprendizaje
se reduzca a cero. Adam es AdaGrad con varias mejoras. Ftrl o "seguir al líder regularizado"
funciona bien en los modelos amplios. Actualmente, Adam y Ftrl son buenas
opciones predeterminadas para las redes neuronales profundas
y los modelos lineales. Quién sabe qué nuevas técnicas
de optimización serán las más usadas en el futuro.