Vamos concluir este módulo
com o conceito de otimização. A otimização é outro campo de pesquisa
em evolução no aprendizado de máquina. Há várias técnicas publicadas
e muitas mais no futuro. O objetivo da otimização
é minimizar ou maximizar alguma função
ou efeito com a alteração de X. Alguns dos algoritmos
estão listados aqui. Você já conhece
o gradiente descendente, que altera os valores para
encontrar o mínimo da função. O momentum reduz
a taxa de aprendizado quando os valores
do gradiente são pequenos. AdaGrad confere taxas menores
a recursos mais frequentes. AdaDelta impede que
AdaGrad reduza a taxa para 0. Adam é basicamente AdaGrad
com várias correções. Ftrl, ou siga o mestre regularizado,
funciona bem em modelos brancos. Por hora, Adam e Ftrl são bons padrões para redes
neurais profundas e modelos lineares. Quem sabe quais serão
as melhores técnicas amanhã?