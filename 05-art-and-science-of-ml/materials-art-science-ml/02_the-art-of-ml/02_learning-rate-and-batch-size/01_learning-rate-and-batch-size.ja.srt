1
00:00:00,560 --> 00:00:04,400
モジュールのセクション2に入ります

2
00:00:04,400 --> 00:00:07,080
ここでは2つの重要な
ハイパーパラメータを見ていきます

3
00:00:07,080 --> 00:00:09,640
学習率とバッチサイズです

4
00:00:09,640 --> 00:00:13,930
まずこれらのパラメータを
いつものPlaygroundで使ってみましょう

5
00:00:15,500 --> 00:00:18,270
学習率から始めましょう

6
00:00:18,270 --> 00:00:23,380
学習率は重み空間での
ステップサイズを制御します

7
00:00:23,380 --> 00:00:27,800
バッチサイズを30にし
他のすべてのパラメータを一定に維持します

8
00:00:27,800 --> 00:00:31,585
最初に学習率を0.01に設定しました

9
00:00:31,585 --> 00:00:37,965
TF Playgroundの開始点は無作為なので
皆さんの結果は異なる可能性があります

10
00:00:37,965 --> 00:00:43,440
誤差曲線におかしな跳ね返りがありますが
急速に収束しています

11
00:00:43,450 --> 00:00:49,120
私の場合 テストデータで
誤差値は0.139でした

12
00:00:49,120 --> 00:00:52,860
エポック数は300未満でした

13
00:00:52,860 --> 00:00:58,520
学習率を0.001に変更すると
ずっと遅くなりました

14
00:00:58,520 --> 00:01:05,280
前回の実験に匹敵するテスト誤差に
達するのに約3,000エポック必要でした

15
00:01:05,280 --> 00:01:10,210
ただ 誤差曲線に
奇妙な跳ね返りはありません

16
00:01:10,210 --> 00:01:14,890
ゆっくり なだらかに収束しています

17
00:01:14,890 --> 00:01:18,710
ここからバッチサイズの効果を実験しましょう

18
00:01:18,710 --> 00:01:24,560
バッチサイズは
勾配計算に使うサンプル数を制御します

19
00:01:24,560 --> 00:01:29,910
学習率を0.01にし
他のすべてのパラメータを一定に維持します

20
00:01:29,910 --> 00:01:33,185
最初はバッチサイズを100で試しました

21
00:01:33,185 --> 00:01:39,720
皆様が試すとき どうやってバッチサイズを
30以上に上げるか悩むかもしれません

22
00:01:39,720 --> 00:01:42,780
心配不要です 壊れてはいません
設計どおりです

23
00:01:42,780 --> 00:01:46,630
UIは30を超えるバッチサイズを
受け入れません

24
00:01:46,630 --> 00:01:49,520
しかしURLから変更できます

25
00:01:49,520 --> 00:01:51,880
バッチサイズを100とします

26
00:01:51,880 --> 00:01:54,615
収束はかなりゆっくりとしています

27
00:01:54,615 --> 00:02:00,745
前回の実験に近い誤差値に達するまで
1,000以上のエポックが必要でした

28
00:02:00,745 --> 00:02:04,990
しかしノイズのあるステップは
ありませんでした

29
00:02:04,990 --> 00:02:09,840
バッチサイズを5に下げると
非常に早く結果が得られました

30
00:02:09,840 --> 00:02:16,160
たったの 65エポックだけで
前回の実験に近いテスト誤差に達しました

31
00:02:16,160 --> 00:02:19,745
しかし誤差曲線には
ノイズのあるステップがありました

32
00:02:19,745 --> 00:02:25,100
モデルのパフォーマンスはバッチサイズと
学習率の影響を強く受けることがわかりました

33
00:02:25,100 --> 00:02:30,300
まるで 技巧が求められる
楽器の調律のようだと思いませんか

34
00:02:32,370 --> 00:02:35,120
では結果を要約します

35
00:02:35,120 --> 00:02:39,905
学習率は重み空間での
ステップのサイズを制御します

36
00:02:39,905 --> 00:02:44,515
ステップが小さすぎると
トレーニングに長時間かかります

37
00:02:44,515 --> 00:02:48,555
一方ステップが大きすぎると
跳ね返りがあります

38
00:02:48,555 --> 00:02:51,465
そして最適点を失うこともあり得ます

39
00:02:51,465 --> 00:02:59,190
学習率が0.001とは
ステップサイズ=入力空間の1/1000です

40
00:02:59,190 --> 00:03:05,010
最適化表面が大きければ
この学習率は小さすぎるかもしれません

41
00:03:05,010 --> 00:03:12,275
たとえばTensorFlowライブラリでの
線形回帰estimatorのデフォルト値は0.2

42
00:03:12,275 --> 00:03:15,670
または 
1/(特徴数の平方根) に設定されます

43
00:03:15,670 --> 00:03:20,180
これは特徴とラベルが
小さい数であることを想定しています

44
00:03:21,750 --> 00:03:23,940
もう1つの設定はバッチサイズです

45
00:03:23,940 --> 00:03:27,870
勾配が計算されるサンプル数を制御します

46
00:03:27,870 --> 00:03:31,120
バッチサイズが小さすぎると
跳ね返ってしまいます

47
00:03:31,120 --> 00:03:35,170
これはバッチがデータ全体を象徴するのに
不十分である可能性があるからです

48
00:03:35,170 --> 00:03:41,145
一方 バッチサイズが大きすぎると
トレーニングに非常に長時間かかります

49
00:03:41,145 --> 00:03:47,290
経験則として
バッチサイズの適切な範囲は40～100です

50
00:03:47,290 --> 00:03:51,330
最高では500まで可能です

51
00:03:53,450 --> 00:03:56,180
バッチの話題を続けます

52
00:03:56,180 --> 00:03:58,985
バッチシャッフリングを忘れてはいけません

53
00:03:58,985 --> 00:04:03,580
サンプルのシャッフリングは良いことだと
聞いたことがあるはずです

54
00:04:03,580 --> 00:04:06,080
ここに示す書名を取り上げましょう

55
00:04:06,080 --> 00:04:11,250
次に読む本をユーザーに提案する機能を
トレーニングしているとします

56
00:04:11,250 --> 00:04:14,235
書名はアルファベット順です

57
00:04:14,235 --> 00:04:17,850
このデータセットをそのまま使用すると

58
00:04:17,850 --> 00:04:21,040
各トレーニングバッチには

59
00:04:21,040 --> 00:04:24,700
連続するアルファベットに基づいた
書名のサブセットが含まれます

60
00:04:24,700 --> 00:04:30,395
一部のアルファベットで始まる
非常に限られた書名しか見ていないモデルは

61
00:04:30,395 --> 00:04:32,845
書名の全容を解明することができません

62
00:04:32,845 --> 00:04:35,495
これではモデルがかわいそうですね

63
00:04:35,495 --> 00:04:40,565
そこで すべてのバッチがデータセット全体を
象徴するようにしたいわけです

64
00:04:40,565 --> 00:04:43,880
多くのデータセットは
一定の順番を持つ傾向があります

65
00:04:43,880 --> 00:04:49,040
アルファベット順の書名、
郵便番号順の顧客記録、

66
00:04:49,040 --> 00:04:52,450
季節や年度ごとに保管された購入記録などです

67
00:04:52,450 --> 00:04:55,440
データセットを適切にシャッフルすると

68
00:04:55,440 --> 00:04:59,570
各バッチは
データセット全体を象徴するものになります

69
00:04:59,570 --> 00:05:03,210
勾配はバッチ内で計算されることを
覚えておきましょう

70
00:05:03,210 --> 00:05:09,675
バッチが全体を象徴していない場合
バッチごとに誤差が大きく変動します