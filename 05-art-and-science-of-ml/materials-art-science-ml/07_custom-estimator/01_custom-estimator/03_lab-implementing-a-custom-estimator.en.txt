In this lab, our goal is to learn how to write a custom estimator. We will assume that we have a TensorFlow function, that takes a set of input tensors and creates a set of output tensors. Our job will be to wrap this function into the estimator framework, so that we get all the high-level benefits that estimator provides. In reality, when you get a working model that doesn't use estimators, it will have some way of reading data and outputting models. You will essentially throw those parts of the model away and keep only the mathematical core of the model: the model that turns the input tensor into the output tensor. So in this lab, we are essentially looking at how you would take a model and wrap it with the estimator framework, your own custom module function. So in order to illustrate this as in the slides, we are essentially using a time series model. Let's not worry about how the time series model actually works, we'll look at sequence models later in the specialization, but for now we just treat it as a black box. The basic, but we'll look at the external wrapper of how this thing works. So in this case what we have is, let me just go to an import TensorFlow, and what we will do is that we basically create, simulate a bunch of data, each of these data has essentially different sine waves of different amplitudes that are basically going in different frequencies that are essentially getting created. So here are five examples of this time series. We will actually create lots and lots of this data, and that is the data that we are going to be training, and the idea is that we will give the neural network nine values. So, zero, one, two, three. All right? We'll give it actually up to eight, then nav we'll give it nine values and have it predict the tenth. So we'll teach it based on a bunch of existing data and have it learn based on nine values what the tenth one ought to be. So in order to do that, we'll go ahead and create a CSV file, two CSV, give it a file name, tell it how many sequences we want, then what we'll do is we basically open up the file, write it, and create a time series. How many times series? N of them. So in this case I'm calling train.csv.n equals thousand. So I'm going to get a file with a thousand sequences. My train.csv is going to contain a thousand sequences, value.csv is going to contain 50 sequences. So, and they're all going to be separated by commas. So I can run this, and then having run it, I can look at the first five lines of train.csv, those are the first five lines, and the first five lines of value.csv. As you can see, this is essentially one time series and our training input features are going to be these, and this is going to be our label, and that's essentially what we want our model to learn. So where does something like this come in? I mean some, even though you're not going to talk about time series, it's probably good to kind of think about what the situation we're illustrating is. The situation that we are illustrating is something like let's say, you're running a retail store and you have thousands of items, and each item has its own seasonality, and you want to basically look at the past eight time periods or plus nine time periods, and use it to predict that tenth time period. That's essentially what you're doing. This is not the time series thing where you're trying to predict the future value of a stock market. That is different, that is one very very very long time series. Instead here, we have thousands of shark time series. So it's a different problem, it's a different problem. This problem is the retail example, where you have thousands of products, each of them have their own seasonality, but they all have seasonality, and you want to basically learn that idea of the seasonality, so that you can look at just that one product's time series and predict the next one. So that's our training dataset, and based on that we're basically going to train our model. The model that you're going to train is called recurring neural network. Again we're not going to worry too much about the internals of the model itself, but we're going to be worried about how we set it up. So, in this case again, we import TensorFlow, and then we have to read our data. Our data is essentially our sequence length. So we basically have defaults is 0.01, so these are all floating point numbers for X range of zero to sequence length. So we basically have 10 numbers. Our batch size, this is how many? We going to compute a gradient descent on, our batch size is going to be 20. The time series column in our data is going to be called raw data, and in our sequence, the number of outputs is one, that's the final output, and the number of inputs is a sequence length minus the number of outputs. So in other words, the first nine are the inputs, and the last one is output. So that's the set of constants if you're defining, and then we basically write our read dataset. This is like creating an input function, here our decode csv given a line, it's basically going to say "Go ahead and read them all as floating point numbers", so you're going to get all data, which is going to be 10 numbers, but remember that it's going to read them one batch at a time. So this thing is not one line, it is actually the data corresponding to typically 20 lines because we're reading it batch by batch. So this is 20 lines, and of those, we're going to slice the first nine of them, and those become the inputs, and we're going to slice the last column, and that's going to become the labels. So that's basically what we're doing here. We're slicing the first nine values and that's our inputs, last value and that's our labels. So again, inputs is going to be of length batch size and width nine, and labels is going to be of height batch size and width one, number of outputs. So we basically take those things, and these are all separate values and we stack them together, so that we basically get a matrix. So that's our input. So we're stacking it to form a matrix, we are stacking this to form a matrix, the matrix here, the second dimension is one, but it's still not in our matrix, rather than the list of lists. We don't want a list of lists, we want a matrix. So that's what the stack does, and then we basically say the time series call, raw data, the tensor is the inputs and now we can return the features and labels. So features contains only one, it's a dictionary contains one feature, that feature is a matrix. Earlier, all of our features were single columns, but here our feature is a matrix. Okay? So that's why you're doing the stack here. So having done that, now how do you do the read dataset? When somebody says read dataset giving us a file name they may actually give us a file path. So what we basically going to do is that we're going to do glob, match all the files that have a wildcard in it for example that gives us a file list, and read it as a text line and call decode csv has to get back our dataset, and if we are doing training, we will shuffle the dataset. If we're doing evaluation there's no need to shuffle, so we just don't shuffle. If we're doing training we'll read indefinitely, if you're reading, during evaluation you want to read the entire dataset once, so the number of epochs is one. So we basically repeat the dataset for the number of epochs. For evaluation we do it once, for training we do it forever and we batch it by batch size. So 20 rows at a time, 20 sequences at a time, and then we basically return the iterator. So that is our reading of our dataset. Now, the model itself, let's not worry about how this works, the key thing is that we have a metrical simple RNN that takes our features, our labels and our mode, and what it does is that it pulls out the sequence X out of the features, and then it does something to them. So let's not worry about these, until it gets to the predictions. This is the output of our time series model. So given the input, we basically have an output, and that's what pretty much every model function is. Having done that we now have to decide what our last function is. Remember that there is a time series problem, we are predicting the last value. In other words, we are predicting a value. So is this regression or is a classification? Regression, right? And because it's regression, my loss is going to be mean squared error. I could use root-mean- squared error, I can also use treatment the mean squared error. My training operation is going to be to minimize the loss with a specific learning rate and with the specific optimizer, and my evaluation metrics is going to be the rmc this time. The root-mean-squared error given the labels and given the predictions. If it's not training and it's not evaluation, the loss, the train op and the eval metrics are all none. There are none because we don't have a label. We can't during prediction, we won't have a label. So we can do evaluation, we can not do training, we cannot do loss. So we set all of those operations to be none. Our predictions dictionaries are essentially the output predictions, we're just calling it, giving it a name "Predicted", and when we are exporting we'll call it regression export outputs, and what we basically do is to basically take those predictions and write them out. In this case we don't have any embedding that we want to write out, so we're just writing one thing out. If you had multiple things to write out, again this is just a dictionary. So we could basically go down here and say embedding, right? And let's say up here in our embedding let's say we had some tensor. Okay. So let's say this weight tensor wasn't an embedding, you would basically go down here and say embedding weight, and that's it. So when we export our model, we'll now be exporting two things. We'll be exporting the regression output and we'll be exporting an embedding. So having done that, we can basically write out an estimators spec passing in the mode, passing in the predictions dict, passing in the loss, the train op, the valuation metrics and the things that we want to export. And that's pretty much it. The rest of it is essentially the same as before, you basically create your training, your validation functions. These have to take no parameters or input functions, so this is what I'm doing, I'm just giving it a get train, that passes in train.csv and train for the mode, and then our serving input function essentially takes the time series col, and says these are all 14 point numbers, we col the train and evaluate, and we try it out as a standalone module, and we can also train it on ML engine, remembering to change the bucket to be a qwiklabs bucket.