Let's say you read a research paper and they talk about a cool model to predict the future values of a time series. They even give you the code, it uses strange concepts like LSTMCell, unfamiliar TensorFlow functions like static_rnn and even does a bit of low-level TensorFlow operations like matmul. For now, let's not worry about what these mean or how they work. We will look at sequence models in the ninth course of the specialization. For now though, let's just treat this function as a Blackbox. Essentially, the input is a time series, an array of values organized by time. N_INPUTS is a length of this time series. The predictions is the output of the model. There N_OUTPUTS numbers in the output array. These represent the future values of the time series as predicted by this model. So, you have a model and you'd like to train it. Notice something here, this is just the model, just the math if you will. We still need to read in the data, we still need to train to evaluate, et cetera, and we want to do this in a distributed fault-tolerant way. We want to add the engineering. Naturally, we want to wrap it in estimator framework because that's the way to get distributed training scaled prediction, et cetera. So, let's look at how to do this. The first thing is to go look at the train_and_evaluate function. Notice that it takes three parameters: estimator, train_spec, eval_spec. Train_spec and eval_spec are the same as in a canned estimator. These control how to feed the input data using input function and data set. They also control how long to train, how often to evaluate, when to export. What's different here is the first parameter to train and evaluate, the estimator. Before, we would create an estimator by creating a linear regressor or DNN regressor or linear classifier, et cetera, we would create a canned or pre-built estimator. Now though, we create a base class estimator. Notice that I'm setting the estimator to just tf.estimators.Estimator. The base class estimator takes two parameters, the second parameter, just [inaudible] canned estimators, is where to save the check points, the output directory. The first parameter is the model function. What does this model function, myfunc, looks like? Myfunc is an estimator_spec. What I mean is that myfunc returns a tf.estimater.EstimatorSpec. It takes three parameters: features, targets and mode. Features and targets should be familiar. This is what gets returned, for example, from a training input function. Features is a dictionary of features, so in this case I take that dictionary and pull out the tensor corresponding to INCOL. Targets is simply the label, again what gets returned from the training input function. The mode is one of three values: train, eval or predict. We look at why you might want this mode shortly. From these three input values, the job of myfunc is to create and return an estimator_spec. There are six things in an estimator_spec. The first parameter mode can simply be passed through, what you get in, just pass it out. The second parameter is the predictions, the outputs of the model. The predictions have to be a dictionary, provide a key name and the corresponding tensor. So here, my predictions_dict consist of just one key, I'm calling it predicted, and the tensor, of course, is the output of the model from research paper. We use the predictions to create the exported outputs. The idea is that we could export things other than just the predictions. For example, you might want to export a trained embedding from the model, this is where you would do that, specify a key and the corresponding tensor. Let's look at other things that form the estimator_spec; the loss, the training operation and the evaluation metric operation. The training operation needs to be carried out only if the mode is trained. The evaluation metric needs to be computed only if the mode is eval. So, I set the loss to be the mean squared error between the targets, the labels and the predictions. The training op consist of optimizing the loss function using, in this case, Stochastic Gradian Descent. Presumably because in the research paper we saw this model described in, they used SGD. The evaluation metric is a dictionary consisting of all the metrics that we want to evaluate. Here, I'm computing just a root mean squared error. During predictions, none of these operations should be executed. Why? Because we won't have a label. So, we set all these ops to be none, and that's it. So in summary, we call train_and_evaluate with a base class estimator, passing in a function that returns an estimator_spec, and that's it, we have a custom estimator.