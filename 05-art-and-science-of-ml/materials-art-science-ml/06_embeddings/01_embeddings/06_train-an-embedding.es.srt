1
00:00:00,695 --> 00:00:03,121
Imagine que crea una incorporación

2
00:00:03,281 --> 00:00:05,299
para representar la palabra clave

3
00:00:05,552 --> 00:00:07,131
en un anuncio de bienes raíces.

4
00:00:07,461 --> 00:00:11,441
Ignoremos cómo se elige
esta palabra importante.

5
00:00:12,090 --> 00:00:15,191
Las palabras en un anuncio
pertenecen al lenguaje natural

6
00:00:15,481 --> 00:00:18,551
por lo que el diccionario posible
es muy grande.

7
00:00:18,929 --> 00:00:22,972
En este caso, podría ser la lista
de todas las palabras en inglés.

8
00:00:23,599 --> 00:00:25,309
Serían miles de palabras

9
00:00:25,709 --> 00:00:28,976
incluso si ignoráramos palabras raras
y la jerga científica.

10
00:00:29,531 --> 00:00:32,548
Por eso, a pesar de que la primera capa

11
00:00:32,548 --> 00:00:35,090
toma una palabra
del anuncio de bienes raíces

12
00:00:35,300 --> 00:00:37,110
y realiza una codificación one-hot

13
00:00:37,240 --> 00:00:41,623
su representación en la memoria
será un vector disperso.

14
00:00:42,203 --> 00:00:45,238
De esta forma,
TensorFlow puede ser eficiente

15
00:00:45,388 --> 00:00:46,710
en el uso de la memoria.

16
00:00:47,750 --> 00:00:50,894
Una vez que tenemos la representación
de la codificación one-hot

17
00:00:51,092 --> 00:00:53,576
la pasamos por un nodo de tres capas.

18
00:00:54,066 --> 00:00:55,511
Esta es nuestra incorporación.

19
00:00:55,691 --> 00:00:58,394
Ya que usamos tres nodos en esa capa

20
00:00:58,714 --> 00:01:00,561
es una incorporación tridimensional.

21
00:01:01,387 --> 00:01:05,025
Observe que, aunque
sparse_word y embedded_word

22
00:01:05,195 --> 00:01:06,972
son en realidad columnas de atributos

23
00:01:07,252 --> 00:01:09,647
las muestro como capas
de redes neuronales.

24
00:01:09,857 --> 00:01:15,075
Esto se debe a que, matemáticamente,
son como capas de redes neuronales.

25
00:01:15,648 --> 00:01:19,445
En este caso,
una incorporación es similar

26
00:01:19,445 --> 00:01:21,865
a cualquier otra capa oculta de una red.

27
00:01:22,235 --> 00:01:24,971
Es como un adaptador útil

28
00:01:25,321 --> 00:01:30,866
que permite que la red incorpore debidamente
datos dispersos y categóricos

29
00:01:31,429 --> 00:01:33,801
En estas diapositivas,
es importante mostrarle

30
00:01:34,071 --> 00:01:36,894
que puede realizar esto
con un problema de regresión

31
00:01:36,894 --> 00:01:39,594
de clasificación o de calificación.

32
00:01:41,429 --> 00:01:44,102
Cuando usa una red neuronal profunda

33
00:01:44,322 --> 00:01:46,480
los pesos se aprenden
por propagación inversa

34
00:01:46,750 --> 00:01:48,604
como en las otras capas.

35
00:01:49,417 --> 00:01:51,176
Supongamos que usamos la incorporación

36
00:01:51,176 --> 00:01:53,336
para las palabras
de un anuncio de bienes raíces

37
00:01:53,336 --> 00:01:56,862
como una de las entradas de un modelo
que predice el precio de venta.

38
00:01:57,804 --> 00:01:59,585
Entrenaríamos el modelo

39
00:01:59,895 --> 00:02:03,465
según los precios de venta
históricos reales de las casas.

40
00:02:04,119 --> 00:02:06,057
Además de la palabra en el anuncio

41
00:02:06,337 --> 00:02:11,560
podemos usar la cantidad de cuartos
y de habitaciones como entradas.

42
00:02:12,143 --> 00:02:14,880
Este es un problema
de regresión con datos estructurados

43
00:02:15,220 --> 00:02:17,283
como el problema de la tarifa de taxi.

44
00:02:18,954 --> 00:02:23,210
¿Ve lo que ocurre si intenta
optimizar los pesos de todas las capas

45
00:02:23,540 --> 00:02:26,614
para minimizar el error
en la predicción del precio de venta?

46
00:02:27,787 --> 00:02:30,713
Debe ajustar
todos los pesos de todas las capas.

47
00:02:31,549 --> 00:02:34,406
Los pesos se ajustan de manera que

48
00:02:34,786 --> 00:02:38,750
los números de incorporación
de una palabra se vuelvan relevantes

49
00:02:39,120 --> 00:02:42,294
para la predicción
de los precios de venta.

50
00:02:42,979 --> 00:02:47,350
Si el anuncio incluyera
una palabra como “vista” o “lago”

51
00:02:47,735 --> 00:02:49,933
el precio de venta debería ser mayor.

52
00:02:50,343 --> 00:02:53,675
Mientras que si el anuncio incluyera
una frase como “hipotecada”

53
00:02:54,762 --> 00:02:56,344
el precio debería ser menor.

54
00:02:56,982 --> 00:03:01,492
Los pesos de todas las capas
se ajustarán para aprender esto.

55
00:03:02,815 --> 00:03:03,975
Matemáticamente

56
00:03:04,135 --> 00:03:08,694
una incorporación es similar
a cualquier otra capa oculta de una red.

57
00:03:09,200 --> 00:03:11,198
Puede considerarla como un adaptador útil

58
00:03:11,358 --> 00:03:15,584
que permite que la red incorpore debidamente
datos dispersos y categóricos.

59
00:03:16,151 --> 00:03:17,896
Cuando usa una red neuronal profunda

60
00:03:17,896 --> 00:03:19,960
los pesos se aprenden
por propagación inversa

61
00:03:20,290 --> 00:03:22,266
como en las otras capas.

62
00:03:22,442 --> 00:03:27,063
Puede hacer esto con un problema
de regresión o de clasificación.

63
00:03:28,459 --> 00:03:33,314
Recuerde un hecho clave
sobre la primera capa, la azul.

64
00:03:34,210 --> 00:03:35,845
A diferencia de los nodos amarillos

65
00:03:36,035 --> 00:03:38,970
la capa azul usa codificación one-hot.

66
00:03:39,346 --> 00:03:41,240
Por lo que si usa la palabra “vista”

67
00:03:41,780 --> 00:03:45,475
se activará solo uno de esos nodos.

68
00:03:46,202 --> 00:03:48,032
Digamos que es este negro.

69
00:03:48,809 --> 00:03:53,668
Los pesos de los vínculos
del nodo negro hacia la siguiente capa

70
00:03:53,858 --> 00:03:58,001
capturarán la relevancia
de la palabra “vista” en este problema.

71
00:03:58,612 --> 00:04:05,076
Por lo tanto, cada palabra
se representa solo con tres números.

72
00:04:05,975 --> 00:04:08,924
Cada uno de los tres nodos
se pueden considerar

73
00:04:08,924 --> 00:04:12,697
como una dimensión
en la que se proyectan las palabras.

74
00:04:13,533 --> 00:04:16,979
Los pesos de los bordes
entre una película y una capa oculta

75
00:04:17,239 --> 00:04:21,374
son los valores de las coordenadas
en esta proyección dimensional inferior.