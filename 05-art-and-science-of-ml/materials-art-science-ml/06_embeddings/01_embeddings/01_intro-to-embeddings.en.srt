1
00:00:00,000 --> 00:00:03,260
Welcome back. I'm Lak,

2
00:00:03,260 --> 00:00:06,955
and I lead the team that put together this specialization.

3
00:00:06,955 --> 00:00:14,100
In this module, we will go back and revisit an important concept called embeddings.

4
00:00:14,100 --> 00:00:17,740
In this course, we're looking at a variety of things

5
00:00:17,740 --> 00:00:21,355
that every ML practitioner should have in their toolkit.

6
00:00:21,355 --> 00:00:26,125
We're talking about the art and science of machine learning.

7
00:00:26,125 --> 00:00:31,495
Let's now look in detail at the advantages that embeddings provide us.

8
00:00:31,495 --> 00:00:37,940
In this module, you will learn how to use embeddings to manage sparse data;

9
00:00:37,940 --> 00:00:40,665
to make machine learning models that use

10
00:00:40,665 --> 00:00:45,475
sparse data consume less memory and train faster.

11
00:00:45,475 --> 00:00:51,710
Embeddings are also a way to do dimensionality reduction and in that way,

12
00:00:51,710 --> 00:00:56,610
make models simpler and more generalizable.

13
00:00:56,610 --> 00:01:01,510
So we will use embeddings as a way to reduce the dimensionality of

14
00:01:01,510 --> 00:01:06,480
problematic inputs and increase model generalization.

15
00:01:06,480 --> 00:01:13,060
The embeddings also become helpful if you want to do clustering of the observations,

16
00:01:13,060 --> 00:01:17,360
but embeddings are not just about one machine learning model.

17
00:01:17,360 --> 00:01:23,335
Embeddings often work best when you think about families of machine learning models.

18
00:01:23,335 --> 00:01:26,860
The embedding created on one model can be

19
00:01:26,860 --> 00:01:30,820
used to jump-start another model in the same family.

20
00:01:30,820 --> 00:01:35,710
So you will learn how to create reusable embeddings.

21
00:01:35,710 --> 00:01:37,959
Embeddings are so useful,

22
00:01:37,959 --> 00:01:40,960
that creating and embedding can be thought

23
00:01:40,960 --> 00:01:44,425
of as a machine learning problem in its own right.

24
00:01:44,425 --> 00:01:47,065
You might have a team maintaining

25
00:01:47,065 --> 00:01:52,750
particularly important embeddings that are used widely throughout an enterprise.

26
00:01:52,750 --> 00:01:58,920
Think of a good reusable embedding as being similar to a software library.

27
00:01:58,920 --> 00:02:01,655
Because embeddings are so useful,

28
00:02:01,655 --> 00:02:04,269
it can be helpful to visualize them,

29
00:02:04,269 --> 00:02:08,330
and TensorBoard gives us a way to do this.