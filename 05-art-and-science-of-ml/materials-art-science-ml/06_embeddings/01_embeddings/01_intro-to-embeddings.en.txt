Welcome back. I'm Lak, and I lead the team that put together this specialization. In this module, we will go back and revisit an important concept called embeddings. In this course, we're looking at a variety of things that every ML practitioner should have in their toolkit. We're talking about the art and science of machine learning. Let's now look in detail at the advantages that embeddings provide us. In this module, you will learn how to use embeddings to manage sparse data; to make machine learning models that use sparse data consume less memory and train faster. Embeddings are also a way to do dimensionality reduction and in that way, make models simpler and more generalizable. So we will use embeddings as a way to reduce the dimensionality of problematic inputs and increase model generalization. The embeddings also become helpful if you want to do clustering of the observations, but embeddings are not just about one machine learning model. Embeddings often work best when you think about families of machine learning models. The embedding created on one model can be used to jump-start another model in the same family. So you will learn how to create reusable embeddings. Embeddings are so useful, that creating and embedding can be thought of as a machine learning problem in its own right. You might have a team maintaining particularly important embeddings that are used widely throughout an enterprise. Think of a good reusable embedding as being similar to a software library. Because embeddings are so useful, it can be helpful to visualize them, and TensorBoard gives us a way to do this.