1
00:00:00,790 --> 00:00:02,430
Imagine que você está criando

2
00:00:02,430 --> 00:00:07,640
uma incorporação para representar a
palavra-chave de um anúncio imobiliário.

3
00:00:07,640 --> 00:00:12,090
Vamos ignorar como escolher
essa palavra por enquanto.

4
00:00:12,090 --> 00:00:15,500
As palavras de um anúncio
são de linguagem natural,

5
00:00:15,500 --> 00:00:18,880
então o dicionário
possível é enorme.

6
00:00:18,880 --> 00:00:23,560
Nesse caso, seriam todas
as palavras em inglês.

7
00:00:23,560 --> 00:00:25,730
Dezenas de milhares
de palavras.

8
00:00:25,730 --> 00:00:29,510
Mesmo se ignorarmos palavras
raras e jargões científicos.

9
00:00:29,510 --> 00:00:32,940
Obviamente, mesmo
se a primeira camada

10
00:00:32,940 --> 00:00:37,250
escolher uma palavra do
anúncio e a outra codificá-la,

11
00:00:37,250 --> 00:00:42,160
a representação na memória
será como um vetor esparso.

12
00:00:42,160 --> 00:00:46,520
Assim, o TensorFlow pode usar
a memória de modo eficiente.

13
00:00:47,770 --> 00:00:51,140
Depois de codificar
uma representação,

14
00:00:51,140 --> 00:00:54,140
ela é transferida por
uma camada de três nós.

15
00:00:54,140 --> 00:00:55,880
Essa é a incorporação.

16
00:00:55,880 --> 00:01:00,350
Como usamos três nós na camada,
é uma incorporação tridimensional.

17
00:01:01,420 --> 00:01:03,700
Veja que, mesmo
que a palavra esparsa

18
00:01:03,700 --> 00:01:07,250
e a palavra incorporada
sejam colunas de recursos,

19
00:01:07,250 --> 00:01:09,890
estou mostrando-as como
camadas da rede neural.

20
00:01:09,890 --> 00:01:15,660
Isso porque, matematicamente, elas
são como outras camadas mais novas.

21
00:01:15,660 --> 00:01:18,260
Matematicamente,
uma incorporação

22
00:01:18,260 --> 00:01:22,250
não é diferente de outra
camada oculta na rede.

23
00:01:22,250 --> 00:01:26,690
Você pode ver isso como um
adaptador útil que permite que a rede

24
00:01:26,690 --> 00:01:30,350
incorpore dados
esparsos ou categóricos.

25
00:01:31,450 --> 00:01:36,780
É essencial mostrar que você
pode fazer isso com um problema

26
00:01:36,780 --> 00:01:39,290
de regressão ou classificação.

27
00:01:41,670 --> 00:01:46,720
Os pesos em uma rede plural
são aprendidos pela retropropagação,

28
00:01:46,720 --> 00:01:49,530
assim como em outras camadas.

29
00:01:49,530 --> 00:01:53,260
Vamos usar a incorporação nas
palavras do anúncio imobiliário

30
00:01:53,260 --> 00:01:56,730
como uma das entradas do modelo
que prevê o preço de venda.

31
00:01:57,730 --> 00:02:04,160
Podemos treinar o modelo com base
no preço histórico real de casas.

32
00:02:04,160 --> 00:02:08,979
Além da palavra usada no anúncio,
podemos usar o número de cômodos,

33
00:02:08,979 --> 00:02:12,110
número de quartos,
etc., como entradas.

34
00:02:12,110 --> 00:02:15,260
Esse é um problema de
regressão de dados estruturados.

35
00:02:15,260 --> 00:02:16,980
Assim como o problema do táxi.

36
00:02:18,940 --> 00:02:23,520
Viu o que acontece se você tentar
otimizar o peso de todas as camadas

37
00:02:23,520 --> 00:02:26,530
para minimizar os erros no
preço de venda previsto?

38
00:02:27,820 --> 00:02:31,490
Todos os pesos das camadas
precisam ser ajustados.

39
00:02:31,490 --> 00:02:36,530
Os pesos são ajustados de maneira
que os números incorporados

40
00:02:36,530 --> 00:02:38,970
a uma palavra
se tornem relevantes

41
00:02:38,970 --> 00:02:42,970
para a capacidade de
prever o preço de venda.

42
00:02:42,970 --> 00:02:46,680
Talvez se o anúncio incluir
palavras como vista ou

43
00:02:46,680 --> 00:02:50,330
lago, o preço seja maior,

44
00:02:50,330 --> 00:02:56,970
enquanto que uma palavra
como hipoteca abaixe o preço.

45
00:02:56,970 --> 00:03:01,529
O peso das camadas é
ajustado para aprender isso.

46
00:03:02,840 --> 00:03:06,650
Matematicamente, uma
incorporação não é diferente

47
00:03:06,650 --> 00:03:09,220
de outra camada
oculta na rede.

48
00:03:09,220 --> 00:03:11,470
Você pode ver isso
como um adaptador útil

49
00:03:11,470 --> 00:03:13,530
que permite que
uma rede incorpore

50
00:03:13,530 --> 00:03:16,090
dados esparsos ou categóricos.

51
00:03:16,090 --> 00:03:20,320
Os pesos de uma rede neural profunda
são aprendidos com a retropropagação,

52
00:03:20,320 --> 00:03:22,410
assim como em outras camadas.

53
00:03:22,410 --> 00:03:25,420
E você pode usar um
problema de regressão

54
00:03:25,420 --> 00:03:26,980
ou de classificação.

55
00:03:28,450 --> 00:03:33,120
Lembre-se de algo crucial sobre
a primeira camada, a azul.

56
00:03:34,190 --> 00:03:39,290
Ao contrário dos nós amarelos,
a camada azul não é codificada.

57
00:03:39,290 --> 00:03:46,220
Se você usar a palavra vista,
só um desses nós será ativado.

58
00:03:46,220 --> 00:03:48,740
Digamos que seja
este, em preto.

59
00:03:48,740 --> 00:03:53,890
Então, o peso dos links desse
nó preto para a próxima camada

60
00:03:53,890 --> 00:03:57,868
capturarão a relevância
da palavra para o problema.

61
00:03:57,868 --> 00:04:05,960
Assim, cada palavra é
representada por três números.

62
00:04:05,960 --> 00:04:10,490
Cada um dos três nós pode
ser considerado uma dimensão

63
00:04:10,490 --> 00:04:13,520
em que as palavras
são projetadas.

64
00:04:13,520 --> 00:04:16,089
Os pesos de borda
entre um filme

65
00:04:16,089 --> 00:04:19,100
e uma camada oculta
são os valores coordenados

66
00:04:19,100 --> 00:04:21,850
nessa projeção
de dimensão inferior.