Storing the input vector as a one heart encoded array is a bad idea. A dense representation is extremely inefficient, both for storage and for compute. Notice that we're calling anything where we store all the values for an input tensor, a dense tensor. This has nothing to do with the actual data in the tensor, just about how we are storing it. But consider the data in this matrix. Do you think this matrix is filled densely or sparsely? It's extremely sparse of course. Each example around this matrix represents movies that have been watched by the user. Think back to your experience, how many movies have you rated? So, we don't want to store the inputs in a dense form. We do not want to store all the values for the tensor. So, we don't want to store the inputs in a dense form, we don't want to store all the values for the tensor, what should we do instead? It would be good to store the data in a sparse manner, in a compressed way in memory. It would be good to be able to do computations like matrix multiplication directly on the sparse tensors, without having to convert them into dense representations. The way we do this is to build a dictionary mapping from each feature to an integer. So, Shrek might be integers zero and Harry Potter might be the integer 300 or 230, some arbitrary number. Remember that there is no embedding at this point. At this point, each movie just has an arbitrary integer associated with it. Then, when you have a row of the Matrix which represents the movie set a specific user has seen, we simply store the movie IDs for the movies that the user has seen. In the example row, the user has seen three movies, so the sparse tensor has three entries in it. Any integer not present in this list, that movie is assumed to not have been watched. So, the three entries are one, and the rest are zero in the equivalent dense representation. So, there are two steps here. The preprocessing step computes the dictionary, and the second step uses the dictionary to create an efficient sparse representation. If you're thinking that this seems familiar and just like vocabulary building for categorical columns, you're absolutely correct. Categorical columns are represented by tensor flow as sparse tensors. So, categorical columns are an example of something that is sparse. Tensor flow can do math operations on sparse tensors without having to convert them into dense. This saves memory and optimizes compute. We looked at how to create a feature cross from categorical columns. That was an example of Math that was carried out completely in terms of sparse tensors. This is why, even though we crossed discretized columns of latitude and longitude, and then feature across the pickup points and drop off points in our taxi for example, there was no problem with memory or with computation speed. We looked at how to create an embedding column from a feature across. The same code works for a single categorical column of course, and that's what I'm showing here. The ability to deal with sparse tensors is why the code to create an embedding column from categorical data in tensor flow can work without causing memory or speed issues. It's one of those magic implementation details. Recall that we said that no separate training process is needed to do embeddings. We just take two steps. First take the original input and represent the input. Second send it through an embedding layer. The first step is done by taking the input and representing it as a sparse tensor. The second step is done through the call to embedding column, but how does that line of code really work?