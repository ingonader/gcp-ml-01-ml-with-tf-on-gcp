1
00:00:00,000 --> 00:00:01,710
Na lição passada,

2
00:00:01,710 --> 00:00:06,500
falamos sobre como criar
embeddings manualmente com regras.

3
00:00:06,500 --> 00:00:10,170
Usamos atributos como
a idade do espectador

4
00:00:10,170 --> 00:00:12,230
e as vendas de ingressos

5
00:00:12,230 --> 00:00:17,310
para projetar nossos filmes
que teriam 500 mil dimensões

6
00:00:17,310 --> 00:00:20,500
em um espaço bidimensional.

7
00:00:21,070 --> 00:00:23,670
No caso do embedding
bidimensional,

8
00:00:23,670 --> 00:00:26,835
demos aos eixos
nomes como idade

9
00:00:26,835 --> 00:00:33,225
e ingressos vendidos, criança
versus adulto, arte versus bilheteria.

10
00:00:33,225 --> 00:00:37,740
No entanto, não é essencial
que eles tenham nomes.

11
00:00:37,740 --> 00:00:42,765
O mais importante é que
passamos de 500 mil para dois.

12
00:00:43,175 --> 00:00:47,290
Para isso, vimos os atributos
dos filmes manualmente.

13
00:00:48,260 --> 00:00:54,015
Qual o impacto de reduzir as
dimensões de 500 mil para duas?

14
00:00:54,555 --> 00:01:01,100
O embedding em 2D para cada filme
está associado a dois valores reais

15
00:01:01,100 --> 00:01:05,495
e você pode representar cada filme
nesse espaço bidimensional.

16
00:01:06,455 --> 00:01:08,545
Por que fazer
esse embedding?

17
00:01:08,995 --> 00:01:11,000
Um motivo principal é este:

18
00:01:11,530 --> 00:01:14,535
digamos que estamos
treinando um modelo

19
00:01:14,535 --> 00:01:16,905
para prever se um usuário
gostará de um filme.

20
00:01:17,475 --> 00:01:21,154
É mais fácil treinar um
modelo com D entradas

21
00:01:21,154 --> 00:01:24,880
que um com N entradas.

22
00:01:24,880 --> 00:01:29,340
Lembre-se que N
é muito maior que D.

23
00:01:30,020 --> 00:01:32,540
Quanto menor o número
de nós de entrada,

24
00:01:32,540 --> 00:01:35,645
menos pesos
precisamos otimizar.

25
00:01:35,645 --> 00:01:39,240
Isso significa que o modelo
é treinado mais rapidamente

26
00:01:39,240 --> 00:01:42,235
e as chances de
sobreajuste são menores.

27
00:01:42,765 --> 00:01:46,285
O embedding
facilita o problema.

28
00:01:46,835 --> 00:01:48,630
No entanto, precisamos fazer

29
00:01:48,630 --> 00:01:53,040
essa redução de modo
a não perder informações.

30
00:01:53,770 --> 00:01:58,410
Como criar um
embedding adequado?

31
00:01:59,050 --> 00:02:01,745
Você pode aprender
o embedding dos dados

32
00:02:01,745 --> 00:02:04,395
como parte do processo
de treinamento.

33
00:02:04,695 --> 00:02:07,470
Não é preciso ter
um processo separado.

34
00:02:07,470 --> 00:02:10,240
Primeiro, veja
a entrada original

35
00:02:10,240 --> 00:02:14,310
e represente-a como
uma matriz codificada.

36
00:02:14,310 --> 00:02:17,580
Em seguida, envie-a por
uma camada de embedding.

37
00:02:18,270 --> 00:02:22,785
Nessa abordagem, a camada de
embedding é uma camada oculta

38
00:02:22,785 --> 00:02:25,525
com uma unidade por dimensão.

39
00:02:26,015 --> 00:02:28,790
Como estamos treinando
um modelo com rótulos,

40
00:02:28,790 --> 00:02:32,815
o embedding muda
com base nesses rótulos.

41
00:02:33,795 --> 00:02:37,250
Intuitivamente, as unidades
ocultas descobrem

42
00:02:37,250 --> 00:02:41,080
como organizar os
itens nas D dimensões

43
00:02:41,080 --> 00:02:45,390
para otimizar para
um objetivo final.

44
00:02:46,110 --> 00:02:47,865
Há um pequeno problema.

45
00:02:48,455 --> 00:02:52,240
Quanta memória é necessária
para armazenar a entrada?

46
00:02:52,910 --> 00:02:55,380
Você tem uma variável
de entrada categórica,

47
00:02:55,380 --> 00:02:58,145
mas 500 mil valores possíveis.

48
00:02:58,705 --> 00:03:03,320
Por isso, precisa criar
500 mil nós de entrada

49
00:03:03,320 --> 00:03:09,340
e fazer um cálculo
com matrizes enormes.