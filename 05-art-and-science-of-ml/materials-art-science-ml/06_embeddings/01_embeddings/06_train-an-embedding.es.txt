Imagine que crea una incorporación para representar la palabra clave en un anuncio de bienes raíces. Ignoremos cómo se elige
esta palabra importante. Las palabras en un anuncio
pertenecen al lenguaje natural por lo que el diccionario posible
es muy grande. En este caso, podría ser la lista
de todas las palabras en inglés. Serían miles de palabras incluso si ignoráramos palabras raras
y la jerga científica. Por eso, a pesar de que la primera capa toma una palabra
del anuncio de bienes raíces y realiza una codificación one-hot su representación en la memoria
será un vector disperso. De esta forma,
TensorFlow puede ser eficiente en el uso de la memoria. Una vez que tenemos la representación
de la codificación one-hot la pasamos por un nodo de tres capas. Esta es nuestra incorporación. Ya que usamos tres nodos en esa capa es una incorporación tridimensional. Observe que, aunque
sparse_word y embedded_word son en realidad columnas de atributos las muestro como capas
de redes neuronales. Esto se debe a que, matemáticamente,
son como capas de redes neuronales. En este caso,
una incorporación es similar a cualquier otra capa oculta de una red. Es como un adaptador útil que permite que la red incorpore debidamente
datos dispersos y categóricos En estas diapositivas,
es importante mostrarle que puede realizar esto
con un problema de regresión de clasificación o de calificación. Cuando usa una red neuronal profunda los pesos se aprenden
por propagación inversa como en las otras capas. Supongamos que usamos la incorporación para las palabras
de un anuncio de bienes raíces como una de las entradas de un modelo
que predice el precio de venta. Entrenaríamos el modelo según los precios de venta
históricos reales de las casas. Además de la palabra en el anuncio podemos usar la cantidad de cuartos
y de habitaciones como entradas. Este es un problema
de regresión con datos estructurados como el problema de la tarifa de taxi. ¿Ve lo que ocurre si intenta
optimizar los pesos de todas las capas para minimizar el error
en la predicción del precio de venta? Debe ajustar
todos los pesos de todas las capas. Los pesos se ajustan de manera que los números de incorporación
de una palabra se vuelvan relevantes para la predicción
de los precios de venta. Si el anuncio incluyera
una palabra como “vista” o “lago” el precio de venta debería ser mayor. Mientras que si el anuncio incluyera
una frase como “hipotecada” el precio debería ser menor. Los pesos de todas las capas
se ajustarán para aprender esto. Matemáticamente una incorporación es similar
a cualquier otra capa oculta de una red. Puede considerarla como un adaptador útil que permite que la red incorpore debidamente
datos dispersos y categóricos. Cuando usa una red neuronal profunda los pesos se aprenden
por propagación inversa como en las otras capas. Puede hacer esto con un problema
de regresión o de clasificación. Recuerde un hecho clave
sobre la primera capa, la azul. A diferencia de los nodos amarillos la capa azul usa codificación one-hot. Por lo que si usa la palabra “vista” se activará solo uno de esos nodos. Digamos que es este negro. Los pesos de los vínculos
del nodo negro hacia la siguiente capa capturarán la relevancia
de la palabra “vista” en este problema. Por lo tanto, cada palabra
se representa solo con tres números. Cada uno de los tres nodos
se pueden considerar como una dimensión
en la que se proyectan las palabras. Los pesos de los bordes
entre una película y una capa oculta son los valores de las coordenadas
en esta proyección dimensional inferior.