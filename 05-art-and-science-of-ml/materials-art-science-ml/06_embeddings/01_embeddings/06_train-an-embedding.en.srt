1
00:00:00,790 --> 00:00:02,430
Imagine that you're creating and

2
00:00:02,430 --> 00:00:07,640
embedding to represent the key
word in a real estate ad.

3
00:00:07,640 --> 00:00:12,090
Let's ignore for now,
how you choose this important word.

4
00:00:12,090 --> 00:00:18,880
Now words in an ad are natural language,
so the potential dictionary is vast.

5
00:00:18,880 --> 00:00:23,560
In this case,
it could be the list of all English words.

6
00:00:23,560 --> 00:00:25,730
Tens of thousands of words.

7
00:00:25,730 --> 00:00:29,510
Even if we ignore rare words and
scientific jargon.

8
00:00:29,510 --> 00:00:32,940
So obviously,
even though the first layer here,

9
00:00:32,940 --> 00:00:37,250
takes a word in the real estate ad and
one hard encodes it.

10
00:00:37,250 --> 00:00:42,160
The representation of this in memory,
will be as sparse vector.

11
00:00:42,160 --> 00:00:46,520
That way TensorFlow can be
efficient in its use of memory.

12
00:00:47,770 --> 00:00:51,140
Once we have the one hard
encoded representation,

13
00:00:51,140 --> 00:00:54,140
we pass it through a three node layer.

14
00:00:54,140 --> 00:00:55,880
This is how embedding and

15
00:00:55,880 --> 00:01:00,350
because we use three nodes in that layer,
it's at three dimensional embedding.

16
00:01:01,420 --> 00:01:07,250
Notice that even though those password and
embedded word are really feature columns.

17
00:01:07,250 --> 00:01:09,890
I'm showing them as neural network layers.

18
00:01:09,890 --> 00:01:15,660
That is because mathematically they
are just like newer network layers.

19
00:01:15,660 --> 00:01:18,260
Mathematically and embedding in this case,

20
00:01:18,260 --> 00:01:22,250
isn't really different from any
other hidden layer in a network.

21
00:01:22,250 --> 00:01:26,690
You can view it as a handy adapter
that allows the network to

22
00:01:26,690 --> 00:01:30,350
incorporate sparse or
categorical data well.

23
00:01:31,450 --> 00:01:36,780
Key to these slides is to show you that
you can do this with a regression,

24
00:01:36,780 --> 00:01:39,290
classification or a ranking problem.

25
00:01:41,670 --> 00:01:46,720
The weights when using a plural net
are learned by back propagation

26
00:01:46,720 --> 00:01:49,530
just as with the other layers.

27
00:01:49,530 --> 00:01:53,260
Let's say we use the embedding for
the words in the real estate ad.

28
00:01:53,260 --> 00:01:56,730
As one of the inputs to the model
that predicts sales price.

29
00:01:57,730 --> 00:02:04,160
We would train such a model based on
actual historical sale prices for houses.

30
00:02:04,160 --> 00:02:08,980
In addition to the word in the ad,
we might also use number of rooms,

31
00:02:08,980 --> 00:02:12,110
number of bedrooms, etc, as inputs.

32
00:02:12,110 --> 00:02:15,260
So this is a structure
data regression problem.

33
00:02:15,260 --> 00:02:16,980
Just like the taxi fare problem.

34
00:02:18,940 --> 00:02:23,520
Do you see what happens if you try to
optimize the weights in all the layers

35
00:02:23,520 --> 00:02:26,530
to minimize the error in
the predicted sales price?

36
00:02:27,820 --> 00:02:31,490
All the weights in all
the layers have to be tuned.

37
00:02:31,490 --> 00:02:36,530
The weights get tuned in such a way
that the embedding numbers for

38
00:02:36,530 --> 00:02:38,970
a word become relevant.

39
00:02:38,970 --> 00:02:42,970
To its ability to predict sales prices.

40
00:02:42,970 --> 00:02:46,680
Perhaps if the ad includes
a word like view or

41
00:02:46,680 --> 00:02:50,330
lake, then the sales
price has to be higher,

42
00:02:50,330 --> 00:02:56,970
whereas if the ad includes a word like
foreclosure, the weight has to be lower.

43
00:02:56,970 --> 00:03:01,529
The weights in all the layers
will adjust to learn this.

44
00:03:02,840 --> 00:03:06,650
Mathematically, an embedding
isn't really different from

45
00:03:06,650 --> 00:03:09,220
any other hidden layer in a network.

46
00:03:09,220 --> 00:03:14,060
You can view it as a handy adapter that
allows a network to incorporate spores or

47
00:03:14,060 --> 00:03:16,090
categorical data well.

48
00:03:16,090 --> 00:03:20,320
The waits when using a deep neural
net are learned with back propagation

49
00:03:20,320 --> 00:03:22,410
just as with other layers.

50
00:03:22,410 --> 00:03:25,420
And you can do this with
a regression problem.

51
00:03:25,420 --> 00:03:26,980
All with a classification problem.

52
00:03:28,450 --> 00:03:33,120
Now, remember a key fact about
the very first layer, the blue layer.

53
00:03:34,190 --> 00:03:39,290
Unlike the yellow nodes,
the blue layer is one hot encoded.

54
00:03:39,290 --> 00:03:46,220
So if you use the word view, then only
one of these nodes will get turned on.

55
00:03:46,220 --> 00:03:48,740
Let's say it's the one in black here.

56
00:03:48,740 --> 00:03:53,890
Then the weight for the links from
that black note to the next layer

57
00:03:53,890 --> 00:03:57,868
will capture the relevance of
the word view to this problem.

58
00:03:57,868 --> 00:04:05,960
Therefore, each word is being
represented by just three numbers.

59
00:04:05,960 --> 00:04:10,490
Each of the three nodes can
be considered as a dimension

60
00:04:10,490 --> 00:04:13,520
into which words are being projected.

61
00:04:13,520 --> 00:04:16,090
Edge weights between a movie and

62
00:04:16,090 --> 00:04:21,200
a hidden layer are the coordinate values
in this lower dimensional projection.