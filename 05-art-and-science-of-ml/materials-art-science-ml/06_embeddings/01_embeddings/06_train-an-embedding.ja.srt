1
00:00:00,760 --> 00:00:07,530
たとえば不動産広告の中のキーワードを表す
埋め込みを作成するとします

2
00:00:07,530 --> 00:00:12,090
この重要な語をどうやって選ぶかは
とりあえず置いておくとして

3
00:00:12,090 --> 00:00:18,880
広告に含まれる語は自然言語なので
潜在的に可能な語数は膨大になります

4
00:00:18,880 --> 00:00:23,560
この場合はあらゆる単語が
含まれることになります

5
00:00:23,560 --> 00:00:29,470
まれな単語や専門用語は省いたとしても
数万語にのぼります

6
00:00:29,470 --> 00:00:32,940
ですからこの第1の層に
不動産広告の単語を含めて

7
00:00:32,940 --> 00:00:37,250
それをワンホットエンコードしても

8
00:00:37,250 --> 00:00:42,160
メモリ内でのその表現は
疎ベクトルになります

9
00:00:42,160 --> 00:00:47,130
このようにTensorFlowでは
メモリが効率的に利用されます

10
00:00:47,130 --> 00:00:51,140
ワンホットエンコードによる
表現ができたら

11
00:00:51,140 --> 00:00:54,140
それを3ノード層に渡します

12
00:00:54,140 --> 00:00:56,030
これが埋め込みであり

13
00:00:56,030 --> 00:01:01,240
この層では3つのノードを使用しているため
これは3次元埋め込みになります

14
00:01:01,240 --> 00:01:07,250
パスワードや埋め込まれた単語は
実際は特徴列ですが

15
00:01:07,250 --> 00:01:09,890
ニューラルネットワークとして
表示しています

16
00:01:09,890 --> 00:01:14,840
これらは数学的には
ニューラルネットワークに似ているからです

17
00:01:14,840 --> 00:01:18,260
数学的には
この場合の埋め込みは

18
00:01:18,260 --> 00:01:22,250
ネットワーク内の他の隠れ層と
あまり変わりません

19
00:01:22,250 --> 00:01:26,510
これはネットワークに疎データや
カテゴリデータも取り込むための

20
00:01:26,510 --> 00:01:31,150
便利なアダプタのようなものと
考えることができます

21
00:01:31,150 --> 00:01:35,640
これらのスライドの要点は
これを回帰や分類のほか

22
00:01:35,640 --> 00:01:40,010
ランキングの問題にも
応用できるということです

23
00:01:41,670 --> 00:01:45,640
ディープニューラルネットを
使用する際の重みは

24
00:01:45,640 --> 00:01:49,530
他の層と同様
誤差逆伝搬によって学習されます

25
00:01:49,530 --> 00:01:53,260
たとえば不動産広告の単語に
埋め込みを使用し

26
00:01:53,260 --> 00:01:57,730
それを販売価格を予測するモデルへの
入力の1つにしてみます

27
00:01:57,730 --> 00:02:04,160
このモデルを実際の過去の不動産価格に
基づいてトレーニングするとします

28
00:02:04,160 --> 00:02:08,979
広告に含まれる単語だけでなく
部屋の数や寝室の数なども

29
00:02:08,979 --> 00:02:12,110
入力に使用することができます

30
00:02:12,110 --> 00:02:15,300
これは構造データの回帰問題です

31
00:02:15,300 --> 00:02:18,530
タクシー料金の問題と同様です

32
00:02:18,530 --> 00:02:23,530
すべての層の重みを最適化して
予想される販売価格のエラーを

33
00:02:23,530 --> 00:02:27,810
最小限に抑えようとすると
どうなるでしょうか

34
00:02:27,810 --> 00:02:31,800
すべての層のすべての重みを
調整しなければならなくなります

35
00:02:31,800 --> 00:02:36,770
重みの調整は
単語に対する埋め込みの数値が

36
00:02:36,770 --> 00:02:42,970
その単語の販売価格予測能力に
適合するように行われます

37
00:02:42,970 --> 00:02:47,250
たとえば広告に「景観」や
「湖」などの語が含まれる場合

38
00:02:47,250 --> 00:02:50,330
販売価格は高くなるはずですが

39
00:02:50,330 --> 00:02:56,970
一方「抵当流れ」といった語が含まれていれば
価格は低くなるはずです

40
00:02:56,970 --> 00:03:02,019
すべての層の重みは
これを学習するように調整されます

41
00:03:02,840 --> 00:03:06,020
埋め込みは数学的には
ネットワーク内の

42
00:03:06,020 --> 00:03:09,060
他の隠れ層と
あまり変わりません

43
00:03:09,060 --> 00:03:13,360
ネットワークに疎データや
カテゴリデータも取り込むための

44
00:03:13,360 --> 00:03:16,190
便利なアダプタとみなすことができます

45
00:03:16,190 --> 00:03:19,860
ディープニューラルネットを
使用する際の重みは

46
00:03:19,860 --> 00:03:23,190
他の層と同様に
誤差逆伝搬によって学習されます

47
00:03:23,190 --> 00:03:27,860
これは回帰の問題や
分類の問題にも応用できます

48
00:03:27,860 --> 00:03:33,120
ここで最初の青い層に関する
重要な点を思い出してください

49
00:03:34,190 --> 00:03:39,290
黄色の各ノードと違い
青い層はワンホットエンコードされています

50
00:03:39,290 --> 00:03:46,030
たとえば「景観」という語を使うと
このノードのうち1つだけがオンになります

51
00:03:46,030 --> 00:03:49,190
ここではこの黒いノードとしましょう

52
00:03:49,190 --> 00:03:53,760
するとこの黒いノードから
次の層へのリンクの重みによって

53
00:03:53,760 --> 00:03:58,538
「景観」という語の
この問題に対する関連性が取得されます

54
00:03:58,538 --> 00:04:05,750
このためそれぞれの語は
たった3つの数値で表されることになります

55
00:04:05,750 --> 00:04:08,720
3つのノードはそれぞれ

56
00:04:08,720 --> 00:04:13,220
単語が投影される
次元とみなすことができます

57
00:04:13,220 --> 00:04:21,849
映画と隠れ層の間のエッジ重みは
この低次元投影における座標値です