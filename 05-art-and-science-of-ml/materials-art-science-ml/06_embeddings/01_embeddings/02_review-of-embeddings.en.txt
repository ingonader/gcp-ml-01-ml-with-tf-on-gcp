You learned about embeddings briefly in the previous course along with feature crosses, but embeddings are everywhere in modern machine learning and they are not limited to feature crosses or even to structured data. In fact, you will use them quite a bit in image models and in text models. Let's do a quick recap of embeddings the way we understand them. We said that we might be building a machine learning model to predict something about the traffic, perhaps at a time before the next vehicle arrives at an intersection and we have a number of inputs into our model. We look specifically at categorical inputs, hour of day, and day of week. We said that the machine learning model would be greatly improved if instead of treating the hour of day and day after week as independent inputs, we are essentially concatenated them to create a feature across. We said that if we used a large number of hash buckets when doing this feature cross, we could be relatively confident that each of the buckets contained only one-hour-day combination. This was a point at which we introduced embeddings. We said that if instead of one hot encoding the feature cross and using it as is, we could pass it to a dense layer and then train the model to predict traffic as before. This dense layer, shown by the yellow and green nodes, creates an embedding. The embeddings are real valued numbers because they're a weighted sum of the feature crossed values. The thing to realise is that the weights that go into the embedding layer, the yellow and green nodes, the embedding layer, these weights are learned from the data. The point is, that by training these weights on a dataset, so that you are solving a useful problem, something neat happens. The feature cross of day hour has a hardened 68 unique values but we are forcing it to be represented with just two real value numbers. So, the model learns how to embed the feature cross in lower dimensional space. We suggested that perhaps the green box tends to capture pedestrian traffic while the yellow tends to capture automobile traffic, but it doesn't matter what exactly those two dimensions are capturing. The important thing is that all the information in the hour of day and day of week as it pertains to traffic at city intersections, is shoehorned into just two numbers. If you do this on a large enough and good enough dataset, these numbers have one very useful property times that are similar in terms of traffic, get real value numbers that are close together and times that are different in terms of traffic, get real value numbers that are different. We then looked at how to create an embedding TensorFlow. To create an embedding, use the embedding column method in tf.feature column and pass in the categorical column that you want to embed. This works with any categorical column, not just a feature across. You do an embedding of any categorical column. Finally, we glanced quickly at how you could take the embeddings that you learned on a problem and apply it to another similar machine learning problem. Perhaps you learned how to represent hour of day and day of the week with two real value numbers by training on traffic data in London. As a QuickStart, you can use the same weights to jumpstart your frank for tomorrow. You might even be able to use the embedding that you learned on the traffic problem to predict the viewership of a TV show. The idea being that bought street traffic and TV viewership depend on the same latent factor, namely, are the people in the city on the move or are they at home or at work? Transfer learning might work on seemingly very different problems as long as they share the same latent factors.