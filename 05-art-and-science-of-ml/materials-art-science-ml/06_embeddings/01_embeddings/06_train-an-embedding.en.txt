Imagine that you're creating and embedding to represent the key
word in a real estate ad. Let's ignore for now,
how you choose this important word. Now words in an ad are natural language,
so the potential dictionary is vast. In this case,
it could be the list of all English words. Tens of thousands of words. Even if we ignore rare words and
scientific jargon. So obviously,
even though the first layer here, takes a word in the real estate ad and
one hard encodes it. The representation of this in memory,
will be as sparse vector. That way TensorFlow can be
efficient in its use of memory. Once we have the one hard
encoded representation, we pass it through a three node layer. This is how embedding and because we use three nodes in that layer,
it's at three dimensional embedding. Notice that even though those password and
embedded word are really feature columns. I'm showing them as neural network layers. That is because mathematically they
are just like newer network layers. Mathematically and embedding in this case, isn't really different from any
other hidden layer in a network. You can view it as a handy adapter
that allows the network to incorporate sparse or
categorical data well. Key to these slides is to show you that
you can do this with a regression, classification or a ranking problem. The weights when using a plural net
are learned by back propagation just as with the other layers. Let's say we use the embedding for
the words in the real estate ad. As one of the inputs to the model
that predicts sales price. We would train such a model based on
actual historical sale prices for houses. In addition to the word in the ad,
we might also use number of rooms, number of bedrooms, etc, as inputs. So this is a structure
data regression problem. Just like the taxi fare problem. Do you see what happens if you try to
optimize the weights in all the layers to minimize the error in
the predicted sales price? All the weights in all
the layers have to be tuned. The weights get tuned in such a way
that the embedding numbers for a word become relevant. To its ability to predict sales prices. Perhaps if the ad includes
a word like view or lake, then the sales
price has to be higher, whereas if the ad includes a word like
foreclosure, the weight has to be lower. The weights in all the layers
will adjust to learn this. Mathematically, an embedding
isn't really different from any other hidden layer in a network. You can view it as a handy adapter that
allows a network to incorporate spores or categorical data well. The waits when using a deep neural
net are learned with back propagation just as with other layers. And you can do this with
a regression problem. All with a classification problem. Now, remember a key fact about
the very first layer, the blue layer. Unlike the yellow nodes,
the blue layer is one hot encoded. So if you use the word view, then only
one of these nodes will get turned on. Let's say it's the one in black here. Then the weight for the links from
that black note to the next layer will capture the relevance of
the word view to this problem. Therefore, each word is being
represented by just three numbers. Each of the three nodes can
be considered as a dimension into which words are being projected. Edge weights between a movie and a hidden layer are the coordinate values
in this lower dimensional projection.