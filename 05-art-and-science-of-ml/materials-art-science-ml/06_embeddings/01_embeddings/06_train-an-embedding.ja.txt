たとえば不動産広告の中のキーワードを表す
埋め込みを作成するとします この重要な語をどうやって選ぶかは
とりあえず置いておくとして 広告に含まれる語は自然言語なので
潜在的に可能な語数は膨大になります この場合はあらゆる単語が
含まれることになります まれな単語や専門用語は省いたとしても
数万語にのぼります ですからこの第1の層に
不動産広告の単語を含めて それをワンホットエンコードしても メモリ内でのその表現は
疎ベクトルになります このようにTensorFlowでは
メモリが効率的に利用されます ワンホットエンコードによる
表現ができたら それを3ノード層に渡します これが埋め込みであり この層では3つのノードを使用しているため
これは3次元埋め込みになります パスワードや埋め込まれた単語は
実際は特徴列ですが ニューラルネットワークとして
表示しています これらは数学的には
ニューラルネットワークに似ているからです 数学的には
この場合の埋め込みは ネットワーク内の他の隠れ層と
あまり変わりません これはネットワークに疎データや
カテゴリデータも取り込むための 便利なアダプタのようなものと
考えることができます これらのスライドの要点は
これを回帰や分類のほか ランキングの問題にも
応用できるということです ディープニューラルネットを
使用する際の重みは 他の層と同様
誤差逆伝搬によって学習されます たとえば不動産広告の単語に
埋め込みを使用し それを販売価格を予測するモデルへの
入力の1つにしてみます このモデルを実際の過去の不動産価格に
基づいてトレーニングするとします 広告に含まれる単語だけでなく
部屋の数や寝室の数なども 入力に使用することができます これは構造データの回帰問題です タクシー料金の問題と同様です すべての層の重みを最適化して
予想される販売価格のエラーを 最小限に抑えようとすると
どうなるでしょうか すべての層のすべての重みを
調整しなければならなくなります 重みの調整は
単語に対する埋め込みの数値が その単語の販売価格予測能力に
適合するように行われます たとえば広告に「景観」や
「湖」などの語が含まれる場合 販売価格は高くなるはずですが 一方「抵当流れ」といった語が含まれていれば
価格は低くなるはずです すべての層の重みは
これを学習するように調整されます 埋め込みは数学的には
ネットワーク内の 他の隠れ層と
あまり変わりません ネットワークに疎データや
カテゴリデータも取り込むための 便利なアダプタとみなすことができます ディープニューラルネットを
使用する際の重みは 他の層と同様に
誤差逆伝搬によって学習されます これは回帰の問題や
分類の問題にも応用できます ここで最初の青い層に関する
重要な点を思い出してください 黄色の各ノードと違い
青い層はワンホットエンコードされています たとえば「景観」という語を使うと
このノードのうち1つだけがオンになります ここではこの黒いノードとしましょう するとこの黒いノードから
次の層へのリンクの重みによって 「景観」という語の
この問題に対する関連性が取得されます このためそれぞれの語は
たった3つの数値で表されることになります 3つのノードはそれぞれ 単語が投影される
次元とみなすことができます 映画と隠れ層の間のエッジ重みは
この低次元投影における座標値です