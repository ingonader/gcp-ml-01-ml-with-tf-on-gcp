1
00:00:00,000 --> 00:00:02,700
In this course, we learn the diverse set of

2
00:00:02,700 --> 00:00:07,155
topics that typically come up when you start doing machine learning.

3
00:00:07,155 --> 00:00:09,975
In the module on art of machine learning,

4
00:00:09,975 --> 00:00:14,570
you learned about ways beyond just early stopping to limit overfitting.

5
00:00:14,570 --> 00:00:20,265
We learned about L1 and L2 regularisation and how to implement them in tensorflow.

6
00:00:20,265 --> 00:00:24,330
We also learned that part of the art of ML involves

7
00:00:24,330 --> 00:00:28,245
choosing an appropriate learning rate and an appropriate batch size,

8
00:00:28,245 --> 00:00:30,810
and that these vary from problem to problem.

9
00:00:30,810 --> 00:00:33,240
Then you learned how to use

10
00:00:33,240 --> 00:00:38,040
hyper parameter tuning and Cloud ML engine to help choose the best value,

11
00:00:38,040 --> 00:00:41,370
not just of general hyper parameters like learning grid,

12
00:00:41,370 --> 00:00:45,345
but also model specific parameters like number of layers,

13
00:00:45,345 --> 00:00:47,415
number of hash buckets, etc.

14
00:00:47,415 --> 00:00:51,185
Finally, we went back to the problem of overfitting.

15
00:00:51,185 --> 00:00:54,140
Overfitting of course, is a key problem in

16
00:00:54,140 --> 00:00:55,790
machine learning and it's not

17
00:00:55,790 --> 00:00:58,880
surprising that we have to look at it from different angles.

18
00:00:58,880 --> 00:01:03,860
This time, we looked at why different regularization methods work and how to

19
00:01:03,860 --> 00:01:10,280
recognize overfitting when it happens not in the full dataset but just on parts of it.

20
00:01:10,280 --> 00:01:12,980
We then looked at a variety of tricks,

21
00:01:12,980 --> 00:01:15,740
from scaling of inputs to different types of

22
00:01:15,740 --> 00:01:17,705
activation functions to help

23
00:01:17,705 --> 00:01:21,455
neural networks converge faster and to a more accurate point.

24
00:01:21,455 --> 00:01:23,845
We also looked at loss functions,

25
00:01:23,845 --> 00:01:26,140
like noise contrastive loss,

26
00:01:26,140 --> 00:01:31,510
that helped to reduce the computation costs without unduly affecting accuracy.

27
00:01:31,510 --> 00:01:33,640
We then looked at embeddings from

28
00:01:33,640 --> 00:01:37,150
a different perspective of doing dimensionality reduction,

29
00:01:37,150 --> 00:01:40,480
to deal with sparse data in an effective way.

30
00:01:40,480 --> 00:01:42,550
You learned how to take advantage of

31
00:01:42,550 --> 00:01:46,900
the similarity property of embeddings in a variety of situations.

32
00:01:46,900 --> 00:01:50,800
Finally, you learned how to implement a custom estimator

33
00:01:50,800 --> 00:01:55,590
and how to use a Keras model from within the estimator framework.