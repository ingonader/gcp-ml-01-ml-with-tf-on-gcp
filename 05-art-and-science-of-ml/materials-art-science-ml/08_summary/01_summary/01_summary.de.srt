1
00:00:00,000 --> 00:00:03,450
In diesem Kurs haben wir
verschiedene Themen kennengelernt,

2
00:00:03,450 --> 00:00:07,155
die typischerweise
bei maschinellem Lernen auftauchen.

3
00:00:07,155 --> 00:00:10,895
Im Modul "Kunst des maschinellen Lernens"
haben Sie Wege kennengelernt,

4
00:00:10,895 --> 00:00:14,570
die über frühes Beenden zur Einschränkung
der Überanpassung hinausgehen.

5
00:00:14,570 --> 00:00:17,417
Wir haben die L1- und L2-Regularisierung

6
00:00:17,417 --> 00:00:20,265
und ihre Implementierung
in TensorFlow kennengelernt.

7
00:00:20,265 --> 00:00:24,330
Wir haben auch gelernt, dass
ein Teil der Kunst des ML darin besteht,

8
00:00:24,330 --> 00:00:28,245
eine geeignete Lernrate
und Batchgröße auszuwählen,

9
00:00:28,245 --> 00:00:30,970
und dass diese je nach Problem variieren.

10
00:00:30,970 --> 00:00:34,430
Dann haben Sie gelernt, 
mithilfe von Hyperparameter-Abstimmung

11
00:00:34,430 --> 00:00:38,040
und Cloud ML Engine
den besten Wert auszuwählen,

12
00:00:38,040 --> 00:00:41,370
nicht nur für allgemeine
Hyperparameter wie die Lernrate,

13
00:00:41,370 --> 00:00:45,345
sondern auch für modellspezifische
Parameter wie die Anzahl der Ebenen,

14
00:00:45,345 --> 00:00:47,415
die Anzahl der Hash-Buckets usw.

15
00:00:47,415 --> 00:00:51,185
Abschließend ging es wieder
um das Problem der Überanpassung.

16
00:00:51,185 --> 00:00:55,070
Überanpassung ist ein
zentrales Problem beim maschinellen Lernen

17
00:00:55,070 --> 00:00:58,880
und muss daher aus verschiedenen
Blickwinkeln betrachtet werden.

18
00:00:58,880 --> 00:01:03,340
Wir haben untersucht, warum bestimmte
Regularisierungsmethoden funktionieren

19
00:01:03,340 --> 00:01:05,370
und wie man Überanpassungen erkennt,

20
00:01:05,370 --> 00:01:10,330
die nicht im gesamten Dataset,
sondern nur in Teilen davon auftreten.

21
00:01:10,330 --> 00:01:12,980
Wir haben uns dann 
einige Tricks angesehen,

22
00:01:12,980 --> 00:01:15,180
von der Skalierung der Eingabedaten

23
00:01:15,180 --> 00:01:17,195
bis zu den verschiedenen Arten
von Aktivierungsfunktionen,

24
00:01:17,195 --> 00:01:18,545
die dazu beizutragen,

25
00:01:18,545 --> 00:01:21,975
dass neuronale Netze schneller
und genauer zu einem Punkt konvergieren.

26
00:01:21,975 --> 00:01:26,165
Wir haben auch Verlustfunktionen wie
Noise Contrastive Loss betrachtet,

27
00:01:26,165 --> 00:01:28,777
die Rechenkosten reduzieren helfen,

28
00:01:28,777 --> 00:01:31,390
ohne die Genauigkeit
zu sehr zu beeinträchtigen.

29
00:01:31,390 --> 00:01:34,720
Wir haben dann Einbettungen
aus einer anderen Perspektive betrachtet,

30
00:01:34,720 --> 00:01:37,150
nämlich der der räumlichen Reduktion,

31
00:01:37,150 --> 00:01:40,480
um effektiv
mit kleinen Datenmengen zu verfahren.

32
00:01:40,480 --> 00:01:44,650
Sie haben gelernt,
die Property "similarity" von Einbettungen

33
00:01:44,650 --> 00:01:46,900
in vielen Situationen zu nutzen.

34
00:01:46,900 --> 00:01:48,740
Schließlich haben Sie gelernt,

35
00:01:48,740 --> 00:01:51,280
einen benutzerdefinierten
Estimator zu implementieren

36
00:01:51,280 --> 00:01:55,550
und ein Keras-Modell aus
dem Estimator-Framework heraus zu nutzen.