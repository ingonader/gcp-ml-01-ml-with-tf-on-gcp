En este curso,
aprendimos sobre varios temas comunes que surgen cuando comienza
a trabajar con aprendizaje automático. En el módulo
sobre el arte del aprendizaje automático aprendió técnicas distintas
de la interrupción anticipada para evitar el sobreajuste. También hablamos
sobre las regularizaciones L1 y L2 y cómo implementarlas en TensorFlow. Aprendimos también que el arte del AA
implica elegir una tasa de aprendizaje y un tamaño de lote adecuados,
ya que varían según cada problema. Luego, aprendió a usar el ajuste
de hiperparámetros en Cloud ML Engine para seleccionar los mejores valores no solo en hiperparámetros generales
como la tasa de aprendizaje sino en parámetros específicos del modelo,
como la cantidad de capas y de depósitos de hash,
entre otros. Por último,
hablamos del sobreajuste. Este es un problema clave
en el aprendizaje automático y usualmente se debe abordar
desde diferentes perspectivas. Analizamos por qué funcionan
algunos métodos de regularización y cómo reconocer el sobreajuste
cuando no ocurre en todo el conjunto de datos sino solo en algunas partes. También aprendimos varios trucos desde escalar entradas
hasta varios tipos de funciones de activación para acelerar la convergencia
de las redes neuronales en un punto más preciso. También analizamos las funciones de pérdida como la pérdida contrastiva de ruido que permite reducir los costos
de procesamiento sin afectar excesivamente la precisión. Analizamos las incorporaciones
desde una perspectiva distinta de reducir la dimensionalidad a fin de lidiar con los datos dispersos
de manera eficaz. Aprendió a aprovechar
la propiedad de similitud de incorporaciones en situaciones diversas. También aprendió a implementar
un estimador personalizado y cómo usar un modelo Keras
en el marco de trabajo del estimador.