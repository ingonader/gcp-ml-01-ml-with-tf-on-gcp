1
00:00:00,330 --> 00:00:03,440
En este curso,
aprendimos sobre varios temas comunes

2
00:00:03,660 --> 00:00:06,695
que surgen cuando comienza
a trabajar con aprendizaje automático.

3
00:00:07,425 --> 00:00:09,935
En el módulo
sobre el arte del aprendizaje automático

4
00:00:10,025 --> 00:00:13,140
aprendió técnicas distintas
de la interrupción anticipada

5
00:00:13,310 --> 00:00:14,735
para evitar el sobreajuste.

6
00:00:14,885 --> 00:00:17,720
También hablamos
sobre las regularizaciones L1 y L2

7
00:00:17,830 --> 00:00:20,130
y cómo implementarlas en TensorFlow.

8
00:00:20,580 --> 00:00:26,415
Aprendimos también que el arte del AA
implica elegir una tasa de aprendizaje

9
00:00:26,415 --> 00:00:30,920
y un tamaño de lote adecuados,
ya que varían según cada problema.

10
00:00:31,930 --> 00:00:35,690
Luego, aprendió a usar el ajuste
de hiperparámetros en Cloud ML Engine

11
00:00:35,980 --> 00:00:37,850
para seleccionar los mejores valores

12
00:00:38,060 --> 00:00:41,330
no solo en hiperparámetros generales
como la tasa de aprendizaje

13
00:00:41,510 --> 00:00:45,340
sino en parámetros específicos del modelo,
como la cantidad de capas

14
00:00:45,340 --> 00:00:47,105
y de depósitos de hash,
entre otros.

15
00:00:47,825 --> 00:00:51,095
Por último,
hablamos del sobreajuste.

16
00:00:51,355 --> 00:00:54,930
Este es un problema clave
en el aprendizaje automático

17
00:00:55,170 --> 00:00:58,720
y usualmente se debe abordar
desde diferentes perspectivas.

18
00:00:59,220 --> 00:01:03,240
Analizamos por qué funcionan
algunos métodos de regularización

19
00:01:03,400 --> 00:01:08,270
y cómo reconocer el sobreajuste
cuando no ocurre en todo el conjunto de datos

20
00:01:08,450 --> 00:01:10,200
sino solo en algunas partes.

21
00:01:10,670 --> 00:01:12,760
También aprendimos varios trucos

22
00:01:12,940 --> 00:01:17,110
desde escalar entradas
hasta varios tipos de funciones de activación

23
00:01:17,390 --> 00:01:19,765
para acelerar la convergencia
de las redes neuronales

24
00:01:19,785 --> 00:01:21,415
en un punto más preciso.

25
00:01:21,795 --> 00:01:23,935
También analizamos las funciones de pérdida

26
00:01:24,025 --> 00:01:25,955
como la pérdida contrastiva de ruido

27
00:01:26,185 --> 00:01:28,535
que permite reducir los costos
de procesamiento

28
00:01:28,765 --> 00:01:31,100
sin afectar excesivamente la precisión.

29
00:01:32,090 --> 00:01:34,970
Analizamos las incorporaciones
desde una perspectiva distinta

30
00:01:34,970 --> 00:01:37,000
de reducir la dimensionalidad

31
00:01:37,160 --> 00:01:40,290
a fin de lidiar con los datos dispersos
de manera eficaz.

32
00:01:40,870 --> 00:01:44,705
Aprendió a aprovechar
la propiedad de similitud de incorporaciones

33
00:01:44,925 --> 00:01:46,550
en situaciones diversas.

34
00:01:46,900 --> 00:01:50,700
También aprendió a implementar
un estimador personalizado

35
00:01:51,000 --> 00:01:54,870
y cómo usar un modelo Keras
en el marco de trabajo del estimador.