This brings us to the end of the fifth and final course of the machine learning on
Google Cloud platform specialization. In the first course, we learn how to
formulate machine learning problems. In the second course,
we learn how machine learning works and how to create good machine
learning data sets. In the third course, we'll learn how to write TensorFlow models
to solve machine learning problems. At that point we had learned the basics. In the fourth course, we learn how
to carry out feature engineering as a way to improve
the performance of our models. In this course, the fifth course,
we continued learning techniques to improve the accuracy of
machine learning models. We planned a ten course journey
on machine learning and you're now at the halfway point. How much do you know? And how much more do you need to know? There are a couple of papers that
throw some light on this question. One is a very famous paper from Google called the unreasonable
effectiveness of data. The other is a more recent paper,
published in 2018, that looked at how predictable the scaling of deep
learning models are with more data. Shown here is an actual
graph from the second paper on machine learning translation quality
with more data and a bigger model. The graph here helps you
interpret what's going on. So even though it's a cartoon, it's based on real empirical work
across a large variety of models. The first thing to note is that as you
increase the size of your data set the accuracy of your model
increases rather dramatically, more data really, really helps. On the other hand, using a better
model with small amounts of data doesn't have that much of an impact,
better models don't really help. Just stick with simple models, the canned
estimators will take you a long way. At some point though, the error improvements plateau
when you add more data. At that point,
you should try hyperparameter tuning. That's what I mean by mole compute and you will typically see a relatively
large improvement in accuracy. So the invest in data collection,
data augmentation and enrichment. Then top it off with
hyperparameter tuning. In the next specialization, and
we hope you will join us for that, you will learn how to
do machine learning at scale. How to build specialized
machine learning models for images, sequences and recommendations. Thank you, and see you around.