In diesem Kurs haben wir
verschiedene Themen kennengelernt, die typischerweise
bei maschinellem Lernen auftauchen. Im Modul "Kunst des maschinellen Lernens"
haben Sie Wege kennengelernt, die über frühes Beenden zur Einschränkung
der Überanpassung hinausgehen. Wir haben die L1- und L2-Regularisierung und ihre Implementierung
in TensorFlow kennengelernt. Wir haben auch gelernt, dass
ein Teil der Kunst des ML darin besteht, eine geeignete Lernrate
und Batchgröße auszuwählen, und dass diese je nach Problem variieren. Dann haben Sie gelernt, 
mithilfe von Hyperparameter-Abstimmung und Cloud ML Engine
den besten Wert auszuwählen, nicht nur für allgemeine
Hyperparameter wie die Lernrate, sondern auch für modellspezifische
Parameter wie die Anzahl der Ebenen, die Anzahl der Hash-Buckets usw. Abschließend ging es wieder
um das Problem der Überanpassung. Überanpassung ist ein
zentrales Problem beim maschinellen Lernen und muss daher aus verschiedenen
Blickwinkeln betrachtet werden. Wir haben untersucht, warum bestimmte
Regularisierungsmethoden funktionieren und wie man Überanpassungen erkennt, die nicht im gesamten Dataset,
sondern nur in Teilen davon auftreten. Wir haben uns dann 
einige Tricks angesehen, von der Skalierung der Eingabedaten bis zu den verschiedenen Arten
von Aktivierungsfunktionen, die dazu beizutragen, dass neuronale Netze schneller
und genauer zu einem Punkt konvergieren. Wir haben auch Verlustfunktionen wie
Noise Contrastive Loss betrachtet, die Rechenkosten reduzieren helfen, ohne die Genauigkeit
zu sehr zu beeinträchtigen. Wir haben dann Einbettungen
aus einer anderen Perspektive betrachtet, nämlich der der räumlichen Reduktion, um effektiv
mit kleinen Datenmengen zu verfahren. Sie haben gelernt,
die Property "similarity" von Einbettungen in vielen Situationen zu nutzen. Schließlich haben Sie gelernt, einen benutzerdefinierten
Estimator zu implementieren und ein Keras-Modell aus
dem Estimator-Framework heraus zu nutzen.