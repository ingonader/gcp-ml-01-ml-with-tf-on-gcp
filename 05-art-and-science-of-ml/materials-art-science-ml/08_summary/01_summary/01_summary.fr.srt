1
00:00:00,200 --> 00:00:04,180
Dans ce cours, nous allons
aborder plusieurs sujets typiques

2
00:00:04,180 --> 00:00:06,805
sur l'utilisation du machine learning.

3
00:00:07,345 --> 00:00:11,665
Le module sur l'art du machine learning
vous a montré que les arrêts prématurés

4
00:00:11,665 --> 00:00:14,460
n'étaient pas le seul moyen
de limiter le surapprentissage.

5
00:00:14,570 --> 00:00:19,995
On a présenté les régularisations L1 et L2
et comment les utiliser dans TensorFlow.

6
00:00:20,835 --> 00:00:24,880
Vous avez aussi appris que
l'art du ML implique le choix

7
00:00:24,880 --> 00:00:28,245
du bon taux d'apprentissage
et de la bonne taille de lots,

8
00:00:28,245 --> 00:00:31,020
mais aussi que ces derniers
varient selon le problème.

9
00:00:31,780 --> 00:00:35,840
Vous avez ensuite appris à régler
les hyperparamètres dans Cloud ML Engine

10
00:00:35,840 --> 00:00:38,040
pour choisir la meilleure valeur,

11
00:00:38,040 --> 00:00:41,420
et non pas un hyperparamètre général
comme la recherche sur grille,

12
00:00:41,420 --> 00:00:44,155
mais aussi des paramètres
propres au modèle,

13
00:00:44,155 --> 00:00:46,855
comme le nombre de couches,
de buckets de hachage, etc.

14
00:00:47,845 --> 00:00:51,065
Enfin, nous sommes revenus
sur le problème de surapprentissage.

15
00:00:51,185 --> 00:00:55,040
Le surapprentissage est
un problème courant du machine learning.

16
00:00:55,330 --> 00:00:58,680
Nous devions donc l'aborder
sous différents angles.

17
00:00:59,050 --> 00:01:03,340
Nous avons vu pourquoi les méthodes
de régularisation étaient efficaces,

18
00:01:03,340 --> 00:01:09,960
et comment détecter le surapprentissage sur
une partie des données, non l'intégralité.

19
00:01:10,700 --> 00:01:12,980
Nous avons ensuite présenté
plusieurs astuces,

20
00:01:12,980 --> 00:01:17,110
allant du scaling des entrées
à divers types de fonctions d'activation,

21
00:01:17,110 --> 00:01:21,255
pour converger rapidement les réseaux
de neurones vers un point plus précis.

22
00:01:21,645 --> 00:01:23,845
Nous avons aussi étudié
les fonctions de perte,

23
00:01:23,845 --> 00:01:26,040
telles que la perte contrastive du bruit,

24
00:01:26,040 --> 00:01:31,025
qui permettent de réduire les coûts
de calcul sans trop affecter la précision.

25
00:01:31,990 --> 00:01:35,010
Nous avons étudié
les représentations vectorielles continues

26
00:01:35,010 --> 00:01:37,150
dans le cadre de réduction de dimensions,

27
00:01:37,150 --> 00:01:40,280
pour gérer efficacement
les données éparses.

28
00:01:40,670 --> 00:01:43,850
Vous avez appris à exploiter
la propriété de similarité

29
00:01:43,850 --> 00:01:46,810
des représentations vectorielles
continues dans différents cas.

30
00:01:46,970 --> 00:01:50,800
Enfin, vous avez vu comment mettre en
œuvre une instance Estimator personnalisée

31
00:01:50,800 --> 00:01:55,060
et comment utiliser un modèle Keras
depuis le framework de l'instance Estimator.