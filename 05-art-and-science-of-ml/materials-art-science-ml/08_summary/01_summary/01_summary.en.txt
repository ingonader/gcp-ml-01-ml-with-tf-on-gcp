In this course, we learn the diverse set of topics that typically come up when you start doing machine learning. In the module on art of machine learning, you learned about ways beyond just early stopping to limit overfitting. We learned about L1 and L2 regularisation and how to implement them in tensorflow. We also learned that part of the art of ML involves choosing an appropriate learning rate and an appropriate batch size, and that these vary from problem to problem. Then you learned how to use hyper parameter tuning and Cloud ML engine to help choose the best value, not just of general hyper parameters like learning grid, but also model specific parameters like number of layers, number of hash buckets, etc. Finally, we went back to the problem of overfitting. Overfitting of course, is a key problem in machine learning and it's not surprising that we have to look at it from different angles. This time, we looked at why different regularization methods work and how to recognize overfitting when it happens not in the full dataset but just on parts of it. We then looked at a variety of tricks, from scaling of inputs to different types of activation functions to help neural networks converge faster and to a more accurate point. We also looked at loss functions, like noise contrastive loss, that helped to reduce the computation costs without unduly affecting accuracy. We then looked at embeddings from a different perspective of doing dimensionality reduction, to deal with sparse data in an effective way. You learned how to take advantage of the similarity property of embeddings in a variety of situations. Finally, you learned how to implement a custom estimator and how to use a Keras model from within the estimator framework.