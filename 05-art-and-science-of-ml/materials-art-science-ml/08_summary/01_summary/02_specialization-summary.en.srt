1
00:00:01,240 --> 00:00:03,380
This brings us to the end of the fifth and

2
00:00:03,380 --> 00:00:08,250
final course of the machine learning on
Google Cloud platform specialization.

3
00:00:08,250 --> 00:00:12,910
In the first course, we learn how to
formulate machine learning problems.

4
00:00:12,910 --> 00:00:16,540
In the second course,
we learn how machine learning works and

5
00:00:16,540 --> 00:00:19,600
how to create good machine
learning data sets.

6
00:00:19,600 --> 00:00:21,000
In the third course,

7
00:00:21,000 --> 00:00:25,940
we'll learn how to write TensorFlow models
to solve machine learning problems.

8
00:00:25,940 --> 00:00:28,650
At that point we had learned the basics.

9
00:00:28,650 --> 00:00:32,770
In the fourth course, we learn how
to carry out feature engineering

10
00:00:32,770 --> 00:00:36,290
as a way to improve
the performance of our models.

11
00:00:36,290 --> 00:00:40,490
In this course, the fifth course,
we continued learning techniques

12
00:00:40,490 --> 00:00:44,010
to improve the accuracy of
machine learning models.

13
00:00:44,010 --> 00:00:47,400
We planned a ten course journey
on machine learning and

14
00:00:47,400 --> 00:00:50,150
you're now at the halfway point.

15
00:00:50,150 --> 00:00:51,610
How much do you know?

16
00:00:51,610 --> 00:00:52,960
And how much more do you need to know?

17
00:00:54,140 --> 00:00:58,550
There are a couple of papers that
throw some light on this question.

18
00:00:58,550 --> 00:01:01,560
One is a very famous paper from Google

19
00:01:01,560 --> 00:01:04,820
called the unreasonable
effectiveness of data.

20
00:01:04,820 --> 00:01:09,750
The other is a more recent paper,
published in 2018, that looked at

21
00:01:09,750 --> 00:01:15,340
how predictable the scaling of deep
learning models are with more data.

22
00:01:15,340 --> 00:01:19,200
Shown here is an actual
graph from the second paper

23
00:01:19,200 --> 00:01:24,670
on machine learning translation quality
with more data and a bigger model.

24
00:01:24,670 --> 00:01:28,120
The graph here helps you
interpret what's going on.

25
00:01:28,120 --> 00:01:30,080
So even though it's a cartoon,

26
00:01:30,080 --> 00:01:35,140
it's based on real empirical work
across a large variety of models.

27
00:01:35,140 --> 00:01:40,000
The first thing to note is that as you
increase the size of your data set

28
00:01:40,000 --> 00:01:44,520
the accuracy of your model
increases rather dramatically,

29
00:01:44,520 --> 00:01:48,520
more data really, really helps.

30
00:01:48,520 --> 00:01:53,140
On the other hand, using a better
model with small amounts of data

31
00:01:53,140 --> 00:01:57,290
doesn't have that much of an impact,
better models don't really help.

32
00:01:57,290 --> 00:02:04,110
Just stick with simple models, the canned
estimators will take you a long way.

33
00:02:04,110 --> 00:02:05,800
At some point though,

34
00:02:05,800 --> 00:02:09,340
the error improvements plateau
when you add more data.

35
00:02:09,340 --> 00:02:12,860
At that point,
you should try hyperparameter tuning.

36
00:02:12,860 --> 00:02:15,460
That's what I mean by mole compute and

37
00:02:15,460 --> 00:02:20,160
you will typically see a relatively
large improvement in accuracy.

38
00:02:20,160 --> 00:02:26,790
So the invest in data collection,
data augmentation and enrichment.

39
00:02:26,790 --> 00:02:29,230
Then top it off with
hyperparameter tuning.

40
00:02:31,050 --> 00:02:34,390
In the next specialization, and
we hope you will join us for

41
00:02:34,390 --> 00:02:38,380
that, you will learn how to
do machine learning at scale.

42
00:02:38,380 --> 00:02:42,130
How to build specialized
machine learning models for

43
00:02:42,130 --> 00:02:45,380
images, sequences and recommendations.

44
00:02:45,380 --> 00:02:46,810
Thank you, and see you around.