Dans ce cours, nous allons
aborder plusieurs sujets typiques sur l'utilisation du machine learning. Le module sur l'art du machine learning
vous a montré que les arrêts prématurés n'étaient pas le seul moyen
de limiter le surapprentissage. On a présenté les régularisations L1 et L2
et comment les utiliser dans TensorFlow. Vous avez aussi appris que
l'art du ML implique le choix du bon taux d'apprentissage
et de la bonne taille de lots, mais aussi que ces derniers
varient selon le problème. Vous avez ensuite appris à régler
les hyperparamètres dans Cloud ML Engine pour choisir la meilleure valeur, et non pas un hyperparamètre général
comme la recherche sur grille, mais aussi des paramètres
propres au modèle, comme le nombre de couches,
de buckets de hachage, etc. Enfin, nous sommes revenus
sur le problème de surapprentissage. Le surapprentissage est
un problème courant du machine learning. Nous devions donc l'aborder
sous différents angles. Nous avons vu pourquoi les méthodes
de régularisation étaient efficaces, et comment détecter le surapprentissage sur
une partie des données, non l'intégralité. Nous avons ensuite présenté
plusieurs astuces, allant du scaling des entrées
à divers types de fonctions d'activation, pour converger rapidement les réseaux
de neurones vers un point plus précis. Nous avons aussi étudié
les fonctions de perte, telles que la perte contrastive du bruit, qui permettent de réduire les coûts
de calcul sans trop affecter la précision. Nous avons étudié
les représentations vectorielles continues dans le cadre de réduction de dimensions, pour gérer efficacement
les données éparses. Vous avez appris à exploiter
la propriété de similarité des représentations vectorielles
continues dans différents cas. Enfin, vous avez vu comment mettre en
œuvre une instance Estimator personnalisée et comment utiliser un modèle Keras
depuis le framework de l'instance Estimator.