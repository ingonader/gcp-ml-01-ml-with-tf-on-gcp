1
00:00:00,000 --> 00:00:03,470
Neste curso, veremos
os diversos tópicos

2
00:00:03,470 --> 00:00:07,155
que surgem em um projeto
de aprendizado de máquina.

3
00:00:07,155 --> 00:00:09,975
No módulo sobre a arte
do aprendizado de máquina,

4
00:00:09,975 --> 00:00:14,750
você viu coisas além de parada
antecipada até o sobreajuste de limites.

5
00:00:14,750 --> 00:00:20,265
Aprendemos sobre a regularização de
L1 e L2 e como implementar no TensorFlow.

6
00:00:20,265 --> 00:00:24,330
Também vimos que
parte dessa arte envolve

7
00:00:24,330 --> 00:00:28,245
escolher uma taxa de aprendizado
e um tamanho de lote certos,

8
00:00:28,245 --> 00:00:30,980
que variam de acordo
com o problema.

9
00:00:31,720 --> 00:00:33,240
Você viu como usar

10
00:00:33,240 --> 00:00:35,900
o ajuste de hiperparâmetros
no Cloud ML Engine

11
00:00:35,900 --> 00:00:38,040
para escolher o melhor valor,

12
00:00:38,040 --> 00:00:41,370
não apenas hiperparâmetros gerais
como a grade de aprendizado,

13
00:00:41,370 --> 00:00:45,345
mas também parâmetros específicos
do modelo, como o número de camadas,

14
00:00:45,345 --> 00:00:47,725
o números de
intervalos de hash, etc.

15
00:00:47,725 --> 00:00:51,185
Finalmente, voltamos
para o sobreajuste.

16
00:00:51,185 --> 00:00:54,140
Esse é um problema importante

17
00:00:54,140 --> 00:00:56,400
do aprendizado de máquina
e, sem surpresas,

18
00:00:56,400 --> 00:00:59,100
temos que vê-lo
de diferentes ângulos.

19
00:00:59,100 --> 00:01:03,350
Dessa vez, vimos como diferentes
métodos de regularização funcionam

20
00:01:03,350 --> 00:01:10,180
e como reconhecer o sobreajuste quando
ele ocorre em partes do conjunto de dados.

21
00:01:10,720 --> 00:01:12,980
Depois, vimos
diversos truques,

22
00:01:12,980 --> 00:01:14,690
desde o escalonamento
de entradas

23
00:01:14,690 --> 00:01:17,185
até diferentes
funções de ativação

24
00:01:17,185 --> 00:01:19,875
para ajudar as redes neurais a
convergir mais rapidamente

25
00:01:19,875 --> 00:01:21,455
e em um ponto mais preciso.

26
00:01:21,455 --> 00:01:23,845
Também vimos
as funções de perda,

27
00:01:23,845 --> 00:01:26,140
como perda
contrastante com ruído,

28
00:01:26,140 --> 00:01:31,320
que ajudou a reduzir os custos de
computação sem afetar a precisão.

29
00:01:32,030 --> 00:01:33,640
Vimos também as incorporações

30
00:01:33,640 --> 00:01:37,150
de uma perspectiva diferente
da redução de dimensionalidade,

31
00:01:37,150 --> 00:01:40,480
para lidar com dados esparsos
de maneira eficaz.

32
00:01:40,480 --> 00:01:42,550
Você viu como aproveitar

33
00:01:42,550 --> 00:01:46,900
a propriedade de similaridade da
incorporação em várias situações.

34
00:01:46,900 --> 00:01:50,800
Finalmente, você viu como
implementar um Estimator personalizado

35
00:01:50,800 --> 00:01:55,130
e como usar um modelo Keras
a partir da estrutura do Estimator.