Neste curso, veremos
os diversos tópicos que surgem em um projeto
de aprendizado de máquina. No módulo sobre a arte
do aprendizado de máquina, você viu coisas além de parada
antecipada até o sobreajuste de limites. Aprendemos sobre a regularização de
L1 e L2 e como implementar no TensorFlow. Também vimos que
parte dessa arte envolve escolher uma taxa de aprendizado
e um tamanho de lote certos, que variam de acordo
com o problema. Você viu como usar o ajuste de hiperparâmetros
no Cloud ML Engine para escolher o melhor valor, não apenas hiperparâmetros gerais
como a grade de aprendizado, mas também parâmetros específicos
do modelo, como o número de camadas, o números de
intervalos de hash, etc. Finalmente, voltamos
para o sobreajuste. Esse é um problema importante do aprendizado de máquina
e, sem surpresas, temos que vê-lo
de diferentes ângulos. Dessa vez, vimos como diferentes
métodos de regularização funcionam e como reconhecer o sobreajuste quando
ele ocorre em partes do conjunto de dados. Depois, vimos
diversos truques, desde o escalonamento
de entradas até diferentes
funções de ativação para ajudar as redes neurais a
convergir mais rapidamente e em um ponto mais preciso. Também vimos
as funções de perda, como perda
contrastante com ruído, que ajudou a reduzir os custos de
computação sem afetar a precisão. Vimos também as incorporações de uma perspectiva diferente
da redução de dimensionalidade, para lidar com dados esparsos
de maneira eficaz. Você viu como aproveitar a propriedade de similaridade da
incorporação em várias situações. Finalmente, você viu como
implementar um Estimator personalizado e como usar um modelo Keras
a partir da estrutura do Estimator.