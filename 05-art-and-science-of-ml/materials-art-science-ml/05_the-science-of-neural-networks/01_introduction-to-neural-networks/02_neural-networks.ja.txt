すでにニューラルネットワークは
説明しましたが ここではその技術を説明します このような問題で特徴クロスが役立つことが
わかってきました x1が水平寸法で x2が垂直寸法の場合 この分布を示す2つの特徴の
線形組み合わせはありませんでした 特徴エンジニアリングを行って
x1とx2を交差させるまで 新しい特徴x3を得られませんでした x3はx1掛けるx2と同じで
このデータ分布を示せます 手動による特徴エンジニアリングで 非線形問題はすべて簡単に解決できます ですよね？残念ながら現実世界では こんなに簡単に分布を描けません 特徴エンジニアリングは
天才たちが何年研究しても これが限界です たとえばこの分布のモデル化には
どんな特徴クロスが必要ですか 2つの円が重なっているようにも
2つのらせんにも見えますが とにかく非常に乱雑です この例はニューラルネットワーク（NN）の
有用性を示します NNはアルゴリズムで複雑な特徴クロスと
変換を作成できます このらせんよりはるかに複雑な空間では NNの使用が必須です NNは特徴を組み合わせて
特徴クロスの代用として機能します NNのアーキテクチャの設計で目指すのは 特徴の組み合わせが存在するような
モデルの構築です 次に他の層を追加して 
既存の組み合わせに重ね さらに層を追加して既存の組み合わせに
重ねていきます どうすれば特徴などの 正しい組み合わせを選択できますか？ もちろんトレーニングでモデルに
学習させるのです これがニューラルネットの基本知識です 特徴クロスより優れているわけではありませんが 多くのケースで役立つ柔軟な代替方法です この図は線形モデルを表しています 3つの入力 x1 x2 x3が青の円で示されています 各エッジに重み与えて組み合わせを行い
出力を生成します 別の偏り項もありますが 簡単にするため表示していません これは線形モデルです
y = w1 x x1 + w2 x x2 + w3 x x3です ここでノードとエッジのネットワークに
隠れ層を追加します 入力層にも隠れ層にもノードが3つあります ここでは隠れノードを見ます これは完全につながった層なので エッジが3 x 3で重みが9になります 確かにこれは非線形モデルなので 非線形問題の解決に使えますよね？ 残念ながら使えません
詳細に見ていきましょう 最初の隠れノードへの入力はw1 x x1 + w4 x x2 + w7 x x3の加重和です 2番目の隠れノードへの入力はw2 x x1 + w5 x x2 + w8 x x3の
加重和です 3番目の隠れノードへの入力はw3 x x1 + w6 x x2 + w9 x x3の
加重和です すべてを出力ノードで組み合わせると w10 x h1 + w11 x h2 + w12 x h3になります 説明したようにh1 h2とh3は入力特徴の線形の組み合わせに
すぎません そのためこれを展開すると 一連の複雑な重み定数に各入力値 x1 x2 x3を掛けたものが残ります 各組の重みを新しい重みに代入できます 見覚えがあるでしょう？ 隠れ層を追加する前の線形モデルと まったく同じです
なぜでしょうか？ 隠れ層をもっと追加したら？ 残念ながらこれも集約され 3つの各入力を掛けた1つの重み行列になります 同じ線形モデルです このプロセスを無限に続けても
同じ結果になります 莫大な計算コストをかけて
複雑なアーキテクチャのトレーニングや 予測を行っても同じです これを線形代数的観点から見ると 複数の行列が1つの連鎖で掛けられています この小さな例では まず3x3の行列を掛け 入力層と隠れ層1間の重み行列の転置を 3x1の入力ベクトルで行うことにより 3x1ベクトルが隠れ層1の各隠れニューロンの
値になります 2番目の隠れ層のニューロンの値を定義して 3x3の重み行列の 転置行列を掛けて隠れ層1を 隠れ層2につなげると
それが隠れ層1の結果のベクトルになります おわかりのように 2つの3x3の重み行列が 1つの3x3行列になります これはまず行列積を左の内側または右側から
計算しています これでも2番目の隠れ層のニューロンの 値のベクトルh2の形は同じです 隠れ層2と出力層の間に最後の層を追加するには 前の手順に先ほどの2つの層の間の 重み行列の転置行列を掛ける必要があります NNによるフィードフォーワードでも 行列の積を右から左に行うには
左から右に適用します この行列要素の大きな連鎖が 値が3つだけのベクトルに集約されました 3つの重みが並んだ単回帰ケースで このモデルをトレーニングし
どちらも最終面で同じ極小値に集約されるなら 膨大な演算で21の重みすべてを計算しても この行列積の連鎖は低次方程式に集約され 重みはトレーニング中の単回帰の重みと
まったく同じになります すべての作業が同じ結果になります 皆様はこう思うでしょう 「NNとはニューロンに層を追加することだ 全層が1つに集約されるなら
ディープラーニングなんてできない」 朗報があります 簡単な解決策があります それは非線形変換層を追加することで シグモイド、Tanh、ReLUなどの
非線形活性化関数で容易になります TensorFlowなどのグラフの場合は 各ニューロンに2つのノードがあると
考えてください 最初のノードは加重和wx + bの結果で 2つ目のノードは 活性化関数を通過した結果です つまり 活性化関数の入力の後に 活性化関数の出力があります 活性化関数が間の転移点として機能します この非線形転換の追加が NNが集約して浅いネットワークになるのを
防ぐ唯一の方法です ネットワークに非線形活性化関数の層が
1つあっても ネットワークのどこかに線形活性化関数が
2つ以上あると 1つのネットワークに集約されます 通常NNでは 最初とマイナス1の層は非線形の層で 最終層の変換は回帰またはシグモイド ソフトマックスの線形になります すべては必要とする出力によって変わります これをもう一度線形代数の観点から考えると 線形変換を行列またはベクトルに適用するときに 行列またはベクトルを掛け合わせて
目的の形状や結果になるようにします 行列のサイズ変更と同様に 定数を掛けることができます しかし実際は単位行列を掛けています その定数を掛けます そのためこれは全対角が定数の対角行列です これは単なる行列積に集約されます しかし非線形性を追加すると 行列で表せなくなります 要素yで関数を入力に適用しているためです たとえば 最初と2番目の隠れ層の間に
非線形活性関数がある場合 最初の隠れ層の重み行列と 入力ベクトルの置換の積の関数を適用します 低次方程式はReLUの活性化関数です 線形代数では変換を表せないので 変換連鎖のその部分はこれ以上集約できず モデルの複雑性は変わらず 入力の線形結合に集約されません 重み行列の2番目の隠れ層と出力層の重み行列は まだ非線形関数を適用していないので
集約されます つまり線形層が2つ以上連続している場合 それが何層でも1つの層に集約されます そのため最も複雑な関数が
ネットワークで作成されています ネットワーク全体に線形活性化関数を入れるのが
最適ですが 最後に異なるタイプの出力を使用する場合は
最後の層を除外します NNに非線形活性化関数を追加することが
重要な理由は？ 正解は複数の層が線形モデルに 集約されないようにするためです 非線形活性化関数は データスクリプト空間で興味深い変換を作成し 深層合成関数も可能にします 説明したように
線形活性化関数に2つ以上の層がある場合 この行列の積は 1つの行列 x 入力特徴ベクトルに
集約されます そのため演算は多いのに 機能的複雑性がすべて排除された
低速モデルになります 非線形性は損失関数に正則性を追加せず 早期中止を呼び出しません 非線形活性化関数がデータ空間に 複素変換を作成したとしても 次元は変化せず 同一ベクトル空間に留まります ただし圧迫や拡大、回転はあります 前のコースでも触れたように シグモイドには多数の非線形活性化関数があり スケール／シフトシグモイドと 双曲正接は最も古いものです ただしすでに述べたように これは飽和状態から勾配消失問題につながります ゼロ勾配では モデルの重みは更新されず 
トレーニングは停止します Rectified Linear Unit（ReLU）は 簡単でうまく機能するためよく使用されます 正領域では線形なので飽和状態になりませんが 負領域では関数が0になります 隠れ活性化にReLUを使うネットワークは 隠れ活性化にシグモイドを使うネットワークと
比べてトレーニング速度が10倍です ただし負領域では関数が常に0なので 実層は消失して終わります これが示しているのは 入力を負領域で開始して 活性化の出力がゼロになると 次の層と正領域での入力に役立たない
ということです これにより多数のゼロ活性化が作成されます 重みを更新する誤差逆伝搬中に 誤差導関数に活性化を掛ける必要があるため 勾配ゼロで終わります このようにデータゼロの重みは変化せず その層のトレーニングは失敗します 幸運にも 巧妙な方法が多数開発されており トレーニングが停止しないよう
ReLUを少し変更しながらも 一般的なReLUのメリットを維持しています 一般的なReLUに戻ります maximum演算子は区分線形式で
表すこともできます ここではゼロ未満で 関数はゼロです またゼロ以上で関数はXです ReLU関数の平滑近似は 1の自然対数の解析関数と 指数関数Xを足したものです これをソフトプラス関数と言います ソフトプラス関数の導関数は
ロジスティック関数です ソフトプラス関数を使用するメリットは ReLU関数と異なり 0でも連続かつ微分可能です ただし自然対数と指数関数により ReLUと比べて追加の計算があるため 実際にはReLUの結果も同様に良好です そのためソフトプラスはディープラーニング
での使用は推奨されません ゼロ活性化によるdying ReLUの問題を
解決するため Leaky ReLUが開発されました ReLU同様 区分線形関数があります ただし負領域には 0ではなく 厳密に言えば0.01の
非ゼロ勾配があります このようにユニットが活性化していなくても Leaky ReLUではわずかな非ゼロ勾配で
それらを通過できるため 重みの更新やトレーニングの継続ができると
考えられます このLeakyの考え方を一歩先に進めたのが
Parametric ReLU（PReLU）です これは負領域でXを0.01として 任意で通過させるのではなく Xをαとして通過させます しかしパラメータαはどうなるべきですか？ グラフでは可視化するために
αを0.5に設定しました しかし実際には他のNNパラメータとともに トレーニングから得た学習済みパラメータです このようにこの値は設定するのではなく データを使ったトレーニング中に決定され 優先順位で設定するより適した値を学習します αが0未満の場合 式はmaximumを使ってコンパクトフォームに
書き直せます 具体的にはmaxはXまたはαxです ランダム化したLeaky ReLUもあり
これではαをトレーニングするのではなく 一様分布からランダムにサンプルを抽出します 厳密には各α値のネットワークは異なるため これには脱落と類似した効果があります そのため集合と類似したものが形成されます テスト時にすべてのα値は 予測に使用する目的で
決定論的値に平均化されます ReLU6という変種もあります これも区分線形関数で線分が3本あります 通常のReLU同様 負領域では0ですが 正領域ではReLU6は6のままです 「なぜ6のままなのか？」と思うでしょう これらのReLUユニットの1つは シフトベルヌーイユニットによって複製された
6のみを持ちます ハードキャップの場合は膨大な量になります 通常これらはReLU nユニットと呼ばれます ここでnはキャップ値です テストで6が最適値であるとわかりました ReLU6ユニットではモデルはスパース性を
より早く学習できます まずCIFAR-10画像データセットの畳込み
深層エリートネットワークで使われました ネットワーク準備に有用なプロパティもあり
推論用の正確な固定小数点を得られます 上限が有界でない場合 固定小数点数のQ部分で多数のビットを失い 上限が6の場合は 数値の端数部分に十分なビットが残り 十分な数値を示し適切な推論を行えます 最後は指数線形ユニット（ELU）です 入力空間の非負領域ではほぼ線形です 入力の負領域では平坦かつ単調で 最も重要なことですが非ゼロでです また通常のReLUよりゼロ中心なので
学習が高速化します ELUの主な欠点は指数関数を計算する
必要があるため ReLUより組成的に高価なことです NNは任意に複雑になる可能性があります 多くの層や 層ごとのニューロン、出力、入力 さまざまな種類の活性化関数などがあります 複数層の目的は何でしょうか？ 層を追加するごとに 作成できる関数の複雑性が追加されます 以降の各層は前の関数の合成です 隠れ層では非線形活性化関数を使用しているので データを回転、拡張、圧迫する 大量のデータ変換を作成しています こうしたことを行う目的は 超平面をデータに合わせて回帰させる方法か 超平面でデータを分離して分類する方法により データを転送することです 元の特徴空間から新しい複雑な特徴空間に
マッピングしています ニューロンを層に追加するとどうなるのか？ ニューロンを追加するごとに
新しい次元がベクトル空間に追加されます 3つの入力ニューロンで開始する場合は R3ベクトル空間で始めます しかし次の層にはR4ベクトル空間に移動した
4つのニューロンがあります 前のコースでカーネル法を説明したとき 超平面で簡単に分離できないデータセットが 元の入力ベクトル空間にありました しかし次元を追加して
新しい次元が埋まるように 理にかなった方法でデータを変換することで データクラス間に明確なスライスを
簡単に作成できるようになりました 同じことをここでNNに適用します 複数の出力ノードがあるとどうなるでしょうか？ 複数の出力ノードがあると 複数のラベルを比較して対応する領域を
後方へ伝搬できます 各画像内に複数のエンテイティまたはクラスが
ある場合に 画像分類を行うようなものです クラスは多数あるので
1つのクラスの予測はできません そのためこの柔軟性は役立ちます NNは任意に複雑でなければなりません 隠れ次元を増やすために ＿＿＿を追加できます 関数合成を増やすには ＿＿＿を追加できます たとえば複数のラベルがあると ＿＿＿を追加できます 正解は、ニューロン、層、出力です 隠れ次元を変更するために
ニューロンの層の数を変更できます そのため中間ベクトルが入るような
ベクトル空間の次元を決定します 層に4つのニューロンがあれば それは4ベクトル空間にあります 層に500個のニューロンがあれば
それは500ベクトル空間にあります つまり実次元が500あるという意味です 層を追加しても前の層の次元は変わらず ニューロンの数が前の層と異ならなければ その層の次元さえも変わりません 層を追加することで関数合成が向上します Go of F of Xは 入力Xでの関数Gと関数Fの合成です したがって まずXをFで変換して 次にその結果をGで変換します 層が多いほどネストした関数が深くなります これは非線形関数を組み合わせて 複雑な特徴マップを作るのに適しています これは人間よりコンピュータが得意なことです 私たちはデータを整形して そこから洞察を得ることができます 洞察は出力層から受け取ります 推測中 洞察がMLによって明確化された
問題への答えになります 画像が犬になる確率を知りたいだけなら 出力ノードが1つあれば十分ですが 画像が猫、犬、鳥、ネズミになる
確率を知りたければ それぞれに1つずつのノードが必要になります 他の3つの答えはすべて間違いです
2つ以上の用語が間違っています