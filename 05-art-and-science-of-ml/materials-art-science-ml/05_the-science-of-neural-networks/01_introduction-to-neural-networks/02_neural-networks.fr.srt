1
00:00:00,000 --> 00:00:02,940
Nous avons déjà parlé
des réseaux de neurones.

2
00:00:02,940 --> 00:00:05,845
Nous allons maintenant les aborder
sous l'angle de la science.

3
00:00:05,845 --> 00:00:08,515
Nous avons vu
que les croisements de caractéristiques

4
00:00:08,515 --> 00:00:10,325
étaient efficaces dans ce type de cas.

5
00:00:10,325 --> 00:00:12,360
Si x1 est la dimension horizontale

6
00:00:12,360 --> 00:00:14,170
et x2 est la dimension verticale,

7
00:00:14,170 --> 00:00:16,080
il n'y a aucune combinaison linéaire

8
00:00:16,080 --> 00:00:18,965
des deux caractéristiques
décrivant cette distribution.

9
00:00:18,965 --> 00:00:22,485
Ce n'est que lorsque nous avons extrait
des caractéristiques

10
00:00:22,485 --> 00:00:27,505
et croisé x1 et x2 pour obtenir
une nouvelle caractéristique x3 (x1x2),

11
00:00:27,505 --> 00:00:30,160
que nous avons pu décrire
notre distribution de données.

12
00:00:30,160 --> 00:00:33,190
Donc, l'extraction manuelle
des caractéristiques peut

13
00:00:33,190 --> 00:00:38,290
facilement résoudre tous les problèmes
de non-linéarité. OK ?

14
00:00:38,290 --> 00:00:41,195
Malheureusement, le monde réel
ne présente pratiquement jamais

15
00:00:41,195 --> 00:00:43,145
des distributions si facilement décrites.

16
00:00:43,145 --> 00:00:46,785
L'extraction des caractéristiques,
malgré des années de travail poussé,

17
00:00:46,785 --> 00:00:48,635
a des limites.

18
00:00:48,635 --> 00:00:51,970
Par exemple, quels croisements
de caractéristiques seraient nécessaires

19
00:00:51,970 --> 00:00:53,720
pour modéliser cette distribution ?

20
00:00:53,720 --> 00:00:57,945
On dirait deux cercles l'un sur l'autre,
ou deux spirales,

21
00:00:57,945 --> 00:01:00,730
mais en fait, ce n'est pas très clair.

22
00:01:00,730 --> 00:01:04,530
Cet exemple démontre l'utilité
des réseaux de neurones qui peuvent créer,

23
00:01:04,530 --> 00:01:08,010
avec un algorithme, des transformations
et croisements de caractéristiques

24
00:01:08,010 --> 00:01:09,090
très complexes.

25
00:01:09,090 --> 00:01:13,630
Vous pouvez imaginer des distributions
encore plus complexes que cette spirale,

26
00:01:13,630 --> 00:01:16,380
qui requièrent l'utilisation
des réseaux de neurones.

27
00:01:16,380 --> 00:01:20,990
Les réseaux de neurones peuvent être utilisés
à la place des croisements de caractéristiques

28
00:01:20,990 --> 00:01:22,610
en combinant des caractéristiques.

29
00:01:22,610 --> 00:01:25,800
Lors de la conception de l'architecture
de notre réseau de neurones,

30
00:01:25,800 --> 00:01:27,370
nous voulons structurer le modèle

31
00:01:27,370 --> 00:01:29,700
pour obtenir des combinaisons
de caractéristiques.

32
00:01:29,700 --> 00:01:33,325
Nous voulons ensuite ajouter une autre 
couche pour lier ces combinaisons,

33
00:01:33,325 --> 00:01:36,980
puis une autre encore pour lier
ces combinaisons et ainsi de suite.

34
00:01:36,980 --> 00:01:39,090
Comment choisir les bonnes combinaisons

35
00:01:39,090 --> 00:01:42,315
de caractéristiques et les combinaisons
de combinaisons ?

36
00:01:42,315 --> 00:01:45,780
Vous devez entraîner le modèle, bien sûr.

37
00:01:45,780 --> 00:01:49,095
C'est ce qui constitue l'intuition de base
des réseaux de neurones.

38
00:01:49,095 --> 00:01:52,500
Cette approche n'est pas supérieure
aux croisements de caractéristiques,

39
00:01:52,500 --> 00:01:57,930
mais c'est une alternative flexible
qui s'adapte à de nombreux cas.

40
00:01:57,930 --> 00:02:00,580
Voici une représentation graphique
d'un modèle linéaire.

41
00:02:00,580 --> 00:02:02,565
Nous avons trois entrées : x1,

42
00:02:02,565 --> 00:02:05,885
x2 et x3, comme indiqué
par les cercles bleus.

43
00:02:05,885 --> 00:02:08,529
Elles sont combinées,
et chaque arête est pondérée

44
00:02:08,529 --> 00:02:11,219
pour produire une sortie.

45
00:02:11,219 --> 00:02:13,410
Il y a souvent
un terme pondéré supplémentaire,

46
00:02:13,410 --> 00:02:15,835
mais pour plus de simplicité,
il n'apparaît pas ici.

47
00:02:15,835 --> 00:02:18,555
C'est un modèle linéaire,
car il se présente sous la forme

48
00:02:18,555 --> 00:02:24,100
y = w1x1 + w2x2 + w3x3
(où w est la pondération).

49
00:02:24,100 --> 00:02:28,575
Ajoutons maintenant une couche cachée
à notre réseau de nœuds et d'arêtes.

50
00:02:28,575 --> 00:02:33,000
Notre couche d'entrée comporte trois nœuds
et notre couche cachée trois également.

51
00:02:33,000 --> 00:02:35,790
Parlons des nœuds cachés.

52
00:02:35,790 --> 00:02:38,170
Comme il s'agit d'une couche
entièrement connectée,

53
00:02:38,170 --> 00:02:42,265
il y a 3 x 3 arêtes, ou 9 pondérations.

54
00:02:42,265 --> 00:02:45,640
Voici donc un modèle non linéaire
que nous pouvons utiliser

55
00:02:45,640 --> 00:02:48,900
pour résoudre des problèmes
de non-linéarité, n'est-ce pas ?

56
00:02:48,900 --> 00:02:52,095
Malheureusement, non.
Regardons cela de plus près.

57
00:02:52,095 --> 00:02:55,570
L'entrée du premier nœud caché est égale
à la somme pondérée de :

58
00:02:55,570 --> 00:03:02,405
w1x1 + w4x2 + w7x3.

59
00:03:02,405 --> 00:03:05,495
L'entrée du deuxième nœud caché est égale
à la somme pondérée de :

60
00:03:05,495 --> 00:03:10,670
w2x1 + w5x2 + w8x3.

61
00:03:10,670 --> 00:03:14,460
L'entrée du troisième nœud caché est égale
à la somme pondérée de :

62
00:03:14,460 --> 00:03:20,205
w3x1 + w6x2 + w9x3.

63
00:03:20,205 --> 00:03:23,200
Une fois que l'on combine tout cela
au niveau du nœud de sortie,

64
00:03:23,200 --> 00:03:30,065
on obtient :
w10h1 + w11h2 + w12h3.

65
00:03:30,065 --> 00:03:32,085
Rappelez-vous toutefois que h1,

66
00:03:32,085 --> 00:03:38,155
h2 et h3 ne sont que des combinaisons
linéaires des caractéristiques d'entrée.

67
00:03:38,155 --> 00:03:40,640
Si nous développons tout cela,

68
00:03:40,640 --> 00:03:44,165
nous obtenons un ensemble complexe
de constantes de pondération multipliées

69
00:03:44,165 --> 00:03:47,210
par chaque valeur d'entrée, x1, x2 et x3.

70
00:03:48,870 --> 00:03:52,020
Nous pouvons substituer
chaque pondération double par une nouvelle.

71
00:03:52,020 --> 00:03:53,675
Ça vous semble familier ?

72
00:03:53,675 --> 00:03:56,455
Il s'agit du même modèle linéaire qu'avant,

73
00:03:56,455 --> 00:03:59,600
malgré le fait que nous avons ajouté
une couche cachée de neurones.

74
00:03:59,600 --> 00:04:04,225
Que s'est-il passé ? Et si on ajoutait
une autre couche cachée ?

75
00:04:04,225 --> 00:04:06,850
Malheureusement, on obtient encore

76
00:04:06,850 --> 00:04:11,670
une matrice de pondération unique
multipliée par chacune des trois entrées.

77
00:04:11,670 --> 00:04:14,285
Il s'agit du même modèle linéaire.

78
00:04:14,285 --> 00:04:18,400
On peut poursuivre ce processus
à l'infini et obtenir le même résultat,

79
00:04:18,400 --> 00:04:22,160
encore que ce serait très coûteux
en termes de calcul pour l'apprentissage

80
00:04:22,160 --> 00:04:26,105
ou la prédiction, et avec une architecture
bien plus compliquée que nécessaire.

81
00:04:27,390 --> 00:04:30,110
Si l'on considère cela
sous l'angle de l'algèbre linéaire,

82
00:04:30,110 --> 00:04:34,170
on multiplie un enchaînement de matrices.

83
00:04:34,170 --> 00:04:35,440
Dans ce petit exemple,

84
00:04:35,440 --> 00:04:37,300
je multiplie d'abord une matrice 3 x 3,

85
00:04:37,300 --> 00:04:41,675
la transposition de la matrice de pondération
entre les couches entrée et cachée h1

86
00:04:41,675 --> 00:04:46,075
par le vecteur d'entrée 3 x 1,
ce qui donne le vecteur 3 x 1,

87
00:04:46,075 --> 00:04:48,405
c'est-à-dire les valeurs
pour chaque neurone caché

88
00:04:48,405 --> 00:04:49,600
dans la couche cachée h1.

89
00:04:49,600 --> 00:04:52,700
Pour trouver les valeurs de neurones
de la deuxième couche cachée,

90
00:04:52,700 --> 00:04:54,595
je multiplie la transposition

91
00:04:54,595 --> 00:04:57,785
de sa matrice de pondération 3 x 1
qui connecte la couche cachée h1

92
00:04:57,785 --> 00:05:01,540
à la couche cachée h2 par le vecteur
résultant au niveau de la couche cachée h1.

93
00:05:01,540 --> 00:05:03,195
Comme vous pouvez le deviner,

94
00:05:03,195 --> 00:05:05,010
les deux matrices de pondération 3 x 3

95
00:05:05,010 --> 00:05:07,300
peuvent être combinées
en une seule matrice 3 x 3

96
00:05:07,300 --> 00:05:09,780
en calculant d'abord
le produit des matrices

97
00:05:09,780 --> 00:05:13,070
à partir de la gauche,
du milieu ou de la droite.

98
00:05:13,070 --> 00:05:16,070
On obtient toujours
la même forme pour h2,

99
00:05:16,070 --> 00:05:19,120
le vecteur de valeur de neurone
de la deuxième couche cachée.

100
00:05:19,120 --> 00:05:21,900
Lorsque j'ajoute la couche finale
entre la couche cachée h2

101
00:05:21,900 --> 00:05:24,980
et la couche de sortie,
je dois multiplier les étapes précédentes

102
00:05:24,980 --> 00:05:27,315
par la transposition
de la matrice de pondération

103
00:05:27,315 --> 00:05:28,950
entre les deux dernières couches.

104
00:05:28,950 --> 00:05:32,610
Bien que dans une propagation avant
via un réseau de neurones, vous effectuiez

105
00:05:32,610 --> 00:05:36,660
la multiplication de la matrice de droite
à gauche en l'appliquant de gauche à droite,

106
00:05:36,660 --> 00:05:39,440
vous pouvez voir
que notre longue chaîne de multiplications

107
00:05:39,440 --> 00:05:43,210
se réduit à un vecteur à trois valeurs.

108
00:05:43,210 --> 00:05:46,210
Si vous entraînez ce modèle
dans une régression linéaire simple

109
00:05:46,210 --> 00:05:50,750
de trois pondérations côte à côte
avec une surface de perte identique,

110
00:05:50,760 --> 00:05:54,965
et même si vous avez fait des tonnes
de calculs pour les 21 pondérations,

111
00:05:54,965 --> 00:05:58,549
la chaîne des produits des matrices
se réduit à l'équation inférieure

112
00:05:58,549 --> 00:06:01,350
et la pondération correspond exactement
aux pondérations

113
00:06:01,350 --> 00:06:03,640
de la régression linéaire simple
d'apprentissage.

114
00:06:03,640 --> 00:06:05,465
Tout ce travail pour le même résultat.

115
00:06:05,465 --> 00:06:08,995
Vous vous dites probablement :
"Je pensais que les réseaux de neurones

116
00:06:08,995 --> 00:06:11,620
consistaient à ajouter
de multiples couches de neurones.

117
00:06:11,620 --> 00:06:14,695
Comment faire du deep learning
si toutes les couches se réduisent

118
00:06:14,695 --> 00:06:17,410
à une seule ?"
J'ai de bonnes nouvelles pour vous.

119
00:06:17,410 --> 00:06:19,025
La solution est très simple.

120
00:06:19,025 --> 00:06:22,970
Il suffit d'ajouter une couche
de transformation non linéaire,

121
00:06:22,970 --> 00:06:25,630
à l'aide d'une fonction d'activation
non linéaire

122
00:06:25,630 --> 00:06:28,820
telle que la fonction sigmoïde,
TanH ou ReLU.

123
00:06:28,820 --> 00:06:32,180
En termes de graphe dans TensorFlow,

124
00:06:32,180 --> 00:06:35,735
vous pouvez imaginer
que chaque neurone dispose de deux nœuds.

125
00:06:35,735 --> 00:06:40,395
Le premier nœud est le résultat
de la somme pondérée wx + b,

126
00:06:40,395 --> 00:06:44,590
et le deuxième nœud correspond à ce résultat
une fois passé par la fonction d'activation.

127
00:06:44,590 --> 00:06:47,830
En d'autres termes, il s'agit des entrées
de la fonction d'activation

128
00:06:47,830 --> 00:06:50,065
suivies des sorties
de la fonction d'activation,

129
00:06:50,065 --> 00:06:54,610
de sorte que la fonction d'activation
fait office de point de transition.

130
00:06:54,610 --> 00:06:57,520
L'ajout de cette transformation
non linéaire est le seul moyen

131
00:06:57,520 --> 00:07:00,990
d'empêcher le réseau de neurones
d'être réduit à un réseau peu profond.

132
00:07:00,990 --> 00:07:04,085
Même si vous disposez d'une couche
avec des fonctions d'activation

133
00:07:04,085 --> 00:07:06,755
non linéaires dans le réseau,
si ailleurs dans le réseau,

134
00:07:06,755 --> 00:07:10,755
vous disposez de deux ou plusieurs couches
avec des fonctions d'activation linéaires,

135
00:07:10,755 --> 00:07:12,980
elles peuvent être réduites
à un seul réseau.

136
00:07:12,980 --> 00:07:16,290
Habituellement, les couches des réseaux
de neurones sont non linéaires

137
00:07:16,290 --> 00:07:21,245
(couches 1 et -1), avec une transformation
de couche finale linéaire pour une régression,

138
00:07:21,245 --> 00:07:24,065
une fonction sigmoïde ou softmax
que nous évoquerons bientôt

139
00:07:24,065 --> 00:07:25,830
dans les problèmes de classification.

140
00:07:25,830 --> 00:07:28,460
Tout dépend de la sortie voulue.

141
00:07:28,460 --> 00:07:31,090
Si on considère cela sous l'angle
de l'algèbre linéaire,

142
00:07:31,090 --> 00:07:34,610
lorsqu'on applique une transformation linéaire
à une matrice ou un vecteur,

143
00:07:34,610 --> 00:07:37,060
nous le multiplions
par une matrice ou un vecteur

144
00:07:37,060 --> 00:07:40,050
afin d'obtenir
la forme et le résultat voulus.

145
00:07:40,050 --> 00:07:42,525
Ainsi, lorsque je veux
mettre à l'échelle une matrice

146
00:07:42,525 --> 00:07:44,429
je peux la multiplier par une constante.

147
00:07:44,429 --> 00:07:47,190
Mais en fait, vous la multipliez
par une matrice d'identité

148
00:07:47,190 --> 00:07:49,470
qui est multipliée par cette constante.

149
00:07:49,470 --> 00:07:53,090
Il s'agit donc d'une matrice diagonale
avec cette constante sur la diagonale.

150
00:07:53,090 --> 00:07:56,490
Le tout pourrait être réduit
à un produit de matrice.

151
00:07:56,490 --> 00:07:59,070
Toutefois, si j'ajoute une non-linéarité,

152
00:07:59,070 --> 00:08:02,720
il m'est impossible de la représenter
par une matrice.

153
00:08:02,720 --> 00:08:06,560
En effet, d'un point de vue des éléments,
j'ajoute une fonction à mon entrée.

154
00:08:06,560 --> 00:08:10,185
Par exemple, si j'ai une fonction
d'activation non linéaire entre la première

155
00:08:10,185 --> 00:08:13,280
et la deuxième couches cachées,
j'applique une fonction du produit

156
00:08:13,280 --> 00:08:15,665
entre la transposition
de la matrice de pondération

157
00:08:15,665 --> 00:08:18,280
de ma première couche cachée
et de mon vecteur d'entrée.

158
00:08:18,280 --> 00:08:21,440
L'équation du bas est ma fonction
d'activation dans une unité ReLU.

159
00:08:21,440 --> 00:08:23,770
Comme je ne peux pas
représenter la transformation

160
00:08:23,770 --> 00:08:27,010
sous forme algébrique linéaire,
je ne peux plus réduire cette portion

161
00:08:27,010 --> 00:08:30,370
de la chaîne de transformation.
Le modèle demeure donc complexe

162
00:08:30,370 --> 00:08:34,795
et ne peut pas être réduit
à une combinaison linéaire des entrées.

163
00:08:34,795 --> 00:08:38,989
Je peux toujours réduire la matrice
de pondération de la deuxième couche cachée

164
00:08:38,989 --> 00:08:41,419
et la matrice de pondération
de la couche de sortie,

165
00:08:41,419 --> 00:08:43,710
car aucune fonction non linéaire
n'est appliquée.

166
00:08:43,710 --> 00:08:47,775
Cela veut dire qu'en présence de deux
ou plusieurs couches linéaires consécutives,

167
00:08:47,775 --> 00:08:51,605
celles-ci peuvent toujours être réduites
à une couche, quel que soit leur nombre.

168
00:08:51,605 --> 00:08:55,290
Les fonctions les plus complexes
sont donc créées par votre réseau.

169
00:08:55,290 --> 00:08:58,390
Il est recommandé d'appliquer
des fonctions d'activation linéaires

170
00:08:58,390 --> 00:09:00,775
à votre réseau,
à l'exception de la dernière couche

171
00:09:00,775 --> 00:09:03,350
au cas où vous utiliseriez
un autre type de sortie.

172
00:09:03,350 --> 00:09:06,915
Pourquoi est-il important d'ajouter
des fonctions d'activation non linéaires

173
00:09:06,915 --> 00:09:08,875
aux réseaux de neurones ?

174
00:09:08,875 --> 00:09:13,260
Parce que cela empêche les couches
d'être réduites à un modèle linéaire.

175
00:09:13,260 --> 00:09:17,450
Les fonctions d'activation non linéaires 
créent d'intéressantes transformations

176
00:09:17,450 --> 00:09:21,450
via un espace de caractéristiques
de données, et permettent d'utiliser

177
00:09:21,450 --> 00:09:24,550
des fonctions de composition profondes.
Pour rappel, en présence

178
00:09:24,550 --> 00:09:27,360
d'au moins deux couches
de fonctions d'activation linéaires,

179
00:09:27,360 --> 00:09:29,835
ce produit des matrices
peut se résumer à une matrice

180
00:09:29,835 --> 00:09:32,455
multipliée par le vecteur
des caractéristiques d'entrée.

181
00:09:32,455 --> 00:09:36,170
Vous obtenez ainsi un modèle plus lent
avec une puissance de calcul supérieure,

182
00:09:36,170 --> 00:09:39,205
mais avec une complexité
fonctionnelle réduite.

183
00:09:39,205 --> 00:09:42,720
Les non-linéarités n'ajoutent pas
de régularisation à la fonction de perte

184
00:09:42,720 --> 00:09:45,370
et ne déclenchent pas
d'arrêt prématuré.

185
00:09:45,370 --> 00:09:48,000
Bien que les fonctions d'activation
non linéaires créent

186
00:09:48,000 --> 00:09:50,520
des transformations complexes
dans l'espace vectoriel,

187
00:09:50,520 --> 00:09:53,380
cette dimension ne change pas.
C'est le même espace vectoriel

188
00:09:53,380 --> 00:09:57,220
même s'il est étiré, rapetissé ou pivoté.

189
00:09:57,220 --> 00:09:59,860
Comme mentionné
dans l'un des cours précédents,

190
00:09:59,860 --> 00:10:03,385
il existe de nombreuses fonctions
d'activation non linéaire avec sigmoïde.

191
00:10:03,385 --> 00:10:06,575
La sigmoïde mise à l'échelle et décalée,
dite tangente hyperbolique,

192
00:10:06,575 --> 00:10:08,300
fait partie des toutes premières.

193
00:10:08,300 --> 00:10:11,780
Toutefois, elles peuvent arriver
à saturation, ce qui entraîne

194
00:10:11,780 --> 00:10:13,545
la disparition du gradient.

195
00:10:13,545 --> 00:10:16,645
Dans ce cas, les pondérations des modèles
ne sont pas mises à jour

196
00:10:16,645 --> 00:10:18,320
et l'entraînement s'interrompt.

197
00:10:18,320 --> 00:10:22,295
L'unité de rectification linéaire (ReLU)
est l'une de nos fonctions préférées,

198
00:10:22,295 --> 00:10:24,920
car elle est simple
et fonctionne parfaitement.

199
00:10:24,920 --> 00:10:28,680
Dans le domaine positif, elle est
linéaire, donc pas de saturation,

200
00:10:28,680 --> 00:10:31,345
et dans le domaine négatif, la fonction
est égale à zéro.

201
00:10:31,345 --> 00:10:35,530
Les réseaux avec activation "cachée"
de type ReLU apprennent

202
00:10:35,530 --> 00:10:39,520
10 fois plus vite que les réseaux
avec activation "cachée" de type sigmoïde.

203
00:10:39,520 --> 00:10:43,070
Cependant, la fonction étant toujours
égale à zéro dans le domaine négatif,

204
00:10:43,070 --> 00:10:45,810
on peut en arriver à la disparition
des couches ReLU.

205
00:10:45,810 --> 00:10:49,485
En effet, lorsque vous commencez à obtenir
des entrées dans le domaine négatif

206
00:10:49,485 --> 00:10:53,050
et que la sortie de l'activation est
égale à zéro, c'est au détriment

207
00:10:53,050 --> 00:10:56,310
de la couche suivante et des entrées
données dans le domaine positif.

208
00:10:56,310 --> 00:11:00,020
Cet effet cumulé aboutit à la création
de nombreuses activations égales à zéro,

209
00:11:00,020 --> 00:11:02,940
durant la rétropropagation
et la mise à jour des pondérations,

210
00:11:02,940 --> 00:11:06,370
car on doit multiplier la dérivée
des erreurs par la valeur d'activation,

211
00:11:06,370 --> 00:11:10,320
et on obtient alors un gradient de zéro.
La pondération est alors égale à zéro,

212
00:11:10,320 --> 00:11:14,480
les pondérations ne changent pas,
et l'apprentissage de cette couche échoue.

213
00:11:14,480 --> 00:11:17,670
Heureusement, on a développé
plusieurs méthodes astucieuses

214
00:11:17,670 --> 00:11:21,770
pour modifier légèrement la fonction ReLU
sans interruption de l'apprentissage,

215
00:11:21,770 --> 00:11:24,720
tout en bénéficiant des avantages
de la fonction ReLU standard.

216
00:11:24,720 --> 00:11:25,920
La voici.

217
00:11:25,920 --> 00:11:28,390
L'opérateur maximal
peut aussi être représenté

218
00:11:28,390 --> 00:11:30,400
par une application linéaire par morceaux.

219
00:11:30,400 --> 00:11:33,510
Si x est inférieur à zéro,
la fonction est égale à zéro,

220
00:11:33,510 --> 00:11:36,510
et si x est supérieur ou égal à zéro,
la fonction est égale à x.

221
00:11:36,510 --> 00:11:39,275
Il existe une approximation douce
de la fonction ReLU.

222
00:11:39,275 --> 00:11:41,895
C'est la fonction logistique :
logarithme népérien de 1

223
00:11:41,895 --> 00:11:45,360
plus exponentielle de x.
Il s'agit de la fonction SoftPlus.

224
00:11:45,360 --> 00:11:49,740
Notez que la dérivée de la fonction SoftPlus
est une fonction logistique.

225
00:11:49,740 --> 00:11:53,080
L'utilisation de la fonction SoftPlus
présente des avantages :

226
00:11:53,080 --> 00:11:56,360
elle est continue et dérivable en 0,
contrairement à la fonction ReLU.

227
00:11:56,360 --> 00:11:59,930
Toutefois, du fait du logarithme népérien
et de la fonction exponentielle,

228
00:11:59,930 --> 00:12:02,979
la fonction requière davantage de calculs
que les fonctions ReLU,

229
00:12:02,979 --> 00:12:06,145
qui offrent tout de même
de bons résultats dans la pratique.

230
00:12:06,145 --> 00:12:10,530
C'est pourquoi SoftPlus n'est généralement
pas recommandée en deep learning.

231
00:12:10,530 --> 00:12:13,485
Pour essayer de résoudre le problème
de la disparition des ReLU

232
00:12:13,485 --> 00:12:15,395
en raison des activations égales à zéro,

233
00:12:15,395 --> 00:12:17,305
la fonction Leaky ReLU a été développée.

234
00:12:17,305 --> 00:12:20,975
Comme la fonction ReLU, Leaky ReLU
est une fonction linéaire par morceaux.

235
00:12:20,975 --> 00:12:24,630
Cependant, dans le domaine négatif,
la fonction n'est pas égale à zéro.

236
00:12:24,630 --> 00:12:28,615
Elle a une courbe non nulle,
spécifiquement, 0,01.

237
00:12:28,615 --> 00:12:32,865
Ainsi, lorsque l'unité n'est pas activée,
la fonction Leaky ReLU autorise tout de même

238
00:12:32,865 --> 00:12:37,019
le passage d'un gradient infime non nul,
qui, avec un peu de chance,

239
00:12:37,019 --> 00:12:40,760
permet de poursuivre la mise à jour
des pondérations et de l'apprentissage.

240
00:12:40,760 --> 00:12:46,590
L'unité ReLU paramétrique, ou PReLU,
est une autre variante.

241
00:12:46,590 --> 00:12:50,595
Plutôt que d'autoriser arbitrairement
le passage d'un centième de x

242
00:12:50,595 --> 00:12:54,700
dans le domaine négatif,
elle laisse passer αx.

243
00:12:54,700 --> 00:12:57,190
Qu'est-ce que ce paramètre alpha ?

244
00:12:57,190 --> 00:13:01,585
Dans le graphe, je donne à alpha
la valeur de 0,5 à des fins de visualisation,

245
00:13:01,585 --> 00:13:05,250
mais dans la pratique, il s'agit
d'un paramètre appris durant l'apprentissage,

246
00:13:05,250 --> 00:13:08,545
tout comme les autres paramètres
de réseaux de neurones.

247
00:13:08,545 --> 00:13:11,045
Ainsi, nous ne définissons pas cette valeur.

248
00:13:11,045 --> 00:13:14,800
Celle-ci est déterminée
durant l'entraînement grâce aux données

249
00:13:14,800 --> 00:13:17,260
et la valeur d'apprentissage
devrait être supérieure

250
00:13:17,260 --> 00:13:21,520
à celle que nous aurions pu définir.
Notez que si α est inférieur à 1,

251
00:13:21,520 --> 00:13:25,850
la formule peut être réduite à nouveau
à sa forme compacte avec la valeur maximale,

252
00:13:25,850 --> 00:13:28,950
spécifiquement, la valeur maximale de x,
ou alpha multiplié par x.

253
00:13:28,950 --> 00:13:31,490
Il existe également
une fonction Leaky ReLU aléatoire

254
00:13:31,490 --> 00:13:35,110
dans laquelle alpha n'est pas entraîné,
mais échantillonné de façon aléatoire

255
00:13:35,110 --> 00:13:36,870
à partir d'une distribution uniforme.

256
00:13:36,870 --> 00:13:40,540
L'effet peut être similaire à un abandon,
car vous obtenez un réseau différent

257
00:13:40,540 --> 00:13:41,930
pour chaque valeur d'alpha.

258
00:13:41,930 --> 00:13:44,510
Vous obtenez ainsi un résultat
semblable à un ensemble.

259
00:13:44,510 --> 00:13:47,555
Lors des tests, on fait la moyenne
de toutes les valeurs d'alpha

260
00:13:47,555 --> 00:13:50,510
pour obtenir une valeur déterministe
à des fins de prédiction.

261
00:13:50,510 --> 00:13:52,505
Il existe aussi la variante ReLU6,

262
00:13:52,505 --> 00:13:56,380
une autre fonction linéaire par morceaux
avec trois segments.

263
00:13:56,380 --> 00:13:59,900
Tout comme une ReLU normale,
elle est égale à zéro dans le domaine négatif.

264
00:13:59,900 --> 00:14:03,235
En revanche, dans le domaine positif,
la limite pour ReLU6 est de six.

265
00:14:03,235 --> 00:14:06,520
Vous vous demandez peut-être pourquoi.

266
00:14:06,520 --> 00:14:09,370
Imaginez que chaque unité ReLU dispose

267
00:14:09,370 --> 00:14:12,510
de seulement six unités répliquées
avec un biais de type Bernoulli,

268
00:14:12,510 --> 00:14:15,520
plutôt que d'un nombre infini
en raison du plafond fixe.

269
00:14:15,520 --> 00:14:18,520
On les appelle généralement
les unités n ReLU,

270
00:14:18,520 --> 00:14:20,265
où n représente la valeur plafond.

271
00:14:20,265 --> 00:14:24,270
Lors des tests, on a trouvé
que six était la valeur optimale.

272
00:14:24,270 --> 00:14:27,195
Les unités ReLU6 peuvent aider
les modèles à apprendre

273
00:14:27,195 --> 00:14:30,870
des caractéristiques creuses plus rapidement.
Elles ont été d'abord utilisées

274
00:14:30,870 --> 00:14:34,010
dans des réseaux CDBN sur un ensemble
de données d'images CIFAR-10.

275
00:14:34,010 --> 00:14:36,730
Elles sont également utiles,
car elles préparent le réseau

276
00:14:36,730 --> 00:14:39,399
pour une précision à virgule fixe
à des fins d'inférence.

277
00:14:39,399 --> 00:14:42,125
En l'absence de limite supérieure,
on perd trop d'éléments

278
00:14:42,125 --> 00:14:45,360
dans la partie décimale du nombre
à virgule fixe, mais si on dispose

279
00:14:45,360 --> 00:14:48,170
d'une limite supérieure de six,
on a suffisamment d'éléments

280
00:14:48,170 --> 00:14:52,160
dans la partie décimale du nombre
à des fins d'inférence.

281
00:14:52,160 --> 00:14:55,540
Enfin, on a l'unité
exponentielle linéaire, ou ELU.

282
00:14:55,540 --> 00:14:59,585
Elle est pour ainsi dire linéaire
dans la partie non négative des entrées.

283
00:14:59,585 --> 00:15:02,390
C'est une fonction douce,
monotone et, fait plus important,

284
00:15:02,390 --> 00:15:05,320
non nulle
dans la partie négative des entrées.

285
00:15:05,320 --> 00:15:09,115
Cette fonction est mieux centrée sur zéro
que la fonction ReLU standard,

286
00:15:09,115 --> 00:15:11,020
ce qui accélère l'apprentissage.

287
00:15:11,020 --> 00:15:14,445
Le principal inconvénient de l'unité ELU
c'est qu'en termes de calcul,

288
00:15:14,445 --> 00:15:18,255
elle est plus coûteuse qu'une ReLU,
car elle doit générer l'exponentielle.

289
00:15:18,255 --> 00:15:21,420
Les réseaux de neurones peuvent être
arbitrairement complexes :

290
00:15:21,420 --> 00:15:24,230
couches nombreuses,
nombreux neurones par couche,

291
00:15:24,230 --> 00:15:27,560
entrées et sorties, plusieurs types
de fonctions d'activation, etc.

292
00:15:27,560 --> 00:15:29,380
À quoi servent ces multiples couches ?

293
00:15:29,380 --> 00:15:33,080
Chaque couche accroît la complexité
des fonctions que je peux créer.

294
00:15:33,080 --> 00:15:36,830
Chaque couche subséquente est une
composition des fonctions précédentes.

295
00:15:36,830 --> 00:15:40,750
Comme j'utilise des fonctions d'activation
non linéaires dans les couches cachées,

296
00:15:40,750 --> 00:15:43,555
je crée une pile de transformations
de données qui pivotent,

297
00:15:43,555 --> 00:15:46,110
étirent ou rapetissent mes données.

298
00:15:46,110 --> 00:15:49,350
N'oubliez pas que je fais tout cela,
soit pour transférer mes données

299
00:15:49,350 --> 00:15:52,440
afin qu'elles tiennent dans un hyperplan
à des fins de régression,

300
00:15:52,440 --> 00:15:54,730
soit pour séparer mes données
dans des hyperplans

301
00:15:54,730 --> 00:15:56,520
à des fins de classification.

302
00:15:56,520 --> 00:15:59,325
On fait correspondre
l'espace des caractéristiques d'origine

303
00:15:59,325 --> 00:16:01,900
et le nouvel espace
des caractéristiques convolutives.

304
00:16:01,900 --> 00:16:04,530
Quel est l'intérêt d'ajouter
des neurones à une couche ?

305
00:16:04,530 --> 00:16:08,370
Chaque neurone ajoute une nouvelle
dimension dans mon espace vectoriel.

306
00:16:08,370 --> 00:16:12,225
Si je commence avec trois neurones d'entrée,
je débute dans l'espace vectoriel R3.

307
00:16:12,225 --> 00:16:14,650
Mais si ma couche suivante
comporte quatre neurones,

308
00:16:14,650 --> 00:16:17,270
je suis dans un espace vectoriel R4.

309
00:16:17,270 --> 00:16:20,700
Quand nous avons abordé les méthodes
de noyau dans le cours précédent,

310
00:16:20,700 --> 00:16:24,460
nous avions un ensemble de données
qui n'étaient pas facilement séparées

311
00:16:24,460 --> 00:16:27,315
avec un hyperplan dans l'espace vectoriel
d'entrée d'origine.

312
00:16:27,315 --> 00:16:30,310
Mais, en ajoutant la dimension,
puis en transformant les données

313
00:16:30,310 --> 00:16:34,070
pour remplir cette nouvelle dimension
de façon adaptée, nous avons pu facilement

314
00:16:34,070 --> 00:16:37,425
séparer les différentes
classes de données.

315
00:16:37,425 --> 00:16:39,870
Il en va de même
pour les réseaux de neurones.

316
00:16:40,720 --> 00:16:43,430
À quoi sert-il d'avoir
plusieurs nœuds de sortie ?

317
00:16:43,430 --> 00:16:46,710
L'utilisation de plusieurs nœuds de sortie
vous permet de les comparer

318
00:16:46,710 --> 00:16:49,425
à plusieurs étiquettes, puis de faire
une rétropropagation

319
00:16:49,425 --> 00:16:50,860
des erreurs correspondantes.

320
00:16:50,860 --> 00:16:53,015
Imaginons le cas
d'une classification d'images

321
00:16:53,015 --> 00:16:56,390
avec plusieurs entités ou
classes par image.

322
00:16:56,390 --> 00:17:00,055
Il est impossible de prédire une classe,
car il pourrait y en avoir plusieurs.

323
00:17:00,055 --> 00:17:03,159
Cette flexibilité représente
donc un réel atout.

324
00:17:03,159 --> 00:17:06,075
Les réseaux de neurones peuvent être
arbitrairement complexes.

325
00:17:06,075 --> 00:17:09,039
Pour accroître les dimensions cachées,
je peux ajouter [blanc].

326
00:17:09,039 --> 00:17:12,279
Pour accroître la composition
des fonctions, je peux ajouter [blanc].

327
00:17:12,279 --> 00:17:15,504
Si j'ai plusieurs étiquettes, par exemple,
je peux ajouter [blanc].

328
00:17:15,504 --> 00:17:19,142
La bonne réponse est :
neurones, couches, sorties.

329
00:17:19,142 --> 00:17:22,719
Pour modifier les dimensions cachées,
il faut changer le nombre de neurones,

330
00:17:22,719 --> 00:17:26,960
pour déterminer les dimensions de l'espace
vectoriel où se situe le vecteur intermédiaire.

331
00:17:26,960 --> 00:17:30,250
Si une couche a quatre neurones,
elle est dans l'espace vectoriel R4.

332
00:17:30,250 --> 00:17:34,200
Et si une couche comporte 500 neurones,
elle est dans l'espace vectoriel R500.

333
00:17:34,200 --> 00:17:37,050
En d'autres termes,
elle a 500 dimensions réelles.

334
00:17:37,050 --> 00:17:40,425
L'ajout d'une couche ne modifie pas
la dimension de la couche précédente

335
00:17:40,425 --> 00:17:44,070
ni même la dimension de cette couche,

336
00:17:44,070 --> 00:17:47,660
à moins que son nombre de neurones diffère
de celui de la couche précédente.

337
00:17:47,660 --> 00:17:52,100
Les couches supplémentaires permettent
d'optimiser la composition des fonctions.

338
00:17:52,100 --> 00:17:53,945
Rappelez-vous que g ∘ f(x)

339
00:17:53,945 --> 00:17:58,450
correspond à la fonction composée
de g et de f pour l'entrée x.

340
00:17:58,450 --> 00:18:03,305
Je transforme donc x avec f,
puis je transforme ce résultat avec g.

341
00:18:03,305 --> 00:18:06,770
Plus j'ajoute de couches, plus
les fonctions imbriquées sont profondes.

342
00:18:06,770 --> 00:18:09,370
C'est idéal pour combiner
des fonctions non linéaires

343
00:18:09,370 --> 00:18:12,270
afin de créer des cartes
de caractéristiques très convolutives

344
00:18:12,270 --> 00:18:15,370
que le cerveau humain a du mal
à élaborer, mais pas un ordinateur.

345
00:18:15,370 --> 00:18:19,080
Cela nous permet aussi de mieux préparer
nos données à des fins d'apprentissage

346
00:18:19,080 --> 00:18:21,910
et pour obtenir des insights.
À ce propos, on les reçoit

347
00:18:21,910 --> 00:18:24,390
via nos couches de sortie,
durant l'inférence.

348
00:18:24,390 --> 00:18:27,410
Ce sont les réponses
au problème formulé en termes de ML.

349
00:18:27,410 --> 00:18:30,360
Si vous voulez uniquement
connaître la probabilité qu'une image

350
00:18:30,360 --> 00:18:33,885
soit celle d'un chien, vous pouvez
vous contenter d'un seul nœud de sortie.

351
00:18:33,885 --> 00:18:36,935
Mais si vous voulez reconnaître
les images d'un chat, d'un chien,

352
00:18:36,935 --> 00:18:39,700
d'un oiseau ou d'un orignal,
vous devez alors avoir un nœud

353
00:18:39,700 --> 00:18:40,850
pour chacun d'entre eux.

354
00:18:40,850 --> 00:18:45,950
Les trois autres réponses sont fausses,
au moins deux des termes étant incorrects.