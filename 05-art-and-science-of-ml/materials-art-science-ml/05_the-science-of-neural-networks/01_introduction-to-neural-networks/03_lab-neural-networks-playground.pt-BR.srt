1
00:00:00,000 --> 00:00:04,295
Agora, vamos brincar com o playground
intensivo das redes neurais.

2
00:00:04,295 --> 00:00:06,305
Neste laboratório,

3
00:00:06,305 --> 00:00:07,730
usaremos playground intensivo

4
00:00:07,730 --> 00:00:10,450
para testar e criar redes neurais
para aprender os dados.

5
00:00:10,450 --> 00:00:13,290
Quero que você resolva esses problemas
de duas maneiras.

6
00:00:13,290 --> 00:00:17,640
Primeiro, vamos tentar treinar os modelos
usando a engenharia manual de atributos,

7
00:00:17,640 --> 00:00:19,840
em que usamos nosso
conhecimento para adivinhar

8
00:00:19,840 --> 00:00:23,720
a combinação correta e a transformação
de atributos para aprender os dados.

9
00:00:23,720 --> 00:00:25,830
Em seguida, vamos nos entregar

10
00:00:25,830 --> 00:00:27,090
ao poder das redes neurais

11
00:00:27,090 --> 00:00:30,070
e adicionar mais camadas e neurônios
usando um conjunto simples

12
00:00:30,070 --> 00:00:34,065
de atributos de entrada para ver se ele
mesmo realiza engenharia de atributos.

13
00:00:34,065 --> 00:00:36,105
Bem-vindo de volta
ao playground intensivo.

14
00:00:36,105 --> 00:00:38,770
Neste laboratório,
veremos se a engenharia de atributos

15
00:00:38,770 --> 00:00:41,565
pode superar nossas redes neurais.

16
00:00:41,565 --> 00:00:44,820
Tenho a sensação de que este
não será o caso. Vamos investigar.

17
00:00:44,820 --> 00:00:49,420
Certo, neste diagrama,
estamos tentando classificar

18
00:00:49,420 --> 00:00:54,420
esses pontos azuis e laranja,
é um problema de classificação.

19
00:00:54,420 --> 00:00:57,870
O que você notará é que eles se
parecem círculos concêntricos.

20
00:00:57,870 --> 00:01:00,000
No entanto, neste caso, há muito ruído.

21
00:01:00,000 --> 00:01:03,090
Portanto, há muita mistura aqui.

22
00:01:03,090 --> 00:01:08,760
O que vou tentar fazer é ver como
o X1 e o X2 se comportam no treino.

23
00:01:10,660 --> 00:01:14,400
Como você pode ver,
não estão aprendendo muito.

24
00:01:14,400 --> 00:01:17,400
Está tudo meio borrado junto,
está muito branco.

25
00:01:17,400 --> 00:01:19,640
Então, não está,
de um jeito ou de outro,

26
00:01:19,640 --> 00:01:22,060
de acordo com a escala abaixo, -101.

27
00:01:22,060 --> 00:01:23,450
Não aprendeu muito.

28
00:01:23,450 --> 00:01:25,350
Vamos ver se você
consegue melhorar.

29
00:01:25,350 --> 00:01:28,095
Com a engenharia de atributos,
sei que isso é um círculo.

30
00:01:28,095 --> 00:01:31,005
Então, faço X1 ao quadrado
e X2 ao quadrado,

31
00:01:31,005 --> 00:01:34,530
e testo agora, vamos ver.

32
00:01:34,530 --> 00:01:37,290
Olhe para isso,
parece uma elipse.

33
00:01:37,290 --> 00:01:42,295
Isso significa que está
quase descobrindo o que é essa função.

34
00:01:42,295 --> 00:01:43,970
Sabemos que é um círculo,

35
00:01:43,970 --> 00:01:45,550
mas há muito ruído e tudo mais,

36
00:01:45,550 --> 00:01:47,465
e está um pouco afastado.

37
00:01:48,755 --> 00:01:52,480
Talvez, porém, eu possa deixar minha
perda menor que 0,275,

38
00:01:52,480 --> 00:01:55,600
vamos tentar nos livrar de X1 e X2,
as formas lineares.

39
00:01:55,600 --> 00:01:57,210
Vamos tentar agora.

40
00:01:58,150 --> 00:02:00,060
2,85.

41
00:02:00,960 --> 00:02:02,715
Parece um pouco mais circular.

42
00:02:02,715 --> 00:02:05,790
No entanto, nossa perda de teste
está um pouco melhor.

43
00:02:05,790 --> 00:02:09,389
Vamos ver agora se podemos fazer o mesmo
com redes neurais.

44
00:02:09,389 --> 00:02:12,525
Vamos voltar para apenas X1 e X2,

45
00:02:12,525 --> 00:02:15,855
que, como vimos anteriormente,
fizeram um trabalho muito ruim.

46
00:02:15,855 --> 00:02:19,570
Vamos adicionar uma camada oculta
e dois domínios extras.

47
00:02:22,290 --> 00:02:27,975
Como vemos, temos dificuldade
para descobrir qual é essa função.

48
00:02:27,975 --> 00:02:31,950
O problema é que não há capacidade
suficiente nesses dois neurônios

49
00:02:31,950 --> 00:02:35,715
nem representação gráfica alta o
suficiente para aprender a distribuição.

50
00:02:35,715 --> 00:02:37,895
Então, vamos fazer uma pausa aqui

51
00:02:37,895 --> 00:02:39,590
e adicionar outro neurônio.

52
00:02:39,590 --> 00:02:42,570
Talvez esse tenha capacidade
suficiente para aprender a função.

53
00:02:42,970 --> 00:02:44,070
Certo.

54
00:02:44,700 --> 00:02:49,635
Ainda não está conseguindo.

55
00:02:49,670 --> 00:02:50,580
Talvez...

56
00:02:52,595 --> 00:02:53,985
Veja isto.

57
00:02:53,985 --> 00:02:55,540
Demorou muito tempo,

58
00:02:55,540 --> 00:02:58,810
mas está descobrindo
lentamente a forma da função.

59
00:02:58,810 --> 00:03:02,540
Isso é algum tipo de formato retangular.

60
00:03:02,540 --> 00:03:06,440
O que isso significa é que
estamos voltando à extremidade

61
00:03:06,440 --> 00:03:10,355
da quantidade de neurônios capazes
de representar essa distribuição.

62
00:03:10,355 --> 00:03:14,170
Vamos ver se podemos acelerar
o tempo ao adicionar um neurônio extra.

63
00:03:16,130 --> 00:03:17,070
Olhe para isso.

64
00:03:17,070 --> 00:03:19,020
Foi muito mais rápido.

65
00:03:19,020 --> 00:03:21,000
Temos apenas quatro neurônios aqui.

66
00:03:21,000 --> 00:03:24,850
Mas vamos ver o que acontece
se adicionamos muitos neurônios extras.

67
00:03:26,750 --> 00:03:29,490
Vamos colocar
um molde de quatro

68
00:03:29,490 --> 00:03:32,260
e ver o que acontece.

69
00:03:32,260 --> 00:03:34,030
Isso é o treino.

70
00:03:37,240 --> 00:03:38,460
É bem mais lento.

71
00:03:38,460 --> 00:03:41,765
Há mais massa para processar
passando por todas essas semicamadas.

72
00:03:41,765 --> 00:03:43,765
Acho que uma hora vai conseguir.

73
00:03:43,765 --> 00:03:47,410
Mas estou preocupado, pois pode ter
um pouco de sobreajuste, como você vê.

74
00:03:47,410 --> 00:03:50,675
Isso não é mais
uma forma circular simples.

75
00:03:50,675 --> 00:03:52,480
É algum polígono estranho.

76
00:03:52,480 --> 00:03:56,290
Portanto, está sobreajustando os dados e
não está indo bem com a perda de teste,

77
00:03:56,290 --> 00:03:58,305
que é muito mais alta
do que costumava ser.

78
00:03:59,375 --> 00:04:01,560
Vamos ver outras distribuições.

79
00:04:02,980 --> 00:04:05,910
Aqui, estamos distribuindo
nosso clássico Xr.

80
00:04:05,910 --> 00:04:09,260
Quando X e Y são positivos ou negativos,

81
00:04:09,260 --> 00:04:14,240
temos azuis ou temos a classe laranja.

82
00:04:14,240 --> 00:04:16,860
Vamos ver se podemos
aprender isso apenas com X1e X2.

83
00:04:21,130 --> 00:04:23,069
Como você pode ver, assim como antes,

84
00:04:23,069 --> 00:04:27,515
X1 e X2 não são fortes o suficiente
para descrever essa função.

85
00:04:27,515 --> 00:04:29,590
É basicamente zero em todo o quadro.

86
00:04:29,590 --> 00:04:33,120
Vamos ver se podemos descobrir isso
usando a engenharia de atributos.

87
00:04:33,810 --> 00:04:35,060
Com ela,

88
00:04:35,060 --> 00:04:38,475
vou escolher o X e X2 porque sei
como eles são.

89
00:04:38,475 --> 00:04:40,340
Vamos treinar isso.

90
00:04:41,340 --> 00:04:42,150
Veja isto.

91
00:04:42,150 --> 00:04:45,175
Muito bom, é uma perda de teste de 0,17.

92
00:04:45,175 --> 00:04:46,455
Isso é ótimo.

93
00:04:47,045 --> 00:04:49,010
Achei facilmente,

94
00:04:49,010 --> 00:04:52,220
e aqui está minha ponderação, 0,19,
isso é ótimo.

95
00:04:52,220 --> 00:04:54,800
Sim, há ruído, então temos
algumas coisas erradas,

96
00:04:54,800 --> 00:04:57,635
mas, na maior parte, ficou bem correto.

97
00:04:57,635 --> 00:05:00,240
Vamos ver agora
se o aprendizado de máquina,

98
00:05:00,240 --> 00:05:03,255
usando redes neurais,
pode fazer um trabalho melhor.

99
00:05:03,255 --> 00:05:06,330
Vamos colocar X1 e X2 juntos novamente,

100
00:05:06,330 --> 00:05:08,950
e vamos adicionar uma camada oculta.

101
00:05:08,950 --> 00:05:10,850
Mais uma vez, vou tentar ver.

102
00:05:10,850 --> 00:05:13,440
Quero ter a menor quantia que puder,

103
00:05:13,440 --> 00:05:17,210
então, vou tentar reduzir isso a apenas
dois neurônios e aprender isso.

104
00:05:18,440 --> 00:05:19,920
No entanto, como você pode ver,

105
00:05:19,920 --> 00:05:21,720
não é possível descobrir isso.

106
00:05:21,720 --> 00:05:24,450
Não há complexidade suficiente,
nem capacidade no modelo.

107
00:05:24,450 --> 00:05:28,230
Então vamos passar isso aqui
e tentar adicionar um terceiro neurônio.

108
00:05:28,920 --> 00:05:30,420
Vamos tentar treinar novamente.

109
00:05:34,100 --> 00:05:35,250
Como você pode ver aqui,

110
00:05:35,250 --> 00:05:37,605
está tendo dificuldade
para aprender esta função.

111
00:05:37,605 --> 00:05:39,450
Talvez seja apenas na borda,

112
00:05:39,450 --> 00:05:42,590
e eu tenho que esperar um pouco mais
para ver se vai aprender.

113
00:05:42,590 --> 00:05:44,340
Mas parece travado.

114
00:05:44,340 --> 00:05:46,875
Talvez outra inicialização
conserte isso.

115
00:05:46,875 --> 00:05:48,650
Vamos ver.

116
00:05:48,650 --> 00:05:49,650
Aí está.

117
00:05:49,650 --> 00:05:52,065
Então, tentamos executar a inicialização

118
00:05:52,065 --> 00:05:54,290
e, de certa forma,
aprenderemos a função aqui.

119
00:05:54,290 --> 00:05:57,930
Parece mais com uma ampulheta
diagonal, na verdade.

120
00:05:57,930 --> 00:06:00,470
No entanto, essa não é
exatamente a função.

121
00:06:00,470 --> 00:06:02,375
Você pode ver
que a perda é muito maior.

122
00:06:02,375 --> 00:06:03,570
Então, vamos para quatro,

123
00:06:03,570 --> 00:06:06,570
isso talvez faça o trabalho, vamos ver.

124
00:06:06,570 --> 00:06:10,040
Ainda estamos com a ampulheta,

125
00:06:10,040 --> 00:06:12,800
mas ela está se tornando parecida
a uma série de quadrados,

126
00:06:12,800 --> 00:06:15,470
que é o que a nossa função realmente é,
está melhorando.

127
00:06:15,470 --> 00:06:18,620
Agora, vamos ver, adicionando mais,

128
00:06:18,620 --> 00:06:23,350
e checar se há sobreajuste.

129
00:06:27,790 --> 00:06:31,235
Como você pode ver, é muito mais lento
na perda de treinamento.

130
00:06:32,195 --> 00:06:36,425
No entanto, esses estão mais
parecidos com um quadrado.

131
00:06:37,855 --> 00:06:39,150
Isso parece ótimo.

132
00:06:44,760 --> 00:06:47,945
Vamos tentar outro tipo de distribuição.

133
00:06:49,095 --> 00:06:50,910
Aqui nós temos uma espiral,

134
00:06:50,910 --> 00:06:53,090
duas, na verdade, uma ao redor da outra.

135
00:06:53,090 --> 00:06:55,685
Muito parecido com
a foto de uma galáxia.

136
00:06:55,685 --> 00:06:58,810
Vamos ver se podemos treinar com X1 e X2.

137
00:06:59,270 --> 00:07:01,080
Eu duvido que possamos.

138
00:07:02,330 --> 00:07:03,580
Como você pode ver aqui,

139
00:07:03,580 --> 00:07:05,615
ele realmente não aprendeu a distribuição.

140
00:07:05,615 --> 00:07:07,790
Está muito próximo de zero

141
00:07:07,790 --> 00:07:09,515
e não consegue decidir
o que é o quê.

142
00:07:09,515 --> 00:07:12,930
O que podemos testar é
a engenharia de atributos.

143
00:07:12,930 --> 00:07:14,600
Vamos testar.

144
00:07:14,600 --> 00:07:16,335
O que você acha?

145
00:07:16,335 --> 00:07:18,900
Vamos tentar círculos, talvez?

146
00:07:20,530 --> 00:07:22,710
Não, vamos tentar adicionar estes.

147
00:07:22,710 --> 00:07:26,170
Será seno e cosseno,
ou senoX1 e senoX2.

148
00:07:26,760 --> 00:07:28,160
Está testando.

149
00:07:28,160 --> 00:07:31,340
Eu tenho seis atributos brutos
entrando aqui

150
00:07:31,340 --> 00:07:33,045
e estão quase conseguindo.

151
00:07:33,045 --> 00:07:34,230
Como você vê no topo,

152
00:07:34,230 --> 00:07:36,390
está lentamente entrando aqui.

153
00:07:36,390 --> 00:07:39,290
Há uma grande lacuna aqui, que não
sei para onde está indo.

154
00:07:39,290 --> 00:07:43,370
Está realmente extrapolando aqui.

155
00:07:43,370 --> 00:07:45,000
Não é um grande trabalho,

156
00:07:45,000 --> 00:07:46,830
e está parado, como você pode ver.

157
00:07:46,830 --> 00:07:50,115
Vamos ver se podemos fazer isso melhor
com redes neurais.

158
00:07:50,115 --> 00:07:52,150
Vamos desativar isso

159
00:07:52,150 --> 00:07:53,990
e adicionar uma camada oculta.

160
00:07:53,990 --> 00:07:57,290
Primeiro começamos com dois neurônios
e veremos se podemos fazer isso.

161
00:07:58,930 --> 00:08:03,255
Como você pode ver aqui, não é muito
melhor do que ter X1 e X2 puros.

162
00:08:03,255 --> 00:08:06,050
Não há capacidade suficiente
para aprender este modelo.

163
00:08:06,050 --> 00:08:09,230
Vamos para três,
ver se consegue aprender.

164
00:08:12,500 --> 00:08:16,615
Está indo um pouco melhor
que a última vez com extrapolação aqui.

165
00:08:16,615 --> 00:08:18,810
No entanto, ainda não está
sendo tão bom quanto

166
00:08:18,810 --> 00:08:23,455
salvar todos os seis atributos ativados,
ou sete atributos.

167
00:08:24,055 --> 00:08:27,815
Vamos ver se podemos
adicionar mais um neurônio

168
00:08:28,665 --> 00:08:30,290
ou outra camada, talvez.

169
00:08:31,500 --> 00:08:32,800
Vamos ver se isso funciona.

170
00:08:34,480 --> 00:08:37,500
Tudo pronto, e você pode ver que temos

171
00:08:37,500 --> 00:08:40,889
uma perda de treino muito baixa para
as perdas de teste e está indo bem.

172
00:08:41,159 --> 00:08:42,700
Está travado.

173
00:08:45,300 --> 00:08:48,555
Vamos tentar mais um pouco, adicionando
mais algumas camadas ocultas.

174
00:08:49,425 --> 00:08:50,840
Vamos colocá-los para quatro.

175
00:08:51,970 --> 00:08:53,360
Espero que seja o suficiente.

176
00:08:54,410 --> 00:08:56,375
Vamos ver o que conseguimos.

177
00:08:57,445 --> 00:08:59,390
Ambas caíram um pouco.

178
00:09:00,210 --> 00:09:03,605
No entanto, ainda não tomou uma decisão,
pois toda a tela está branca.

179
00:09:04,345 --> 00:09:07,190
Aí está: tenho um ponto de inflexão

180
00:09:07,190 --> 00:09:09,040
e minha perda está diminuindo.

181
00:09:11,400 --> 00:09:14,880
No entanto, você pode ver
que a perda de teste também está subindo.

182
00:09:16,350 --> 00:09:19,660
Agora está constante.
Isso não tem capacidade suficiente.

183
00:09:19,660 --> 00:09:24,350
Vamos o mais longe possível e adicionar
oito neurônios a cada camada.

184
00:09:24,350 --> 00:09:28,810
Esperamos que seja o suficiente para
aprender esta função complexa e com ruído.

185
00:09:30,720 --> 00:09:31,770
Certo.

186
00:09:33,230 --> 00:09:34,870
Vamos tentar treinar isso.

187
00:09:36,960 --> 00:09:40,920
Como você vê, está indo muito devagar,
quando faz este treino aqui.

188
00:09:40,920 --> 00:09:45,430
Esperamos que descubra uma maneira
de fazer essa função funcionar.

189
00:09:47,350 --> 00:09:49,110
Minha perda de treino
está diminuindo.

190
00:09:49,110 --> 00:09:51,120
Porém, minha perda de teste está subindo.

191
00:10:00,760 --> 00:10:03,510
É uma espécie de nivelamento,
minha perda de teste.

192
00:10:05,650 --> 00:10:07,480
Quando você está fazendo isso sozinho,

193
00:10:07,480 --> 00:10:11,675
seus resultados podem variar, devido
a inicializações aleatórias da rede.

194
00:10:11,675 --> 00:10:13,350
Vamos tentar algo diferente.

195
00:10:17,070 --> 00:10:19,820
Este talvez seja um pouco mais promissor.

196
00:10:26,620 --> 00:10:28,970
Certo, isso parece
um pouco mais promissor.

197
00:10:30,290 --> 00:10:35,480
Veja o que ele está fazendo, está
aprendendo esses modos aqui, preenchendo.

198
00:10:37,390 --> 00:10:43,110
Parece que sobreajustamos, porque a perda
de teste está divergente, isso não é bom.

199
00:10:50,770 --> 00:10:51,840
E aqui vamos.

200
00:10:51,840 --> 00:10:53,565
Então, como você vê,

201
00:10:53,565 --> 00:10:55,245
mesmo com esse monte de redes,

202
00:10:55,245 --> 00:10:59,000
não podemos aprender
muito bem essa distribuição.

203
00:10:59,000 --> 00:11:00,785
Temos todas essas extrapolações

204
00:11:00,785 --> 00:11:06,470
e suposições amplas, e isso não vai ser
bom na nossa perda de teste.

205
00:11:06,470 --> 00:11:07,880
Veja isso.

206
00:11:07,880 --> 00:11:11,090
Nossa perda de teste está diminuindo,
de repente, e isso é ótimo.

207
00:11:21,310 --> 00:11:24,155
Certo, a função está cada vez
mais aprendida.

208
00:11:24,155 --> 00:11:28,760
No entanto, está indo muito devagar
devido ao tamanho dessa rede.

209
00:11:32,770 --> 00:11:34,725
Lembre-se, entre cada uma dessas camadas,

210
00:11:34,725 --> 00:11:37,845
há 64 ponderações entre cada uma.

211
00:11:37,845 --> 00:11:39,240
Como tenho seis camadas,

212
00:11:39,240 --> 00:11:41,850
eu tenho 6 vezes 64 ali.

213
00:11:41,850 --> 00:11:46,325
Não incluindo entre minha camada de
atributo e minha camada superior,

214
00:11:46,325 --> 00:11:48,640
em que recebo mais oito em cada.

215
00:11:51,690 --> 00:11:54,255
Aqui vamos nós, veja isto.
Isto é ótimo.

216
00:11:54,255 --> 00:11:56,530
Então, estou aprendendo
muito bem essa função.

217
00:11:56,530 --> 00:11:59,470
No entanto, há essas extrapolações,

218
00:11:59,470 --> 00:12:01,160
interpolações acontecendo aqui,

219
00:12:01,160 --> 00:12:04,810
como este pico laranja,
que atravessa a espiral.

220
00:12:08,890 --> 00:12:11,330
Ainda está melhorando com o tempo.

221
00:12:11,330 --> 00:12:13,950
Como você pode ver, a perda do teste
está diminuindo.

222
00:12:13,950 --> 00:12:17,910
No entanto, esta forma está
muito sobreajustada.

223
00:12:24,140 --> 00:12:26,190
Pronto. Como pode você pode ver,

224
00:12:26,190 --> 00:12:30,335
conseguimos, finalmente, encontrar as
formas de tudo isso, usando redes neurais.

225
00:12:30,335 --> 00:12:33,140
Às vezes, é um trabalho melhor

226
00:12:33,140 --> 00:12:36,200
ou o trabalho completo,
no caso da espiral,

227
00:12:36,200 --> 00:12:37,920
pois foi possível descobrir a forma.