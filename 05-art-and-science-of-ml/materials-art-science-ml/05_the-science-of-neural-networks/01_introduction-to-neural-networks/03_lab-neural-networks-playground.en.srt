1
00:00:00,000 --> 00:00:04,605
Now, to get our hands dirty by playing with Neural Networks intensible playground.

2
00:00:04,605 --> 00:00:06,585
In this Neural Networks playground lab,

3
00:00:06,585 --> 00:00:08,160
we will use intensible playground,

4
00:00:08,160 --> 00:00:10,450
to try and build Neural Networks to learn our data.

5
00:00:10,450 --> 00:00:13,290
I want you to solve these problems in two ways.

6
00:00:13,290 --> 00:00:17,640
First, we should try training our models using manual feature engineering,

7
00:00:17,640 --> 00:00:19,560
where we use our own knowledge to try and guess

8
00:00:19,560 --> 00:00:23,720
the right combination and transformation of features to learn the data.

9
00:00:23,720 --> 00:00:26,010
Next, we're going to hand over the reins,

10
00:00:26,010 --> 00:00:27,450
to the power of Neural Networks,

11
00:00:27,450 --> 00:00:29,790
and add more layers and neurons using a simple set of

12
00:00:29,790 --> 00:00:34,065
input features to see it can perform the feature engineering itself.

13
00:00:34,065 --> 00:00:35,925
Welcome back to intensible playground.

14
00:00:35,925 --> 00:00:38,310
In this lab, we're going to see if feature engineering

15
00:00:38,310 --> 00:00:41,565
can outperform our Neural Networks.

16
00:00:41,565 --> 00:00:44,820
I have a feeling this is not going be the case. Let's investigate.

17
00:00:44,820 --> 00:00:48,390
All right. In this diagram here,

18
00:00:48,390 --> 00:00:54,750
we're trying to classify these blue and orange dots.This is a classification problem.

19
00:00:54,750 --> 00:00:57,840
Which you'll notice, is that they look like two concentric circles.

20
00:00:57,840 --> 00:01:00,000
However, in this case there's a lot of noise.

21
00:01:00,000 --> 00:01:03,090
So, therefore it is a lot of intermixing going on here.

22
00:01:03,090 --> 00:01:09,340
Okay. What I'm going to try to do is let's see how X1 and X2 to do in training.

23
00:01:09,740 --> 00:01:14,400
As you can see, it's really not learning very much at all.

24
00:01:14,400 --> 00:01:17,400
It's all kind of blurred together, it's pretty white.

25
00:01:17,400 --> 00:01:19,640
So, therefore it's not one way or the other,

26
00:01:19,640 --> 00:01:22,060
according to the scale down here, negative 101.

27
00:01:22,060 --> 00:01:25,220
So, it hasn't learned much. Let's see if you can do better.

28
00:01:25,220 --> 00:01:28,095
With feature engineering, I know this is a circle.

29
00:01:28,095 --> 00:01:31,005
So, I do X1 squared and X2 squared,

30
00:01:31,005 --> 00:01:34,530
and I try it now, let's see.

31
00:01:34,530 --> 00:01:37,290
Wow! Look at that. Was going to ellipse.

32
00:01:37,290 --> 00:01:42,295
So, that means it's almost figuring out what this function is.

33
00:01:42,295 --> 00:01:44,470
We know it's a circle, but however,

34
00:01:44,470 --> 00:01:45,550
there's a lot of noise and everything.

35
00:01:45,550 --> 00:01:47,465
So, it kind of offshores a little bit.

36
00:01:47,465 --> 00:01:50,820
Okay. Perhaps though, I can get my loss

37
00:01:50,820 --> 00:01:53,900
lower from 0.275 when we try to get rid of X1 and X2,

38
00:01:53,900 --> 00:01:57,210
the linear forms. Let's try it now.

39
00:01:57,370 --> 00:02:00,060
Uh Huh, 2.85.

40
00:02:00,060 --> 00:02:02,715
Okay. So, it looks a little more circular.

41
00:02:02,715 --> 00:02:05,790
However, our test loss is slightly better.

42
00:02:05,790 --> 00:02:09,390
Let's see now if we can do the same with Neural Networks.

43
00:02:09,390 --> 00:02:12,525
So, let's go back to just X1 and X2,

44
00:02:12,525 --> 00:02:15,855
which as we saw earlier it did a really poor job.

45
00:02:15,855 --> 00:02:19,570
Let's add a hidden layer. Let's add two extra domains.

46
00:02:21,530 --> 00:02:27,975
As you can see here, it's having quite a hard time figuring out what this function is.

47
00:02:27,975 --> 00:02:31,950
The problem with that is, is that there is not enough capacity in these two neurons,

48
00:02:31,950 --> 00:02:35,715
enough high dimensional representation to learn this distribution.

49
00:02:35,715 --> 00:02:37,995
So, let's pause this here and see.

50
00:02:37,995 --> 00:02:39,150
Let's add another neuron.

51
00:02:39,150 --> 00:02:41,510
Maybe this is enough capacity to learn this function.

52
00:02:41,510 --> 00:02:43,250
Alright.

53
00:02:43,250 --> 00:02:49,635
It is still not quite getting it.

54
00:02:49,635 --> 00:02:53,985
Maybe, look at that.

55
00:02:53,985 --> 00:02:55,540
Took a long time,

56
00:02:55,540 --> 00:02:58,810
but it's slowly figuring out that shape of the function.

57
00:02:58,810 --> 00:03:02,540
That's some kind of rectangular shape here.

58
00:03:02,540 --> 00:03:06,440
So, what this means is that we're kind of re-sailing on the cusp right here

59
00:03:06,440 --> 00:03:10,355
of the amount of neurons able to represent this distribution here.

60
00:03:10,355 --> 00:03:13,540
So, let's see if we can easier time if we just add one extra neuron.

61
00:03:13,540 --> 00:03:17,070
Alright. Look at that.

62
00:03:17,070 --> 00:03:19,020
That was much much faster.

63
00:03:19,020 --> 00:03:21,000
We only have four neurons here.

64
00:03:21,000 --> 00:03:25,510
But let's see what happens if we add many many extra neurons.

65
00:03:25,700 --> 00:03:29,490
Alright. Let's see. Let's put a mold of four.

66
00:03:29,490 --> 00:03:32,260
See what happens.

67
00:03:32,260 --> 00:03:36,960
That's train already.

68
00:03:36,960 --> 00:03:38,460
It's quite a lot slower here.

69
00:03:38,460 --> 00:03:41,765
There is a lot more mass to perform going through all these semi-layers.

70
00:03:41,765 --> 00:03:44,005
I think it's eventually going to get it.

71
00:03:44,005 --> 00:03:47,290
I'm a little nervous it might overfit a little bit as you can see.

72
00:03:47,290 --> 00:03:50,675
It is no longer as a simple circular shape.

73
00:03:50,675 --> 00:03:52,480
Some crazy polygon here.

74
00:03:52,480 --> 00:03:56,110
So, therefore it's overfitting the data and not doing so well on the test loss.

75
00:03:56,110 --> 00:03:58,055
Which is much higher than it used to be.

76
00:03:58,055 --> 00:04:01,950
Alright. Let's see some other distributions.

77
00:04:02,030 --> 00:04:05,910
Here, we are distribution our classic Xr.

78
00:04:05,910 --> 00:04:09,260
When both X and Y are positive, or negative,

79
00:04:09,260 --> 00:04:14,240
we have blues and when they're either r we have the orange class.

80
00:04:14,240 --> 00:04:17,540
Let's see if we can learn this with just X1 X2.

81
00:04:20,240 --> 00:04:23,069
As you can see, just like before,

82
00:04:23,069 --> 00:04:27,515
Iq and X2 is not powerful enough to be able to describe this function.

83
00:04:27,515 --> 00:04:29,480
It's basically zero all across the board.

84
00:04:29,480 --> 00:04:33,120
Let's see if we can figure this out using feature engineering.

85
00:04:33,120 --> 00:04:35,580
With feature engineering, I'm going to

86
00:04:35,580 --> 00:04:38,475
introduce X1 X2 because I know that's what it looks like.

87
00:04:38,475 --> 00:04:42,150
So let's train this. Look at that.

88
00:04:42,150 --> 00:04:46,205
Very nice. It's a test loss of zero point one seven. That is great.

89
00:04:46,205 --> 00:04:49,010
Okay. Found it really easily,

90
00:04:49,010 --> 00:04:52,220
and here's my weight, 0.19. That's great.

91
00:04:52,220 --> 00:04:54,800
Yes there's been noise so we got some wrong,

92
00:04:54,800 --> 00:04:57,635
but for the most part it got pretty much right.

93
00:04:57,635 --> 00:05:00,240
Let's see now if Machine Learning,

94
00:05:00,240 --> 00:05:03,255
using Neural Networks, can do a better job.

95
00:05:03,255 --> 00:05:06,330
So we're going to put X1 and X2 back together,

96
00:05:06,330 --> 00:05:08,950
and let's add a hidden layer.

97
00:05:08,950 --> 00:05:10,850
So once again, I'm going to try to see.

98
00:05:10,850 --> 00:05:13,440
I want to have a small amount as I can.

99
00:05:13,440 --> 00:05:17,670
So I'm going try to get this down to just two neurons and learn this.

100
00:05:17,670 --> 00:05:19,920
However, as you can see,

101
00:05:19,920 --> 00:05:21,720
it's not able to figure that out.

102
00:05:21,720 --> 00:05:24,450
It's not enough complexity, not enough capacity in this model.

103
00:05:24,450 --> 00:05:27,240
So let's pause this here and try add the third neuron.

104
00:05:27,240 --> 00:05:30,910
Let's try training again.

105
00:05:32,690 --> 00:05:35,250
As you can see here,

106
00:05:35,250 --> 00:05:37,605
it's having quite a hard time learning this function.

107
00:05:37,605 --> 00:05:39,450
Now, maybe it's just on the edge,

108
00:05:39,450 --> 00:05:42,260
and I have to wait a little longer to see if it will learn it.

109
00:05:42,260 --> 00:05:44,340
But it's kind of stuck.

110
00:05:44,340 --> 00:05:46,875
Perhaps another initialization maybe will fix it.

111
00:05:46,875 --> 00:05:49,650
Let's see. There we go.

112
00:05:49,650 --> 00:05:52,065
So, we tried everything, running initialization,

113
00:05:52,065 --> 00:05:54,020
and it will somewhat learn the function here.

114
00:05:54,020 --> 00:05:57,930
It looks more like like a diagonal hourglass a bit, actually.

115
00:05:57,930 --> 00:06:00,540
However, that's not quite the function.

116
00:06:00,540 --> 00:06:02,115
You can see the loss is much higher.

117
00:06:02,115 --> 00:06:03,570
So, let's go to four,

118
00:06:03,570 --> 00:06:06,570
that might do the job. Let's see.

119
00:06:06,570 --> 00:06:10,040
Here we are still with the arrow glass but it's becoming

120
00:06:10,040 --> 00:06:12,800
more and more like a series of squares.

121
00:06:12,800 --> 00:06:15,470
Which is what our function actually is. It's getting better.

122
00:06:15,470 --> 00:06:18,620
Now, let's see if I add a whole bunch more,

123
00:06:18,620 --> 00:06:23,350
and see if we overfit it.

124
00:06:27,110 --> 00:06:31,575
As you can see, it's a lot slower in it's training loss.

125
00:06:31,575 --> 00:06:36,425
However, those are much more square shaped.

126
00:06:36,425 --> 00:06:43,120
This is looking great.

127
00:06:43,940 --> 00:06:48,375
Let's try another distribution type.

128
00:06:48,375 --> 00:06:50,910
Right here we have a spiral,

129
00:06:50,910 --> 00:06:53,090
two spirals actually spiraling around each other.

130
00:06:53,090 --> 00:06:55,685
Much like a picture of a Galaxy.

131
00:06:55,685 --> 00:06:58,810
So, let's see if we can train with X1 X2.

132
00:06:58,810 --> 00:07:01,080
I highly doubt we were able to.

133
00:07:01,080 --> 00:07:03,580
As you can see here,

134
00:07:03,580 --> 00:07:05,615
it really didn't learn the distribution at all.

135
00:07:05,615 --> 00:07:07,790
It's basically, pretty close to zero,

136
00:07:07,790 --> 00:07:09,515
and it can't decide what's what.

137
00:07:09,515 --> 00:07:12,930
So, what we can try is now some feature engineering.

138
00:07:12,930 --> 00:07:14,600
Let's try.

139
00:07:14,600 --> 00:07:16,335
What do you think?

140
00:07:16,335 --> 00:07:19,690
Let's try circles, perhaps?

141
00:07:19,730 --> 00:07:22,710
Nope. Let's try adding these.

142
00:07:22,710 --> 00:07:24,120
Will be sine and cosine,

143
00:07:24,120 --> 00:07:28,160
or sineX1 and sinex2. It's trying.

144
00:07:28,160 --> 00:07:31,570
I have six raw features going on here,

145
00:07:31,570 --> 00:07:33,045
and it's sort of getting in.

146
00:07:33,045 --> 00:07:34,230
As you can see up top,

147
00:07:34,230 --> 00:07:36,390
it's slowly gaining here.

148
00:07:36,390 --> 00:07:39,570
There's a big gap here. I don't know where it's going.

149
00:07:39,570 --> 00:07:43,370
It's really extrapolating very strongly here.

150
00:07:43,370 --> 00:07:45,000
So, it's not really a greater job,

151
00:07:45,000 --> 00:07:46,830
it's kind of stalled out as you can see.

152
00:07:46,830 --> 00:07:50,115
Let's see if we can do this better with Neural Networks.

153
00:07:50,115 --> 00:07:52,150
Less turn this all off,

154
00:07:52,150 --> 00:07:53,990
and else add a hidden layer.

155
00:07:53,990 --> 00:07:57,100
Once we get started off with two neurons and see if we can do it.

156
00:07:58,100 --> 00:08:03,515
As you can see here, it's really no much better than just having pure X1 and X2.

157
00:08:03,515 --> 00:08:06,050
It's not enough capacity to learn this model.

158
00:08:06,050 --> 00:08:09,870
Let's go to three. See if you can learn.

159
00:08:11,600 --> 00:08:16,615
It's doing slightly better than last time with extrapolation right here.

160
00:08:16,615 --> 00:08:18,880
However, it's still not doing as good as just saving

161
00:08:18,880 --> 00:08:23,455
all six features activated, or seven features.

162
00:08:23,455 --> 00:08:27,835
Alright. Let's see if we can add one more neuron,

163
00:08:27,835 --> 00:08:33,220
another layer perhaps. Let's see if this can do it.

164
00:08:33,470 --> 00:08:37,340
All ready. So, you can see we have

165
00:08:37,340 --> 00:08:40,290
a really low train loss for the test losses and doing so well.

166
00:08:40,290 --> 00:08:45,300
So, it's kind of stuck.

167
00:08:45,300 --> 00:08:48,755
Let's try some more. Let's add some more hidden layers.

168
00:08:48,755 --> 00:08:52,040
Let's all put them to four.

169
00:08:52,040 --> 00:08:53,180
Hopefully that's enough.

170
00:08:53,180 --> 00:08:56,605
Let's see what we get.

171
00:08:56,605 --> 00:08:59,170
Alright. Both have gone down quite a bit.

172
00:08:59,170 --> 00:09:04,035
However, it hasn't quite made a decision since the whole screen is white.

173
00:09:04,035 --> 00:09:07,190
There it is. I have an inflection point.

174
00:09:07,190 --> 00:09:09,630
My loss is going down a lot.

175
00:09:10,490 --> 00:09:16,350
However, you can see my test loss is also going up.

176
00:09:16,350 --> 00:09:19,660
Now, it's going flat. So, this doesn't have enough capacity.

177
00:09:19,660 --> 00:09:24,350
So, let's go as far as we can go and add eight neurons each layer.

178
00:09:24,350 --> 00:09:29,290
And hopefully, that's enough to learn this very complex and noisy function.

179
00:09:29,290 --> 00:09:32,720
Alright.

180
00:09:32,720 --> 00:09:37,230
Let's try training this.

181
00:09:37,230 --> 00:09:40,920
As you can see, it's going very slow when it's joining this train right here.

182
00:09:40,920 --> 00:09:45,880
We're hoping that will finally figure out a way to make this function work.

183
00:09:46,640 --> 00:09:49,110
So, my train loss is going down.

184
00:09:49,110 --> 00:09:51,670
How about my test loss? Is going up.

185
00:10:00,220 --> 00:10:04,220
It's kind of leveling out, my test loss.

186
00:10:05,290 --> 00:10:07,480
When you're doing this yourself,

187
00:10:07,480 --> 00:10:09,575
your results might vary quite a bit due to

188
00:10:09,575 --> 00:10:13,170
random initialisations of the network. Let's try a different one.

189
00:10:13,170 --> 00:10:20,250
This one might be a little bit more promising maybe.

190
00:10:25,700 --> 00:10:29,430
Right, this is looking a little more promising.

191
00:10:29,430 --> 00:10:36,000
So, you can see what it is doing, it's learning these ways over here. Fill it in.

192
00:10:36,560 --> 00:10:43,630
What up? Looks like we overfit because now our test loss is diverging, it's not good.

193
00:10:49,340 --> 00:10:51,840
And there you go.

194
00:10:51,840 --> 00:10:53,565
So, as you can see,

195
00:10:53,565 --> 00:10:55,245
even with this mountain network,

196
00:10:55,245 --> 00:10:59,000
we are not able to learn that distribution very well.

197
00:10:59,000 --> 00:11:00,785
We have all these extrapolations,

198
00:11:00,785 --> 00:11:06,470
and broad guessing, and that's not going to do very well in our test loss.

199
00:11:06,470 --> 00:11:07,880
Look at this.

200
00:11:07,880 --> 00:11:12,030
Our test loss is going down all of a sudden. This is

201
00:11:19,940 --> 00:11:24,155
great. Alright. It's getting more and more of the function learned.

202
00:11:24,155 --> 00:11:29,170
However, it's going very slow due to how large this network is.

203
00:11:32,000 --> 00:11:34,725
Remember, between each one of these layers,

204
00:11:34,725 --> 00:11:37,845
there is 64 weights between each one.

205
00:11:37,845 --> 00:11:39,240
They have six layers,

206
00:11:39,240 --> 00:11:41,850
I mean they have six times 64, just there.

207
00:11:41,850 --> 00:11:45,885
Not including between my features layer and my upper layer.

208
00:11:45,885 --> 00:11:49,000
Where I get another eight each.

209
00:11:50,810 --> 00:11:54,255
Well, there you go. Look at this. This is great.

210
00:11:54,255 --> 00:11:56,530
So, I'm learning this function pretty well.

211
00:11:56,530 --> 00:11:59,470
However, there are these extrapolations,

212
00:11:59,470 --> 00:12:00,670
interpolations going on here,

213
00:12:00,670 --> 00:12:05,300
where like this orange peak goes right through the spiral.

214
00:12:07,430 --> 00:12:11,300
It's still getting slightly better and better over time.

215
00:12:11,300 --> 00:12:13,640
As you can see, the test loss is getting lower and lower.

216
00:12:13,640 --> 00:12:18,210
However, this shape is very much overfit.

217
00:12:23,030 --> 00:12:26,170
Now ready. As you can see,

218
00:12:26,170 --> 00:12:30,335
we were finally able to find the shapes of all of these, using Neural Networks,

219
00:12:30,335 --> 00:12:33,140
which is sometimes a better job,

220
00:12:33,140 --> 00:12:36,200
or the job at all in the case of the spiral,

221
00:12:36,200 --> 00:12:38,200
was able to figure out its shape.