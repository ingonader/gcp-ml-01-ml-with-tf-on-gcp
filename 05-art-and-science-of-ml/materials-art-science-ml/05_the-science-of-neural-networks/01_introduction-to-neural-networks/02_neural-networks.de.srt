1
00:00:00,000 --> 00:00:03,240
Neuronale Netzwerke wurden 
in anderen Kursen schon behandelt.

2
00:00:03,240 --> 00:00:05,175
Jetzt wollen wir
in die Tiefe gehen.

3
00:00:05,175 --> 00:00:09,825
Feature Crosses haben sich für
das folgende Problem bereits bewährt:

4
00:00:09,825 --> 00:00:12,285
Wenn x1 die horizontale Dimension und

5
00:00:12,285 --> 00:00:14,220
x2 die vertikale Dimension ist,

6
00:00:14,220 --> 00:00:18,850
ließ sich die Verteilung durch keine lineare
Kombination der beiden Features beschreiben.

7
00:00:18,940 --> 00:00:23,310
Erst mit Feature Engineering 
und der Kreuzung von x1 und x2

8
00:00:23,310 --> 00:00:26,495
zum neuen Feature x3,
das x1 mal x2 entspricht,

9
00:00:26,495 --> 00:00:30,030
konnten wir 
unsere Datenverteilung beschreiben.

10
00:00:30,030 --> 00:00:33,200
Das heißt also, dass wir 
mit manuellem Feature Engineering

11
00:00:33,200 --> 00:00:36,100
mühelos alle nichtlinearen 
Probleme lösen können.

12
00:00:36,100 --> 00:00:39,855
Ist das richtig? 
Leider gibt es in der realen Welt

13
00:00:39,855 --> 00:00:42,735
fast nie solche einfach
zu beschreibenden Verteilungen.

14
00:00:42,735 --> 00:00:46,655
Obwohl seit Jahren die klügsten
Köpfe an Feature Engineering arbeiten,

15
00:00:46,655 --> 00:00:48,495
sind der Methode Grenzen gesetzt.

16
00:00:48,495 --> 00:00:52,950
Welche Feature Crosses brauchen Sie z. B.,
um diese Verteilung zu modellieren?

17
00:00:52,950 --> 00:00:57,945
Das sieht aus wie zwei Kreise 
übereinander oder vielleicht zwei Spiralen.

18
00:00:57,945 --> 00:01:00,630
Was immer es ist, es ist sehr chaotisch.

19
00:01:00,630 --> 00:01:04,370
Diese Art von Beispielen zeigt
den Nutzen neuronaler Netzwerke.

20
00:01:04,370 --> 00:01:08,650
Sie können algorithmisch komplexe Feature
Crosses und Transformationen erstellen.

21
00:01:08,650 --> 00:01:12,400
Es gibt sehr viel 
komplexere Räume als diese Spirale,

22
00:01:12,400 --> 00:01:16,380
die den Einsatz
neuronaler Netze erforderlich machen.

23
00:01:16,790 --> 00:01:21,760
Als Alternative zu Feature Crossing 
können neuronale Netze Features kombinieren.

24
00:01:21,760 --> 00:01:24,920
Beim Entwurf 
der Architektur des neuronalen Netzes

25
00:01:24,920 --> 00:01:28,980
möchten wir das Modell so strukturieren,
dass Features kombiniert werden.

26
00:01:29,430 --> 00:01:32,975
Dann möchten wir auf einer weiteren
Schicht die Kombinationen kombinieren

27
00:01:32,975 --> 00:01:36,980
und dann noch eine Schicht hinzufügen,
um diese Kombinationen zu kombinieren usw.

28
00:01:36,980 --> 00:01:39,090
Wie wählen wir 
die richtigen Kombinationen

29
00:01:39,090 --> 00:01:42,315
der Features und die Kombinationen
der kombinierten Features usw.?

30
00:01:42,625 --> 00:01:45,570
Das Modell lernt 
das Ganze durch Training.

31
00:01:45,780 --> 00:01:49,095
Das ist der Grundgedanke
hinter neuronalen Netzwerken.

32
00:01:49,095 --> 00:01:52,410
Der Ansatz ist nicht
unbedingt besser als Feature Crossing.

33
00:01:52,410 --> 00:01:56,660
Er ist aber eine flexible Alternative, 
die oftmals gute Ergebnisse liefert.

34
00:01:57,060 --> 00:02:00,330
Hier eine grafische
Darstellung eines linearen Modells.

35
00:02:00,330 --> 00:02:03,025
Es gibt drei Eingaben: 
x1, x2 und x3.

36
00:02:03,025 --> 00:02:05,885
Sie sind durch blaue Kreise dargestellt.

37
00:02:05,885 --> 00:02:08,829
Sie werden mit Gewichtungen 
von den Rändern kombiniert,

38
00:02:08,829 --> 00:02:10,319
um eine Ausgabe zu erzeugen.

39
00:02:10,319 --> 00:02:12,620
Oft gibt es zusätzlich einen Bias-Term.

40
00:02:12,620 --> 00:02:14,955
Zur Vereinfachung
wird er hier nicht gezeigt.

41
00:02:14,955 --> 00:02:20,045
Dies ist ein lineares Modell. 
Es ist eine Form von y gleich w1 mal x1,

42
00:02:20,045 --> 00:02:22,010
plus w2 mal x2,

43
00:02:22,010 --> 00:02:23,915
plus w3 mal x3.

44
00:02:23,915 --> 00:02:28,200
Nun fügen wir eine Zwischenschicht
in das Netzwerk der Knoten und Kanten ein.

45
00:02:28,350 --> 00:02:32,640
Die Eingabeschicht hat drei Knoten,
die Zwischenschicht ebenfalls drei.

46
00:02:32,640 --> 00:02:34,970
Das sind aber verborgene Knoten.

47
00:02:35,530 --> 00:02:38,325
Da diese Schicht 
vollständig verbunden ist,

48
00:02:38,325 --> 00:02:41,980
gibt es drei mal drei Kanten
oder neun Gewichtungen.

49
00:02:41,980 --> 00:02:44,590
Das ist nun sicher 
ein nichtlineares Modell,

50
00:02:44,590 --> 00:02:47,735
mit dem wir unsere 
nichtlinearen Probleme lösen können.

51
00:02:47,735 --> 00:02:50,800
Stimmts?
Leider nicht. Warum?

52
00:02:51,620 --> 00:02:54,775
Als Eingabe 
in den ersten verborgenen Knoten

53
00:02:54,775 --> 00:02:58,865
dient die gewichtete 
Summe von w1 mal x1, plus w4 mal x2,

54
00:02:58,865 --> 00:03:00,890
plus w7 mal x3.

55
00:03:01,820 --> 00:03:05,010
Die Eingabe 
in den zweiten verborgenen Knoten

56
00:03:05,010 --> 00:03:10,395
ist die gewichtete Summe w2 mal x1 
plus w5 mal x2 plus w8 mal x3.

57
00:03:10,575 --> 00:03:14,060
Als Eingabe 
in den dritten verborgenen Knoten

58
00:03:14,060 --> 00:03:19,385
dient die gewichtete Summe w3 mal x1 
plus w6 mal x2 plus w9 mal x3.

59
00:03:20,205 --> 00:03:23,035
Bei der Kombination im Ausgabeknoten

60
00:03:23,035 --> 00:03:25,685
ergibt sich w10 mal h1,

61
00:03:25,685 --> 00:03:28,080
plus w11 mal h2,

62
00:03:28,080 --> 00:03:30,225
plus w12 mal h3.

63
00:03:30,605 --> 00:03:32,040
Sie erinnern sich:

64
00:03:32,040 --> 00:03:37,160
h1, h2 und h3 sind nur lineare
Kombinationen der Eingabefeatures.

65
00:03:37,760 --> 00:03:40,395
Wenn wir sie erweitern, 
erhalten wir daher

66
00:03:40,395 --> 00:03:43,345
einen komplexen Satz 
an gewichteten Konstanten

67
00:03:43,345 --> 00:03:47,220
multipliziert mit 
den einzelnen Eingabewerten x1, x2 und x3.

68
00:03:48,090 --> 00:03:51,925
Jedes Paar an Gewichtungen kann 
durch eine neue Gewichtung ersetzt werden.

69
00:03:51,925 --> 00:03:53,420
Kommt Ihnen das bekannt vor?

70
00:03:53,540 --> 00:03:56,530
Das ist genau dasselbe
lineare Modell wie zuvor.

71
00:03:56,530 --> 00:03:59,635
Außer der zusätzlichen 
Zwischenschicht mit Neuronen.

72
00:03:59,695 --> 00:04:00,995
Was ist passiert?

73
00:04:01,185 --> 00:04:04,380
Was passiert, wenn wir eine
weitere Zwischenschicht einfügen?

74
00:04:04,380 --> 00:04:07,680
Leider fällt auch
dieses Modell in sich zusammen

75
00:04:07,680 --> 00:04:11,925
zu einer Matrix mit einer Gewichtung
multipliziert jeweils mit den drei Eingaben.

76
00:04:11,925 --> 00:04:13,790
Es ist dasselbe lineare Modell.

77
00:04:13,910 --> 00:04:16,620
Wir können diesen Prozess 
immer weiter fortführen.

78
00:04:16,620 --> 00:04:18,450
Das Ergebnis wäre immer dasselbe.

79
00:04:18,600 --> 00:04:22,780
Allerdings wären Training 
und Vorhersage viel rechenintensiver

80
00:04:22,780 --> 00:04:26,030
für eine weitaus 
komplexere Architektur als nötig.

81
00:04:27,150 --> 00:04:29,380
Aus Sicht der linearen Algebra

82
00:04:29,380 --> 00:04:33,255
werden mehrere Matrizen
in einer Kette multipliziert.

83
00:04:33,885 --> 00:04:36,395
Hier multipliziere ich
eine Drei-mal-drei-Matrix,

84
00:04:36,395 --> 00:04:39,040
also die transponierte 
Gewichtungsmatrix zwischen

85
00:04:39,040 --> 00:04:41,090
Eingabe- und 
erster Zwischenschicht,

86
00:04:41,090 --> 00:04:45,170
mit dem Drei-mal-eins-Eingabevektor.
Heraus kommt der Drei-mal-eins-Vektor.

87
00:04:45,170 --> 00:04:49,275
Das sind die Werte in den einzelnen
verborgenen Neuronen in Zwischenschicht eins.

88
00:04:49,275 --> 00:04:52,405
Zur Definition der Werte 
der Neuronen der zweiten Zwischenschicht

89
00:04:52,405 --> 00:04:56,090
habe ich die transponierte 
Drei-mal-drei-Gewichtungsmatrix multipliziert,

90
00:04:56,090 --> 00:04:58,615
die Zwischenschicht eins 
mit Zwischenschicht zwei

91
00:04:58,615 --> 00:05:01,590
zum resultierenden Vektor 
in Zwischenschicht eins verbindet.

92
00:05:01,610 --> 00:05:05,050
Man kann jetzt
beide Drei-mal-drei-Gewichtungsmatrizen

93
00:05:05,050 --> 00:05:06,880
zu einer zusammenfassen.

94
00:05:06,970 --> 00:05:10,580
Dafür wird zuerst
das Matrixprodukt von links innen

95
00:05:10,580 --> 00:05:12,570
statt von rechts berechnet.

96
00:05:12,570 --> 00:05:15,190
Wir erhalten immer
noch dieselbe Form für h2,

97
00:05:15,190 --> 00:05:18,360
den Wertevektor des Neurons 
der zweiten Zwischenschicht.

98
00:05:18,470 --> 00:05:21,435
Wenn ich die finale Schicht 
zwischen zweiter Zwischenschicht

99
00:05:21,435 --> 00:05:23,075
und Ausgabeschicht hinzufüge,

100
00:05:23,075 --> 00:05:25,770
muss ich alles mit 
der transponierten Gewichtungsmatrix

101
00:05:25,770 --> 00:05:28,490
zwischen den letzten 
beiden Schichten multiplizieren.

102
00:05:28,580 --> 00:05:31,740
Auch wenn Sie beim Einspeisen 
durch ein neuronales Netzwerk

103
00:05:31,740 --> 00:05:34,350
die Matrix von 
rechts nach links multiplizieren,

104
00:05:34,350 --> 00:05:36,730
indem Sie sie 
von links nach rechts anwenden,

105
00:05:36,730 --> 00:05:38,670
sehen Sie, dass unsere lange Kette

106
00:05:38,670 --> 00:05:43,050
der Matrixverflechtungen zu einem
Vektor mit nur drei Werten kollabiert.

107
00:05:43,160 --> 00:05:46,480
Wenn Sie dieses Modell für eine
einfache lineare Regression von drei

108
00:05:46,480 --> 00:05:50,760
Gewichtungen nebeneinander trainieren und
beide unten beim selben Minimum ankommen,

109
00:05:50,760 --> 00:05:54,865
wird sich die Matrixproduktkette trotz des
massiven Rechenaufwands zur Berechnung

110
00:05:54,865 --> 00:05:58,179
aller 21 Gewichtungen zur
unteren Gleichung verdichten.

111
00:05:58,179 --> 00:06:02,590
Die Gewichtung wird exakt den einfachen
linearen Regressionsgewichten entsprechen.

112
00:06:02,590 --> 00:06:05,205
Der ganze Aufwand für dasselbe Ergebnis.

113
00:06:05,205 --> 00:06:07,235
Sie mögen jetzt denken, "Wie bitte?

114
00:06:07,235 --> 00:06:11,280
Geht es bei neuronalen Netzen nicht darum,
schichtweise Neuronen einzufügen?

115
00:06:11,280 --> 00:06:16,005
Wie kann ich Deep Learning ausführen, 
wenn alle Schichten zu einer zusammenfallen?"

116
00:06:16,005 --> 00:06:18,820
Dann sage ich Ihnen:
Es gibt eine einfache Lösung.

117
00:06:19,025 --> 00:06:22,610
Wir fügen eine nichtlineare
Transformationsschicht hinzu.

118
00:06:22,610 --> 00:06:26,630
Dafür verwenden wir 
eine nichtlineare Aktivierungsfunktion

119
00:06:26,630 --> 00:06:28,530
wie Sigmoid, Tanh oder ReLU.

120
00:06:28,530 --> 00:06:31,550
Für die Grafik ist, 
genau wie in TensorFlow,

121
00:06:31,550 --> 00:06:35,580
vorstellbar, dass jedes Neuron
tatsächlich zwei Knoten hat.

122
00:06:35,580 --> 00:06:39,725
Der erste Knoten ist das Ergebnis
der gewichteten Summe aus wx plus b,

123
00:06:39,725 --> 00:06:41,865
der zweite das Ergebnis der Übergabe

124
00:06:41,865 --> 00:06:44,430
des Ergebnisses 
an die Aktivierungsfunktion.

125
00:06:44,430 --> 00:06:47,000
Es gibt also
Eingaben der Aktivierungsfunktion

126
00:06:47,000 --> 00:06:49,770
gefolgt von den 
Ausgaben der Aktivierungsfunktion.

127
00:06:49,770 --> 00:06:53,615
Die Aktivierungsfunktion
fungiert also als Übergangspunkt.

128
00:06:53,735 --> 00:06:56,540
Nur durch Aufnahme dieser 
nichtlinearen Transformation

129
00:06:56,540 --> 00:06:58,630
verhindern wir, 
dass das neuronale Netz

130
00:06:58,630 --> 00:07:00,810
zu einem flachen Netz komprimiert wird.

131
00:07:00,810 --> 00:07:04,240
Wenn es eine Schicht mit
nichtlinearen Aktivierungsfunktionen gibt,

132
00:07:04,240 --> 00:07:06,135
an anderer Stelle aber zwei oder mehr

133
00:07:06,135 --> 00:07:11,915
Schichten mit linearen Aktivierungsfunktionen,
können sie zu einem Netz verdichtet werden.

134
00:07:11,915 --> 00:07:14,840
In der Regel sind in 
neuronalen Netzwerken alle Schichten,

135
00:07:14,840 --> 00:07:17,840
von der ersten bis
zur vorletzten Schicht, nichtlinear.

136
00:07:17,840 --> 00:07:21,245
Die letzte Transformationsschicht
ist dann linear für Regression,

137
00:07:21,245 --> 00:07:25,175
Sigmoid- und Softmax-Funktionen, 
die ich bei der Klassifizierung bespreche.

138
00:07:25,175 --> 00:07:27,620
Alles hängt 
von der geplanten Ausgabe ab.

139
00:07:27,620 --> 00:07:31,040
Beleuchten wir das Problem noch 
einmal aus Sicht der linearen Algebra:

140
00:07:31,040 --> 00:07:34,700
Bei Anwendung von linearer 
Transformation auf Matrizen oder Vektoren

141
00:07:34,700 --> 00:07:39,540
werden Matrizen oder Vektoren multipliziert,
um das gewünschte Ergebnis zu erhalten.

142
00:07:39,540 --> 00:07:41,710
Genauso kann ich 
Matrizen für die Skalierung

143
00:07:41,710 --> 00:07:43,475
mit einer Konstanten multiplizieren.

144
00:07:43,475 --> 00:07:46,749
Eigentlich werden sie aber
mit einer Identitätsmatrix multipliziert,

145
00:07:46,749 --> 00:07:48,880
die mit der Konstanzen multipliziert wird.

146
00:07:48,880 --> 00:07:52,600
Es ergibt sich eine Diagonalmatrix
mit der Konstanten in der Diagonalen.

147
00:07:52,600 --> 00:07:55,690
Sie würde zu einem 
einfachen Matrixprodukt kollabiert.

148
00:07:55,690 --> 00:07:59,070
Füge ich jedoch eine Nichtlinearität ein,

149
00:07:59,070 --> 00:08:02,560
lässt sich der Vorgang
nicht in einer Matrix darstellen,

150
00:08:02,560 --> 00:08:05,800
da ich die Funktion
elementweise auf die Eingabe anwende.

151
00:08:05,800 --> 00:08:08,800
Beispiel: Ich habe 
eine nichtlineare Aktivierungsfunktion

152
00:08:08,800 --> 00:08:11,025
zwischen erster 
und zweiter Zwischenschicht.

153
00:08:11,025 --> 00:08:13,120
Ich wende hier
eine Funktion des Produkts

154
00:08:13,120 --> 00:08:17,375
aus transponierter Gewichtungsmatrix der 
ersten Zwischenschicht und Eingabevektor an.

155
00:08:17,375 --> 00:08:20,740
Die niedrigere Gleichung ist die
Aktivierungsfunktion in ReLU.

156
00:08:20,740 --> 00:08:24,560
Da ich die Transformation nicht mithilfe
der linearen Algebra darstellen kann,

157
00:08:24,560 --> 00:08:27,890
kann dieser Teil der Transformationskette 
nicht kollabiert werden.

158
00:08:27,890 --> 00:08:30,800
Die Komplexität 
des Modells bleibt also erhalten

159
00:08:30,800 --> 00:08:34,385
und wird nicht in eine rein lineare
Kombination der Eingaben überführt.

160
00:08:34,385 --> 00:08:38,299
Die Gewichtungsmatrizen der zweiten
Zwischenschicht und der Ausgabeschicht

161
00:08:38,299 --> 00:08:42,909
können weiter kollabiert werden, da hier
keine nichtlineare Funktion angewendet wird.

162
00:08:42,909 --> 00:08:46,970
Das heißt, zwei oder mehr
aufeinanderfolgende lineare Schichten

163
00:08:46,970 --> 00:08:51,515
können unabhängig von ihrer Anzahl
immer zu einer Schicht reduziert werden.

164
00:08:51,515 --> 00:08:55,105
Wenn Sie also komplexe Funktionen
in Ihrem Netzwerk erstellen möchten,

165
00:08:55,105 --> 00:08:58,630
sind lineare Aktivierungsfunktionen 
im gesamten Netzwerk zu empfehlen,

166
00:08:58,630 --> 00:09:02,770
außer in der letzten Schicht, wenn Sie zum
Schluss eine andere Ausgabe benötigen.

167
00:09:03,150 --> 00:09:06,075
Warum sind nichtlineare 
Aktivierungsfunktionen

168
00:09:06,075 --> 00:09:08,035
in neuronalen Netzen wichtig?

169
00:09:08,035 --> 00:09:10,200
Sie verhindern, dass die Schichten

170
00:09:10,200 --> 00:09:12,985
zu einem rein
linearen Modell reduziert werden.

171
00:09:12,985 --> 00:09:15,415
Sie unterstützen nicht nur die Erstellung

172
00:09:15,415 --> 00:09:18,400
interessanter Transformationen
im Featureraum,

173
00:09:18,400 --> 00:09:21,260
sondern auch tiefe
kompositorische Funktionen.

174
00:09:21,260 --> 00:09:24,280
Wie erläutert, lässt sich 
bei zwei oder mehr Schichten

175
00:09:24,280 --> 00:09:28,330
mit linearen Aktivierungsfunktionen
das Produkt der Matrizen zusammenzufassen

176
00:09:28,330 --> 00:09:31,560
als eine Matrix multipliziert
mit dem Eingabe-Feature-Vektor.

177
00:09:31,830 --> 00:09:36,210
Das resultierende Modell ist dann 
langsamer und berechnungsintensiver

178
00:09:36,210 --> 00:09:38,435
aber funktional weniger komplex.

179
00:09:38,605 --> 00:09:41,050
Nichtlinearitäten 
fügen der Verlustfunktion

180
00:09:41,050 --> 00:09:45,015
keine Regularisierung hinzu und 
führen nicht zur vorzeitigen Beendigung.

181
00:09:45,015 --> 00:09:46,930
Nichtlineare Aktivierungsfunktionen

182
00:09:46,930 --> 00:09:49,710
erzeugen zwar komplexe 
Transformationen im Vektorraum,

183
00:09:49,710 --> 00:09:53,200
die Dimension ändert sich jedoch nicht.
Der Vektorraum bleibt gleich.

184
00:09:53,200 --> 00:09:56,530
Auch wenn er gedehnt,
gedrückt oder gedreht wird.

185
00:09:57,120 --> 00:09:59,020
Wie in einem früheren Kurs erwähnt,

186
00:09:59,020 --> 00:10:02,170
gibt es viele nichtlineare
Aktivierungsfunktionen mit Sigmoid

187
00:10:02,170 --> 00:10:04,790
und der skalierten
und verschobenen Sigmoid-Funktion.

188
00:10:04,790 --> 00:10:07,915
Hier war der hyperbolische 
Tangens eine der ersten Funktionen.

189
00:10:07,915 --> 00:10:09,915
Wie erwähnt, 
kann es zur Sättigung

190
00:10:09,915 --> 00:10:13,310
und damit zum Problem
der verschwindenden Gradienten kommen.

191
00:10:13,310 --> 00:10:14,670
Bei Nullgradienten werden

192
00:10:14,670 --> 00:10:17,945
die Modellgewichtungen nicht
aktualisiert und das Training stoppt.

193
00:10:17,945 --> 00:10:20,320
Die rektifizierte
Lineareinheit, kurz ReLU,

194
00:10:20,320 --> 00:10:24,565
gehört zu meinen Lieblingsfunktionen,
denn sie ist einfach und funktioniert gut.

195
00:10:24,565 --> 00:10:26,660
In der positiven Domain ist sie linear,

196
00:10:26,660 --> 00:10:30,440
sodass es keine Sättigung gibt, 
in der negativen Domain ist sie null.

197
00:10:30,770 --> 00:10:35,595
In Netzen mit ReLU als Aktivierung
in der Zwischenschicht ist das Training

198
00:10:35,595 --> 00:10:39,050
oft 10 mal schneller 
als in Netzen mit Sigmoid.

199
00:10:39,050 --> 00:10:42,320
Da jedoch die Funktion
in negativen Domains immer null ist,

200
00:10:42,320 --> 00:10:45,210
kann es passieren,
dass die echten Schichten sterben.

201
00:10:45,210 --> 00:10:46,910
Was meine ich damit?

202
00:10:46,910 --> 00:10:48,260
Wenn Sie die ersten Eingaben

203
00:10:48,260 --> 00:10:51,845
in der negativen Domain erhalten
und die Ausgabe der Aktivierung null ist,

204
00:10:51,845 --> 00:10:55,950
hilft das in der nächsten Schicht nicht, 
Eingaben in der positiven Domain zu erhalten.

205
00:10:55,950 --> 00:10:58,960
Das Problem potenziert sich 
und erzeugt viele Nullaktivierungen

206
00:10:58,960 --> 00:11:02,270
bei der Backpropagation, bei der
die Gewichtungen aktualisiert werden,

207
00:11:02,270 --> 00:11:05,670
denn die Ableitung der Fehler 
wird mit ihrer Aktivierung multipliziert.

208
00:11:05,670 --> 00:11:07,310
Heraus kommt ein Nullgradient.

209
00:11:07,310 --> 00:11:10,100
Ergebnis: eine Gewichtung 
von null, die Gewichtungen

210
00:11:10,100 --> 00:11:13,800
ändern sich nicht und das Training
schlägt für diese Schicht fehl.

211
00:11:13,800 --> 00:11:17,220
Zum Glück wurden viele
clevere Methoden entwickelt,

212
00:11:17,220 --> 00:11:20,980
die ReLu so modifizieren,
dass das Training nicht zum Stocken kommt,

213
00:11:20,980 --> 00:11:24,120
und trotzdem viele Vorteile 
von vanilla ReLu genutzt werden.

214
00:11:24,120 --> 00:11:25,920
Hier sehen wir nochmal vanilla ReLu.

215
00:11:25,920 --> 00:11:28,510
Der max-Operator kann 
auch durch die abschnittsweise

216
00:11:28,510 --> 00:11:30,270
lineare Gleichung dargestellt werden.

217
00:11:30,270 --> 00:11:33,060
Hier ergibt die Funktion 
bei kleiner als null "null"

218
00:11:33,060 --> 00:11:36,190
und bei größer als 
oder gleich null ergibt die Funktion "X".

219
00:11:36,190 --> 00:11:38,520
Eine reibungslose
Approximation der ReLU-Funktion

220
00:11:38,520 --> 00:11:41,865
ist die analytische Funktion des
natürlichen Logarithmus von eins,

221
00:11:41,865 --> 00:11:43,185
plus das exponenzielle X.

222
00:11:43,185 --> 00:11:45,360
Das ist die Softplus-Funktion.

223
00:11:45,630 --> 00:11:50,150
Interessanterweise ist die Ableitung der
Softplus-Funktion eine logistische Funktion.

224
00:11:50,150 --> 00:11:52,210
Die Vorteile der Softplus-Funktion:

225
00:11:52,210 --> 00:11:54,570
Sie ist stetig und 
bei Null differenzierbar,

226
00:11:54,570 --> 00:11:56,380
im Gegensatz zur ReLu-Funktion.

227
00:11:56,380 --> 00:11:59,449
Aufgrund des natürlichen
Logarithmus und der Exponentialfunktion

228
00:11:59,449 --> 00:12:02,605
gibt es im Vergleich zu ReLUs
zusätzlichen Berechnungsaufwand

229
00:12:02,605 --> 00:12:06,030
und ReLUs erzielen in der Praxis
immer noch genauso gute Ergebnisse.

230
00:12:06,030 --> 00:12:10,195
Daher wird in der Regel von 
Softplus für Deep Learning abgeraten.

231
00:12:10,195 --> 00:12:14,895
Um das Problem der sterbenden ReLUs 
aufgrund von Nullaktivierungen zu lösen,

232
00:12:14,895 --> 00:12:16,995
wurde Leaky ReLU entwickelt.

233
00:12:16,995 --> 00:12:20,975
Wie ReLUs haben auch Leaky ReLUs
eine abschnittsweise lineare Funktion.

234
00:12:21,215 --> 00:12:24,090
In der negativen Domain gibt es jedoch

235
00:12:24,090 --> 00:12:28,115
statt null eine Neigung
ungleich null, genauer 0,01.

236
00:12:28,235 --> 00:12:31,005
Auch wenn die Einheit nicht aktiviert ist,

237
00:12:31,005 --> 00:12:35,389
ermöglichen Leaky ReLUs kleinen
Gradienten ungleich null den Durchgang,

238
00:12:35,389 --> 00:12:40,000
sodass die Gewichtungen aktualisiert
und die Trainingserfolge fortgesetzt werden.

239
00:12:40,650 --> 00:12:46,290
Eine Weiterentwicklung von Leaky ist
die parametrische ReLU oder PReLU.

240
00:12:46,730 --> 00:12:51,530
Statt willkürlich ein Hundertstel eines X
in die negative Domain durchzulassen,

241
00:12:51,530 --> 00:12:53,990
lässt diese Funktion Alpha von X durch.

242
00:12:54,150 --> 00:12:57,190
Was aber ist der Parameter Alpha?

243
00:12:57,190 --> 00:13:01,585
Im Grafen stelle ich Alpha
für Visualisierungszwecke auf 0,5 ein.

244
00:13:01,585 --> 00:13:04,520
In der Praxis handelt es sich 
tatsächlich um einen Parameter,

245
00:13:04,520 --> 00:13:07,735
der im Training zusammen 
mit den anderen Parametern erlernt wird.

246
00:13:08,015 --> 00:13:11,045
So müssen wir diesen Wert
nicht selbst einrichten.

247
00:13:11,045 --> 00:13:14,440
Er wird während des Trainings
über die Daten ermittelt.

248
00:13:14,440 --> 00:13:18,910
Der erlernte Wert sollte besser sein,
als der über die Priorität eingerichtete.

249
00:13:18,910 --> 00:13:22,690
Beachten Sie, dass bei Alpha 
unter eins die Formel mit dem max-Wert

250
00:13:22,690 --> 00:13:25,590
in die kompakte Form 
zurückgeschrieben werden kann.

251
00:13:25,590 --> 00:13:28,480
Genauer, der max-Wert
von x oder Alpha mal x.

252
00:13:28,480 --> 00:13:32,360
Es gibt auch randomisierte Leaky ReLUs, 
bei denen Alpha nicht trainiert,

253
00:13:32,360 --> 00:13:36,140
sondern nach Zufallsprinzip aus einer
gleichmäßigen Verteilung gewählt wird.

254
00:13:36,140 --> 00:13:38,400
Der Effekt kann einem
Abbruch gleichkommen,

255
00:13:38,400 --> 00:13:41,970
weil Sie technisch gesehen für 
jeden Alpha-Wert ein anderes Netz haben.

256
00:13:41,970 --> 00:13:43,980
Daher entsteht etwas wie ein Ensemble.

257
00:13:43,980 --> 00:13:46,280
In der Testphase
werden alle Werte von Alpha

258
00:13:46,280 --> 00:13:50,215
zu einem deterministischen Wert gemittelt,
der für Vorhersagen verwendet wird.

259
00:13:50,345 --> 00:13:52,550
Eine weitere Variante ist ReLU6.

260
00:13:52,550 --> 00:13:56,485
Das ist ebenfalls eine abschnittsweise
lineare Funktion mit drei Segmenten.

261
00:13:56,485 --> 00:13:59,910
Wie die normale ReLU-Funktion,
ist sie in der negativen Domain null.

262
00:13:59,910 --> 00:14:03,105
In der positiven Domain hat
ReLU6 jedoch immer den Wert sechs.

263
00:14:03,225 --> 00:14:06,100
Warum bleibt der Wert bei sechs?

264
00:14:06,520 --> 00:14:09,370
Es gibt ReLU-Einheiten, 
die einen Wert von sechs

265
00:14:09,370 --> 00:14:12,220
repliziert durch verschobene
Bernoulli-Einheiten haben

266
00:14:12,220 --> 00:14:15,520
und keine unendliche Menge.
Das liegt an der festen Obergrenze,

267
00:14:15,520 --> 00:14:18,650
Diese Einheiten werden
im Allgemeinen ReLU n-Einheiten genannt.

268
00:14:18,650 --> 00:14:20,265
Dabei ist "n" die Obergrenze.

269
00:14:20,605 --> 00:14:24,060
In Tests wurde sechs
als optimaler Wert ermittelt.

270
00:14:24,480 --> 00:14:28,235
Mit ReLU6-Einheiten können Modelle
wenige Features schneller trainieren.

271
00:14:28,235 --> 00:14:31,430
Sie wurden zuerst 
in Deep Convolutional Elite Networks

272
00:14:31,430 --> 00:14:33,740
für CIFAR-10-Bild-Datasets verwendet.

273
00:14:33,740 --> 00:14:36,840
Sie sind außerdem nützlich 
für die Vorbereitung des Netzwerks

274
00:14:36,840 --> 00:14:38,940
auf Festkomma-Genauigkeit für Inferenzen.

275
00:14:38,940 --> 00:14:41,179
Wenn es für 
die Obergrenze keine Grenze gibt,

276
00:14:41,179 --> 00:14:44,095
verlieren Sie zu viele Bits
an den Q-Teil der Festkommazahl.

277
00:14:44,095 --> 00:14:45,570
Bei der Obergrenze "sechs"

278
00:14:45,570 --> 00:14:48,320
bleiben aber genug 
Bit für den Bruchteil der Zahl

279
00:14:48,320 --> 00:14:51,600
sodass die Repräsentation
für eine gute Inferenz ausreicht.

280
00:14:52,260 --> 00:14:55,850
Schließlich gibt es die exponentielle
lineare Einheit oder ELU.

281
00:14:55,850 --> 00:15:00,115
Sie ist im nicht negativen Teil des
Eingabebereichs annähernd linear,

282
00:15:00,115 --> 00:15:02,820
dazu gleichmäßig, 
monoton und, noch wichtiger,

283
00:15:02,820 --> 00:15:05,240
im negativen Teil der Eingabe nicht null.

284
00:15:05,610 --> 00:15:08,435
Sie ist auch besser 
nullzentriert als vanilla ReLUs,

285
00:15:08,435 --> 00:15:10,315
was das Lernen beschleunigen kann.

286
00:15:10,525 --> 00:15:11,870
Hauptnachteil der ELUs:

287
00:15:11,870 --> 00:15:14,470
Sie sind kompositionell 
aufwändiger als ReLUs

288
00:15:14,470 --> 00:15:17,475
weil die Exponentialfunktion
berechnet werden muss.

289
00:15:17,675 --> 00:15:20,285
Neuronale Netzwerke
können beliebig komplex sein.

290
00:15:20,285 --> 00:15:23,210
Es kann viele Schichten geben,
viele Neuronen pro Schicht,

291
00:15:23,210 --> 00:15:26,520
Ausgaben, Eingaben,
verschiedene Aktivierungsfunktionen usw.

292
00:15:26,650 --> 00:15:28,700
Was ist der Zweck mehrerer Schichten?

293
00:15:28,700 --> 00:15:32,750
Mit jeder erhöht sich die Komplexität
der Funktionen, die ich erstellen kann.

294
00:15:32,870 --> 00:15:36,720
Jede nachfolgende Schicht ist eine
Komposition der vorherigen Funktionen.

295
00:15:36,870 --> 00:15:40,785
Da wir in Zwischenschichten nichtlineare 
Aktivierungsfunktionen verwenden,

296
00:15:40,785 --> 00:15:43,390
wird ein Stack an 
Datentransformationen erstellt,

297
00:15:43,390 --> 00:15:45,510
die die Daten drehen, 
dehnen und pressen.

298
00:15:45,510 --> 00:15:46,450
Zur Erinnerung:

299
00:15:46,450 --> 00:15:49,360
Der Zweck all dessen ist 
die Übertragung der Daten,

300
00:15:49,360 --> 00:15:51,620
sodass sie gut auf eine Hyperebene passen

301
00:15:51,620 --> 00:15:55,635
und Regression oder Trennung der Daten
für die Klassifizierung möglich ist.

302
00:15:55,725 --> 00:16:00,930
Das Mapping erfolgt vom ursprünglichen in
einen neuen, verschachtelten Featureraum.

303
00:16:01,430 --> 00:16:04,870
Was passiert, wenn ich einer Schicht
zusätzliche Neuronen hinzufüge?

304
00:16:04,980 --> 00:16:08,370
Jedes neue Neuron fügt dem
Vektorraum eine neue Dimension hinzu.

305
00:16:08,620 --> 00:16:11,025
Wenn ich mit 
drei Eingabeneuronen beginne,

306
00:16:11,025 --> 00:16:12,650
starte ich im R3-Vektorraum.

307
00:16:12,780 --> 00:16:17,160
Wenn die nächste Schicht 4 Neuronen hat,
befinde ich mich in einem R4-Vektorraum.

308
00:16:17,580 --> 00:16:20,210
Als es im vorherigen Kurs
um Kernel-Methoden ging,

309
00:16:20,210 --> 00:16:23,320
konnten wir das Dataset
im ursprünglichen Eingabevektorraum

310
00:16:23,320 --> 00:16:25,785
nicht einfach mit einer
Hyperebene separieren.

311
00:16:26,085 --> 00:16:30,150
Erst nach Hinzufügen der Dimension
und Umwandlung der Daten,

312
00:16:30,150 --> 00:16:33,790
die genau an die neue
Dimension angepasst wurden,

313
00:16:33,790 --> 00:16:37,495
war es möglich,
die Datenklassen sauber zu trennen.

314
00:16:37,695 --> 00:16:40,170
Das gilt auch
für die neuronalen Netzwerke.

315
00:16:40,560 --> 00:16:43,340
Welchen Effekt 
haben mehrere Ausgabeknoten?

316
00:16:43,530 --> 00:16:47,270
Bei mehreren Ausgabeknoten 
können Sie mit mehreren Labels vergleichen

317
00:16:47,270 --> 00:16:49,995
und die entsprechenden 
Bereiche zurückpropagieren.

318
00:16:50,175 --> 00:16:52,790
Nehmen Sie als Beispiel 
die Bildklassifizierung.

319
00:16:52,790 --> 00:16:56,045
In jedem Bild befinden sich 
mehrere Entitäten oder Klassen.

320
00:16:56,295 --> 00:16:59,910
Wir können nicht nur eine Klasse
vorhersagen, es kann viele geben.

321
00:16:59,910 --> 00:17:02,405
Hier ist diese 
Flexibilität also großartig.

322
00:17:02,405 --> 00:17:04,929
Neuronale Netzwerke
sollten beliebig komplex sein.

323
00:17:04,929 --> 00:17:08,375
Brauche ich mehr verborgene 
Dimensionen, kann ich XXX hinzufügen,

324
00:17:08,375 --> 00:17:11,969
Um die funktionale Komposition 
zu erweitern, kann ich XXX hinzufügen.

325
00:17:11,969 --> 00:17:14,520
Habe ich mehrere Labels,
kann ich XXX hinzufügen.

326
00:17:15,220 --> 00:17:18,345
Die korrekte Antwort lautet:
Neuronen, Schichten, Ausgaben,

327
00:17:18,345 --> 00:17:21,980
Um verborgene Dimensionen zu ändern,
kann ich die Neuronenzahl ändern.

328
00:17:21,980 --> 00:17:25,780
So wird die Dimensionen des Vektorraums
für den Zwischenvektor festgelegt.

329
00:17:25,780 --> 00:17:27,370
Hat eine Schicht vier Neuronen,

330
00:17:27,370 --> 00:17:30,010
befindet sie sich 
in einem Raum mit vier Vektoren.

331
00:17:30,010 --> 00:17:33,340
Eine Schicht mit 500 Neuronen
befindet sich im R500-Vektorraum.

332
00:17:33,480 --> 00:17:36,635
Das heißt, sie hat
500 echte Dimensionen.

333
00:17:36,935 --> 00:17:40,850
Das Hinzufügen einer Schicht ändert 
nicht die Dimension der vorherigen Schicht,

334
00:17:40,850 --> 00:17:43,970
vielleicht nicht einmal
die Dimension der zugehörigen Schicht,

335
00:17:43,970 --> 00:17:47,220
außer die Zahl der Neuronen ist
in der vorherigen Schicht anders.

336
00:17:47,700 --> 00:17:51,995
Zusätzliche Schichten sorgen für
eine bessere Komposition der Funktionen.

337
00:17:51,995 --> 00:17:53,960
Erinnern Sie sich: 
g von f von x

338
00:17:53,960 --> 00:17:58,355
ist die Komposition der Funktion g
mit der Funktion f für die Eingabe x.

339
00:17:58,765 --> 00:18:02,390
Daher wandle ich zuerst x mit f
und dann das Ergebnis mit g um.

340
00:18:02,390 --> 00:18:06,530
Je größer die Zahl der Schichten, 
desto tiefer die verschachtelten Funktionen.

341
00:18:06,820 --> 00:18:09,960
Das ist ideal für die Zusammensetzung 
nichtlinearer Funktionen

342
00:18:09,960 --> 00:18:11,610
zu verschachtelten Feature Maps,

343
00:18:11,610 --> 00:18:14,700
die für Menschen schwer, 
aber für Computer gut geeignet sind,

344
00:18:14,700 --> 00:18:17,150
und helfen, 
Daten so vor- und aufzubereiten,

345
00:18:17,150 --> 00:18:19,640
dass wir sie 
analysieren und verwerten können.

346
00:18:19,770 --> 00:18:23,240
Die Informationen und Erkenntnisse
erhalten wir in den Ausgabeschichten.

347
00:18:23,240 --> 00:18:27,275
Sie enthalten während der Interferenz
die Antworten auf das formulierte ML-Problem.

348
00:18:27,275 --> 00:18:30,615
Wenn Sie nur wissen möchten, 
wie groß die Wahrscheinlichkeit ist,

349
00:18:30,615 --> 00:18:33,520
dass das Bild einen Hund zeigt, 
reicht ein Ausgabeknoten.

350
00:18:33,840 --> 00:18:35,680
Wenn Sie die Wahrscheinlichkeit

351
00:18:35,680 --> 00:18:38,270
für Katze, Hund, 
Vogel oder Elch interessiert,

352
00:18:38,270 --> 00:18:40,840
brauchen Sie
für jede Möglichkeit einen Knoten.

353
00:18:41,020 --> 00:18:45,750
Die anderen drei Antworten sind falsch.
Mindestens zwei Wörter stimmen nicht.