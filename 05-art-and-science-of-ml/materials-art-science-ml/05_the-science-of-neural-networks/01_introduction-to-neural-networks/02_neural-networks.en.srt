1
00:00:00,000 --> 00:00:02,940
We've talked about neural networks in previous courses and modules,

2
00:00:02,940 --> 00:00:05,175
but now, let's learn some of the science behind them.

3
00:00:05,175 --> 00:00:09,825
We recently saw that feature crosses did very well in a problem like this.

4
00:00:09,825 --> 00:00:12,285
If x1 is the horizontal dimension,

5
00:00:12,285 --> 00:00:14,220
and x2 is the vertical dimension,

6
00:00:14,220 --> 00:00:18,480
there was no linear combination of the two features to describe this distribution.

7
00:00:18,480 --> 00:00:23,640
It wasn't until we did some feature engineering and crossed x1 and x2 to get

8
00:00:23,640 --> 00:00:26,355
a new feature x3 which equals

9
00:00:26,355 --> 00:00:30,030
x1 times x2 where we are able to describe our data distribution.

10
00:00:30,030 --> 00:00:32,910
So, manual handcraft to feature

11
00:00:32,910 --> 00:00:36,260
engineering can easily solve all of our nonlinear problems.

12
00:00:36,260 --> 00:00:39,855
Right? Unfortunately, the real world

13
00:00:39,855 --> 00:00:42,735
almost never has such easily described distributions.

14
00:00:42,735 --> 00:00:46,785
So, feature engineering, even after years the brightest people working on it,

15
00:00:46,785 --> 00:00:48,495
can only get so far.

16
00:00:48,495 --> 00:00:52,950
For instance, what feature crosses would you need to model this distribution?

17
00:00:52,950 --> 00:00:57,945
It looks like two circles over top of each other or maybe two spirals,

18
00:00:57,945 --> 00:01:00,630
but whatever it is, it's very messy.

19
00:01:00,630 --> 00:01:04,550
This example sets up the usefulness of neural networks so they can

20
00:01:04,550 --> 00:01:08,650
algorithmically create very complex feature crosses and transformations.

21
00:01:08,650 --> 00:01:12,240
You can imagine much more complicated spaces that

22
00:01:12,240 --> 00:01:16,380
even this spiral that really necessitate the use of neural networks.

23
00:01:16,380 --> 00:01:21,990
Neural networks can help as an alternative to feature crossing by combining features.

24
00:01:21,990 --> 00:01:25,110
When we were designing our neural network architecture we want to

25
00:01:25,110 --> 00:01:29,130
structure the model in such a way that there are features combined.

26
00:01:29,130 --> 00:01:32,975
Then we want to add another layer to combine our combinations

27
00:01:32,975 --> 00:01:36,980
and then add another layer to combine those combinations and so on.

28
00:01:36,980 --> 00:01:39,090
How do we choose the right combinations of

29
00:01:39,090 --> 00:01:42,315
our features and the combinations of those etc?

30
00:01:42,315 --> 00:01:45,780
You get the model to learn them through training of course.

31
00:01:45,780 --> 00:01:49,095
This is the basic intuition behind neural nets.

32
00:01:49,095 --> 00:01:52,500
This approach isn't necessarily better than feature crosses,

33
00:01:52,500 --> 00:01:56,350
but is a flexible alternative that works well in many cases.

34
00:01:56,350 --> 00:02:00,040
Here is a graphical representation of a linear model.

35
00:02:00,040 --> 00:02:02,355
We have three inputs x1,

36
00:02:02,355 --> 00:02:05,885
x2 and x3 shown by the blue circles.

37
00:02:05,885 --> 00:02:09,890
They're combined with some weight given on each edge to produce an output.

38
00:02:09,890 --> 00:02:12,620
They're often is an extra biased term,

39
00:02:12,620 --> 00:02:14,955
but for simplicity it isn't shown here.

40
00:02:14,955 --> 00:02:20,045
This is a linear model since is a form of y equals w1 times x1,

41
00:02:20,045 --> 00:02:22,010
plus w2 times x2,

42
00:02:22,010 --> 00:02:23,915
plus w3 times x3.

43
00:02:23,915 --> 00:02:28,040
Now, let's add a hidden layer to our network of nodes and edges.

44
00:02:28,040 --> 00:02:32,640
Our input layer has three nodes and our hidden layer also has three.

45
00:02:32,640 --> 00:02:35,210
But now, hidden nodes.

46
00:02:35,210 --> 00:02:37,565
Since this is a completely connected layer,

47
00:02:37,565 --> 00:02:41,980
there are three times three edges or nine weights.

48
00:02:41,980 --> 00:02:44,870
Surely, this is a nonlinear model now that we can

49
00:02:44,870 --> 00:02:48,045
use to solve our nonlinear problems, right?

50
00:02:48,045 --> 00:02:51,340
Unfortunately, not. Let's break it down.

51
00:02:51,340 --> 00:02:56,415
The input to the first hidden node is the weighted sum of w1 times x1,

52
00:02:56,415 --> 00:02:58,515
plus w4 times x2,

53
00:02:58,515 --> 00:03:01,350
plus w7 times x3.

54
00:03:01,350 --> 00:03:05,640
The input to the second hidden node is the weighted sum w2 times

55
00:03:05,640 --> 00:03:10,395
x1 plus w5 times x2 plus w8 times x3.

56
00:03:10,395 --> 00:03:14,520
The input to the third hidden node is the weighted sum w3

57
00:03:14,520 --> 00:03:19,575
times x1 plus w6 times x2 plus w9 times x3.

58
00:03:19,575 --> 00:03:23,035
Combining it all together at the output node,

59
00:03:23,035 --> 00:03:25,685
we have w10 times h1,

60
00:03:25,685 --> 00:03:28,080
plus w11 times h2,

61
00:03:28,080 --> 00:03:30,225
plus w12 times h3.

62
00:03:30,225 --> 00:03:32,550
Remember though, that h1,

63
00:03:32,550 --> 00:03:37,370
h2 and h3 are just linear combinations of the input features.

64
00:03:37,370 --> 00:03:40,055
Therefore, expanding it out,

65
00:03:40,055 --> 00:03:43,165
we're left with a complex set of weight constants multiplied by

66
00:03:43,165 --> 00:03:47,570
each input value x1, x2, and x3.

67
00:03:47,820 --> 00:03:51,935
We can substitute each couple of weights for a new weight.

68
00:03:51,935 --> 00:03:53,250
Look familiar?

69
00:03:53,250 --> 00:03:56,530
This is exactly the same linear model as before

70
00:03:56,530 --> 00:04:00,995
despite adding a hidden layer of neurons. So, what happened?

71
00:04:00,995 --> 00:04:04,050
What if we added another hidden layer?

72
00:04:04,050 --> 00:04:07,830
Unfortunately, this once again and collapses all the way back down

73
00:04:07,830 --> 00:04:11,835
into a single weight matrix multiplied by each of the three inputs.

74
00:04:11,835 --> 00:04:13,790
It is the same linear model.

75
00:04:13,790 --> 00:04:18,450
We can continue this process add infinitum and it would still be the same result,

76
00:04:18,450 --> 00:04:23,250
albeit a lot more costly computationally for training or prediction for a much,

77
00:04:23,250 --> 00:04:26,260
much more complicated architecture than needed.

78
00:04:26,410 --> 00:04:29,600
Thinking about this from a linear algebra perspective,

79
00:04:29,600 --> 00:04:33,455
you're multiplying multiple matrices together in a chain.

80
00:04:33,455 --> 00:04:34,985
In this small example,

81
00:04:34,985 --> 00:04:37,085
I first multiply a three by three matrix,

82
00:04:37,085 --> 00:04:41,290
the transpose of the weight matrix between the input layer and a hidden layer one,

83
00:04:41,290 --> 00:04:44,390
by the three by one input vector resulting in the three by

84
00:04:44,390 --> 00:04:48,845
one vector which are the values at each hidden neuron in the hidden layer one.

85
00:04:48,845 --> 00:04:52,175
Define their second hidden layers neuron's values,

86
00:04:52,175 --> 00:04:54,030
I multiplied the transpose of it's

87
00:04:54,030 --> 00:04:56,395
three by three weight matrix that connects hidden layer

88
00:04:56,395 --> 00:05:01,190
one with hidden layer two to my resultant vector at hidden layer one.

89
00:05:01,190 --> 00:05:03,440
As you can guess, the two,

90
00:05:03,440 --> 00:05:06,050
three by three weight matrices can be combined into on,

91
00:05:06,050 --> 00:05:08,400
three by thre matrix by first calculating

92
00:05:08,400 --> 00:05:12,570
the matrix product from the left inside or from the right.

93
00:05:12,570 --> 00:05:15,450
This still gives the same shape for h2,

94
00:05:15,450 --> 00:05:18,720
the second hidden layer neuron's value vector.

95
00:05:18,720 --> 00:05:23,075
Adding in the final layer between hidden layer two and the output layer,

96
00:05:23,075 --> 00:05:25,350
I need to multiply the preceding steps by

97
00:05:25,350 --> 00:05:28,400
the transpose of the weight matrix between the last two layers.

98
00:05:28,400 --> 00:05:31,740
Even though when feeding forward through a neural network you perform

99
00:05:31,740 --> 00:05:36,360
the matrix multiplication from right to left by applying it from left to right,

100
00:05:36,360 --> 00:05:38,670
you can see that our large chain of

101
00:05:38,670 --> 00:05:42,750
matrix complications collapses down into just a three valued vector.

102
00:05:42,750 --> 00:05:46,160
If you train this model in just a simple linear regression case of

103
00:05:46,160 --> 00:05:50,760
three weight side by side and they both fall into the same minimum on the low surface,

104
00:05:50,760 --> 00:05:54,865
than even though I did a ton of computation to calculate all 21 weights

105
00:05:54,865 --> 00:05:58,859
my matrix product chain will condense down into the lower equation,

106
00:05:58,859 --> 00:06:02,590
the weight will exactly match the training simple linear regressions weights.

107
00:06:02,590 --> 00:06:05,205
All of the work for the same result.

108
00:06:05,205 --> 00:06:07,455
You're probably thinking now, "Hey,

109
00:06:07,455 --> 00:06:11,280
I thought neural networks are all about adding layers upon layers in neurons.

110
00:06:11,280 --> 00:06:15,765
How can I do deep learning when all of my layers collapse into just one?"

111
00:06:15,765 --> 00:06:17,190
I've got good news for you.

112
00:06:17,190 --> 00:06:19,025
There is an easy solution.

113
00:06:19,025 --> 00:06:23,690
The solution is adding a non-linear transformation layer which is facilitated by

114
00:06:23,690 --> 00:06:28,530
a nonlinear activation function such as sigmoid, Tanh or ReLU.

115
00:06:28,530 --> 00:06:31,980
And thinking of terms of the graph such as you're making TensorFlow,

116
00:06:31,980 --> 00:06:35,580
you can imagine each neuron actually having two nodes.

117
00:06:35,580 --> 00:06:39,865
The first node being the result of the weighted sum wx plus b,

118
00:06:39,865 --> 00:06:41,865
and the second node being the result

119
00:06:41,865 --> 00:06:44,430
of that being passed through the activation function.

120
00:06:44,430 --> 00:06:46,350
In other words, there are inputs of

121
00:06:46,350 --> 00:06:49,770
the activation function followed by the outputs of the activation function,

122
00:06:49,770 --> 00:06:53,735
so the activation function acts as the transition point between.

123
00:06:53,735 --> 00:06:57,110
Adding in this non-linear transformation is the only way

124
00:06:57,110 --> 00:06:59,990
to stop the neural network from condensing back into a shallow network.

125
00:06:59,990 --> 00:07:04,280
Even if you have a layer with nonlinear activation of functions your network,

126
00:07:04,280 --> 00:07:09,585
if elsewhere in the network you have two or more layers with linear activation functions,

127
00:07:09,585 --> 00:07:12,525
those can still be collapsed into just one network.

128
00:07:12,525 --> 00:07:14,840
Usually, neural networks have all layers

129
00:07:14,840 --> 00:07:17,840
nonlinear for the first and minus one layers and then have

130
00:07:17,840 --> 00:07:21,245
the final layer transformation be linear for regression or

131
00:07:21,245 --> 00:07:25,175
sigmoid or softmax which we'll talk about soon for classification.

132
00:07:25,175 --> 00:07:27,620
It all depends on what you want the output to be.

133
00:07:27,620 --> 00:07:29,720
Thinking about this again from

134
00:07:29,720 --> 00:07:34,700
a linear algebra perspective when we apply a linear transformation to a matrix or vector,

135
00:07:34,700 --> 00:07:39,540
we are multiplying a matrix or vector to it leading to our desired shape and result.

136
00:07:39,540 --> 00:07:41,840
Such as when I want to scale a matrix,

137
00:07:41,840 --> 00:07:43,475
I can multiply it by a constant.

138
00:07:43,475 --> 00:07:46,819
But truly, what you are doing is multiplying it by an identity matrix,

139
00:07:46,819 --> 00:07:48,410
multiplied by that constant.

140
00:07:48,410 --> 00:07:52,600
So, it is a diagonal matrix with that constant all on the diagonal.

141
00:07:52,600 --> 00:07:55,690
This would be collapsed into just a matrix product.

142
00:07:55,690 --> 00:07:59,070
However, if I add a non-linearity,

143
00:07:59,070 --> 00:08:02,660
what I am doing is not able to be represented by a matrix.

144
00:08:02,660 --> 00:08:05,800
Since, I am element y it's applying a function into my input.

145
00:08:05,800 --> 00:08:07,550
For instance, if I have

146
00:08:07,550 --> 00:08:11,315
a nonlinear activation function between my first and second hidden layers,

147
00:08:11,315 --> 00:08:13,110
I'm applying a function of the product of

148
00:08:13,110 --> 00:08:17,185
the transpose of my first hidden layers weight matrix and my input vector.

149
00:08:17,185 --> 00:08:20,740
The lower equation is my activation function in a ReLU.

150
00:08:20,740 --> 00:08:24,560
Since I cannot represent the transformation in terms of linear algebra,

151
00:08:24,560 --> 00:08:27,890
I can no longer collapse that portion of my transformation chain

152
00:08:27,890 --> 00:08:30,800
thus complexity to my model remains and

153
00:08:30,800 --> 00:08:34,325
doesn't collapse into just one linear combination of the inputs.

154
00:08:34,325 --> 00:08:38,300
Note that I can still collapse second hidden layer of weight matrix and

155
00:08:38,300 --> 00:08:42,910
the output layer weight matrix since there is no nonlinear function being applied here.

156
00:08:42,910 --> 00:08:47,540
This means that whenever there are two or more linear layers consecutively,

157
00:08:47,540 --> 00:08:51,515
they can always be collapsed back into one layer no matter how many they are.

158
00:08:51,515 --> 00:08:55,415
Therefore, they have the most complex functions being created by your network,

159
00:08:55,415 --> 00:08:58,790
it's best to have your entire network have a linear activation functions,

160
00:08:58,790 --> 00:09:02,710
except at the last layer in case you might use a different type of output at the end.

161
00:09:02,710 --> 00:09:08,035
Why is it important adding non-linear activation functions to neural networks?

162
00:09:08,035 --> 00:09:10,690
The correct answer is because it stops the layers

163
00:09:10,690 --> 00:09:12,985
from collapsing back into just a linear model.

164
00:09:12,985 --> 00:09:15,415
Not only do nonlinear activation functions help

165
00:09:15,415 --> 00:09:18,400
create interesting transformations through our data scripture space,

166
00:09:18,400 --> 00:09:21,260
but it allows for deep compositional functions.

167
00:09:21,260 --> 00:09:26,590
As we explained, if there are any two or more layers with linear activation functions,

168
00:09:26,590 --> 00:09:28,600
this product of matrices can be summarized by

169
00:09:28,600 --> 00:09:31,560
just one matrix times the input feature vector.

170
00:09:31,560 --> 00:09:34,420
Therefore, you end up with slower model with

171
00:09:34,420 --> 00:09:38,605
more computation but with all of your functional complexity reduced.

172
00:09:38,605 --> 00:09:41,620
Non-linearities do not add regularization to

173
00:09:41,620 --> 00:09:45,015
the loss function and they do not invoke early stopping.

174
00:09:45,015 --> 00:09:47,650
Even though nonlinear activation functions do

175
00:09:47,650 --> 00:09:49,910
create complex transformations in the vector space,

176
00:09:49,910 --> 00:09:53,200
that dimension does not change it remains the same vector space.

177
00:09:53,200 --> 00:09:56,790
Albeit stretched, squished or rotated.

178
00:09:56,790 --> 00:09:59,680
As mentioned in one of our previous courses,

179
00:09:59,680 --> 00:10:03,200
there are many nonlinear activation functions with sigmoid,

180
00:10:03,200 --> 00:10:05,000
and the scaled and shifted sigmoid,

181
00:10:05,000 --> 00:10:07,535
hyperbolic tangent being some of the earliest.

182
00:10:07,535 --> 00:10:09,725
However, as mentioned before,

183
00:10:09,725 --> 00:10:13,310
these can have saturation which leads to the vanishing gradient problem,

184
00:10:13,310 --> 00:10:14,510
where with zero gradients,

185
00:10:14,510 --> 00:10:17,945
the models weights don't update and training halts.

186
00:10:17,945 --> 00:10:21,140
The Rectified Linear Unit or ReLU for short is one of

187
00:10:21,140 --> 00:10:24,215
our favorites because it's simple and works well.

188
00:10:24,215 --> 00:10:26,450
In the positive domain it is linear,

189
00:10:26,450 --> 00:10:30,440
so we don't have saturation whereas the negative domain the function is zero.

190
00:10:30,440 --> 00:10:33,365
Networks with ReLU hidden activation,

191
00:10:33,365 --> 00:10:39,050
often have 10 times the speed of training than networks with sigmoid, hidden activations.

192
00:10:39,050 --> 00:10:42,590
However, due to negative domains function always being zero,

193
00:10:42,590 --> 00:10:45,210
we can end up with the real layers dying.

194
00:10:45,210 --> 00:10:46,910
What I mean by this is that,

195
00:10:46,910 --> 00:10:48,230
when you start getting inputs in

196
00:10:48,230 --> 00:10:51,845
the negative domain and the output of the activation will be zero,

197
00:10:51,845 --> 00:10:54,980
which doesn't help in the next layer and given inputs in the positive domain.

198
00:10:54,980 --> 00:10:59,150
This compounds and creates a lot of zero activations,

199
00:10:59,150 --> 00:11:02,270
during back propagation when updating the weights since

200
00:11:02,270 --> 00:11:05,170
we have to multiply our errors derivative by their activation,

201
00:11:05,170 --> 00:11:06,810
we end up with a gradient of zero.

202
00:11:06,810 --> 00:11:09,650
Thus, a weight of data zero and thus the weights

203
00:11:09,650 --> 00:11:13,800
don't change and the training fails for that layer.

204
00:11:13,800 --> 00:11:17,220
Fortunately, a lot of clever methods have been developed to

205
00:11:17,220 --> 00:11:20,980
slightly modify the ReLU to ensure training doesn't stall,

206
00:11:20,980 --> 00:11:24,120
but still, with bunch of the benefits of the vanilla ReLU.

207
00:11:24,120 --> 00:11:25,920
Here again is the vanilla ReLu,

208
00:11:25,920 --> 00:11:30,270
the maximum operator can also be represented by the piecewise linear equation,

209
00:11:30,270 --> 00:11:32,790
where less than zero, function is zero.

210
00:11:32,790 --> 00:11:36,190
And greater than or equal to zero, the function is X.

211
00:11:36,190 --> 00:11:38,520
A smooth approximation of the ReLUs function

212
00:11:38,520 --> 00:11:41,205
is the analytic function of the natural log of one,

213
00:11:41,205 --> 00:11:43,185
plus the exponential X.

214
00:11:43,185 --> 00:11:45,360
This is called the Softplus function.

215
00:11:45,360 --> 00:11:49,740
Interestingly, the derivative the Softplus function is a logistic function.

216
00:11:49,740 --> 00:11:52,210
The pros of using the Softplus function are,

217
00:11:52,210 --> 00:11:54,570
it's continuous and differentiable at zero,

218
00:11:54,570 --> 00:11:56,380
unlike the ReLu function.

219
00:11:56,380 --> 00:11:59,449
However, due to the natural log and exponential,

220
00:11:59,449 --> 00:12:02,295
there's added computation compared to ReLUs,

221
00:12:02,295 --> 00:12:06,030
and ReLUs still have as good of results in practice.

222
00:12:06,030 --> 00:12:10,195
Therefore, Softplus is usually discouraged to be using deep learning.

223
00:12:10,195 --> 00:12:14,895
To try and solve our issue of dying ReLUs due to zero activations,

224
00:12:14,895 --> 00:12:16,995
the Leaky ReLU was developed.

225
00:12:16,995 --> 00:12:20,975
Just like ReLUs, Leaky ReLUs have a piecewise linear function.

226
00:12:20,975 --> 00:12:23,180
However, in the negative domain,

227
00:12:23,180 --> 00:12:28,225
rather than zero, they have a non-zero slope specifically, 0.01.

228
00:12:28,225 --> 00:12:31,345
This way, when the unit is not activated,

229
00:12:31,345 --> 00:12:35,999
Leaky ReLUs still allow a small non-zero gradient to pass through them,

230
00:12:35,999 --> 00:12:40,000
which hopefully will allow weight updating and training to continue.

231
00:12:40,000 --> 00:12:46,290
Taking this Leaky idea one step further is the parametric ReLU or PReLU for short.

232
00:12:46,290 --> 00:12:48,640
Here, rather than arbitrarily allowing

233
00:12:48,640 --> 00:12:51,625
one hundredth of an X through in the negative domain,

234
00:12:51,625 --> 00:12:53,910
it lets Alpha of X through.

235
00:12:53,910 --> 00:12:57,190
But, what is the parameter Alpha supposed to be?

236
00:12:57,190 --> 00:13:01,585
In the graph, I set Alpha to be 0.5 for visualization purposes.

237
00:13:01,585 --> 00:13:04,420
But in practice, it is actually a learned parameter

238
00:13:04,420 --> 00:13:07,735
from training along with the other neural network parameters.

239
00:13:07,735 --> 00:13:11,045
This way, rather than us setting this value,

240
00:13:11,045 --> 00:13:14,800
the value will be determined during training via the data and should

241
00:13:14,800 --> 00:13:18,910
learn a more optimal value than we our priority could set.

242
00:13:18,910 --> 00:13:21,520
Notice that when Alpha is less than one,

243
00:13:21,520 --> 00:13:25,270
the formula can be rewritten back into the compact form using the maximum.

244
00:13:25,270 --> 00:13:28,480
Specifically, the max of X or alpha times x.

245
00:13:28,480 --> 00:13:33,010
There are also randomized Leaky ReLUs where instead of Alpha being trained,

246
00:13:33,010 --> 00:13:35,740
it is a sampled from a uniform distribution randomly.

247
00:13:35,740 --> 00:13:38,080
This can have an effect similar to drop out since you

248
00:13:38,080 --> 00:13:40,990
technically have a different network for each value of Alpha.

249
00:13:40,990 --> 00:13:43,980
And therefore, it is making something similar to an ensemble.

250
00:13:43,980 --> 00:13:46,720
At test time, all the values of Alpha are

251
00:13:46,720 --> 00:13:50,005
averaged together to a deterministic value to use for predictions.

252
00:13:50,005 --> 00:13:52,550
There is also the ReLU6 variant,

253
00:13:52,550 --> 00:13:56,565
this is another piecewise linear function with three segments.

254
00:13:56,565 --> 00:13:58,040
Like a normal ReLU,

255
00:13:58,040 --> 00:13:59,890
it is zero in the negative domain,

256
00:13:59,890 --> 00:14:03,105
however the positive domain the ReLU6 is kept at six.

257
00:14:03,105 --> 00:14:06,520
You're probably thinking, "Why is it kept at six?

258
00:14:06,520 --> 00:14:09,370
You can imagine one of these ReLU units having only

259
00:14:09,370 --> 00:14:12,220
six replicated by a shifted bernoulli units,

260
00:14:12,220 --> 00:14:15,520
rather than an infinite amount due to the hard cap.

261
00:14:15,520 --> 00:14:18,520
In general, these are called the ReLU n units,

262
00:14:18,520 --> 00:14:20,265
where n is the cap value.

263
00:14:20,265 --> 00:14:24,270
In testing, six was found to be the most optimal value.

264
00:14:24,270 --> 00:14:28,235
ReLU6 units can help models learn sparse features sooner.

265
00:14:28,235 --> 00:14:33,740
They were first used convolutional deep elite networks on a CIFAR-10 image data set.

266
00:14:33,740 --> 00:14:36,280
They also have useful property of preparing

267
00:14:36,280 --> 00:14:38,700
the network for fixed point precision for inference.

268
00:14:38,700 --> 00:14:40,639
If the upper limit is unbounded,

269
00:14:40,639 --> 00:14:44,095
then you lose too many bits to the Q part of a fixed point number,

270
00:14:44,095 --> 00:14:45,550
whereas with an upper limit of six,

271
00:14:45,550 --> 00:14:47,620
it leaves enough bits to the fractional part of

272
00:14:47,620 --> 00:14:51,600
the number making it represented well enough to do good inference.

273
00:14:51,600 --> 00:14:55,540
Lastly, there is the exponential linear unit or ELU.

274
00:14:55,540 --> 00:15:00,925
It is approximately linear in the non-negative portion of the input space and is smooth,

275
00:15:00,925 --> 00:15:02,680
monotonic and most importantly,

276
00:15:02,680 --> 00:15:05,320
non-zero in the negative portion of the input.

277
00:15:05,320 --> 00:15:10,315
They are also better zero centered than vanilla ReLUs which can speed up learning.

278
00:15:10,315 --> 00:15:13,960
The main drawback of ELUs are that they are more compositionally

279
00:15:13,960 --> 00:15:17,475
expensive than ReLUs due to having to calculate the exponential.

280
00:15:17,475 --> 00:15:20,285
Neural networks can be arbitrarily complex,

281
00:15:20,285 --> 00:15:21,710
there can be many layers,

282
00:15:21,710 --> 00:15:23,930
neurons per layer, outputs, inputs,

283
00:15:23,930 --> 00:15:26,650
different types activation functions et cetra.

284
00:15:26,650 --> 00:15:29,050
What does the purpose of multiple layers?

285
00:15:29,050 --> 00:15:30,390
Each layer I add,

286
00:15:30,390 --> 00:15:32,620
adds the complexity of the functions I can create.

287
00:15:32,620 --> 00:15:36,790
Each subsequent layer is a composition of the previous functions.

288
00:15:36,790 --> 00:15:40,255
Since we are using nonlinear activation functions in my hidden layers,

289
00:15:40,255 --> 00:15:43,570
I'm creating a stack of data transformations that rotate,

290
00:15:43,570 --> 00:15:45,510
stretch and squeeze my data.

291
00:15:45,510 --> 00:15:48,190
Remember, the purpose of doing all of this is

292
00:15:48,190 --> 00:15:50,630
to transfer my data in such a way that can nicely

293
00:15:50,630 --> 00:15:52,840
fit hyper plane to it for regression or

294
00:15:52,840 --> 00:15:55,635
separate my data with a hyper planes for classification.

295
00:15:55,635 --> 00:16:01,140
We are mapping from the original feature space to some new convoluted feature space.

296
00:16:01,140 --> 00:16:04,530
What does adding additional neurons to a layer do?

297
00:16:04,530 --> 00:16:08,370
Each neuron I add, adds a new dimension to my vector space.

298
00:16:08,370 --> 00:16:10,515
If I begin with three input neurons,

299
00:16:10,515 --> 00:16:12,510
I start in R3 vector space.

300
00:16:12,510 --> 00:16:17,270
But if my next layer has four neurons that I moved to an R4 vector space.

301
00:16:17,270 --> 00:16:20,100
Back when we talked about Kernel methods in our previous course,

302
00:16:20,100 --> 00:16:22,450
we had a data set that couldn't be easily separated

303
00:16:22,450 --> 00:16:25,695
with a hyper plane in the original input vector space.

304
00:16:25,695 --> 00:16:28,750
But, by adding the dimension and then transform

305
00:16:28,750 --> 00:16:32,120
the data to fill that new dimension in just the right way,

306
00:16:32,120 --> 00:16:37,425
we were then easily able to make a clean slice between the classes of data.

307
00:16:37,425 --> 00:16:39,570
The same applies here with neural networks.

308
00:16:39,570 --> 00:16:43,100
What might having multiple output nodes do?

309
00:16:43,100 --> 00:16:45,920
Having multiple output nodes allows you to

310
00:16:45,920 --> 00:16:49,995
compare to multiple labels and then propagate the corresponding areas backwards.

311
00:16:49,995 --> 00:16:52,850
You can imagine doing image classification where there are

312
00:16:52,850 --> 00:16:56,045
multiple entities or classes within each image.

313
00:16:56,045 --> 00:16:59,660
We can't just predict one class because there maybe many,

314
00:16:59,660 --> 00:17:02,405
so having this flexibility is great.

315
00:17:02,405 --> 00:17:05,120
Neural networks should be arbitrarily complex.

316
00:17:05,120 --> 00:17:06,275
To increase hidden dimensions,

317
00:17:06,275 --> 00:17:07,820
I can add blank.

318
00:17:07,820 --> 00:17:09,350
To increase function composition,

319
00:17:09,350 --> 00:17:10,745
I can add blank.

320
00:17:10,745 --> 00:17:12,530
If I have multiple labels for example,

321
00:17:12,530 --> 00:17:14,600
I can add blank.

322
00:17:14,600 --> 00:17:18,875
The correct answer is neuron's, layers, outputs.

323
00:17:18,875 --> 00:17:21,920
To change hidden dimensions, I can change the layers number of neurons.

324
00:17:21,920 --> 00:17:25,780
So that determines the dimensions of the vector space like intermediate vector is in.

325
00:17:25,780 --> 00:17:27,380
If a layer has four neurons,

326
00:17:27,380 --> 00:17:29,060
than it is in our four vector space,

327
00:17:29,060 --> 00:17:33,080
and if a layer has 500 neurons it is in R 500 vector space.

328
00:17:33,080 --> 00:17:36,935
Meaning, it has 500 real dimensions.

329
00:17:36,935 --> 00:17:41,970
Adding a layer doesn't change the dimension of the previous layer and it might not even

330
00:17:41,970 --> 00:17:43,970
change the dimension in its layer

331
00:17:43,970 --> 00:17:47,330
unless it has a different number of neurons in the previous layer.

332
00:17:47,330 --> 00:17:51,995
What additional layers do add is a greater composition of functions.

333
00:17:51,995 --> 00:17:53,960
Remember, Go of F of X,

334
00:17:53,960 --> 00:17:58,355
is the composition of the function G with the function F on the input X.

335
00:17:58,355 --> 00:18:01,460
Therefore, I first transform X by F and then

336
00:18:01,460 --> 00:18:04,790
transform that result by G. The more layers I have,

337
00:18:04,790 --> 00:18:06,530
the deeper the nested functions go.

338
00:18:06,530 --> 00:18:09,860
This is great for combining non-linear functions together to make

339
00:18:09,860 --> 00:18:12,170
very convoluted feature maps that are hard for

340
00:18:12,170 --> 00:18:14,700
humans to construct but great for computers,

341
00:18:14,700 --> 00:18:17,150
and allow us to better get our data into

342
00:18:17,150 --> 00:18:19,580
a shape that we can learn and gain insights from.

343
00:18:19,580 --> 00:18:23,160
Speaking of insights, we receive those through our output layers,

344
00:18:23,160 --> 00:18:27,275
where during inference, those will be the answers to our ML formulated problem.

345
00:18:27,275 --> 00:18:30,865
If you only want to know the probability of an image being a dog,

346
00:18:30,865 --> 00:18:33,440
then you can get by with only one output node.

347
00:18:33,440 --> 00:18:36,490
But if he wanted to know the probability of an image being a cat, dog,

348
00:18:36,490 --> 00:18:37,800
bird or a moose,

349
00:18:37,800 --> 00:18:40,450
then you would need to have a node for each one.

350
00:18:40,450 --> 00:18:45,960
The other three answers are all wrong since they get two or more of the words wrong.