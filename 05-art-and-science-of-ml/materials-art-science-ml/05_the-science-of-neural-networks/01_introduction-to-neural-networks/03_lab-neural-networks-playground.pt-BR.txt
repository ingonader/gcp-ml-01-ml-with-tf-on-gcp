Agora, vamos brincar com o playground
intensivo das redes neurais. Neste laboratório, usaremos playground intensivo para testar e criar redes neurais
para aprender os dados. Quero que você resolva esses problemas
de duas maneiras. Primeiro, vamos tentar treinar os modelos
usando a engenharia manual de atributos, em que usamos nosso
conhecimento para adivinhar a combinação correta e a transformação
de atributos para aprender os dados. Em seguida, vamos nos entregar ao poder das redes neurais e adicionar mais camadas e neurônios
usando um conjunto simples de atributos de entrada para ver se ele
mesmo realiza engenharia de atributos. Bem-vindo de volta
ao playground intensivo. Neste laboratório,
veremos se a engenharia de atributos pode superar nossas redes neurais. Tenho a sensação de que este
não será o caso. Vamos investigar. Certo, neste diagrama,
estamos tentando classificar esses pontos azuis e laranja,
é um problema de classificação. O que você notará é que eles se
parecem círculos concêntricos. No entanto, neste caso, há muito ruído. Portanto, há muita mistura aqui. O que vou tentar fazer é ver como
o X1 e o X2 se comportam no treino. Como você pode ver,
não estão aprendendo muito. Está tudo meio borrado junto,
está muito branco. Então, não está,
de um jeito ou de outro, de acordo com a escala abaixo, -101. Não aprendeu muito. Vamos ver se você
consegue melhorar. Com a engenharia de atributos,
sei que isso é um círculo. Então, faço X1 ao quadrado
e X2 ao quadrado, e testo agora, vamos ver. Olhe para isso,
parece uma elipse. Isso significa que está
quase descobrindo o que é essa função. Sabemos que é um círculo, mas há muito ruído e tudo mais, e está um pouco afastado. Talvez, porém, eu possa deixar minha
perda menor que 0,275, vamos tentar nos livrar de X1 e X2,
as formas lineares. Vamos tentar agora. 2,85. Parece um pouco mais circular. No entanto, nossa perda de teste
está um pouco melhor. Vamos ver agora se podemos fazer o mesmo
com redes neurais. Vamos voltar para apenas X1 e X2, que, como vimos anteriormente,
fizeram um trabalho muito ruim. Vamos adicionar uma camada oculta
e dois domínios extras. Como vemos, temos dificuldade
para descobrir qual é essa função. O problema é que não há capacidade
suficiente nesses dois neurônios nem representação gráfica alta o
suficiente para aprender a distribuição. Então, vamos fazer uma pausa aqui e adicionar outro neurônio. Talvez esse tenha capacidade
suficiente para aprender a função. Certo. Ainda não está conseguindo. Talvez... Veja isto. Demorou muito tempo, mas está descobrindo
lentamente a forma da função. Isso é algum tipo de formato retangular. O que isso significa é que
estamos voltando à extremidade da quantidade de neurônios capazes
de representar essa distribuição. Vamos ver se podemos acelerar
o tempo ao adicionar um neurônio extra. Olhe para isso. Foi muito mais rápido. Temos apenas quatro neurônios aqui. Mas vamos ver o que acontece
se adicionamos muitos neurônios extras. Vamos colocar
um molde de quatro e ver o que acontece. Isso é o treino. É bem mais lento. Há mais massa para processar
passando por todas essas semicamadas. Acho que uma hora vai conseguir. Mas estou preocupado, pois pode ter
um pouco de sobreajuste, como você vê. Isso não é mais
uma forma circular simples. É algum polígono estranho. Portanto, está sobreajustando os dados e
não está indo bem com a perda de teste, que é muito mais alta
do que costumava ser. Vamos ver outras distribuições. Aqui, estamos distribuindo
nosso clássico Xr. Quando X e Y são positivos ou negativos, temos azuis ou temos a classe laranja. Vamos ver se podemos
aprender isso apenas com X1e X2. Como você pode ver, assim como antes, X1 e X2 não são fortes o suficiente
para descrever essa função. É basicamente zero em todo o quadro. Vamos ver se podemos descobrir isso
usando a engenharia de atributos. Com ela, vou escolher o X e X2 porque sei
como eles são. Vamos treinar isso. Veja isto. Muito bom, é uma perda de teste de 0,17. Isso é ótimo. Achei facilmente, e aqui está minha ponderação, 0,19,
isso é ótimo. Sim, há ruído, então temos
algumas coisas erradas, mas, na maior parte, ficou bem correto. Vamos ver agora
se o aprendizado de máquina, usando redes neurais,
pode fazer um trabalho melhor. Vamos colocar X1 e X2 juntos novamente, e vamos adicionar uma camada oculta. Mais uma vez, vou tentar ver. Quero ter a menor quantia que puder, então, vou tentar reduzir isso a apenas
dois neurônios e aprender isso. No entanto, como você pode ver, não é possível descobrir isso. Não há complexidade suficiente,
nem capacidade no modelo. Então vamos passar isso aqui
e tentar adicionar um terceiro neurônio. Vamos tentar treinar novamente. Como você pode ver aqui, está tendo dificuldade
para aprender esta função. Talvez seja apenas na borda, e eu tenho que esperar um pouco mais
para ver se vai aprender. Mas parece travado. Talvez outra inicialização
conserte isso. Vamos ver. Aí está. Então, tentamos executar a inicialização e, de certa forma,
aprenderemos a função aqui. Parece mais com uma ampulheta
diagonal, na verdade. No entanto, essa não é
exatamente a função. Você pode ver
que a perda é muito maior. Então, vamos para quatro, isso talvez faça o trabalho, vamos ver. Ainda estamos com a ampulheta, mas ela está se tornando parecida
a uma série de quadrados, que é o que a nossa função realmente é,
está melhorando. Agora, vamos ver, adicionando mais, e checar se há sobreajuste. Como você pode ver, é muito mais lento
na perda de treinamento. No entanto, esses estão mais
parecidos com um quadrado. Isso parece ótimo. Vamos tentar outro tipo de distribuição. Aqui nós temos uma espiral, duas, na verdade, uma ao redor da outra. Muito parecido com
a foto de uma galáxia. Vamos ver se podemos treinar com X1 e X2. Eu duvido que possamos. Como você pode ver aqui, ele realmente não aprendeu a distribuição. Está muito próximo de zero e não consegue decidir
o que é o quê. O que podemos testar é
a engenharia de atributos. Vamos testar. O que você acha? Vamos tentar círculos, talvez? Não, vamos tentar adicionar estes. Será seno e cosseno,
ou senoX1 e senoX2. Está testando. Eu tenho seis atributos brutos
entrando aqui e estão quase conseguindo. Como você vê no topo, está lentamente entrando aqui. Há uma grande lacuna aqui, que não
sei para onde está indo. Está realmente extrapolando aqui. Não é um grande trabalho, e está parado, como você pode ver. Vamos ver se podemos fazer isso melhor
com redes neurais. Vamos desativar isso e adicionar uma camada oculta. Primeiro começamos com dois neurônios
e veremos se podemos fazer isso. Como você pode ver aqui, não é muito
melhor do que ter X1 e X2 puros. Não há capacidade suficiente
para aprender este modelo. Vamos para três,
ver se consegue aprender. Está indo um pouco melhor
que a última vez com extrapolação aqui. No entanto, ainda não está
sendo tão bom quanto salvar todos os seis atributos ativados,
ou sete atributos. Vamos ver se podemos
adicionar mais um neurônio ou outra camada, talvez. Vamos ver se isso funciona. Tudo pronto, e você pode ver que temos uma perda de treino muito baixa para
as perdas de teste e está indo bem. Está travado. Vamos tentar mais um pouco, adicionando
mais algumas camadas ocultas. Vamos colocá-los para quatro. Espero que seja o suficiente. Vamos ver o que conseguimos. Ambas caíram um pouco. No entanto, ainda não tomou uma decisão,
pois toda a tela está branca. Aí está: tenho um ponto de inflexão e minha perda está diminuindo. No entanto, você pode ver
que a perda de teste também está subindo. Agora está constante.
Isso não tem capacidade suficiente. Vamos o mais longe possível e adicionar
oito neurônios a cada camada. Esperamos que seja o suficiente para
aprender esta função complexa e com ruído. Certo. Vamos tentar treinar isso. Como você vê, está indo muito devagar,
quando faz este treino aqui. Esperamos que descubra uma maneira
de fazer essa função funcionar. Minha perda de treino
está diminuindo. Porém, minha perda de teste está subindo. É uma espécie de nivelamento,
minha perda de teste. Quando você está fazendo isso sozinho, seus resultados podem variar, devido
a inicializações aleatórias da rede. Vamos tentar algo diferente. Este talvez seja um pouco mais promissor. Certo, isso parece
um pouco mais promissor. Veja o que ele está fazendo, está
aprendendo esses modos aqui, preenchendo. Parece que sobreajustamos, porque a perda
de teste está divergente, isso não é bom. E aqui vamos. Então, como você vê, mesmo com esse monte de redes, não podemos aprender
muito bem essa distribuição. Temos todas essas extrapolações e suposições amplas, e isso não vai ser
bom na nossa perda de teste. Veja isso. Nossa perda de teste está diminuindo,
de repente, e isso é ótimo. Certo, a função está cada vez
mais aprendida. No entanto, está indo muito devagar
devido ao tamanho dessa rede. Lembre-se, entre cada uma dessas camadas, há 64 ponderações entre cada uma. Como tenho seis camadas, eu tenho 6 vezes 64 ali. Não incluindo entre minha camada de
atributo e minha camada superior, em que recebo mais oito em cada. Aqui vamos nós, veja isto.
Isto é ótimo. Então, estou aprendendo
muito bem essa função. No entanto, há essas extrapolações, interpolações acontecendo aqui, como este pico laranja,
que atravessa a espiral. Ainda está melhorando com o tempo. Como você pode ver, a perda do teste
está diminuindo. No entanto, esta forma está
muito sobreajustada. Pronto. Como pode você pode ver, conseguimos, finalmente, encontrar as
formas de tudo isso, usando redes neurais. Às vezes, é um trabalho melhor ou o trabalho completo,
no caso da espiral, pois foi possível descobrir a forma.