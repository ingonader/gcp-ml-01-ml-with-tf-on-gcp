1
00:00:00,000 --> 00:00:02,940
Falamos sobre redes neurais
em cursos e módulos anteriores.

2
00:00:02,940 --> 00:00:05,175
Agora, vamos aprender
a ciência por trás delas.

3
00:00:05,175 --> 00:00:09,825
Vimos que os cruzamentos de atributos
funcionavam bem em um problema como esse.

4
00:00:09,825 --> 00:00:12,285
Se x1 é a dimensão horizontal

5
00:00:12,285 --> 00:00:14,220
e x2 é a dimensão vertical,

6
00:00:14,220 --> 00:00:18,480
não houve combinação linear dos dois
atributos para descrever a distribuição.

7
00:00:18,480 --> 00:00:23,640
Não até fazermos engenharia de atributos
e cruzarmos x1 e x2

8
00:00:23,640 --> 00:00:26,085
para conseguir um novo atributo, x3,

9
00:00:26,085 --> 00:00:30,030
que equivale a x1 vezes x2, em que podemos
descrever nossa distribuição de dados.

10
00:00:30,030 --> 00:00:33,520
Assim, o artesanato manual
para a engenharia de atributos

11
00:00:33,520 --> 00:00:36,260
pode resolver facilmente os
problemas não lineares.

12
00:00:36,260 --> 00:00:37,285
Certo?

13
00:00:38,225 --> 00:00:39,855
Infelizmente, o mundo real

14
00:00:39,855 --> 00:00:42,735
quase nunca tem distribuições
tão facilmente descritas.

15
00:00:42,735 --> 00:00:46,785
A engenharia de atributos, mesmo com
pessoas brilhantes trabalhando nela,

16
00:00:46,785 --> 00:00:48,495
só consegue chegar até esse ponto.

17
00:00:48,495 --> 00:00:52,950
Por exemplo, qual cruzamento de atributos
é preciso para modelar essa distribuição?

18
00:00:52,950 --> 00:00:57,945
Parecem dois círculos em cima um do
outro ou talvez duas espirais,

19
00:00:57,945 --> 00:01:00,630
mas seja o que for, é muito bagunçado.

20
00:01:00,630 --> 00:01:04,550
Este exemplo configura a utilidade das
redes neurais para que elas possam criar

21
00:01:04,550 --> 00:01:08,650
algoritmicamente cruzamentos
e transformações muito complexos.

22
00:01:08,650 --> 00:01:12,240
Você pode imaginar espaços
muito mais complicados

23
00:01:12,240 --> 00:01:16,380
que esta espiral que realmente
requerem o uso de redes neurais.

24
00:01:16,970 --> 00:01:21,990
Elas são uma alternativa ao cruzamento
de atributos, por combinar atributos.

25
00:01:21,990 --> 00:01:25,110
Quando estávamos projetando
nossa arquitetura de rede neural,

26
00:01:25,110 --> 00:01:28,770
queríamos estruturar o modelo de modo
que haja atributos combinados.

27
00:01:29,360 --> 00:01:32,975
Depois, pretendíamos adicionar outra
camada para combinar nossas combinações

28
00:01:32,975 --> 00:01:36,980
e, em seguida, adicionar outra camada para
combinar essas combinações etc.

29
00:01:37,270 --> 00:01:39,090
Como escolhemos as combinações certas

30
00:01:39,090 --> 00:01:42,315
de nossos atributos e as combinações
deles etc.?

31
00:01:42,555 --> 00:01:45,780
Você consegue o modelo para aprendê-los
por meio de treino, é claro.

32
00:01:45,780 --> 00:01:49,095
Essa é a intuição básica
por trás das redes neurais.

33
00:01:49,095 --> 00:01:52,600
Essa abordagem não é necessariamente
melhor que cruzamentos de atributos,

34
00:01:52,600 --> 00:01:56,350
mas é uma alternativa flexível
que funciona bem em muitos casos.

35
00:01:57,370 --> 00:02:00,040
Aqui está uma representação
gráfica de um modelo linear.

36
00:02:00,040 --> 00:02:02,355
Temos três entradas: x1,

37
00:02:02,355 --> 00:02:05,885
x2 e x3 mostradas nos círculos azuis.

38
00:02:05,885 --> 00:02:09,889
Elas são combinadas com uma ponderação
dada nas bordas para produzir uma saída.

39
00:02:10,319 --> 00:02:12,620
Elas são muitas vezes
um termo extratendencioso,

40
00:02:12,620 --> 00:02:14,955
mas, para simplificar,
isso não é mostrado aqui.

41
00:02:14,955 --> 00:02:20,045
Este é um modelo linear, pois é uma forma
de y igual a w1 vezes x1,

42
00:02:20,045 --> 00:02:22,010
mais w2 vezes x2,

43
00:02:22,010 --> 00:02:23,915
mais w3 vezes x3.

44
00:02:23,915 --> 00:02:28,040
Agora, vamos adicionar uma camada oculta
à nossa rede de nós e bordas.

45
00:02:28,040 --> 00:02:32,640
Nossa camada de entrada tem três nós
e nossa camada oculta também tem três.

46
00:02:32,640 --> 00:02:35,210
Mas, agora, nós ocultos.

47
00:02:35,210 --> 00:02:37,565
Como essa é uma camada
completamente conectada,

48
00:02:37,565 --> 00:02:41,980
há três vezes três bordas
ou nove ponderações.

49
00:02:41,980 --> 00:02:45,070
Certamente, agora este é um modelo
não linear que podemos usar

50
00:02:45,070 --> 00:02:48,045
para resolver nossos
problemas não lineares, certo?

51
00:02:48,455 --> 00:02:51,340
Infelizmente, não.
Vamos entender melhor.

52
00:02:52,060 --> 00:02:56,415
A entrada para o primeiro nó oculto
é a soma ponderada de w1 vezes x1,

53
00:02:56,415 --> 00:02:58,515
mais w4 vezes x2,

54
00:02:58,515 --> 00:03:00,970
mais w7 vezes x3.

55
00:03:02,050 --> 00:03:05,640
A entrada para o segundo nó oculto
é a soma ponderada w2 vezes x1,

56
00:03:05,640 --> 00:03:10,395
mais w5 vezes x2, mais w8 vezes x3.

57
00:03:10,395 --> 00:03:15,110
A entrada para o terceiro nó oculto
é a soma ponderada w3 vezes x1,

58
00:03:15,110 --> 00:03:19,575
mais w6 vezes x2,
mais w9 vezes x3.

59
00:03:20,195 --> 00:03:23,035
Combinando tudo junto no nó de saída,

60
00:03:23,035 --> 00:03:25,685
temos w10 vezes h1,

61
00:03:25,685 --> 00:03:28,080
mais w11 vezes h2,

62
00:03:28,080 --> 00:03:30,225
mais w12 vezes h3.

63
00:03:30,225 --> 00:03:32,550
Lembre-se, porém, que h1,

64
00:03:32,550 --> 00:03:37,370
h2 e h3 são apenas combinações lineares
dos atributos de entrada.

65
00:03:37,370 --> 00:03:40,055
Portanto, expandindo isso,

66
00:03:40,055 --> 00:03:43,495
ficamos com um conjunto complexo
de constantes ponderadas multiplicadas

67
00:03:43,495 --> 00:03:46,780
por cada valor de entrada x1, x2 e x3.

68
00:03:48,700 --> 00:03:51,935
Podemos substituir cada grupo
de ponderações por uma nova ponderação.

69
00:03:51,935 --> 00:03:53,250
Parece familiar?

70
00:03:53,250 --> 00:03:56,530
Este é exatamente o mesmo
modelo linear de antes,

71
00:03:56,530 --> 00:04:00,995
apesar da adição de uma camada oculta
de neurônios. Então, o que aconteceu?

72
00:04:01,645 --> 00:04:04,050
E se adicionássemos outra camada oculta?

73
00:04:04,050 --> 00:04:07,830
Infelizmente, isso mais uma vez
recolhe até chegar

74
00:04:07,830 --> 00:04:11,835
a uma matriz de ponderação única,
multiplicada por cada uma das 3 entradas.

75
00:04:11,835 --> 00:04:13,790
É o mesmo modelo linear.

76
00:04:13,790 --> 00:04:18,450
Podemos continuar este processo
e ainda seria o mesmo resultado,

77
00:04:18,450 --> 00:04:23,250
mas exigiria muito mais computação
para treino ou previsão de uma arquitetura

78
00:04:23,250 --> 00:04:25,700
muito mais complicada do que necessário.

79
00:04:27,170 --> 00:04:29,600
Pensando nisso
de uma perspectiva de álgebra linear,

80
00:04:29,600 --> 00:04:33,455
você está multiplicando várias matrizes
juntas em uma cadeia.

81
00:04:33,455 --> 00:04:34,735
Neste exemplo,

82
00:04:34,735 --> 00:04:37,095
primeiro multiplico
uma matriz de 3x3,

83
00:04:37,095 --> 00:04:41,250
a transposição da matriz ponderada entre
a camada de entrada e camada oculta 1,

84
00:04:41,250 --> 00:04:45,280
pelo vetor de entrada 3x1,
resultando no vetor 3x1,

85
00:04:45,280 --> 00:04:48,845
que são os valores em cada neurônio oculto
na camada oculta 1.

86
00:04:49,165 --> 00:04:52,145
Definidas as segundas camadas ocultas
dos valores do neurônio,

87
00:04:52,145 --> 00:04:53,710
eu multiplico a transposição

88
00:04:53,710 --> 00:04:56,395
da matriz ponderada de 3x3
que conecta a camada oculta 1

89
00:04:56,395 --> 00:05:01,190
com a camada oculta 2 ao meu vetor
resultante na camada oculta 1.

90
00:05:01,190 --> 00:05:02,920
Como você pode imaginar,

91
00:05:02,920 --> 00:05:05,720
as duas matrizes ponderadas 3x3
podem ser combinadas

92
00:05:05,720 --> 00:05:08,400
em uma matriz 3x3, calculando primeiro

93
00:05:08,400 --> 00:05:12,570
o produto da matriz da esquerda,
em vez da direita.

94
00:05:12,570 --> 00:05:15,450
Isso ainda dá a mesma forma para h2,

95
00:05:15,450 --> 00:05:18,720
o segundo vetor de valor
do neurônio da camada oculta.

96
00:05:18,720 --> 00:05:23,075
Adicionando a camada final entre
a camada oculta 2 e a camada de saída,

97
00:05:23,075 --> 00:05:25,170
eu preciso multiplicar
as etapas precedentes

98
00:05:25,170 --> 00:05:28,400
pela transposição da matriz ponderada
entre as duas últimas camadas.

99
00:05:28,400 --> 00:05:31,600
Mesmo que, ao avançar por
uma rede neural, você execute

100
00:05:31,600 --> 00:05:34,180
a multiplicação de matrizes
da direita para a esquerda,

101
00:05:34,180 --> 00:05:36,360
aplicando-a da esquerda para a direita,

102
00:05:36,360 --> 00:05:40,010
você pode ver que nossa grande cadeia
de complicações matriciais

103
00:05:40,010 --> 00:05:42,750
se recolhe a apenas
um vetor de três valores.

104
00:05:42,750 --> 00:05:46,200
Se você treinar este modelo em apenas
um caso de regressão linear simples

105
00:05:46,200 --> 00:05:50,760
de 3 ponderações lado a lado e elas caírem
no mesmo mínimo na superfície baixa,

106
00:05:50,760 --> 00:05:54,865
então, mesmo que eu tenha feito muitos
cálculos para todas as 21 ponderações

107
00:05:54,865 --> 00:05:58,859
em minha cadeia de produtos de matriz
que se condensará na equação mais baixa,

108
00:05:58,859 --> 00:06:02,830
a ponderação corresponderá às ponderações
de regressão linear simples de treino.

109
00:06:02,830 --> 00:06:05,205
Todo esse trabalho para o mesmo resultado.

110
00:06:05,205 --> 00:06:07,455
Você provavelmente está pensando agora:

111
00:06:07,455 --> 00:06:11,280
"eu pensei que redes neurais eram
a adição de camadas em neurônios.

112
00:06:11,280 --> 00:06:15,765
Como posso fazer aprendizado profundo
quando minhas camadas se recolhem em uma?"

113
00:06:15,765 --> 00:06:17,190
Tenho boas notícias.

114
00:06:17,190 --> 00:06:19,025
Há uma solução fácil.

115
00:06:19,025 --> 00:06:22,670
A solução é adicionar uma camada
de transformação não linear,

116
00:06:22,670 --> 00:06:28,510
facilitada por uma função de ativação não
linear, como sigmoide, tanh ou ReLU.

117
00:06:28,510 --> 00:06:31,980
E pensando nos termos do gráfico, como
se estivesse fazendo no TensorFlow,

118
00:06:31,980 --> 00:06:35,580
você pode imaginar cada neurônio
tendo, na verdade, dois nós.

119
00:06:35,580 --> 00:06:39,865
O primeiro nó é o resultado
da soma ponderada wx mais b,

120
00:06:39,865 --> 00:06:41,865
e o segundo nó é o resultado

121
00:06:41,865 --> 00:06:44,430
disso sendo passado
pela função de ativação.

122
00:06:44,430 --> 00:06:46,350
Em outras palavras, há entradas

123
00:06:46,350 --> 00:06:49,770
da função de ativação seguidas pelas
saídas da função de ativação,

124
00:06:49,770 --> 00:06:53,735
de modo que a função de ativação atua
como o ponto de transição entre elas.

125
00:06:53,735 --> 00:06:56,790
Adicionar essa transformação
não linear é a única maneira

126
00:06:56,790 --> 00:07:00,280
de impedir que a rede neural volte
a se condensar em uma rede superficial.

127
00:07:00,280 --> 00:07:04,280
Mesmo se você tiver uma camada com
função de ativação não linear na rede,

128
00:07:04,280 --> 00:07:09,585
se em algum lugar você tiver duas ou mais
camadas com funções de ativação linear,

129
00:07:09,585 --> 00:07:12,085
elas ainda poderão ser recolhidas
a uma única rede.

130
00:07:12,095 --> 00:07:15,010
Geralmente, as redes neurais têm
todas as camadas não lineares

131
00:07:15,010 --> 00:07:17,840
para a primeira e -1
camadas e, em seguida,

132
00:07:17,850 --> 00:07:21,245
deixam a transformação da camada final
linear para regressão

133
00:07:21,245 --> 00:07:25,175
ou sigmoide ou softmax, que
falaremos em breve para classificação.

134
00:07:25,175 --> 00:07:27,620
Tudo depende de como você
quer que seja a saída.

135
00:07:27,620 --> 00:07:29,720
Pensando sobre isso novamente,

136
00:07:29,720 --> 00:07:32,090
de uma perspectiva
de álgebra linear, quando

137
00:07:32,090 --> 00:07:34,700
aplicamos transformação
linear a uma matriz ou vetor,

138
00:07:34,700 --> 00:07:39,540
estamos multiplicando-os para
levar ao resultado que queremos.

139
00:07:39,540 --> 00:07:41,840
Como quando quero escalonar uma matriz,

140
00:07:41,840 --> 00:07:43,675
posso multiplicá-la por uma constante.

141
00:07:43,675 --> 00:07:46,789
Mas o que é feito é a multiplicação por
uma matriz de identidade

142
00:07:46,789 --> 00:07:48,410
multiplicada por essa constante.

143
00:07:49,150 --> 00:07:52,600
Então, é uma matriz diagonal
com essa constante toda na diagonal.

144
00:07:52,600 --> 00:07:55,690
Isso pode ser recolhido em apenas
um produto de matriz.

145
00:07:55,690 --> 00:07:59,070
No entanto, se eu adicionar
uma não linearidade,

146
00:07:59,070 --> 00:08:02,660
o que estou fazendo não pode ser
representado por uma matriz,

147
00:08:02,660 --> 00:08:05,800
já que o elemento y está aplicando
uma função na minha entrada.

148
00:08:05,800 --> 00:08:07,550
Por exemplo, se eu tiver

149
00:08:07,550 --> 00:08:11,315
uma função de ativação não linear entre
a primeira e a segunda camada oculta,

150
00:08:11,315 --> 00:08:14,630
aplico uma função do produto
da transposição das primeiras

151
00:08:14,630 --> 00:08:17,545
matrizes ponderadas das camadas
ocultas e do vetor de entrada.

152
00:08:17,545 --> 00:08:20,740
A equação mais baixa é minha função
de ativação em uma ReLU.

153
00:08:21,160 --> 00:08:24,560
Como não posso representar a transformação
em termos de álgebra linear,

154
00:08:24,560 --> 00:08:27,890
não posso mais recolher essa parte da
minha cadeia de transformação,

155
00:08:27,890 --> 00:08:30,800
de modo que a complexidade
do modelo permanece

156
00:08:30,800 --> 00:08:34,325
e não se recolhe em apenas
uma combinação linear das entradas.

157
00:08:34,325 --> 00:08:38,299
Observe que ainda posso recolher a
segunda camada oculta da matriz ponderada

158
00:08:38,299 --> 00:08:42,909
e a matriz ponderada da camada de saída,
já que não há função não linear aplicada.

159
00:08:43,519 --> 00:08:47,540
Isso significa que sempre que houver duas
ou mais camadas lineares consecutivas,

160
00:08:47,540 --> 00:08:51,515
elas poderão ser recolhidas em uma camada,
independentemente de quantas sejam.

161
00:08:51,515 --> 00:08:55,415
Como elas têm as funções mais complexas
sendo criadas pela sua rede,

162
00:08:55,415 --> 00:08:58,790
é melhor ter toda a sua rede
com funções de ativação linear,

163
00:08:58,790 --> 00:09:02,710
exceto na última camada, caso você queira
usar um tipo diferente de saída no final.

164
00:09:03,120 --> 00:09:07,835
Por que é importante adicionar funções
de ativação não linear às redes neurais?

165
00:09:08,495 --> 00:09:11,060
A resposta é que ela impede
que as camadas se recolham

166
00:09:11,060 --> 00:09:12,985
a apenas um modelo linear.

167
00:09:12,985 --> 00:09:15,415
As funções de ativação não linear
não apenas ajudam

168
00:09:15,415 --> 00:09:18,470
a criar transformações por meio
do espaço de escritura de dados,

169
00:09:18,470 --> 00:09:21,260
mas também permitem funções
de composição profundas.

170
00:09:21,260 --> 00:09:26,590
Como explicamos, se há duas ou mais
camadas com funções de ativação linear,

171
00:09:26,590 --> 00:09:28,600
esse produto de matrizes pode ser resumido

172
00:09:28,600 --> 00:09:31,560
por apenas uma matriz
vezes o vetor de atributo de entrada.

173
00:09:31,560 --> 00:09:34,420
Portanto, você acaba com
um modelo mais lento,

174
00:09:34,420 --> 00:09:38,605
com mais computação, mas com toda
a sua complexidade funcional reduzida.

175
00:09:38,605 --> 00:09:41,620
As não linearidades
não adicionam regularização

176
00:09:41,620 --> 00:09:45,015
à função de perda e não invocam
a parada antecipada.

177
00:09:45,015 --> 00:09:47,650
Mesmo que as funções
de ativação não linear criem

178
00:09:47,650 --> 00:09:49,910
transformações complexas
no espaço vetorial,

179
00:09:49,910 --> 00:09:53,200
essa dimensão não muda,
permanece o mesmo espaço vetorial.

180
00:09:53,200 --> 00:09:56,790
Mesmo esticado, esmagado ou girado.

181
00:09:56,790 --> 00:09:59,680
Como mencionado em um
de nossos cursos anteriores,

182
00:09:59,680 --> 00:10:03,110
há muitas funções de ativação
não linear com sigmoide,

183
00:10:03,110 --> 00:10:04,700
sendo que a tangente hiperbólica

184
00:10:04,700 --> 00:10:07,535
com sigmoide escalonado e deslocado
é uma das mais antigas.

185
00:10:07,535 --> 00:10:09,725
No entanto, como mencionado antes,

186
00:10:09,725 --> 00:10:13,030
eles podem ter saturação, o que leva
ao problema do gradiente de fuga,

187
00:10:13,030 --> 00:10:14,510
em que, com gradientes nulos,

188
00:10:14,510 --> 00:10:17,945
as ponderações dos modelos não são
atualizadas e o treino é interrompido.

189
00:10:17,945 --> 00:10:21,140
A Unidade Linear Retificada,
ou ReLU, é um dos

190
00:10:21,140 --> 00:10:24,215
nossos métodos favoritos porque
é simples e funciona bem.

191
00:10:24,215 --> 00:10:26,450
No domínio positivo, ela é linear,

192
00:10:26,450 --> 00:10:30,440
então não temos saturação, enquanto
no domínio negativo a função é zero.

193
00:10:30,440 --> 00:10:34,855
Redes com ativação oculta de ReLU
têm 10 vezes mais

194
00:10:34,855 --> 00:10:39,050
velocidade de treino do que redes
com ativações ocultas de sigmoides.

195
00:10:39,050 --> 00:10:42,590
No entanto, devido à função de domínios
negativos ser sempre zero,

196
00:10:42,590 --> 00:10:45,210
podemos acabar com
as camadas reais morrendo.

197
00:10:45,210 --> 00:10:46,910
O que quero dizer é que,

198
00:10:46,910 --> 00:10:48,570
quando você recebe entradas

199
00:10:48,570 --> 00:10:51,725
no domínio negativo,
a saída da ativação será zero,

200
00:10:51,725 --> 00:10:55,100
o que não ajuda na próxima camada
e recebe entradas no domínio positivo.

201
00:10:55,100 --> 00:10:59,150
Isso compõe e cria
um monte de ativações zero,

202
00:10:59,150 --> 00:11:01,860
durante a propagação de volta
ao atualizar as ponderações,

203
00:11:01,860 --> 00:11:05,170
uma vez que temos que multiplicar
os erros derivados pela ativação,

204
00:11:05,170 --> 00:11:06,950
e acabamos com um gradiente de zero.

205
00:11:06,950 --> 00:11:09,650
Portanto, uma ponderação
de dados zero, as ponderações

206
00:11:09,650 --> 00:11:13,800
não mudam e o treinamento
falha para essa camada.

207
00:11:14,440 --> 00:11:17,220
Felizmente, muitos métodos
inteligentes foram desenvolvidos

208
00:11:17,220 --> 00:11:20,980
para modificar levemente a ReLU e
garantir que o treinamento não pare,

209
00:11:20,980 --> 00:11:24,120
mas ainda assim, com muitos
benefícios da ReLU convencional.

210
00:11:24,120 --> 00:11:25,880
Aqui está a ReLu convencional.

211
00:11:25,880 --> 00:11:28,300
O operador máximo também pode ser
representado

212
00:11:28,300 --> 00:11:30,270
pela equação linear por partes,

213
00:11:30,270 --> 00:11:32,790
em que menos de zero, a função é zero.

214
00:11:32,790 --> 00:11:36,190
E maior que ou igual a zero,
a função é X.

215
00:11:36,190 --> 00:11:38,520
Uma aproximação da função de ReLU

216
00:11:38,520 --> 00:11:41,205
é a função analítica
do registro natural de 1,

217
00:11:41,205 --> 00:11:43,185
mais o X exponencial.

218
00:11:43,185 --> 00:11:45,360
Isso é chamado de função Softplus.

219
00:11:45,360 --> 00:11:49,740
Curiosamente, a derivada da função
Softplus é uma função logística.

220
00:11:49,740 --> 00:11:52,210
Os prós de usar a função Softplus são:

221
00:11:52,210 --> 00:11:54,570
ela é contínua e diferenciável em zero,

222
00:11:54,570 --> 00:11:56,380
ao contrário da função ReLU.

223
00:11:56,380 --> 00:11:59,449
No entanto, devido ao registro
natural e exponencial,

224
00:11:59,449 --> 00:12:02,295
há mais computação
em comparação com as ReLUs,

225
00:12:02,295 --> 00:12:06,030
e as ReLUs ainda têm resultados 
igualmente bons na prática.

226
00:12:06,030 --> 00:12:10,765
Portanto, Softplus, geralmente, não é
recomendado no aprendizado profundo.

227
00:12:10,765 --> 00:12:14,895
Para tentar resolver nosso problema
de ReLUs mortos devido a ativações zero,

228
00:12:14,895 --> 00:12:16,995
o Leaky ReLU foi desenvolvido.

229
00:12:16,995 --> 00:12:20,975
Assim como ReLUs, Leaky ReLUs têm
uma função linear por partes.

230
00:12:20,975 --> 00:12:23,180
No entanto, no domínio negativo,

231
00:12:23,180 --> 00:12:28,225
em vez de zero, há uma inclinação
diferente de zero, especificamente, 0,01.

232
00:12:28,225 --> 00:12:31,345
Dessa forma, quando a unidade
não está ativada,

233
00:12:31,345 --> 00:12:35,999
as Leaky ReLUs ainda permitem que um
pequeno gradiente diferente de zero passe,

234
00:12:35,999 --> 00:12:40,000
o que permitirá que a atualização
de ponderação e o treino continuem.

235
00:12:40,000 --> 00:12:46,290
Um passo adiante da ideia Leaky
é a ReLU paramétrica, ou PReLU.

236
00:12:46,290 --> 00:12:48,640
Aqui, em vez de permitir arbitrariamente

237
00:12:48,640 --> 00:12:51,625
um centésimo de um X no domínio negativo,

238
00:12:51,625 --> 00:12:53,910
ela permite que o alfa de X passe.

239
00:12:53,910 --> 00:12:57,190
Mas qual deveria ser o parâmetro
de alfa?

240
00:12:57,190 --> 00:13:01,585
No gráfico, defino alfa como 0,5
para fins de visualização.

241
00:13:01,585 --> 00:13:05,030
Mas na prática, na verdade, é um parâmetro
aprendido do treinamento

242
00:13:05,030 --> 00:13:07,735
junto com os outros
parâmetros da rede neural.

243
00:13:07,735 --> 00:13:11,045
Dessa forma, em vez de
definirmos esse valor,

244
00:13:11,045 --> 00:13:14,800
o valor será determinado durante o
treinamento por meio dos dados

245
00:13:14,800 --> 00:13:18,910
e provavelmente aprenderá um valor mais
otimizado do que nós definiríamos.

246
00:13:18,910 --> 00:13:21,520
Observe que quando alfa é menor que 1,

247
00:13:21,520 --> 00:13:25,270
a fórmula pode ser reescrita novamente
no formato compacto usando o máximo.

248
00:13:25,270 --> 00:13:28,490
Especificamente, o máximo de X
ou alfa vezes x.

249
00:13:28,490 --> 00:13:33,010
Há também Leaky ReLUs aleatórios
em que, em vez de o alfa ser treinado,

250
00:13:33,010 --> 00:13:35,740
é uma amostra de uma distribuição
uniforme aleatória.

251
00:13:35,740 --> 00:13:37,880
Isso pode ter um efeito
semelhante à exclusão,

252
00:13:37,880 --> 00:13:41,170
já que você tem uma rede
diferente para cada valor de alfa.

253
00:13:41,170 --> 00:13:43,980
E, portanto, está fazendo algo
semelhante a um conjunto.

254
00:13:43,980 --> 00:13:46,720
No momento do teste,
todos os valores de alfa

255
00:13:46,720 --> 00:13:50,005
são comparados juntos a um valor
determinístico para as previsões.

256
00:13:50,005 --> 00:13:52,550
Há também a variante ReLU6,

257
00:13:52,550 --> 00:13:56,565
outra função linear por partes
com três segmentos.

258
00:13:56,565 --> 00:13:58,040
Como uma ReLU normal,

259
00:13:58,040 --> 00:13:59,890
ela é zero no domínio negativo.

260
00:13:59,890 --> 00:14:03,105
No entanto, no domínio positivo,
a ReLU6 é mantida em seis.

261
00:14:03,105 --> 00:14:06,520
Você provavelmente está pensando:
"por que é mantida em seis?"

262
00:14:06,520 --> 00:14:09,370
Você pode imaginar uma dessas
unidades de ReLU tendo apenas

263
00:14:09,370 --> 00:14:12,220
seis unidades replicadas
por uma Bernoulli deslocada,

264
00:14:12,220 --> 00:14:15,520
em vez de uma quantidade
infinita devido ao limite máximo.

265
00:14:15,520 --> 00:14:18,520
Em geral, elas são chamadas
de unidades n de ReLU,

266
00:14:18,520 --> 00:14:20,265
em que n é o valor de limite.

267
00:14:20,265 --> 00:14:24,270
Em testes, seis foi definido
como o valor mais próximo do ideal.

268
00:14:24,270 --> 00:14:28,235
Unidades de ReLU6 podem ajudar os modelos
a aprender atributos esparsos mais cedo.

269
00:14:28,235 --> 00:14:31,360
Seu primeiro uso foi em redes
de elite profundas convolucionais

270
00:14:31,360 --> 00:14:33,740
em um conjunto
de dados de imagem CIFAR-10.

271
00:14:33,740 --> 00:14:36,280
Eles também têm a
propriedade útil de preparar

272
00:14:36,280 --> 00:14:38,700
a rede para precisão de ponto
fixo para inferência.

273
00:14:38,700 --> 00:14:40,639
Se o limite superior é ilimitado,

274
00:14:40,639 --> 00:14:44,005
você perde muitos bits para a
parte Q de um número de ponto fixo,

275
00:14:44,005 --> 00:14:46,090
enquanto que
um limite superior a seis

276
00:14:46,090 --> 00:14:48,830
deixa bits suficientes
para a parte fracionária do número,

277
00:14:48,830 --> 00:14:52,020
fazendo com que seja bem representado
para fazer uma boa inferência.

278
00:14:52,390 --> 00:14:55,540
Por fim, há a unidade linear
exponencial, ou ELU.

279
00:14:55,540 --> 00:14:59,765
É aproximadamente linear na porção
não negativa do espaço de entrada,

280
00:14:59,765 --> 00:15:02,680
e é suave, monotônica e, mais importante,

281
00:15:02,680 --> 00:15:05,320
diferente de zero
na porção negativa da entrada.

282
00:15:05,320 --> 00:15:07,645
Elas também são melhor centradas no zero

283
00:15:07,645 --> 00:15:10,555
do que ReLUs convencionais,
o que pode acelerar o aprendizado.

284
00:15:10,555 --> 00:15:14,340
A principal desvantagem das ELUs é que são
mais caras em termos de composição

285
00:15:14,340 --> 00:15:17,475
do que as ReLUs, devido a terem
que calcular o exponencial.

286
00:15:17,475 --> 00:15:20,285
As redes neurais podem ser
arbitrariamente complexas,

287
00:15:20,285 --> 00:15:21,710
pode haver muitas camadas,

288
00:15:21,710 --> 00:15:23,930
neurônios por camada, saídas, entradas,

289
00:15:23,930 --> 00:15:26,650
diferentes tipos
de funções de ativação etc.

290
00:15:26,650 --> 00:15:29,050
Qual o propósito de múltiplas camadas?

291
00:15:29,050 --> 00:15:30,340
Cada camada adicionada

292
00:15:30,340 --> 00:15:32,800
aumenta a complexidade
das funções que posso criar.

293
00:15:32,800 --> 00:15:36,540
Cada camada subsequente é uma composição
das funções anteriores.

294
00:15:36,540 --> 00:15:40,255
Como estamos usando funções de ativação
não linear nas minhas camadas ocultas,

295
00:15:40,255 --> 00:15:43,570
estou criando uma pilha de transformações
de dados que giram,

296
00:15:43,570 --> 00:15:45,510
esticam e espremem meus dados.

297
00:15:45,510 --> 00:15:48,070
Lembre-se, o propósito de fazer tudo isso

298
00:15:48,070 --> 00:15:50,800
é transferir os dados de modo
que seja possível encaixar

299
00:15:50,800 --> 00:15:52,790
o hiperplano para eles, para regressão,

300
00:15:52,790 --> 00:15:55,895
ou separar meus dados com um
hiperplano para classificação.

301
00:15:55,895 --> 00:16:01,140
Estamos mapeando do espaço de atributo
original para um espaço confuso.

302
00:16:01,910 --> 00:16:04,530
O que acontece se eu adicionar
neurônios a uma camada?

303
00:16:04,530 --> 00:16:08,370
Cada neurônio que adiciono acrescenta uma
nova dimensão ao meu espaço vetorial.

304
00:16:08,370 --> 00:16:10,515
Se eu começar com
três neurônios de entrada,

305
00:16:10,515 --> 00:16:12,510
começo no espaço vetorial R3.

306
00:16:12,510 --> 00:16:17,270
Mas se a próxima camada tiver quatro,
mudo para um espaço vetorial R4.

307
00:16:17,270 --> 00:16:20,140
Quando falamos sobre os métodos
Kernel no curso anterior,

308
00:16:20,140 --> 00:16:22,900
tínhamos um conjunto de dados
que não podia ser separado

309
00:16:22,900 --> 00:16:25,695
com um hiperplano no espaço
vetorial de entrada original.

310
00:16:25,695 --> 00:16:28,750
Mas, adicionando a dimensão
e, em seguida, transformando

311
00:16:28,750 --> 00:16:32,120
os dados para preencher a nova
dimensão, da maneira certa,

312
00:16:32,120 --> 00:16:37,425
conseguimos facilmente criar uma fatia
limpa entre as classes dos dados.

313
00:16:37,425 --> 00:16:39,570
O mesmo se aplica aqui com redes neurais.

314
00:16:40,460 --> 00:16:43,100
O que acontece se tenho
vários nós de saída?

315
00:16:43,920 --> 00:16:46,270
Ter vários nós de saída permite comparar

316
00:16:46,270 --> 00:16:49,995
com vários rótulos e depois propagar
as áreas correspondentes anteriores.

317
00:16:49,995 --> 00:16:52,850
Você pode imaginar a classificação
de imagens em que há

318
00:16:52,850 --> 00:16:56,045
várias entidades ou classes
dentro de cada imagem.

319
00:16:56,045 --> 00:16:59,660
Não podemos apenas prever uma classe
porque talvez haja muitas,

320
00:16:59,660 --> 00:17:02,405
então ter essa flexibilidade é ótimo.

321
00:17:02,675 --> 00:17:05,059
Redes neurais podem ser
arbitrariamente complexas.

322
00:17:05,059 --> 00:17:06,735
Para aumentar as dimensões ocultas,

323
00:17:06,735 --> 00:17:08,129
adiciono o quê?

324
00:17:08,129 --> 00:17:09,919
Para aumentar a composição da função,

325
00:17:09,919 --> 00:17:10,874
adiciono o quê?

326
00:17:10,874 --> 00:17:12,530
Se eu tiver vários rótulos,

327
00:17:12,530 --> 00:17:14,599
adiciono o quê?

328
00:17:15,619 --> 00:17:18,425
A resposta correta é
"neurônios, camadas, saídas".

329
00:17:18,425 --> 00:17:21,980
Para alterar as dimensões ocultas,
altero o número de camadas de neurônios.

330
00:17:21,990 --> 00:17:24,140
Isso determina dimensões
do espaço vetorial,

331
00:17:24,140 --> 00:17:25,950
pois o vetor intermediário
está dentro.

332
00:17:25,950 --> 00:17:27,380
Se uma camada tem 4 neurônios,

333
00:17:27,380 --> 00:17:29,060
está no espaço vetorial R4,

334
00:17:29,060 --> 00:17:33,080
e se uma camada tem 500 neurônios,
está no espaço vetorial R500.

335
00:17:33,080 --> 00:17:36,935
Significa que tem 500 dimensões reais.

336
00:17:36,935 --> 00:17:41,970
Adicionar uma camada não altera a dimensão
da camada anterior, e talvez

337
00:17:41,970 --> 00:17:43,970
nem altere a dimensão na camada dela,

338
00:17:43,970 --> 00:17:47,330
a menos que tenha um número diferente
de neurônios na camada anterior.

339
00:17:47,330 --> 00:17:51,995
O que camadas adicionais acrescentam
é mais composição de funções.

340
00:17:51,995 --> 00:17:54,020
Lembre-se, G de F de X,

341
00:17:54,030 --> 00:17:58,355
é a composição da função G
com a função F na entrada X.

342
00:17:58,355 --> 00:18:01,960
Portanto, primeiro transformo
X por F e depois transformo

343
00:18:01,960 --> 00:18:03,350
esse resultado por G.

344
00:18:03,350 --> 00:18:06,530
Quanto mais camadas, mais
profundamente as funções aninhadas vão.

345
00:18:06,530 --> 00:18:09,650
Isso é ótimo para combinar
funções não lineares em conjunto

346
00:18:09,650 --> 00:18:11,610
para criar mapas de atributos
complicados,

347
00:18:11,610 --> 00:18:14,930
difíceis de serem criados por humanos,
mas ótimos para computadores.

348
00:18:14,930 --> 00:18:17,150
Além disso,
nos permitem colocar nossos dados

349
00:18:17,150 --> 00:18:19,790
em uma forma melhor
para aprender e ter insights dela.

350
00:18:19,790 --> 00:18:23,040
Falando de insights, nós os recebemos por
meio das camadas de saída.

351
00:18:23,040 --> 00:18:27,275
Durante a inferência, serão as respostas
para o problema formulado pelo ML.

352
00:18:27,275 --> 00:18:30,865
Se você só quer saber a probabilidade
de uma imagem ser um cão,

353
00:18:30,865 --> 00:18:33,440
pode conseguir com
apenas um nó de saída.

354
00:18:33,440 --> 00:18:36,580
Mas se quiser saber a probabilidade de uma
imagem ser um gato, cão,

355
00:18:36,580 --> 00:18:37,800
pássaro ou alce,

356
00:18:37,800 --> 00:18:40,450
então você precisa ter um nó para cada um.

357
00:18:41,106 --> 00:18:45,716
As outras três respostas estão erradas,
pois têm duas ou mais palavras erradas.