ではNNのIntensibleプレイグラウンドで
遊びましょう このNNのプレイグラウンドラボでは Intensiveプレイグラウンドを使って NNを構築してデータを学習させます これらの問題を2つの方法で解決します 最初は手動の特徴エンジニアリングを使用して
モデルをトレーニングします ここでは独自の知識を使用して データの学習に最適な特徴の組み合わせと
変換を推測します 次に操作権を ニューラルネットワークに渡し 簡単な入力特徴を使用して 層とニューロンを追加し
特徴エンジニアリングの実施を確認します Intensibleプレイグラウンドに戻ります このラボでは特徴エンジニアリングが NNより優れているかを確認します 私はそうはならないと思います
調べてみましょう この図表では 青とオレンジの点の分類を行います
これは分類問題です これらの点は2つの同心円のように見えます しかしこのケースには多数のノイズがあります そのため多数の混合が生じています ここではX1とX2がトレーニングで
どのような動きをするかを確認します 見てわかるようにほとんど学習していません 全体的に不鮮明で ほとんど白です そのためどちらにもなりません 下のスケールによると-1 0 1です つまりあまり学習していません
改善できるか確認しましょう 特徴エンジニアリングではこれは円になります そこでX1の2乗とX2の2乗を行います ここでやってみます どうですか 見てください 楕円になりました この関数が何かを
もうすぐ把握できるということです 円であることがわかりますが 多くのノイズがあります そのため少し外側にシフトします しかしX1とX2を取り除いて 線形を形成すれば 損失を0.275から 下げることができます
やってみましょう 0.285です もう少し丸くなりました しかしテスト損失が少し改善されました 同じことがNNでできるかどうか見てみましょう X1とX2だけに戻りましょう すでに見たように 非常にひどい結果しか
出ていません 隠れ層を追加します
ドメインを2つ追加します ご覧のようにこの関数の把握に非常に
苦労しています 問題はこの2つのニューロンに
十分な容量がないことです この分布を学習できるほど高次元表現では
ありません ここで停止して見てみましょう ニューロンを追加します この関数の学習にはこの容量で十分でしょう できました まだうまくいきません これを見てみます 時間がかかりましたが 徐々に関数の形状を把握しています ここではある種の長方形です つまりこの分布を示せるニューロンの量の 変わり目をさまよっているようです もう1つニューロンを追加して どうなるか見てみましょう はるかに短時間で終わりました ここではニューロンは4つだけですが さらに多数のニューロンを追加すると
どうなるでしょうか それでは4つの型を入れましょう 何が起こるか見てみましょう トレーニングします かなり低速です これらの中間層の処理に多数の計算が
行われています 最終的にはうまくいくと思います ご覧のように少し過学習かもしれません もう単純な円ではありません 規則性のない多角形です つまりデータの過学習で
テスト損失に対処できていません 前より高くなっています 他の分布を見てみましょう ここに標準の分布Xrがあります XとYの両方が正または負の場合は青になり どちらかの場合はオレンジクラスになります これをX1 X2だけで学習できるか確認します ご覧のように 前と同じように IqとX2はこの関数を表せるほど
強力ではありません ボード全体で基本的に0です 特徴エンジニアリングでこれを把握できるか
確認します 特徴エンジニアリングでは その見た目がわかっている
X1 X2を導入します これをトレーニングします
見てください かなりいいですね テスト損失は0.17です
素晴らしい 非常に簡単でした これが重みで0.19です 素晴らしい 確かにノイズもあるので間違いもありますが 大半の部分がかなり正確です ではNNを使った機械学習が これより優れた処理を行えるか確認します X1とX2を戻して 隠れ層を追加します もう一度見てみましょう できるだけ量は少なくしたいので これを2つのニューロンにして学習させます しかしご覧のように これを把握できていません このモデルは複雑性も容量も不十分です ここで停止して3つ目のニューロンを追加します もう一度トレーニングしましょう ご覧のように この関数の学習に非常に苦労しています おそらくもう少しでしょう もう少し待たないと学習するかどうか
わかりませんが スタックしたようです 初期化すれば直るでしょう どうでしょうか あらゆることを行い 初期化しました ある程度関数を学習するでしょう 斜めの砂時計のようにも見えます しかし関数ではありません 損失も大きく上昇しています 4つに増やしましょう これでうまくいくかもしれません まだ砂時計も見えますが 一連の四角のように見え始めています それがこの関数です よくなっています ここで大量に追加して 過学習になるかどうか見てみましょう ご覧のように非常に低速化し
トレーニングの損失も生じています しかしかなり正方形になっています いいように見えます 他の分布タイプを試します ここにらせんがあります 2つのらせんが互いの周りを回っています 銀河の写真のようです これをX1 X2でトレーニングできるか
確認します おそらくできないでしょう ご覧のように 分布をまったく学習していません ほぼ0に近い状態です これが何かを判断できていません 次は特徴エンジニアリングを試します 試してみましょう どうしますか？ 円を試しましょうか？ いいえ これを追加しましょう 正弦と余弦 sineX1とsineX2です
実行しています ここに6つの未加工の特徴が発生して 入り込んでいます 上部を見ると ここがゆっくりと近づいています ここに大きな隙間があります ここは非常に強く外挿しています あまりうまく動作していません ご覧のように停止しています これをNNでうまくできるか確認します これをすべてオフにし 隠れ層を追加します 2つのニューロンで開始したら
うまくいくか確認します ご覧のようにX1とX2のときと
まったく変わりません このモデルを学習する容量がありません 3つに増やして学習できるか見てみます 前回より少しよくなり
ここに外挿がありますが この活性化した6つの特徴の保存は それほどうまく行えていません ではもう1つニューロンを追加できるか
確認します 層を追加します
これでできるか見てみましょう テスト損失に対して
トレーニング損失が非常低く うまくいっています スタックしています もう少し試します
隠れ層を追加します すべて4まで増やします これで十分でしょう 結果を見てみましょう 両方ともかなり下がりました しかし画面全体が白いので
判断は下せていません そこに変曲点があります 損失は大きく低下しています しかしテスト損失は上昇もしています その後は平坦です
やはり容量が足りません 次は思い切って各層にニューロンを
8つ追加します これならこの複雑でノイズの多い関数を
学習するのに十分でしょう できました トレーニングしてみましょう このトレーニングをここで結合するのに
非常に低速になっています 最後にはこの関数を動かす方法が
見つかるはずです トレーニング損失は低下しています テスト損失は上昇しています テスト損失はどちらかと言えば平坦です 皆さんがこれを行った場合 ネットワークのランダム初期化により 結果はかなり変動するでしょう
他のものを試しましょう これはもう少し期待できそうです これはもう少し期待できそうに見えます これらをここで学習して埋めているのが
わかります テスト損失が分岐しているので過学習のようです
これはよくありません そうです ご覧のように この山のようなネットワークでも この分布をうまく学習できません こうした外挿や 広範な推測が行われ テスト損失の結果も
あまりよくありません これを見てください テスト損失が急に低下しています 
素晴らしい 学習した関数の量が増えています しかしネットワークが巨大なため非常に低速です これらの各層の間には 64の重みがあります 6層あるので 6 x 64の重みがあります 特徴層と上層の間は含みません そこには他の8つがあります これを見てください 
これは素晴らしい この関数をかなりよく学習しています しかしこれらの外挿があり ここには内挿があり このオレンジのピークはらせんの中心を
通過しています 時間が経つとさらに少しずつよくなります テスト損失は低下しています しかしこの形状は非常に過学習です 準備完了です 
ご覧のようにようやく NNを使ってこうした形状を見つけることが
できました らせんの場合はNNの方が 適しているときもあります NNでこの形状を見つけ出せました