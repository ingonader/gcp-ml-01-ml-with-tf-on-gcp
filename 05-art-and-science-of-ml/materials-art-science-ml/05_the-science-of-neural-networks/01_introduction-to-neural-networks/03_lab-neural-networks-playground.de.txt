Im praktischen Teil experimentieren wir mit
neuronalen Netzen in TensorFlow Playground. In diesem Lab mit dem
Neuronale-Netze-Spielplatz versuchen wir mit TensorFlow Playground, neuronale Netzwerke
zu erstellen und Daten zu trainieren. Sie sollen die Probleme
mit zwei Verfahren lösen. Zuerst trainieren Sie die Modelle
manuell mit Feature Engineering. Sie versuchen dabei
anhand Ihres eigenen Wissens abzuschätzen, wie Features am besten
zu kombinieren und transformieren sind. Dann geben Sie die Verantwortung an das neuronale Netzwerk ab und ergänzen es mit Schichten
und Neuronen. Mit einfachen Eingabefeatures prüfen wir, ob es
Feature Engineering selbst ausführen kann. Willkommen zurück
bei TensorFlow Playground. In diesem Lab prüfen wir,
ob Feature Engineering die Leistung des
neuronalen Netzes übertreffen kann. Ich denke, das wird nicht
möglich sein. Wir werden sehen. Ok. In diesem Diagramm hier versuchen wir, die blauen und orangefarbenen Punkte zu
klassifizieren. Ein Klassifizierungsproblem. Die Punkte sind in zwei
konzentrischen Kreisen angeordnet. In diesem Fall gibt es
jedoch sehr viel Rauschen. Daher findet
eine große Vermischung statt. Ich möchte nun sehen, wie
sich X1 und X2 beim Training verhalten. Wie Sie sehen,
lernt das Modell nicht viel. Alles ist irgendwie ineinander
verlaufen und ziemlich weiß. Daher gibt es wenige
Entweder/Oder-Ergebnisse, nach der Skala hier unten,
-1 oder 1. Der Lerneffekt ist also gering.
Das geht bestimmt besser. Mit Feature Engineering
weiß ich, dass das ein Kreis ist. Ich setze also X1 und X2 ins Quadrat und versuche es jetzt. Was passiert? Wow! Schauen Sie. Das wird eine Ellipse. Das heißt, das Modell
hat diese Funktion fast verstanden. Wir wissen, dass
das ein Kreis sein soll, es gibt aber viel Rauschen
und Ähnliches. Deshalb ist er etwas verzogen. Ok. Vielleicht kann ich
den Verlust wenigstens unter 0,275 bringen, wenn ich X1 und X2, die linearen Formen, deaktiviere. Aha, 2,85. Gut, das sieht etwas runder aus. Der Testverlust ist etwas besser. Wir versuchen nun
dasselbe mit neuronalen Netzen. Wir gehen zurück zur
Methode mit X1 und X2. Das Ergebnis vorhin
war ja wirklich schlecht. Wir fügen nun eine Zwischenschicht
und zwei zusätzliche Neuronen hinzu. Wie Sie sehen, scheint es schwierig
zu sein, diese Funktion zu verstehen. Das Problem ist, dass die Kapazität
in diesen beiden Neuronen nicht ausreicht, die hochdimensionale Darstellung
reicht nicht, um diese Verteilung zu lernen. Wir unterbrechen erstmal hier. Lassen Sie uns ein
weiteres Neuron hinzufügen. Vielleicht reicht diese Kapazität
aus, um die Funktion zu trainieren. Gut. Es funktioniert noch nicht richtig. Schauen Sie hier. Das hat lange gedauert, aber die Form dieser
Funktion wird langsam erkannt. Hier das erinnert an eine rechteckige Form. Wir sind hier an einem
Wendepunkt angekommen, was die Menge an Neuronen anbelangt,
die diese Verteilung hier darstellen können. Sehen wir also, ob es mit einem
zusätzlichen Neuron einfacher ist. Gut. Sehen Sie sich das an. Das ging viel schneller. Wir haben hier nur vier Neuronen. Was passiert, wenn wir sehr
viele zusätzliche Neuronen hinzufügen? Okay, mal sehen.
Fügen wir weitere vier hinzu. Was passiert? Das ist bereits trainiert. Es ist jetzt viel langsamer. Der Aufwand ist viel größer, weil alle
Zwischenschichten durchlaufen werden. Früher oder später wird es klappen. Ich befürchte, dass es zu
einer Überanpassung kommt. Sie sehen, die Form
ist nicht mehr einfach rund, sondern eine Art verrücktes Polygon. Das heißt, die Daten werden überangepasst
und der Testverlust ist nicht besonders gut. Er ist viel höher als vorher. Sehen wir uns
einige andere Verteilungen an. Hier haben wir das klassische
"exklusive ODER". Wenn X und Y jeweils
beide positiv oder negativ sind, sehen wir die blaue Klasse,
bei entweder/oder die orangefarbene Klasse. Ist es möglich,
das nur mit X1, X2 zu lernen? Sie sehen, wie schon vorher, sind X1 und X2 nicht leistungsstark
genug, um diese Funktion zu beschreiben. Sie sind praktisch durchweg null. Ist es möglich, die Funktion
mit Feature Engineering zu beschreiben? Mit Feature Engineering führe ich X1, X2 ein, denn ich weiß,
wie die Funktion aussieht. Ich starte das Training. Schauen Sie. Sehr gut. Der Testverlust liegt bei 0,17.
Das ist großartig. Okay. Das war sehr einfach. Und hier ist die Gewichtung: 0,19. Das ist
großartig. Ja, hier gibt es Rauschen,
das heißt, einige Ergebnisse sind falsch, überwiegend ist jedoch alles richtig. Vergleichen wir nun,
ob das mit maschinellem Lernen und neuronalen Netzwerken besser geht. Wir fügen X1 und X2 wieder zusammen und fügen eine Zwischenschicht ein. Auch hier müssen
wir das Ergebnis abwarten. Ich möchte die Menge
so klein wie möglich halten. Ich versuche, die Zahl auf zwei
Neuronen zu beschränken und zu trainieren. Wie Sie sehen, kann die Funktion jedoch nicht nachempfunden werden. Die Komplexität und die Kapazität
dieses Modells reichen nicht aus. Wir halten hier an und
fügen ein drittes Neuron hinzu. Wir starten das Training. Wie Sie sehen, fällt es dem Netz sehr schwer,
die Funktion zu erlernen. Nun, vielleicht steht
es noch auf der Kippe und wir müssen ihm etwas mehr Zeit geben. Es scheint aber zu hängen. Vielleicht hilft eine
erneute Initialisierung. Mal sehen, was passiert. Wir haben alles ausprobiert,
die Initialisierung ausgeführt. Die Funktion scheint
irgendwie trainiert zu werden. Das Ergebnis sieht aber eher
aus wie eine diagonale Sanduhr. Ja, das ist nicht ganz die Funktion. Wie Sie sehen, ist der Verlust viel größer. Erhöhen wir auf vier. Das hilft vielleicht. Noch immer eine Sanduhr. Sie gleicht aber mehr und mehr einer Reihe von Quadraten. Das entspricht genau der Funktion.
Es wird besser ... Ich möchte testen, ob es
mit weiteren Neuronen zu einer Überanpassung kommt. Wie Sie sehen, ist
der Trainingsverlust viel langsamer. Jedoch gleichen
die Formen mehr und mehr Quadraten. Das sieht sehr gut aus. Testen wir eine andere Verteilung. Hier haben wir eine Spirale. Eigentlich zwei Spiralen,
die einander umkreisen. Es sieht aus wie das Bild einer Galaxie. Ich möchte versuchen,
das Modell mit X1 und X2 zu trainieren. Ich bezweifle sehr, dass es möglich ist. Wie Sie hier sehen, wurde die Verteilung
überhaupt nicht erlernt. Im Grunde liegt das nahe bei null. Das Modell kann nicht
entscheiden, was was ist. Wir testen nun Feature Engineering. Los geht's. Was denken Sie? Versuchen wir Kreise. Keine Verbesserung.
Aktivieren wir diese Features. Das sind Sinus und Kosinus, oder sinex1 und sinex2. Es läuft. Ich habe
sechs Features aktiviert. Das Netz scheint zu lernen. Wie Sie sehen, füllt sich der Bereich hier oben. Hier bleibt eine große Lücke.
Ich weiß nicht, wie sich das entwickelt. Die Extrapolation ist
wirklich sehr stark hier. Das ist also nicht wirklich besser. Hier ist der Prozess
irgendwie ins Stocken geraten. Funktioniert das mit
neuronalen Netzwerken besser? Ich deaktiviere alle und füge eine Zwischenschicht hinzu. Ich starte mit zwei Neuronen
und teste, ob sie ausreichen. Das Ergebnis ist wirklich nicht viel
besser als vorher mit nur X1 und X2. Die Kapazität reicht
nicht aus, um dieses Modell zu trainieren. Erhöhen wir auf drei Neuronen.
Gibt es einen Lerneffekt? Es läuft etwas besser als beim letzten Mal.
Hier gibt es Extrapolation. Das Ergebnis ist jedoch
noch nicht so gut wie vorher, als ich alle sechs oder
sieben Features aktiviert hatte. Also gut. Vielleicht sollte ich
ein weiteres Neuron hinzufügen. Vielleicht noch eine weitere Schicht.
Hilft das? Gut. Wie Sie sehen, ist der Trainingsverlust sehr niedrig, aber
der Testverlust ist nicht besonders gut. Hier hängt der Prozess irgendwie. Lassen Sie uns weitere
Zwischenschichten hinzufügen. Insgesamt vier Neuronen pro Schicht. Hoffentlich reicht das aus. Wie sieht das Ergebnis aus? Okay, beide Werte
sind ziemlich stark gesunken. Allerdings wurde noch keine Entscheidung 
getroffen, denn der gesamte Bildschirm ist weiß. Jetzt aber. Hier ist der Wendepunkt. Der Verlust geht stark nach unten. Der Testverlust geht jedoch
gleichzeitig nach oben. Nun verläuft er gerade.
Die Kapazität reicht nicht aus. Ich gehe jetzt bis an die obere Grenze und
füge pro Schicht acht Neuronen hinzu. Das reicht hoffentlich aus, um diese
sehr komplexe und verrauschte Funktion zu trainieren. Also gut. Starten wir das Training. Wie Sie sehen,
verläuft das Training hier sehr langsam. Ich hoffe, dass wir noch herausfinden,
wie diese Funktion am besten funktioniert. Der Trainingsverlust geht nach unten. Was ist mit dem Testverlust?
Er geht nach oben. Jetzt wird der Testverlust aber stabiler. Wenn Sie selbst experimentieren, können die Ergebnisse
etwas abweichen, weil es zu zufälligen Initialisierungen des Netzwerks
kommen kann. Testen wir ein anderes. Dieses ist vielleicht etwas vielversprechender. Ja, tatsächlich,
es sieht vielversprechender aus. Schauen Sie, was passiert.
Es lernt dazu und wird aufgefüllt. Sieht aus, als gäbe es eine Überanpassung, 
denn der Testverlust divergiert. Das ist nicht gut. Wir warten noch etwas. Wie Sie sehen, ist es selbst mit diesem gewaltigen Netzwerk nicht möglich,
diese Verteilung gut zu trainieren. Es gibt viele Extrapolationen und großes Rätselraten und ist
nicht gut für den Testverlust. Schauen Sie hier. Der Testverlust geht plötzlich nach unten.
Sehr gut. Das Netz
erlernt die Funktion zunehmend. Es ist aber sehr langsam,
weil das Netzwerk sehr groß ist. Zur Erinnerung: Zwischen allen Schichten befinden sich jeweils 64 Gewichtungen. Sie haben jeweils sechs Schichten. Das heißt sechs mal 64, nur hier. Dabei sind diejenigen zwischen den
Feature-Schichten und der Ausgabeschicht nicht berücksichtigt. Da sind jeweils weitere acht. Sehen Sie sich das an. Das ist großartig. Die Funktion wird sehr gut erlernt. Hier gibt es jedoch Extrapolationen, und hier Interpolationen. Die orangefarbene Spitze
geht direkt durch die Spirale. Mit der Zeit wird es aber immer besser. Der Testverlust wird immer niedriger. Die Form ist jedoch in
hohem Maße überangepasst. Jetzt ist sie fertig. Wie Sie sehen,
ist es uns endlich gelungen, mit dem neuronalen Netzwerk
all diese Formen zu ermitteln. In manchen Fällen erfüllt das
neuronale Netz die Aufgabe besser, in anderen Fällen, etwa
bei der Spirale, war nur diese Methode überhaupt in der Lage,
die Form zu ermitteln.