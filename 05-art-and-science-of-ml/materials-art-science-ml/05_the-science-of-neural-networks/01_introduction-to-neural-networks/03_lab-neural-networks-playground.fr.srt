1
00:00:00,000 --> 00:00:04,395
Il est temps de passer à la pratique.

2
00:00:04,395 --> 00:00:07,775
Dans cet atelier, nous allons utiliser
TensorFlow Playground

3
00:00:07,775 --> 00:00:10,780
pour créer des réseaux de neurones
à des fins d'apprentissage.

4
00:00:10,780 --> 00:00:13,400
Vous allez devoir résoudre
ces problèmes de deux façons.

5
00:00:13,400 --> 00:00:17,600
D'abord, nous devons entraîner les modèles
avec l'extraction manuelle de caractéristiques

6
00:00:17,600 --> 00:00:20,230
qui nous permet,
grâce à nos connaissances, d'identifier

7
00:00:20,230 --> 00:00:23,670
la combinaison et la transformation
de caractéristiques appropriées.

8
00:00:23,670 --> 00:00:26,940
Ensuite, nous allons céder la place
au réseau de neurones,

9
00:00:26,940 --> 00:00:30,210
et ajouter d'autres couches et neurones
à l'aide d'un simple ensemble

10
00:00:30,210 --> 00:00:34,020
de caractéristiques d'entrées pour voir
si le réseau peut effectuer l'extraction.

11
00:00:34,020 --> 00:00:36,270
Nous voici à nouveau
dans TensorFlow Playground.

12
00:00:36,270 --> 00:00:39,600
Dans cet atelier, nous allons voir
si l'extraction des caractéristiques

13
00:00:39,600 --> 00:00:42,555
peut donner de meilleurs résultats
que les réseaux de neurones.

14
00:00:42,555 --> 00:00:45,835
J'ai l'impression que ce ne sera pas le cas.
Voyons cela de plus près.

15
00:00:45,835 --> 00:00:49,540
Bien. Dans ce graphe,
nous essayons de classifier

16
00:00:49,540 --> 00:00:55,465
les points orange et bleus.
C'est un problème de classification.

17
00:00:55,465 --> 00:00:58,040
Vous noterez qu'ils forment
deux cercles concentriques.

18
00:00:58,040 --> 00:01:00,420
Cependant, dans ce cas,
il y a énormément de bruit.

19
00:01:00,420 --> 00:01:03,790
Tout est mélangé.

20
00:01:03,790 --> 00:01:08,940
Je vais essayer de voir les performances
de x1 et x2 durant l'apprentissage.

21
00:01:10,670 --> 00:01:13,740
Comme vous le voyez,
l'apprentissage n'est pas très efficace.

22
00:01:14,380 --> 00:01:17,550
Tout est flou et assez blanc.

23
00:01:17,550 --> 00:01:19,870
Rien n'est vraiment clairement défini,

24
00:01:19,870 --> 00:01:22,430
à en juger par l'échelle -1, 0, 1.

25
00:01:22,430 --> 00:01:25,810
Le réseau n'a pas appris grand-chose.
Voyons si on peut faire mieux.

26
00:01:25,810 --> 00:01:29,250
Avec l'extraction des caractéristiques,
je sais qu'il s'agit d'un cercle.

27
00:01:29,250 --> 00:01:32,340
J'élève x1 et x2 au carré,

28
00:01:32,340 --> 00:01:34,665
puis je fais un essai. Voyons le résultat.

29
00:01:34,665 --> 00:01:38,085
Cela ressemble à une ellipse.

30
00:01:38,085 --> 00:01:43,070
Le réseau arrive presque
à identifier cette fonction.

31
00:01:43,070 --> 00:01:44,830
Nous savons qu'il s'agit d'un cercle,

32
00:01:44,830 --> 00:01:48,275
mais il y a beaucoup de bruit [inaudible].

33
00:01:48,275 --> 00:01:52,840
Mais je peux peut-être réduire
la perte de données à moins de 0,275.

34
00:01:52,840 --> 00:01:56,270
Lorsqu'on se débarrasse de x1 et de x2,
on a une représentation linéaire.

35
00:01:56,270 --> 00:01:58,065
Faisons un essai.

36
00:01:58,585 --> 00:02:00,670
0,285.

37
00:02:00,670 --> 00:02:02,670
Le cercle est plus net.

38
00:02:02,670 --> 00:02:06,560
Toutefois, la perte de test
est un peu meilleure.

39
00:02:06,560 --> 00:02:09,940
Voyons si nous pouvons faire la même chose
avec les réseaux de neurones.

40
00:02:09,940 --> 00:02:12,435
Revenons à x1 et x2.

41
00:02:12,435 --> 00:02:16,280
Comme nous l'avons vu tout à l'heure,
le résultat n'était pas probant.

42
00:02:16,280 --> 00:02:18,969
Ajoutons une couche cachée
et deux neurones.

43
00:02:22,339 --> 00:02:27,355
Comme vous pouvez le voir, le réseau
ne parvient pas à identifier la fonction.

44
00:02:28,155 --> 00:02:31,585
Le problème, c'est que la capacité
de ces deux neurones est insuffisante,

45
00:02:31,585 --> 00:02:36,250
tout comme la précision géométrique
pour l'apprentissage de cette distribution.

46
00:02:36,250 --> 00:02:37,965
Mettons cela en pause.

47
00:02:37,965 --> 00:02:39,530
Ajoutons un neurone.

48
00:02:39,530 --> 00:02:43,145
La capacité sera peut-être suffisante
pour l'apprentissage de cette fonction.

49
00:02:43,145 --> 00:02:44,495
Bien.

50
00:02:44,495 --> 00:02:52,840
Le réseau ne peut toujours pas l'identifier.

51
00:02:52,840 --> 00:02:54,140
Regardez.

52
00:02:54,140 --> 00:02:55,920
Cela a pris du temps,

53
00:02:55,920 --> 00:02:59,565
mais le réseau arrive petit à petit
à déterminer la forme de la fonction.

54
00:02:59,565 --> 00:03:03,465
Il s'agit d'une forme rectangulaire.

55
00:03:03,465 --> 00:03:06,900
Cela veut dire qu'on a presque
le nombre nécessaire de neurones

56
00:03:06,900 --> 00:03:10,060
pour représenter cette distribution.

57
00:03:10,060 --> 00:03:14,190
Voyons si nous pouvons faciliter
les choses en ajoutant un neurone.

58
00:03:14,190 --> 00:03:17,310
Regardez le résultat.

59
00:03:17,310 --> 00:03:19,545
Cela a été beaucoup plus rapide.

60
00:03:19,545 --> 00:03:21,390
Nous n'avons que quatre neurones.

61
00:03:21,390 --> 00:03:27,330
Voyons maintenant le résultat
si nous ajoutons un grand nombre de neurones.

62
00:03:27,330 --> 00:03:30,550
Ajoutons quatre neurones à chacune des couches.

63
00:03:30,550 --> 00:03:33,200
Voyons le résultat.

64
00:03:33,200 --> 00:03:34,810
Lançons l'entraînement.

65
00:03:36,210 --> 00:03:38,790
C'est beaucoup plus lent.

66
00:03:38,790 --> 00:03:42,510
Le nombre des calculs est bien plus élevé
en raison des couches intermédiaires.

67
00:03:42,510 --> 00:03:45,270
Je pense que le réseau
va finir par identifier la fonction.

68
00:03:45,270 --> 00:03:48,070
Je crains que l'on soit face
à un cas de surapprentissage.

69
00:03:48,070 --> 00:03:52,095
Ce n'est plus une sorte de cercle.
C'est un drôle de polygone.

70
00:03:52,095 --> 00:03:54,655
Le modèle correspond
trop étroitement aux données,

71
00:03:54,655 --> 00:03:56,750
ce qui n'est pas idéal
pour la perte de test,

72
00:03:56,750 --> 00:03:59,595
qui est plus élevée qu'auparavant.

73
00:03:59,595 --> 00:04:03,010
Regardons d'autres distributions.

74
00:04:03,010 --> 00:04:06,030
Voici une distribution classique
de type "x ou y".

75
00:04:06,030 --> 00:04:09,105
Si x et y sont tous les deux positifs,
ou négatifs,

76
00:04:09,105 --> 00:04:12,950
nous obtenons la classe bleue,
et si x et y diffèrent,

77
00:04:12,950 --> 00:04:14,440
nous obtenons la classe orange.

78
00:04:14,440 --> 00:04:17,380
Voyons si nous pouvons
faire un apprentissage avec x1 et x2.

79
00:04:20,790 --> 00:04:23,200
Comme vous l'avez déjà vu,

80
00:04:23,200 --> 00:04:27,520
x1 et x2 ne sont pas assez puissants
pour pouvoir décrire cette fonction.

81
00:04:27,520 --> 00:04:30,359
On obtient 0 de manière générale.

82
00:04:30,359 --> 00:04:34,265
Voyons si nous pouvons résoudre ce problème
avec l'extraction des caractéristiques.

83
00:04:34,265 --> 00:04:37,430
Grâce à l'extraction des caractéristiques,
je vais introduire x1x2,

84
00:04:37,430 --> 00:04:42,040
car je sais à quoi cela ressemble.
Lançons l'entraînement. Regardez.

85
00:04:42,040 --> 00:04:47,050
La perte de test est de 0,170. Excellent.

86
00:04:47,050 --> 00:04:49,395
Je n'ai pas eu de mal à trouver la solution.

87
00:04:49,395 --> 00:04:51,290
Voici la pondération, 0,190.

88
00:04:51,290 --> 00:04:55,675
Il y a du bruit
et tout n'est donc pas parfait.

89
00:04:55,675 --> 00:04:58,140
Mais dans l'ensemble,
le résultat est plutôt bon.

90
00:04:58,140 --> 00:05:00,050
Voyons maintenant si le machine learning

91
00:05:00,050 --> 00:05:03,700
peut faire mieux
avec les réseaux de neurones.

92
00:05:03,700 --> 00:05:06,085
Nous allons à nouveau
prendre à la fois x1 et x2,

93
00:05:06,085 --> 00:05:09,450
et nous allons ajouter une couche cachée.

94
00:05:09,450 --> 00:05:10,895
Une fois de plus, mon objectif

95
00:05:10,895 --> 00:05:13,660
est d'avoir le plus petit nombre possible.

96
00:05:13,660 --> 00:05:18,700
Je vais limiter cela à deux neurones
et lancer l'apprentissage.

97
00:05:18,700 --> 00:05:20,190
Cependant, comme vous le voyez,

98
00:05:20,190 --> 00:05:22,360
le ML ne sait pas
interpréter la fonction.

99
00:05:22,360 --> 00:05:25,230
La complexité et la capacité
de ce modèle sont insuffisantes.

100
00:05:25,230 --> 00:05:29,430
Mettons le processus en pause
et ajoutons un troisième neurone.

101
00:05:29,430 --> 00:05:32,340
Lançons à nouveau l'apprentissage.

102
00:05:34,280 --> 00:05:35,440
Comme vous le voyez ici,

103
00:05:35,440 --> 00:05:37,980
le réseau a du mal
à apprendre cette fonction.

104
00:05:37,980 --> 00:05:42,710
Il y est presque, et il me suffit
peut-être d'attendre un peu plus.

105
00:05:42,710 --> 00:05:44,900
Mais il est coincé.

106
00:05:44,900 --> 00:05:48,455
Une autre initialisation pourrait résoudre
le problème. Voyons cela…

107
00:05:48,455 --> 00:05:52,080
Nous y voici. Nous avons effectué
une autre réinitialisation,

108
00:05:52,080 --> 00:05:54,750
et le réseau apprend la fonction en partie.

109
00:05:54,750 --> 00:05:58,560
Elle ressemble un peu
à un sablier en diagonale.

110
00:05:58,560 --> 00:06:00,965
Mais ce n'est pas tout à fait la fonction.

111
00:06:00,965 --> 00:06:02,880
Vous voyez que la perte est plus élevée.

112
00:06:02,880 --> 00:06:07,595
Passons donc à quatre neurones. Voyons cela…

113
00:06:07,595 --> 00:06:11,680
Nous obtenons toujours un sablier,
mais la forme ressemble de plus en plus

114
00:06:11,680 --> 00:06:14,670
à une série de carrés,
ce qui représente bien notre fonction.

115
00:06:14,670 --> 00:06:19,180
Maintenant, si j'ajoute
toute une série de neurones,

116
00:06:19,180 --> 00:06:23,510
voyons si nous aboutissons
à un cas de surapprentissage.

117
00:06:28,190 --> 00:06:32,680
La perte d'apprentissage
est beaucoup plus lente.

118
00:06:32,680 --> 00:06:36,700
En revanche, la forme des carrés
est beaucoup plus apparente.

119
00:06:38,320 --> 00:06:40,160
C'est très encourageant.

120
00:06:45,230 --> 00:06:49,400
Essayons un autre type de distribution.

121
00:06:49,400 --> 00:06:53,325
Voici une spirale, deux spirales en fait
tournant l'une autour de l'autre.

122
00:06:53,325 --> 00:06:56,065
Un peu comme la photo d'une galaxie.

123
00:06:56,065 --> 00:06:59,410
Voyons si nous pouvons
effectuer l'entraînement avec x1 et x2.

124
00:06:59,410 --> 00:07:02,735
J'en doute fort.

125
00:07:02,735 --> 00:07:05,920
Comme vous le voyez ici,
le réseau n'a pas appris la distribution.

126
00:07:05,920 --> 00:07:10,490
On est proche de 0,
et on ne sait pas de quoi il s'agit.

127
00:07:10,490 --> 00:07:13,645
Essayons maintenant
l'extraction des caractéristiques.

128
00:07:13,645 --> 00:07:15,615
Faisons un essai.

129
00:07:15,615 --> 00:07:19,560
Que pensez-vous d'un cercle ?

130
00:07:20,520 --> 00:07:22,835
Non. Essayons plutôt cela.

131
00:07:22,835 --> 00:07:24,160
Sinus et cosinus,

132
00:07:24,160 --> 00:07:28,395
ou sin(x1) et sin(x2).

133
00:07:28,395 --> 00:07:31,660
J'ai six caractéristiques brutes ici
qui entrent en ligne de compte.

134
00:07:31,660 --> 00:07:34,240
Comme vous le voyez en haut,

135
00:07:34,240 --> 00:07:36,505
on a une progression lente.

136
00:07:36,505 --> 00:07:39,900
Il y a un écart important ici.
Je ne sais pas où cela va aller.

137
00:07:39,900 --> 00:07:43,110
On a une très forte extrapolation ici.

138
00:07:43,110 --> 00:07:47,090
Le résultat n'est pas bien meilleur,
et le processus est bloqué.

139
00:07:47,090 --> 00:07:50,100
Voyons si nous pouvons faire mieux
avec les réseaux de neurones.

140
00:07:50,100 --> 00:07:54,350
Désactivons tout cela,
et ajoutons une couche cachée.

141
00:07:54,350 --> 00:07:57,520
Commençons avec deux neurones
et voyons le résultat.

142
00:07:59,160 --> 00:08:03,555
Comme vous le remarquez ici, le résultat
n'est pas bien meilleur qu'avec x1 et x2.

143
00:08:04,300 --> 00:08:07,260
La capacité est insuffisante
pour l'apprentissage de ce modèle.

144
00:08:07,260 --> 00:08:09,840
Passons à trois.

145
00:08:12,020 --> 00:08:16,615
Le résultat est un tout petit peu meilleur
qu'avant en termes d'extrapolation.

146
00:08:16,615 --> 00:08:24,330
Toutefois, ce n'est pas aussi concluant
qu'avec six ou sept caractéristiques activées.

147
00:08:24,330 --> 00:08:27,220
Bien. Voyons si nous pouvons
ajouter un autre neurone,

148
00:08:29,110 --> 00:08:31,505
ou peut-être une autre couche.

149
00:08:31,505 --> 00:08:34,570
Voyons ce qui va se passer.

150
00:08:34,570 --> 00:08:35,775
Tout est prêt.

151
00:08:35,775 --> 00:08:39,005
Notez que la perte d'apprentissage
est très faible,

152
00:08:39,005 --> 00:08:41,225
mais que la perte de test n'est pas bonne.

153
00:08:41,225 --> 00:08:43,000
On est coincé.

154
00:08:45,280 --> 00:08:49,230
Essayons d'ajouter d'autres couches cachées.

155
00:08:49,230 --> 00:08:52,009
Ajoutons quatre neurones à chaque couche.

156
00:08:52,009 --> 00:08:54,750
Ça devrait être suffisant.

157
00:08:54,750 --> 00:08:57,495
Voyons le résultat.

158
00:08:57,495 --> 00:09:00,680
On observe une baisse significative
des deux chiffres.

159
00:09:00,680 --> 00:09:04,610
Cependant, l'écran blanc
indique l'indécision du réseau.

160
00:09:04,610 --> 00:09:07,285
Voici le point d'inflexion.

161
00:09:07,285 --> 00:09:10,300
La perte est réduite de façon significative.

162
00:09:11,720 --> 00:09:15,825
Mais vous pouvez voir
que la perte de test augmente.

163
00:09:15,825 --> 00:09:20,230
Elle atteint un palier,
ce qui indique un manque de capacité.

164
00:09:20,230 --> 00:09:24,870
Nous allons aller aussi loin que possible
et ajouter huit neurones par couche.

165
00:09:24,870 --> 00:09:28,110
J'espère que ce sera suffisant
pour l'apprentissage de cette fonction

166
00:09:28,110 --> 00:09:30,790
qui est très complexe
et comporte du bruit.

167
00:09:30,790 --> 00:09:32,980
Bien.

168
00:09:32,980 --> 00:09:35,720
Lançons l'apprentissage.

169
00:09:37,130 --> 00:09:40,590
Comme vous le voyez,
l'apprentissage est très lent ici.

170
00:09:41,480 --> 00:09:45,270
Espérons que nous allons arriver
à utiliser cette fonction.

171
00:09:47,560 --> 00:09:49,700
La perte d'apprentissage
est moins importante,

172
00:09:49,700 --> 00:09:52,320
mais la perte de test augmente.

173
00:10:01,010 --> 00:10:03,770
La perte de test se stabilise quelque peu.

174
00:10:06,170 --> 00:10:09,790
Si vous effectuez l'apprentissage par
vous-même, les résultats peuvent varier

175
00:10:09,790 --> 00:10:12,530
en raison des initialisations
aléatoires du réseau.

176
00:10:12,530 --> 00:10:15,810
Essayons autre chose.

177
00:10:17,370 --> 00:10:20,455
Nous obtiendrons peut-être
un résultat plus satisfaisant.

178
00:10:26,885 --> 00:10:30,260
C'est effectivement plus prometteur.

179
00:10:30,260 --> 00:10:33,420
Vous voyez ce qui se passe.
L'apprentissage s'effectue à ce niveau.

180
00:10:33,420 --> 00:10:35,640
Cette section se remplit.

181
00:10:37,780 --> 00:10:41,590
Il y a surapprentissage,
car notre perte de test diverge.

182
00:10:41,960 --> 00:10:43,520
Ce n'est pas bon.

183
00:10:50,770 --> 00:10:52,580
Et voilà.

184
00:10:52,580 --> 00:10:53,720
Comme vous le voyez,

185
00:10:53,720 --> 00:10:56,195
malgré la taille du réseau,

186
00:10:56,195 --> 00:10:59,225
nous n'arrivons pas à apprendre
cette distribution correctement.

187
00:10:59,225 --> 00:11:03,950
Nous avons toutes ces extrapolations…
ces conjectures,

188
00:11:03,950 --> 00:11:07,175
mais le résultat ne va pas être bon
en termes de perte de test.

189
00:11:07,420 --> 00:11:11,190
Notre perte de test
diminue soudain. Excellent.

190
00:11:21,650 --> 00:11:25,010
Le réseau apprend
la fonction de mieux en mieux.

191
00:11:25,010 --> 00:11:29,005
Cependant, le processus est très lent
en raison de la taille même du réseau.

192
00:11:33,095 --> 00:11:36,080
N'oubliez pas
que pour chacune de ces couches,

193
00:11:36,080 --> 00:11:38,145
il y a 64 pondérations.

194
00:11:38,145 --> 00:11:39,715
Nous avons six couches,

195
00:11:39,715 --> 00:11:42,120
je veux dire 6 x 64, juste là.

196
00:11:42,120 --> 00:11:45,440
Cela n'inclut pas la couche
des caractéristiques ni la couche supérieure,

197
00:11:47,030 --> 00:11:49,615
qui en comptent 8 chacune.

198
00:11:51,765 --> 00:11:54,090
Nous y voici. Regardez, c'est excellent.

199
00:11:54,890 --> 00:11:56,825
Le réseau apprend bien cette fonction.

200
00:11:56,825 --> 00:12:00,080
Mais il y a ces extrapolations,

201
00:12:00,080 --> 00:12:01,600
ces intrapolations ici.

202
00:12:01,600 --> 00:12:04,460
Cette pointe orange traverse la spirale.

203
00:12:08,950 --> 00:12:11,840
Le processus s'améliore au fil du temps.

204
00:12:11,840 --> 00:12:14,600
Comme vous le voyez,
la perte de test continue de diminuer.

205
00:12:14,600 --> 00:12:18,070
En revanche, cette forme indique
un surapprentissage.

206
00:12:23,860 --> 00:12:26,000
C'est fait.

207
00:12:26,000 --> 00:12:28,980
Nous avons fini par identifier
toutes ces formes,

208
00:12:28,980 --> 00:12:32,280
à l'aide des réseaux de neurones,
ce qui peut s'avérer plus efficace,

209
00:12:32,280 --> 00:12:34,685
voire indispensable,

210
00:12:34,685 --> 00:12:38,190
comme dans le cas de la spirale,
dont nous avons identifié la forme.