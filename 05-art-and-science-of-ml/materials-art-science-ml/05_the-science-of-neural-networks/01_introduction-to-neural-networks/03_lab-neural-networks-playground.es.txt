Practiquemos con la redes neuronales
y el TensorFlow Playground. En este lab de zona de pruebas
para redes neuronales usaremos TensorFlow Playground para probar y crear redes
neuronales para aprender datos. Quiero que resuelva
estos problemas de dos formas. Primero, vamos a probar
entrenar los modelos con la ingeniería
de funciones manual. con la que usamos nuestros
conocimientos para intentar adivinar la combinación y transformación
de funciones correctas para aprender datos. Después, vamos a pasarle el mando al poder de las redes neuronales y agregaremos más capas y neuronas
con un conjunto simple de funciones de entrada para ver si puede
hacer la ingeniería de funciones sola. Bienvenidos al TensorFlow Playground En este lab, vamos a ver
si la ingeniería de funciones puede tener un mejor rendimiento
que las redes neuronales. Tengo la sensación de
que no sucederá. Investiguemos En este diagrama buscamos clasificar
estos puntos azules y naranjas. Es un problema de clasificación. Como verá, parecen
dos círculos concéntricos. Pero en este caso
hay mucha contaminación Una gran mezcla. Vamos a ver el rendimiento
de X1 y X2 en el entrenamiento. Como verá, no hay mucho aprendizaje. Está todo difuso y hay bastante blanco. No es definitivo de acuerdo con el ajusto de aquí: -101. No aprendió mucho,
Veamos si lo podemos mejorar. Con la ingeniería de funciones,
sé que este es un círculo. Así que hago X1 al cuadrado
y X2 al cuadrado y probaré eso. Ahora sí, parece una elipse. Eso significa que casi
está descifrando esta función. Sabemos que es un círculo pero hay mucha contaminación por lo que no es claro. Quizás pueda bajar
la pérdida de 0.275. Intentaré
deshacerme de X1 y X2 las formas lineales.
intentémoslo ahora. 2.85 Tiene una forma más circular. Pero la pérdida de prueba
está un poco mejor. Veamos si podemos hacer lo mismo
con las redes neuronales. Volvamos a X1 y X2 que, como vimos antes,
fueron deficientes. Agreguemos una capa oculta
y dos neuronas adicionales. Como puede ver, le resulta difícil
descifrar qué función es. El problema es que no hay
suficiente capacidad en estas dos neuronas ni una representación dimensional
lo suficientemente alta para aprender
esta distribución. Detengámonos aquí y veamos. Agreguemos otra neurona. Quizás sea la capacidad suficiente
para aprender esta función. Muy bien. Sigue sin funcionar bien. Miren eso. Le lleva mucho tiempo pero de a poco está descifrando
la forma de la función. Es una especie de rectángulo. Eso significa que [inaudible] de la cantidad de neuronas que
pueden representar esta distribución. Vemos si funciona mejor
con una sola neurona adicional. Miren ahora. Lo hizo mucho más rápido. Solo tenemos cuatro neuronas. Vemos qué pasa si agregamos
muchas neuronas adicionales. Configuremos
todo en cuatro. Veamos qué sucede. Entrenaré. Es bastante más lento. Hay más cantidad para procesar
con todas las capas Pero creo que lo logrará. Me preocupa que sobreajuste un poco. Ya no es una forma circular simple. Es una especie de polígono por lo que está sobreajustando los datos
y la pérdida de prueba no es buena está más alta que antes. Veamos otras distribuciones. Esta es la distribución exclusiva clásica en la que X e Y son positivos o negativos tenemos azules, y con
el "o exclusivo" está la clase naranja, Veamos si podemos aprender
solo con X1 y X2. Al igual que antes X1 y X2 no son lo suficientemente potentes
para poder describir esta función. Se ve cero en todo el tablero. Veamos si podemos descifrarlo
con la ingeniería de funciones. Con la ingeniería
de funciones ingresaré X1 y X2
porque sé que son correctos. Iniciemos el entrenamiento. La pérdida de prueba es 0.07.
Eso es excelente Lo encontró muy fácilmente aquí está el peso, 0.19.
Excelente. Hay un poco de contaminación
así que no es perfecto pero en gran parte
lo descifró muy bien. Veamos si el aprendizaje automático con redes neuronales
puede hacerlo mejor. Volveré a usar X1 y X2 y agregaré una capa oculta. Ahora, lo probaré. Quiero conseguir
la menor cantidad posible. Así que intentaré usar
solo dos neuronas para aprender esto. Sin embargo, no logra descifrarlo. No tiene la complejidad ni la capacidad
suficiente en este modelo Detengámonos y agreguemos
una tercera neurona. Volvamos a entrenar. Como puede ver tiene dificultades
para aprender esta función. Quizás está en el perímetro y tengo que esperar un poco más
para que la aprenda. Pero parece estar atascado. Quizás con otra inicialización
se corrija. Veamos… Probamos todo y pareciera haber aprendido la función. Parece un reloj de arena en diagonal. Sin embargo, esa no es la función. La pérdida es mucho más alta. Probemos con cuatro para ver si funciona. Sigue pareciendo un reloj de arena pero se está convirtiendo en
una serie de cuadrados que es la forma real de la función.
Está mejorando. Probemos agregando otras más y veamos si sobreajusta. Es mucho más lento
y tiene pérdida de entrenamiento. Pero tienen más forma de cuadrados. Excelente. Probemos otro tipo de distribución. Aquí tenemos una espiral. Dos espirales,
una alrededor de la otra. Parece una imagen de la galaxia. Veamos si se puede entrenar
con X1 y X2. Dudo mucho que sea posible. Como se puede ver aquí que no logró aprender
la distribución para nada. Está muy cerca de cero y no puede decidir
qué es cada cosa. Podemos probar usar
algo de ingeniería de funciones. Probemos. ¿Qué les parece? Probemos con círculos. No funciona.
Agreguemos esto. El seno y el coseno o seno(X1) y seno(X2). Tengo seis funciones sin procesar aquí y pareciera descifrarlo como se puede ver arriba y un poco en esta parte. Hay una brecha grande aquí
y no sé dónde está yendo. Se está extrapolando mucho aquí. No funcionó muy bien. Está estancado. Veamos si lo podemos hacer mejor
con las redes neuronales. Desactivemos todo esto y agreguemos una capa oculta. Comencemos con dos neuronas
y veamos si funciona. Como se ve aquí, no es mucho mejor
que con X1 y X2 simplemente. No hay suficiente capacidad
para aprender este modelo. Probemos con tres y veamos si aprende. Funciona un poco mejor que antes
con algo de extrapolación aquí. Sin embargo, no es mejor
que con las seis o siete funciones activadas aquí. Agreguemos una neurona más y otra capa. Veamos si así funciona. Muy bien. Tenemos una pérdida
de entrenamiento muy baja, pero la pérdida
de prueba no es muy buena. Está como atascado. Agreguemos más capas ocultas. Configurémoslas en cuatro. Quizás sean suficientes. Veamos qué obtenemos. Ambas descendieron un poco. Pero no ha tomado una decisión,
ya que toda la pantalla está en blanco. Allí está, tengo un punto de inflexión. La pérdida disminuyó mucho. Pero la pérdida de prueba está subiendo. Y ahora quedó plana.
No tiene la capacidad suficiente. Configuremos el máximo y agreguemos
ocho neuronas en cada capa. Ojalá sean suficientes para aprender
esta función compleja y contaminada. Muy bien. Iniciemos el entrenamiento. Va muy lento con este entrenamiento. Esperemos que encuentre una forma
de descifrar esta función. La pérdida de entrenamiento
está descendiendo. La pérdida de prueba está aumentando. La pérdida de prueba se está nivelando. Cuando realicen esto sus resultados pueden variar bastante por la regularización
aleatoria de la red. Probemos otra cosa. Quizás sea un poco más prometedora. Esta parece ser un poco más prometedora. Lo que está haciendo
es aprender estas formas de aquí. Rellenando. Parece que sobreajustamos porque
la pérdida de prueba se está apartando. Eso es un problema. Listo. Como se puede ver incluso con esta cantidad de red no podemos aprender
bien esta distribución. Obtenemos todas
estas extrapolaciones y estas estimaciones amplias. Eso no tendrá un buen resultado
en la pérdida de prueba. Veamos ahora. La pérdida de prueba
está bajando de pronto. Excelente. Está descifrando más
la función aprendida. Pero va muy lento
dado que la red es tan grande. Entre cada una de estas capas hay 64 pesos porque tengo seis capas es decir, 6 por 64. Sin incluir los pesos entre
la capa de función y la capa superior. Donde hay ocho entre cada una. Listo. Excelente. Está aprendiendo
esta función bastante bien. Sin embargo, estas extrapolaciones e interpolaciones de aquí como este pico naranja
que ingresa en la espiral. Pero está mejorando
un poco con el tiempo. La pérdida de prueba sigue bajando. Sin embargo, esta forma
significa mucho sobreajuste. Listo. Finalmente, pudo encontrar las formas
de todo esto con las redes neuronales que a veces lo descifra mejor o al menos descifra,
su forma como en el caso de la espiral.