Bonjour, je m'appelle Ryan. Je suis chercheur
en machine learning chez Google, et j'adore appliquer les maths et le ML
au big data pour mieux comprendre le monde. Après avoir couvert l'art du ML,
nous allons maintenant parler de science, notamment des réseaux de neurones
et des pratiques d'apprentissage. Initialement, nous avons traité
de la régularisation L2, qui aide à réduire
les pondérations de paramètres, et de l'incidence du taux d'apprentissage
et de la taille de lot sur l'apprentissage. Nous avons alors abordé
le réglage des hyperparamètres, qui est une boucle d'optimisation externe, pour identifier les hyperparamètres permettant
d'obtenir le modèle généralisable optimal. Nous avons aussi appris à effectuer
la régularisation L1 et un arrêt prématuré pour créer des modèles généralisables creux,
sans oublier la régression logistique et en quoi elle peut influer
sur les performances du modèle. Maintenant, nous allons expliquer
la science des réseaux de neurones et les bonnes pratiques d'apprentissage. Enfin, nous aborderons les réseaux de neurones
à classes multiples et leurs emplois.