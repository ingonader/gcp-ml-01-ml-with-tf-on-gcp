1
00:00:00,000 --> 00:00:02,940
すでにニューラルネットワークは
説明しましたが

2
00:00:02,940 --> 00:00:05,175
ここではその技術を説明します

3
00:00:05,175 --> 00:00:09,825
このような問題で特徴クロスが役立つことが
わかってきました

4
00:00:09,825 --> 00:00:12,285
x1が水平寸法で

5
00:00:12,285 --> 00:00:14,220
x2が垂直寸法の場合

6
00:00:14,220 --> 00:00:18,480
この分布を示す2つの特徴の
線形組み合わせはありませんでした

7
00:00:18,480 --> 00:00:23,640
特徴エンジニアリングを行って
x1とx2を交差させるまで

8
00:00:23,640 --> 00:00:26,245
新しい特徴x3を得られませんでした

9
00:00:26,245 --> 00:00:30,030
x3はx1掛けるx2と同じで
このデータ分布を示せます

10
00:00:30,030 --> 00:00:32,910
手動による特徴エンジニアリングで

11
00:00:32,910 --> 00:00:36,260
非線形問題はすべて簡単に解決できます

12
00:00:36,260 --> 00:00:39,855
ですよね？残念ながら現実世界では

13
00:00:39,855 --> 00:00:42,735
こんなに簡単に分布を描けません

14
00:00:42,735 --> 00:00:46,785
特徴エンジニアリングは
天才たちが何年研究しても

15
00:00:46,785 --> 00:00:48,495
これが限界です

16
00:00:48,495 --> 00:00:52,950
たとえばこの分布のモデル化には
どんな特徴クロスが必要ですか

17
00:00:52,950 --> 00:00:57,945
2つの円が重なっているようにも
2つのらせんにも見えますが

18
00:00:57,945 --> 00:01:00,630
とにかく非常に乱雑です

19
00:01:00,630 --> 00:01:04,550
この例はニューラルネットワーク（NN）の
有用性を示します

20
00:01:04,550 --> 00:01:08,650
NNはアルゴリズムで複雑な特徴クロスと
変換を作成できます

21
00:01:08,650 --> 00:01:12,240
このらせんよりはるかに複雑な空間では

22
00:01:12,240 --> 00:01:16,380
NNの使用が必須です

23
00:01:16,380 --> 00:01:21,990
NNは特徴を組み合わせて
特徴クロスの代用として機能します

24
00:01:21,990 --> 00:01:25,110
NNのアーキテクチャの設計で目指すのは

25
00:01:25,110 --> 00:01:29,130
特徴の組み合わせが存在するような
モデルの構築です

26
00:01:29,130 --> 00:01:32,975
次に他の層を追加して 
既存の組み合わせに重ね

27
00:01:32,975 --> 00:01:36,980
さらに層を追加して既存の組み合わせに
重ねていきます

28
00:01:36,980 --> 00:01:39,090
どうすれば特徴などの

29
00:01:39,090 --> 00:01:42,315
正しい組み合わせを選択できますか？

30
00:01:42,315 --> 00:01:45,780
もちろんトレーニングでモデルに
学習させるのです

31
00:01:45,780 --> 00:01:49,095
これがニューラルネットの基本知識です

32
00:01:49,095 --> 00:01:52,500
特徴クロスより優れているわけではありませんが

33
00:01:52,500 --> 00:01:56,350
多くのケースで役立つ柔軟な代替方法です

34
00:01:56,350 --> 00:02:00,040
この図は線形モデルを表しています

35
00:02:00,040 --> 00:02:01,795
3つの入力

36
00:02:01,795 --> 00:02:05,885
x1 x2 x3が青の円で示されています

37
00:02:05,885 --> 00:02:09,889
各エッジに重み与えて組み合わせを行い
出力を生成します

38
00:02:09,889 --> 00:02:12,620
別の偏り項もありますが

39
00:02:12,620 --> 00:02:14,955
簡単にするため表示していません

40
00:02:14,955 --> 00:02:20,045
これは線形モデルです
y = w1 x x1

41
00:02:20,045 --> 00:02:22,010
+ w2 x x2

42
00:02:22,010 --> 00:02:23,915
+ w3 x x3です

43
00:02:23,915 --> 00:02:28,040
ここでノードとエッジのネットワークに
隠れ層を追加します

44
00:02:28,040 --> 00:02:32,640
入力層にも隠れ層にもノードが3つあります

45
00:02:32,640 --> 00:02:35,210
ここでは隠れノードを見ます

46
00:02:35,210 --> 00:02:37,565
これは完全につながった層なので

47
00:02:37,565 --> 00:02:41,980
エッジが3 x 3で重みが9になります

48
00:02:41,980 --> 00:02:44,870
確かにこれは非線形モデルなので

49
00:02:44,870 --> 00:02:48,045
非線形問題の解決に使えますよね？

50
00:02:48,045 --> 00:02:51,340
残念ながら使えません
詳細に見ていきましょう

51
00:02:51,340 --> 00:02:56,415
最初の隠れノードへの入力はw1 x x1

52
00:02:56,415 --> 00:02:58,515
+ w4 x x2

53
00:02:58,515 --> 00:03:01,350
+ w7 x x3の加重和です

54
00:03:01,350 --> 00:03:05,640
2番目の隠れノードへの入力はw2 x x1

55
00:03:05,640 --> 00:03:10,395
+ w5 x x2 + w8 x x3の
加重和です

56
00:03:10,395 --> 00:03:15,480
3番目の隠れノードへの入力はw3 x x1

57
00:03:15,480 --> 00:03:19,575
+ w6 x x2 + w9 x x3の
加重和です

58
00:03:19,575 --> 00:03:23,035
すべてを出力ノードで組み合わせると

59
00:03:23,035 --> 00:03:25,685
w10 x h1

60
00:03:25,685 --> 00:03:28,080
+ w11 x h2

61
00:03:28,080 --> 00:03:30,225
+ w12 x h3になります

62
00:03:30,225 --> 00:03:32,550
説明したようにh1

63
00:03:32,550 --> 00:03:37,370
h2とh3は入力特徴の線形の組み合わせに
すぎません

64
00:03:37,370 --> 00:03:40,055
そのためこれを展開すると

65
00:03:40,055 --> 00:03:43,165
一連の複雑な重み定数に各入力値

66
00:03:43,165 --> 00:03:47,570
x1 x2 x3を掛けたものが残ります

67
00:03:47,820 --> 00:03:51,935
各組の重みを新しい重みに代入できます

68
00:03:51,935 --> 00:03:53,250
見覚えがあるでしょう？

69
00:03:53,250 --> 00:03:56,530
隠れ層を追加する前の線形モデルと

70
00:03:56,530 --> 00:04:00,995
まったく同じです
なぜでしょうか？

71
00:04:00,995 --> 00:04:04,050
隠れ層をもっと追加したら？

72
00:04:04,050 --> 00:04:07,830
残念ながらこれも集約され

73
00:04:07,830 --> 00:04:11,835
3つの各入力を掛けた1つの重み行列になります

74
00:04:11,835 --> 00:04:13,790
同じ線形モデルです

75
00:04:13,790 --> 00:04:18,450
このプロセスを無限に続けても
同じ結果になります

76
00:04:18,450 --> 00:04:23,630
莫大な計算コストをかけて
複雑なアーキテクチャのトレーニングや

77
00:04:23,630 --> 00:04:26,260
予測を行っても同じです

78
00:04:26,410 --> 00:04:29,600
これを線形代数的観点から見ると

79
00:04:29,600 --> 00:04:33,455
複数の行列が1つの連鎖で掛けられています

80
00:04:33,455 --> 00:04:34,985
この小さな例では

81
00:04:34,985 --> 00:04:37,085
まず3x3の行列を掛け

82
00:04:37,085 --> 00:04:41,290
入力層と隠れ層1間の重み行列の転置を

83
00:04:41,290 --> 00:04:44,390
3x1の入力ベクトルで行うことにより

84
00:04:44,390 --> 00:04:48,845
3x1ベクトルが隠れ層1の各隠れニューロンの
値になります

85
00:04:48,845 --> 00:04:52,175
2番目の隠れ層のニューロンの値を定義して

86
00:04:52,175 --> 00:04:54,030
3x3の重み行列の

87
00:04:54,030 --> 00:04:56,395
転置行列を掛けて隠れ層1を

88
00:04:56,395 --> 00:05:01,190
隠れ層2につなげると
それが隠れ層1の結果のベクトルになります

89
00:05:01,190 --> 00:05:02,940
おわかりのように

90
00:05:02,940 --> 00:05:05,570
2つの3x3の重み行列が

91
00:05:05,570 --> 00:05:07,690
1つの3x3行列になります

92
00:05:07,690 --> 00:05:12,570
これはまず行列積を左の内側または右側から
計算しています

93
00:05:12,570 --> 00:05:15,450
これでも2番目の隠れ層のニューロンの

94
00:05:15,450 --> 00:05:18,720
値のベクトルh2の形は同じです

95
00:05:18,720 --> 00:05:23,075
隠れ層2と出力層の間に最後の層を追加するには

96
00:05:23,075 --> 00:05:25,350
前の手順に先ほどの2つの層の間の

97
00:05:25,350 --> 00:05:28,400
重み行列の転置行列を掛ける必要があります

98
00:05:28,400 --> 00:05:31,740
NNによるフィードフォーワードでも

99
00:05:31,740 --> 00:05:36,360
行列の積を右から左に行うには
左から右に適用します

100
00:05:36,360 --> 00:05:38,670
この行列要素の大きな連鎖が

101
00:05:38,670 --> 00:05:42,750
値が3つだけのベクトルに集約されました

102
00:05:42,750 --> 00:05:46,160
3つの重みが並んだ単回帰ケースで

103
00:05:46,160 --> 00:05:50,760
このモデルをトレーニングし
どちらも最終面で同じ極小値に集約されるなら

104
00:05:50,760 --> 00:05:54,865
膨大な演算で21の重みすべてを計算しても

105
00:05:54,865 --> 00:05:58,859
この行列積の連鎖は低次方程式に集約され

106
00:05:58,859 --> 00:06:02,590
重みはトレーニング中の単回帰の重みと
まったく同じになります

107
00:06:02,590 --> 00:06:05,205
すべての作業が同じ結果になります

108
00:06:05,205 --> 00:06:07,455
皆様はこう思うでしょう

109
00:06:07,455 --> 00:06:11,280
「NNとはニューロンに層を追加することだ

110
00:06:11,280 --> 00:06:15,765
全層が1つに集約されるなら
ディープラーニングなんてできない」

111
00:06:15,765 --> 00:06:17,190
朗報があります

112
00:06:17,190 --> 00:06:19,025
簡単な解決策があります

113
00:06:19,025 --> 00:06:22,630
それは非線形変換層を追加することで

114
00:06:22,630 --> 00:06:28,530
シグモイド、Tanh、ReLUなどの
非線形活性化関数で容易になります

115
00:06:28,530 --> 00:06:31,980
TensorFlowなどのグラフの場合は

116
00:06:31,980 --> 00:06:35,580
各ニューロンに2つのノードがあると
考えてください

117
00:06:35,580 --> 00:06:39,865
最初のノードは加重和wx + bの結果で

118
00:06:39,865 --> 00:06:41,865
2つ目のノードは

119
00:06:41,865 --> 00:06:44,430
活性化関数を通過した結果です

120
00:06:44,430 --> 00:06:47,730
つまり 活性化関数の入力の後に

121
00:06:47,730 --> 00:06:49,770
活性化関数の出力があります

122
00:06:49,770 --> 00:06:53,735
活性化関数が間の転移点として機能します

123
00:06:53,735 --> 00:06:56,580
この非線形転換の追加が

124
00:06:56,580 --> 00:06:59,990
NNが集約して浅いネットワークになるのを
防ぐ唯一の方法です

125
00:06:59,990 --> 00:07:04,280
ネットワークに非線形活性化関数の層が
1つあっても

126
00:07:04,280 --> 00:07:09,585
ネットワークのどこかに線形活性化関数が
2つ以上あると

127
00:07:09,585 --> 00:07:12,525
1つのネットワークに集約されます

128
00:07:12,525 --> 00:07:14,370
通常NNでは

129
00:07:14,370 --> 00:07:17,840
最初とマイナス1の層は非線形の層で

130
00:07:17,840 --> 00:07:21,245
最終層の変換は回帰またはシグモイド

131
00:07:21,245 --> 00:07:25,175
ソフトマックスの線形になります

132
00:07:25,175 --> 00:07:27,620
すべては必要とする出力によって変わります

133
00:07:27,620 --> 00:07:31,100
これをもう一度線形代数の観点から考えると

134
00:07:31,100 --> 00:07:34,700
線形変換を行列またはベクトルに適用するときに

135
00:07:34,700 --> 00:07:39,540
行列またはベクトルを掛け合わせて
目的の形状や結果になるようにします

136
00:07:39,540 --> 00:07:41,840
行列のサイズ変更と同様に

137
00:07:41,840 --> 00:07:43,475
定数を掛けることができます

138
00:07:43,475 --> 00:07:46,819
しかし実際は単位行列を掛けています

139
00:07:46,819 --> 00:07:48,410
その定数を掛けます

140
00:07:48,410 --> 00:07:52,600
そのためこれは全対角が定数の対角行列です

141
00:07:52,600 --> 00:07:55,690
これは単なる行列積に集約されます

142
00:07:55,690 --> 00:07:59,070
しかし非線形性を追加すると

143
00:07:59,070 --> 00:08:02,660
行列で表せなくなります

144
00:08:02,660 --> 00:08:05,800
要素yで関数を入力に適用しているためです

145
00:08:05,800 --> 00:08:06,810
たとえば

146
00:08:06,810 --> 00:08:11,315
最初と2番目の隠れ層の間に
非線形活性関数がある場合

147
00:08:11,315 --> 00:08:13,110
最初の隠れ層の重み行列と

148
00:08:13,110 --> 00:08:17,185
入力ベクトルの置換の積の関数を適用します

149
00:08:17,185 --> 00:08:20,740
低次方程式はReLUの活性化関数です

150
00:08:20,740 --> 00:08:24,560
線形代数では変換を表せないので

151
00:08:24,560 --> 00:08:27,890
変換連鎖のその部分はこれ以上集約できず

152
00:08:27,890 --> 00:08:30,800
モデルの複雑性は変わらず

153
00:08:30,800 --> 00:08:34,325
入力の線形結合に集約されません

154
00:08:34,325 --> 00:08:38,299
重み行列の2番目の隠れ層と出力層の重み行列は

155
00:08:38,299 --> 00:08:42,909
まだ非線形関数を適用していないので
集約されます

156
00:08:42,909 --> 00:08:47,540
つまり線形層が2つ以上連続している場合

157
00:08:47,540 --> 00:08:51,515
それが何層でも1つの層に集約されます

158
00:08:51,515 --> 00:08:55,415
そのため最も複雑な関数が
ネットワークで作成されています

159
00:08:55,415 --> 00:08:58,790
ネットワーク全体に線形活性化関数を入れるのが
最適ですが

160
00:08:58,790 --> 00:09:02,710
最後に異なるタイプの出力を使用する場合は
最後の層を除外します

161
00:09:02,710 --> 00:09:08,035
NNに非線形活性化関数を追加することが
重要な理由は？

162
00:09:08,035 --> 00:09:10,690
正解は複数の層が線形モデルに

163
00:09:10,690 --> 00:09:12,985
集約されないようにするためです

164
00:09:12,985 --> 00:09:14,815
非線形活性化関数は

165
00:09:14,815 --> 00:09:18,400
データスクリプト空間で興味深い変換を作成し

166
00:09:18,400 --> 00:09:21,260
深層合成関数も可能にします

167
00:09:21,260 --> 00:09:26,590
説明したように
線形活性化関数に2つ以上の層がある場合

168
00:09:26,590 --> 00:09:27,940
この行列の積は

169
00:09:27,940 --> 00:09:31,560
1つの行列 x 入力特徴ベクトルに
集約されます

170
00:09:31,560 --> 00:09:34,420
そのため演算は多いのに

171
00:09:34,420 --> 00:09:38,605
機能的複雑性がすべて排除された
低速モデルになります

172
00:09:38,605 --> 00:09:41,620
非線形性は損失関数に正則性を追加せず

173
00:09:41,620 --> 00:09:45,015
早期中止を呼び出しません

174
00:09:45,015 --> 00:09:47,650
非線形活性化関数がデータ空間に

175
00:09:47,650 --> 00:09:49,910
複素変換を作成したとしても

176
00:09:49,910 --> 00:09:53,200
次元は変化せず 同一ベクトル空間に留まります

177
00:09:53,200 --> 00:09:56,790
ただし圧迫や拡大、回転はあります

178
00:09:56,790 --> 00:09:59,680
前のコースでも触れたように

179
00:09:59,680 --> 00:10:03,200
シグモイドには多数の非線形活性化関数があり

180
00:10:03,200 --> 00:10:05,000
スケール／シフトシグモイドと

181
00:10:05,000 --> 00:10:07,535
双曲正接は最も古いものです

182
00:10:07,535 --> 00:10:09,725
ただしすでに述べたように

183
00:10:09,725 --> 00:10:13,310
これは飽和状態から勾配消失問題につながります

184
00:10:13,310 --> 00:10:14,510
ゼロ勾配では

185
00:10:14,510 --> 00:10:17,945
モデルの重みは更新されず 
トレーニングは停止します

186
00:10:17,945 --> 00:10:21,140
Rectified Linear Unit（ReLU）は

187
00:10:21,140 --> 00:10:24,215
簡単でうまく機能するためよく使用されます

188
00:10:24,215 --> 00:10:28,000
正領域では線形なので飽和状態になりませんが

189
00:10:28,000 --> 00:10:30,440
負領域では関数が0になります

190
00:10:30,440 --> 00:10:33,365
隠れ活性化にReLUを使うネットワークは

191
00:10:33,365 --> 00:10:39,050
隠れ活性化にシグモイドを使うネットワークと
比べてトレーニング速度が10倍です

192
00:10:39,050 --> 00:10:42,590
ただし負領域では関数が常に0なので

193
00:10:42,590 --> 00:10:45,210
実層は消失して終わります

194
00:10:45,210 --> 00:10:46,910
これが示しているのは

195
00:10:46,910 --> 00:10:48,230
入力を負領域で開始して

196
00:10:48,230 --> 00:10:51,845
活性化の出力がゼロになると

197
00:10:51,845 --> 00:10:54,980
次の層と正領域での入力に役立たない
ということです

198
00:10:54,980 --> 00:10:59,150
これにより多数のゼロ活性化が作成されます

199
00:10:59,150 --> 00:11:02,270
重みを更新する誤差逆伝搬中に

200
00:11:02,270 --> 00:11:05,170
誤差導関数に活性化を掛ける必要があるため

201
00:11:05,170 --> 00:11:06,810
勾配ゼロで終わります

202
00:11:06,810 --> 00:11:10,680
このようにデータゼロの重みは変化せず

203
00:11:10,680 --> 00:11:13,800
その層のトレーニングは失敗します

204
00:11:13,800 --> 00:11:17,220
幸運にも 巧妙な方法が多数開発されており

205
00:11:17,220 --> 00:11:20,980
トレーニングが停止しないよう
ReLUを少し変更しながらも

206
00:11:20,980 --> 00:11:24,120
一般的なReLUのメリットを維持しています

207
00:11:24,120 --> 00:11:25,920
一般的なReLUに戻ります

208
00:11:25,920 --> 00:11:30,270
maximum演算子は区分線形式で
表すこともできます

209
00:11:30,270 --> 00:11:32,790
ここではゼロ未満で 関数はゼロです

210
00:11:32,790 --> 00:11:36,190
またゼロ以上で関数はXです

211
00:11:36,190 --> 00:11:38,520
ReLU関数の平滑近似は

212
00:11:38,520 --> 00:11:41,205
1の自然対数の解析関数と

213
00:11:41,205 --> 00:11:43,185
指数関数Xを足したものです

214
00:11:43,185 --> 00:11:45,360
これをソフトプラス関数と言います

215
00:11:45,360 --> 00:11:49,740
ソフトプラス関数の導関数は
ロジスティック関数です

216
00:11:49,740 --> 00:11:52,210
ソフトプラス関数を使用するメリットは

217
00:11:52,210 --> 00:11:54,570
ReLU関数と異なり

218
00:11:54,570 --> 00:11:56,380
0でも連続かつ微分可能です

219
00:11:56,380 --> 00:11:59,449
ただし自然対数と指数関数により

220
00:11:59,449 --> 00:12:02,295
ReLUと比べて追加の計算があるため

221
00:12:02,295 --> 00:12:06,030
実際にはReLUの結果も同様に良好です

222
00:12:06,030 --> 00:12:10,195
そのためソフトプラスはディープラーニング
での使用は推奨されません

223
00:12:10,195 --> 00:12:14,895
ゼロ活性化によるdying ReLUの問題を
解決するため

224
00:12:14,895 --> 00:12:16,995
Leaky ReLUが開発されました

225
00:12:16,995 --> 00:12:20,975
ReLU同様 区分線形関数があります

226
00:12:20,975 --> 00:12:23,180
ただし負領域には

227
00:12:23,180 --> 00:12:28,225
0ではなく 厳密に言えば0.01の
非ゼロ勾配があります

228
00:12:28,225 --> 00:12:31,345
このようにユニットが活性化していなくても

229
00:12:31,345 --> 00:12:35,999
Leaky ReLUではわずかな非ゼロ勾配で
それらを通過できるため

230
00:12:35,999 --> 00:12:40,000
重みの更新やトレーニングの継続ができると
考えられます

231
00:12:40,000 --> 00:12:46,290
このLeakyの考え方を一歩先に進めたのが
Parametric ReLU（PReLU）です

232
00:12:46,290 --> 00:12:48,640
これは負領域でXを0.01として

233
00:12:48,640 --> 00:12:51,625
任意で通過させるのではなく

234
00:12:51,625 --> 00:12:53,910
Xをαとして通過させます

235
00:12:53,910 --> 00:12:57,190
しかしパラメータαはどうなるべきですか？

236
00:12:57,190 --> 00:13:01,585
グラフでは可視化するために
αを0.5に設定しました

237
00:13:01,585 --> 00:13:04,420
しかし実際には他のNNパラメータとともに

238
00:13:04,420 --> 00:13:07,735
トレーニングから得た学習済みパラメータです

239
00:13:07,735 --> 00:13:11,045
このようにこの値は設定するのではなく

240
00:13:11,045 --> 00:13:14,800
データを使ったトレーニング中に決定され

241
00:13:14,800 --> 00:13:18,910
優先順位で設定するより適した値を学習します

242
00:13:18,910 --> 00:13:21,520
αが0未満の場合

243
00:13:21,520 --> 00:13:25,270
式はmaximumを使ってコンパクトフォームに
書き直せます

244
00:13:25,270 --> 00:13:28,480
具体的にはmaxはXまたはαxです

245
00:13:28,480 --> 00:13:33,010
ランダム化したLeaky ReLUもあり
これではαをトレーニングするのではなく

246
00:13:33,010 --> 00:13:35,740
一様分布からランダムにサンプルを抽出します

247
00:13:35,740 --> 00:13:38,080
厳密には各α値のネットワークは異なるため

248
00:13:38,080 --> 00:13:40,990
これには脱落と類似した効果があります

249
00:13:40,990 --> 00:13:43,980
そのため集合と類似したものが形成されます

250
00:13:43,980 --> 00:13:46,720
テスト時にすべてのα値は

251
00:13:46,720 --> 00:13:50,005
予測に使用する目的で
決定論的値に平均化されます

252
00:13:50,005 --> 00:13:52,550
ReLU6という変種もあります

253
00:13:52,550 --> 00:13:56,565
これも区分線形関数で線分が3本あります

254
00:13:56,565 --> 00:13:58,040
通常のReLU同様

255
00:13:58,040 --> 00:13:59,890
負領域では0ですが

256
00:13:59,890 --> 00:14:03,105
正領域ではReLU6は6のままです

257
00:14:03,105 --> 00:14:06,520
「なぜ6のままなのか？」と思うでしょう

258
00:14:06,520 --> 00:14:09,370
これらのReLUユニットの1つは

259
00:14:09,370 --> 00:14:12,220
シフトベルヌーイユニットによって複製された
6のみを持ちます

260
00:14:12,220 --> 00:14:15,520
ハードキャップの場合は膨大な量になります

261
00:14:15,520 --> 00:14:18,520
通常これらはReLU nユニットと呼ばれます

262
00:14:18,520 --> 00:14:20,265
ここでnはキャップ値です

263
00:14:20,265 --> 00:14:24,270
テストで6が最適値であるとわかりました

264
00:14:24,270 --> 00:14:28,235
ReLU6ユニットではモデルはスパース性を
より早く学習できます

265
00:14:28,235 --> 00:14:33,720
まずCIFAR-10画像データセットの畳込み
深層エリートネットワークで使われました

266
00:14:33,720 --> 00:14:38,700
ネットワーク準備に有用なプロパティもあり
推論用の正確な固定小数点を得られます

267
00:14:38,700 --> 00:14:40,639
上限が有界でない場合

268
00:14:40,639 --> 00:14:44,095
固定小数点数のQ部分で多数のビットを失い

269
00:14:44,095 --> 00:14:45,550
上限が6の場合は

270
00:14:45,550 --> 00:14:47,620
数値の端数部分に十分なビットが残り

271
00:14:47,620 --> 00:14:51,600
十分な数値を示し適切な推論を行えます

272
00:14:51,600 --> 00:14:55,540
最後は指数線形ユニット（ELU）です

273
00:14:55,540 --> 00:15:00,925
入力空間の非負領域ではほぼ線形です

274
00:15:00,925 --> 00:15:02,680
入力の負領域では平坦かつ単調で

275
00:15:02,680 --> 00:15:05,320
最も重要なことですが非ゼロでです

276
00:15:05,320 --> 00:15:10,315
また通常のReLUよりゼロ中心なので
学習が高速化します

277
00:15:10,315 --> 00:15:13,960
ELUの主な欠点は指数関数を計算する
必要があるため

278
00:15:13,960 --> 00:15:17,475
ReLUより組成的に高価なことです

279
00:15:17,475 --> 00:15:20,285
NNは任意に複雑になる可能性があります

280
00:15:20,285 --> 00:15:21,710
多くの層や

281
00:15:21,710 --> 00:15:23,930
層ごとのニューロン、出力、入力

282
00:15:23,930 --> 00:15:26,650
さまざまな種類の活性化関数などがあります

283
00:15:26,650 --> 00:15:29,050
複数層の目的は何でしょうか？

284
00:15:29,050 --> 00:15:30,390
層を追加するごとに

285
00:15:30,390 --> 00:15:32,620
作成できる関数の複雑性が追加されます

286
00:15:32,620 --> 00:15:36,790
以降の各層は前の関数の合成です

287
00:15:36,790 --> 00:15:40,255
隠れ層では非線形活性化関数を使用しているので

288
00:15:40,255 --> 00:15:43,570
データを回転、拡張、圧迫する

289
00:15:43,570 --> 00:15:45,510
大量のデータ変換を作成しています

290
00:15:45,510 --> 00:15:48,190
こうしたことを行う目的は

291
00:15:48,190 --> 00:15:50,630
超平面をデータに合わせて回帰させる方法か

292
00:15:50,630 --> 00:15:52,840
超平面でデータを分離して分類する方法により

293
00:15:52,840 --> 00:15:55,635
データを転送することです

294
00:15:55,635 --> 00:16:01,140
元の特徴空間から新しい複雑な特徴空間に
マッピングしています

295
00:16:01,140 --> 00:16:04,530
ニューロンを層に追加するとどうなるのか？

296
00:16:04,530 --> 00:16:08,370
ニューロンを追加するごとに
新しい次元がベクトル空間に追加されます

297
00:16:08,370 --> 00:16:10,515
3つの入力ニューロンで開始する場合は

298
00:16:10,515 --> 00:16:12,510
R3ベクトル空間で始めます

299
00:16:12,510 --> 00:16:17,270
しかし次の層にはR4ベクトル空間に移動した
4つのニューロンがあります

300
00:16:17,270 --> 00:16:20,100
前のコースでカーネル法を説明したとき

301
00:16:20,100 --> 00:16:22,450
超平面で簡単に分離できないデータセットが

302
00:16:22,450 --> 00:16:25,695
元の入力ベクトル空間にありました

303
00:16:25,695 --> 00:16:28,750
しかし次元を追加して
新しい次元が埋まるように

304
00:16:28,750 --> 00:16:32,120
理にかなった方法でデータを変換することで

305
00:16:32,120 --> 00:16:37,425
データクラス間に明確なスライスを
簡単に作成できるようになりました

306
00:16:37,425 --> 00:16:39,570
同じことをここでNNに適用します

307
00:16:39,570 --> 00:16:43,100
複数の出力ノードがあるとどうなるでしょうか？

308
00:16:43,100 --> 00:16:45,920
複数の出力ノードがあると

309
00:16:45,920 --> 00:16:49,995
複数のラベルを比較して対応する領域を
後方へ伝搬できます

310
00:16:49,995 --> 00:16:52,850
各画像内に複数のエンテイティまたはクラスが
ある場合に

311
00:16:52,850 --> 00:16:56,045
画像分類を行うようなものです

312
00:16:56,045 --> 00:16:59,660
クラスは多数あるので
1つのクラスの予測はできません

313
00:16:59,660 --> 00:17:02,405
そのためこの柔軟性は役立ちます

314
00:17:02,405 --> 00:17:05,119
NNは任意に複雑でなければなりません

315
00:17:05,119 --> 00:17:06,275
隠れ次元を増やすために

316
00:17:06,275 --> 00:17:07,819
＿＿＿を追加できます

317
00:17:07,819 --> 00:17:09,349
関数合成を増やすには

318
00:17:09,349 --> 00:17:10,744
＿＿＿を追加できます

319
00:17:10,744 --> 00:17:12,530
たとえば複数のラベルがあると

320
00:17:12,530 --> 00:17:14,599
＿＿＿を追加できます

321
00:17:14,599 --> 00:17:18,875
正解は、ニューロン、層、出力です

322
00:17:18,875 --> 00:17:21,920
隠れ次元を変更するために
ニューロンの層の数を変更できます

323
00:17:21,920 --> 00:17:25,780
そのため中間ベクトルが入るような
ベクトル空間の次元を決定します

324
00:17:25,780 --> 00:17:27,380
層に4つのニューロンがあれば

325
00:17:27,380 --> 00:17:29,060
それは4ベクトル空間にあります

326
00:17:29,060 --> 00:17:33,080
層に500個のニューロンがあれば
それは500ベクトル空間にあります

327
00:17:33,080 --> 00:17:36,935
つまり実次元が500あるという意味です

328
00:17:36,935 --> 00:17:41,970
層を追加しても前の層の次元は変わらず

329
00:17:41,970 --> 00:17:43,970
ニューロンの数が前の層と異ならなければ

330
00:17:43,970 --> 00:17:47,330
その層の次元さえも変わりません

331
00:17:47,330 --> 00:17:51,995
層を追加することで関数合成が向上します

332
00:17:51,995 --> 00:17:53,960
Go of F of Xは

333
00:17:53,960 --> 00:17:58,355
入力Xでの関数Gと関数Fの合成です

334
00:17:58,355 --> 00:18:01,180
したがって まずXをFで変換して

335
00:18:01,180 --> 00:18:03,370
次にその結果をGで変換します

336
00:18:03,370 --> 00:18:06,530
層が多いほどネストした関数が深くなります

337
00:18:06,530 --> 00:18:09,860
これは非線形関数を組み合わせて

338
00:18:09,860 --> 00:18:12,170
複雑な特徴マップを作るのに適しています

339
00:18:12,170 --> 00:18:14,700
これは人間よりコンピュータが得意なことです

340
00:18:14,700 --> 00:18:17,150
私たちはデータを整形して

341
00:18:17,150 --> 00:18:19,580
そこから洞察を得ることができます

342
00:18:19,580 --> 00:18:23,160
洞察は出力層から受け取ります

343
00:18:23,160 --> 00:18:27,275
推測中 洞察がMLによって明確化された
問題への答えになります

344
00:18:27,275 --> 00:18:30,865
画像が犬になる確率を知りたいだけなら

345
00:18:30,865 --> 00:18:33,440
出力ノードが1つあれば十分ですが

346
00:18:33,440 --> 00:18:37,830
画像が猫、犬、鳥、ネズミになる
確率を知りたければ

347
00:18:37,830 --> 00:18:40,450
それぞれに1つずつのノードが必要になります

348
00:18:40,450 --> 00:18:45,960
他の3つの答えはすべて間違いです
2つ以上の用語が間違っています