1
00:00:00,000 --> 00:00:02,970
Analizamos las redes neuronales
en cursos y módulos anteriores.

2
00:00:02,970 --> 00:00:05,455
Ahora, exploremos la ciencia
que hay detrás de ellas.

3
00:00:05,455 --> 00:00:09,825
Vimos que las combinaciones de atributos
fueron útiles para un problema como este.

4
00:00:09,825 --> 00:00:12,285
Si x1 es la dimensión horizontal

5
00:00:12,285 --> 00:00:14,220
y x2 es la dimensión vertical

6
00:00:14,220 --> 00:00:18,980
no había una combinación lineal de los
atributos para describir su distribución.

7
00:00:18,980 --> 00:00:23,640
Recién cuando usamos ingeniería
de atributos y combinamos x1 y x2

8
00:00:23,640 --> 00:00:27,725
para obtener el atributo nuevo x3,
que es igual a x1 por x2

9
00:00:27,725 --> 00:00:30,319
pudimos describir la distribución
de nuestros datos.

10
00:00:30,473 --> 00:00:33,440
Así que, la ingeniería de atributos manual

11
00:00:33,440 --> 00:00:36,410
puede resolver todos los problemas
no lineales con facilidad.

12
00:00:36,410 --> 00:00:37,650
¿Verdad?

13
00:00:38,376 --> 00:00:40,175
Lamentablemente, en el mundo real

14
00:00:40,175 --> 00:00:43,305
casi no hay distribuciones que
se puedan describir tan fácil.

15
00:00:43,305 --> 00:00:46,785
La ingeniería de atributos, aún con años
de expertos trabajando en ella

16
00:00:46,785 --> 00:00:48,495
es limitada.

17
00:00:48,495 --> 00:00:53,420
Por ejemplo, ¿qué combinaciones de
atributos modelarían esta distribución?

18
00:00:53,420 --> 00:00:57,945
Podrían ser dos círculos uno sobre otro
o, quizás, dos espirales

19
00:00:57,945 --> 00:01:00,630
pero, en cualquier caso,
es muy complicado.

20
00:01:00,630 --> 00:01:04,550
En este ejemplo, se ve la utilidad
de las redes neuronales.

21
00:01:04,550 --> 00:01:09,190
Pueden crear con algoritmos combinaciones
y transformaciones de atributos complejas.

22
00:01:09,190 --> 00:01:13,400
Puede pensar en espacios aún
más complicados que este espiral

23
00:01:13,400 --> 00:01:16,780
que requieran del uso de redes neuronales.

24
00:01:16,780 --> 00:01:19,996
Las redes neuronales puede ser
una alternativa del cruzamiento de atributos

25
00:01:19,996 --> 00:01:21,990
con la combinación de los atributos.

26
00:01:21,990 --> 00:01:25,110
Cuando diseñamos la arquitectura
de nuestra red neuronal

27
00:01:25,110 --> 00:01:29,850
queríamos estructurar el modelo
para que los atributos se combinen.

28
00:01:29,850 --> 00:01:32,975
Luego, queremos agregar otra capa
para combinar las combinaciones

29
00:01:32,975 --> 00:01:36,980
y otra capa para combinar
esas combinaciones, etcétera.

30
00:01:37,190 --> 00:01:39,430
¿Cómo seleccionamos
las combinaciones correctas

31
00:01:39,430 --> 00:01:42,315
de nuestros atributos
y las combinaciones de ellas?

32
00:01:42,315 --> 00:01:45,403
Hace que el modelo las aprenda
a través del entrenamiento.

33
00:01:46,060 --> 00:01:49,295
Esta es la intuición básica
de las redes neuronales.

34
00:01:49,295 --> 00:01:52,500
Este enfoque no es necesariamente mejor
que las combinaciones de atributos

35
00:01:52,500 --> 00:01:56,350
pero es una alternativa flexible
que funciona en muchos casos.

36
00:01:57,420 --> 00:02:00,040
Esta es una representación gráfica
de un modelo lineal.

37
00:02:00,040 --> 00:02:01,875
Tenemos tres entradas:

38
00:02:01,875 --> 00:02:05,815
x1, x2 y x3, que son los círculos azules.

39
00:02:05,815 --> 00:02:10,539
Se combinan con peso atribuido
para producir un resultado.

40
00:02:10,539 --> 00:02:13,300
A menudo, hay un término adicional
de la ordenada al origen

41
00:02:13,300 --> 00:02:15,305
pero, para simplificar,
no se muestra aquí.

42
00:02:15,305 --> 00:02:20,045
Este es un modelo lineal, ya que
la forma de y es igual a w1 x x1

43
00:02:20,045 --> 00:02:22,010
+ w2 x x2

44
00:02:22,010 --> 00:02:23,915
+ w3 x x3.

45
00:02:23,915 --> 00:02:28,040
Agreguemos una capa oculta
de nuestra red de nodos y perímetros.

46
00:02:28,040 --> 00:02:32,640
La capa de entrada tiene tres nodos
y la capa oculta también tiene tres

47
00:02:32,640 --> 00:02:35,210
pero ahora son nodos ocultos.

48
00:02:35,210 --> 00:02:37,565
Como esta capa está totalmente conectada

49
00:02:37,565 --> 00:02:41,750
hay 3 x 3 perímetros o nueve pesos.

50
00:02:41,750 --> 00:02:44,990
Este es un modelo no lineal
que podemos usar

51
00:02:44,990 --> 00:02:48,045
para resolver nuestros problemas
no lineales, ¿verdad?

52
00:02:48,045 --> 00:02:51,340
Lamentablemente, no. Analicémoslo.

53
00:02:52,220 --> 00:02:56,415
La entrada en el primer nodo oculto
es la suma ponderada de w1 x x1

54
00:02:56,415 --> 00:02:58,515
+ w4 x x2

55
00:02:58,515 --> 00:03:01,350
+ w7 x x3.

56
00:03:01,930 --> 00:03:05,640
La entrada en el segundo nodo oculto
es la suma ponderada de w2 x x1

57
00:03:05,640 --> 00:03:10,395
+ w5 x x2, + w8 x x3.

58
00:03:10,395 --> 00:03:14,520
La entrada en el tercer nodo oculto
es la suma ponderada de w3 x 1x

59
00:03:14,520 --> 00:03:19,575
+ w6 x x2, + w9 x x3.

60
00:03:19,575 --> 00:03:23,035
Si combinamos todo en el nodo de salida

61
00:03:23,035 --> 00:03:25,685
obtenemos w10 x h1

62
00:03:25,685 --> 00:03:28,080
+ w12 x h2

63
00:03:28,080 --> 00:03:30,225
+ w12 x h3.

64
00:03:30,225 --> 00:03:32,550
Recuerde que h1

65
00:03:32,550 --> 00:03:37,370
h2 y h3 son solo combinaciones lineales
de los atributos de entrada.

66
00:03:37,370 --> 00:03:40,055
Por lo tanto, si lo expandimos

67
00:03:40,055 --> 00:03:43,165
tenemos un conjunto complejo de constantes
de pesos multiplicadas

68
00:03:43,165 --> 00:03:47,570
por cada valor de entrada: x1, x2 y x3.

69
00:03:48,450 --> 00:03:51,935
Podemos sustituir cada par de pesos
con un peso nuevo.

70
00:03:51,935 --> 00:03:53,250
¿Le resulta familiar?

71
00:03:53,250 --> 00:03:56,530
Es exactamente
el mismo modelo lineal de antes

72
00:03:56,530 --> 00:04:00,995
aunque agregamos una capa
oculta de neuronas. ¿Qué sucedió?

73
00:04:00,995 --> 00:04:04,050
¿Qué pasa si agregamos otra capa oculta?

74
00:04:04,050 --> 00:04:09,520
Lamentablemente, también se contrae
en una sola matriz de peso

75
00:04:09,520 --> 00:04:12,075
multiplicada por cada una
de las tres entradas.

76
00:04:12,075 --> 00:04:13,790
Es el mismo modelo lineal.

77
00:04:13,790 --> 00:04:18,450
Podríamos repetir este proceso
y obtendríamos el mismo resultado

78
00:04:18,450 --> 00:04:23,250
pero con un costo de computación mayor
para el entrenamiento o la predicción

79
00:04:23,250 --> 00:04:26,260
para una arquitectura más complicada
que la necesaria.

80
00:04:27,150 --> 00:04:29,600
Desde la perspectiva de la álgebra lineal

81
00:04:29,600 --> 00:04:33,455
está multiplicando varias matrices
en una cadena.

82
00:04:33,455 --> 00:04:34,985
En este pequeño ejemplo

83
00:04:34,985 --> 00:04:37,085
primero multipliqué una matriz de 3 x 3

84
00:04:37,085 --> 00:04:41,710
la transposición de la matriz de peso
entre la entrada y la primera capa oculta

85
00:04:41,710 --> 00:04:45,930
por el vector de entrada de 3 x 1,
cuyo resultado es el vector de 3 x 1

86
00:04:45,930 --> 00:04:49,345
que son los valores de cada neurona oculta
en la primera capa oculta.

87
00:04:49,345 --> 00:04:52,685
Definí los valores de la neurona
de las segundas capas ocultas

88
00:04:52,685 --> 00:04:54,030
multipliqué la transposición

89
00:04:54,030 --> 00:04:57,605
de su matriz de peso de 3 x 3,
que conecta la capa oculta 1

90
00:04:57,605 --> 00:05:01,270
con la capa oculta 2 en el vector
resultante de la capa oculta 1.

91
00:05:01,270 --> 00:05:04,810
Como puede ver,
las dos matrices de peso 3 x 3

92
00:05:04,810 --> 00:05:07,270
se puede combinar en una matriz de 3 x 3

93
00:05:07,270 --> 00:05:08,870
si primero se calcula

94
00:05:08,870 --> 00:05:12,570
el producto de la matriz de la izquierda
o de la derecha.

95
00:05:12,570 --> 00:05:15,450
Se obtiene la misma forma para h2

96
00:05:15,450 --> 00:05:19,020
el vector de valor de la neurona
de la segunda capa oculta.

97
00:05:19,350 --> 00:05:23,075
Y en la última capa entre la capa oculta 2
y la capa de salida

98
00:05:23,075 --> 00:05:25,290
necesito multiplicar los pasos anteriores

99
00:05:25,290 --> 00:05:28,610
por la transposición de la matriz
de peso entre las últimas dos capas.

100
00:05:28,610 --> 00:05:31,800
Aunque cuando prealimenta
a través de una red neuronal

101
00:05:31,800 --> 00:05:34,861
realiza una multiplicación de matrices
de derecha a izquierda

102
00:05:34,861 --> 00:05:38,670
si la aplica de izquierda a derecha,
nuestra cadena grande

103
00:05:38,670 --> 00:05:42,750
de complicaciones de matriz se contrae
en un vector de tres valores.

104
00:05:42,750 --> 00:05:46,160
Si entrena este modelo en un caso
de regresión lineal simple

105
00:05:46,160 --> 00:05:50,760
de tres pesos de lado a lado y producen
el mismo mínimo en la superficie baja

106
00:05:50,760 --> 00:05:54,865
aunque usé mucha computación
para calcular los 21 pesos

107
00:05:54,865 --> 00:05:58,859
de mi cadena de producto de matrices
se condensará en la ecuación menor

108
00:05:58,859 --> 00:06:02,990
el peso coincidirá con los pesos de
la regresión lineal de entrenamiento.

109
00:06:02,990 --> 00:06:05,325
Todo ese trabajo para obtener
el mismo resultado.

110
00:06:05,375 --> 00:06:07,455
Probablemente esté pensando:

111
00:06:07,455 --> 00:06:11,280
"Creí que las redes neuronales se trataban
de agregar capas y capas de neuronas.

112
00:06:11,280 --> 00:06:15,765
¿Cómo puedo hacer un aprendizaje profundo
si todas las capas se contraen en una?"

113
00:06:15,765 --> 00:06:17,190
Le tengo buenas noticias.

114
00:06:17,190 --> 00:06:19,025
Hay una solución simple.

115
00:06:19,025 --> 00:06:22,560
La solución es agregar
una capa de transformación no lineal

116
00:06:22,560 --> 00:06:25,918
que se obtiene con una función
de activación no lineal,

117
00:06:25,918 --> 00:06:28,530
como una función sigmoide,
tangente hiperbólica o ReLu.

118
00:06:28,530 --> 00:06:31,980
Si pensamos en términos del gráfico,
como con TensorFlow

119
00:06:31,980 --> 00:06:35,580
puede imaginar
a cada neurona con dos nodos.

120
00:06:35,580 --> 00:06:39,865
El primer nodo es el resultado
de la suma ponderada de wx + b

121
00:06:39,865 --> 00:06:41,865
y el segundo nodo es el resultado

122
00:06:41,865 --> 00:06:44,430
de eso después del uso
de la función de activación.

123
00:06:44,430 --> 00:06:46,350
En otras palabras, son las entradas

124
00:06:46,350 --> 00:06:49,770
de la función de activación,
seguidas de los resultados de esa función

125
00:06:49,770 --> 00:06:53,735
es decir que la función de activación
actúa como el punto de transición.

126
00:06:54,457 --> 00:06:57,512
Agregar una transformación no lineal
es la única forma de evitar

127
00:06:57,512 --> 00:07:00,400
que la red neuronal se revierta
a una red superficial.

128
00:07:00,400 --> 00:07:04,280
Incluso si tiene una capa con funciones de
activación no lineales en su red,

129
00:07:04,280 --> 00:07:09,235
si en otro lugar de la red tiene dos
o más capas con funciones

130
00:07:09,235 --> 00:07:12,385
de activación lineales, esas pueden
contraerse en solo una red.

131
00:07:12,385 --> 00:07:15,010
En general, las redes neuronales
tienen todas las capas

132
00:07:15,010 --> 00:07:17,840
no lineales para la primera capa y la -1

133
00:07:17,840 --> 00:07:21,245
y la transformación de la última capa
es lineal para la regresión

134
00:07:21,245 --> 00:07:25,175
sigmoide o softmax, que analizaremos
luego para la clasificación.

135
00:07:25,175 --> 00:07:27,620
Todo depende del resultado
que desea obtener.

136
00:07:28,000 --> 00:07:29,970
Volvamos a analizarlo
desde la perspectiva

137
00:07:29,970 --> 00:07:34,540
de la álgebra lineal, cuando aplicamos
una transformación lineal a una matriz

138
00:07:34,540 --> 00:07:39,670
o vector, estamos multiplicándolos para
obtener la forma y resultados deseados.

139
00:07:39,670 --> 00:07:41,840
Como cuando quiero escalar una matriz

140
00:07:41,840 --> 00:07:43,635
puedo multiplicarla
por una constante.

141
00:07:43,635 --> 00:07:47,039
Pero, en realidad, lo que hago
es multiplicarla por una matriz identidad

142
00:07:47,039 --> 00:07:48,870
multiplicada por esa constante.

143
00:07:48,870 --> 00:07:52,600
Es una matriz diagonal con
esa constante en la diagonal.

144
00:07:52,600 --> 00:07:55,690
Esto se contraería
en solo un producto de matriz.

145
00:07:55,690 --> 00:07:59,070
Sin embargo, si agrego una no linealidad

146
00:07:59,070 --> 00:08:02,660
lo que hago no lo podrá
representar una matriz.

147
00:08:02,660 --> 00:08:05,880
ya que le estos aplicando una función
a mi entrada de a un elemento.

148
00:08:05,880 --> 00:08:08,160
Por ejemplo, si tengo una función

149
00:08:08,160 --> 00:08:11,315
de activación no lineal entre
la primera y la segunda capa oculta

150
00:08:11,315 --> 00:08:14,150
estoy aplicando una función
del producto de la transposición

151
00:08:14,150 --> 00:08:17,645
de la matriz de peso de mis primeras
capas ocultas y el vector de entrada.

152
00:08:17,645 --> 00:08:20,740
La ecuación inferior es mi función
de activación en una ReLu.

153
00:08:21,070 --> 00:08:24,560
Como no puedo representar
la transformación como álgebra lineal

154
00:08:24,560 --> 00:08:27,890
ya no puedo contraer esa porción
de mi cadena de transformación

155
00:08:27,890 --> 00:08:30,800
por lo que mi modelo sigue siendo complejo

156
00:08:30,800 --> 00:08:34,325
y no se contrae en una sola
combinación lineal de entradas.

157
00:08:34,325 --> 00:08:38,599
Aún puedo contraer
la segunda capa oculta de matriz de peso

158
00:08:38,599 --> 00:08:43,459
y la matriz de peso de la capa de salida,
pues no se aplica una función lineal.

159
00:08:43,459 --> 00:08:47,540
Es decir que, cuando hay 
dos o más capas lineales consecutivas

160
00:08:47,540 --> 00:08:51,515
siempre se pueden contraer en una capa
sin importar la cantidad.

161
00:08:51,795 --> 00:08:55,415
Por lo tanto, como tienen las funciones
más complejas que crea su red

162
00:08:55,415 --> 00:08:58,790
es mejor que toda su red tenga funciones
de activación lineales

163
00:08:58,790 --> 00:09:02,710
excepto la última capa, por si usa
un tipo de salida diferente al final.

164
00:09:02,710 --> 00:09:08,295
¿Por qué importa agregar funciones de
activación lineal a las redes neuronales?

165
00:09:08,295 --> 00:09:11,310
Porque evitan que las capas se contraigan

166
00:09:11,310 --> 00:09:13,205
a un solo modelo lineal.

167
00:09:13,205 --> 00:09:15,305
Las funciones de activación no lineales

168
00:09:15,305 --> 00:09:18,510
ayudan a crear transformaciones
con el espacio de escritura de datos

169
00:09:18,510 --> 00:09:21,260
y también admiten
funciones de composición profundas.

170
00:09:21,260 --> 00:09:26,590
Si hay dos o más capas
con funciones de activación lineal

171
00:09:26,590 --> 00:09:28,750
este producto de matrices se puede resumir

172
00:09:28,750 --> 00:09:31,740
en una matriz por el vector
de función de entrada.

173
00:09:31,740 --> 00:09:34,420
Por lo que tendrá un modelo más lento

174
00:09:34,420 --> 00:09:38,605
con más computación,
pero con menos complejidad funcional.

175
00:09:38,605 --> 00:09:41,620
Las no linealidades
no agregan regularización

176
00:09:41,620 --> 00:09:45,015
a la función de pérdida
y no invocan la interrupción anticipada.

177
00:09:45,015 --> 00:09:47,650
Aunque las funciones
de activación no lineales

178
00:09:47,650 --> 00:09:50,310
crean transformaciones
complejas en el espacio vectorial

179
00:09:50,310 --> 00:09:53,670
esa dimensión no cambia,
sigue siendo el mismo espacio vectorial.

180
00:09:53,670 --> 00:09:56,790
Aunque se estire, apriete o rote.

181
00:09:56,990 --> 00:09:59,680
Como comentamos
en uno de los módulos anteriores

182
00:09:59,680 --> 00:10:03,200
hay funciones de activación
no lineales sigmoidal

183
00:10:03,200 --> 00:10:05,380
y de la función sigmoide
escalada y modificada

184
00:10:05,380 --> 00:10:07,605
la tangente hiperbólica
es de las primeras.

185
00:10:07,615 --> 00:10:09,985
Sin embargo, puede haber una saturación

186
00:10:09,985 --> 00:10:13,310
que genere un problema
de desvanecimiento de gradientes.

187
00:10:13,310 --> 00:10:14,510
Sin gradientes

188
00:10:14,510 --> 00:10:18,205
los pesos de los modelos no se actualizan
y se detiene el entrenamiento.

189
00:10:18,205 --> 00:10:21,140
La unidad lineal rectificada o ReLu

190
00:10:21,140 --> 00:10:24,215
es una de nuestra favoritas
porque es simple y funciona bien.

191
00:10:24,215 --> 00:10:26,530
En el dominio positivo, es lineal

192
00:10:26,530 --> 00:10:30,580
así que no hay saturación y en el
dominio negativo, la función es cero.

193
00:10:30,580 --> 00:10:33,365
Las redes con activación oculta de ReLu

194
00:10:33,365 --> 00:10:37,075
a menudo tienen una velocidad
de entrenamiento 10 veces mayor

195
00:10:37,075 --> 00:10:39,470
que las redes con activaciones
ocultas sigmoidales.

196
00:10:39,470 --> 00:10:43,200
Sin embargo, como la función
de los dominios negativos es siempre cero

197
00:10:43,200 --> 00:10:45,210
es posible que
las capas reales se pierdan.

198
00:10:45,210 --> 00:10:46,910
Lo que quiero decir es que

199
00:10:46,910 --> 00:10:48,610
cuando comienza a obtener entradas

200
00:10:48,610 --> 00:10:51,845
en el dominio negativo,
el resultado de la activación será cero

201
00:10:51,845 --> 00:10:55,320
lo que no ayuda en la próxima capa
y las entradas en el dominio positivo.

202
00:10:55,320 --> 00:10:59,150
Esto combina y crea
muchas activaciones en cero

203
00:10:59,150 --> 00:11:02,170
durante la propagación inversa
cuando se actualizan los pesos

204
00:11:02,170 --> 00:11:05,660
como tenemos que multiplicar el derivado
de los errores por su activación

205
00:11:05,660 --> 00:11:07,290
y obtendremos
un gradiente de cero.

206
00:11:07,290 --> 00:11:09,650
Así, tenemos un peso de datos de cero

207
00:11:09,650 --> 00:11:13,800
los pesos no cambiarán y el entrenamiento
no funcionará en esa capa.

208
00:11:14,540 --> 00:11:17,039
Por suerte, se desarrollaron
métodos inteligentes

209
00:11:17,039 --> 00:11:20,980
para modificar un poco la ReLu
y que el entrenamiento no se detenga

210
00:11:20,980 --> 00:11:24,120
pero con muchos
de los beneficios de la ReLu convencional.

211
00:11:24,120 --> 00:11:25,920
Aquí está la ReLu convencional

212
00:11:25,920 --> 00:11:30,270
el operador máximo se puede representar
con la ecuación lineal definida en partes

213
00:11:30,270 --> 00:11:32,880
en la que el valor menor a cero,
es la función en cero.

214
00:11:32,880 --> 00:11:36,190
Y con un valor de cero o mayor que cero,
la función es X.

215
00:11:36,190 --> 00:11:38,520
Una aproximación a la función de ReLu

216
00:11:38,520 --> 00:11:41,205
es la función analítica del
logaritmo natural de uno

217
00:11:41,205 --> 00:11:43,185
más el exponente X.

218
00:11:43,185 --> 00:11:45,360
Esta es la función softplus.

219
00:11:45,360 --> 00:11:49,740
El derivado de la función softplus
es una función logística.

220
00:11:49,740 --> 00:11:52,210
La ventaja de usar la función softplus

221
00:11:52,210 --> 00:11:54,570
es que es continua y diferenciable de cero

222
00:11:54,570 --> 00:11:56,380
a diferencia de la función de ReLu.

223
00:11:56,380 --> 00:11:59,449
Sin embargo, debido al logaritmo natural
y al exponente

224
00:11:59,449 --> 00:12:02,295
se aplica computación adicional
en comparación con las ReLu

225
00:12:02,295 --> 00:12:06,030
y las ReLu dan resultados
iguales de buenos en la práctica.

226
00:12:06,030 --> 00:12:10,195
Por lo tanto, no se recomienda
usar softplus en el aprendizaje profundo.

227
00:12:10,715 --> 00:12:14,895
Para resolver el problema de la pérdida
las ReLu con las activaciones cero

228
00:12:14,895 --> 00:12:16,995
se desarrollaron las "Leaky ReLu".

229
00:12:16,995 --> 00:12:21,225
Al igual que las ReLu, tienen una función
lineal definida en partes.

230
00:12:21,225 --> 00:12:23,180
Sin embargo, en el dominio negativo

231
00:12:23,180 --> 00:12:28,225
tienen un pendiente distinto de cero,
específicamente, de 0.01.

232
00:12:28,225 --> 00:12:31,345
De esta forma,
cuando la unidad no se activa

233
00:12:31,345 --> 00:12:35,999
las "Leaky ReLu" permiten que se propague
un gradiente pequeño distinto de cero

234
00:12:35,999 --> 00:12:40,430
lo que debería permitir que se actualice
el peso y continúe el entrenamiento.

235
00:12:40,590 --> 00:12:46,290
Si avanzamos con esta idea,
está la ReLu paramétrica o PreLu.

236
00:12:46,290 --> 00:12:48,640
En lugar de permitir de forma arbitraria

237
00:12:48,640 --> 00:12:51,625
un centésimo de una X
en el dominio negativo

238
00:12:51,625 --> 00:12:54,390
permite el Alfa de X.

239
00:12:54,390 --> 00:12:57,190
¿Qué se supone que es el parámetro Alfa?

240
00:12:57,190 --> 00:13:01,585
En el gráfico, configuré Alfa
como 0.5 para una mejor visualización.

241
00:13:01,585 --> 00:13:04,420
En la práctica, es un parámetro aprendido

242
00:13:04,420 --> 00:13:07,735
en el entrenamiento junto con
otros parámetros de la red neuronal

243
00:13:07,735 --> 00:13:11,045
Así, en lugar de
que configuremos este valor

244
00:13:11,045 --> 00:13:14,600
el valor se determinará
en el entrenamiento con los datos

245
00:13:14,600 --> 00:13:18,910
y debería aprender un valor mejor
que el que configuraríamos.

246
00:13:18,910 --> 00:13:21,520
Cuando Alfa es menor que uno

247
00:13:21,520 --> 00:13:25,270
la fórmula se puede volver a escribir
de forma compacta con el uso del máximo.

248
00:13:25,270 --> 00:13:28,480
Específicamente, el máximo de x
o Alfa por x.

249
00:13:28,480 --> 00:13:33,010
También hay "Leaky ReLu" aleatorizadas,
en las que en lugar de entrenar el Alfa

250
00:13:33,010 --> 00:13:35,740
es una muestra aleatoria
de una distribución uniforme.

251
00:13:35,740 --> 00:13:38,080
Esto puede tener un efecto similar
a la retirada

252
00:13:38,080 --> 00:13:41,170
ya que técnicamente tiene un red
distinta para cada valor de Alfa.

253
00:13:41,170 --> 00:13:43,980
Por lo tanto, hace algo similar
a un ensamble.

254
00:13:43,980 --> 00:13:46,720
En el momento de las pruebas,
todos los valores de Alfa

255
00:13:46,720 --> 00:13:50,415
se promedian en un valor determinista
que se usa para las predicciones.

256
00:13:50,415 --> 00:13:52,550
También está la variante ReLu6

257
00:13:52,550 --> 00:13:56,565
que es otra función lineal definida
en partes con tres segmentos.

258
00:13:56,565 --> 00:13:58,040
Como una ReLu normal

259
00:13:58,040 --> 00:13:59,890
es de cero en el dominio negativo

260
00:13:59,890 --> 00:14:03,105
pero en el dominio positivo,
la ReLu6 se mantiene en seis.

261
00:14:03,105 --> 00:14:06,520
Probablemente se pregunte
"¿por qué se mantiene en seis?"

262
00:14:06,520 --> 00:14:09,040
Puede imaginar a una
de estas unidades ReLu

263
00:14:09,040 --> 00:14:12,460
con solo seis unidades replicadas
de Bernoulli modificadas por el sesgo.

264
00:14:12,460 --> 00:14:15,520
en lugar de una cantidad infinita
debido al límite máximo.

265
00:14:15,520 --> 00:14:18,520
En general, se conocen
como unidades n de ReLu

266
00:14:18,520 --> 00:14:20,475
en las que la "n" es el valor límite.

267
00:14:20,475 --> 00:14:24,270
En las pruebas, se descubrió que seis
es el valor óptimo.

268
00:14:24,270 --> 00:14:28,235
Las unidades ReLu6 ayudan a los modelos
a aprender los atributos más rápido.

269
00:14:28,235 --> 00:14:32,020
Primero, se usaron para redes
convolucionales de creencia profunda

270
00:14:32,020 --> 00:14:34,180
en un conjunto de datos
de imágenes CIFAR-10.

271
00:14:34,180 --> 00:14:36,280
También son útiles para preparar la red

272
00:14:36,280 --> 00:14:38,700
para la precisión
de punto fijo en la inferencia.

273
00:14:38,700 --> 00:14:40,639
Si el límite superior es indefinido

274
00:14:40,639 --> 00:14:44,095
perderá muchos bits en la parte Q
de un número de punto fijo

275
00:14:44,095 --> 00:14:46,230
mientras que con un límite
superior de seis

276
00:14:46,230 --> 00:14:48,660
deja suficientes bits
en la parte fraccional

277
00:14:48,660 --> 00:14:52,280
del número que lo representa de tal forma
que permitirá una buena inferencia.

278
00:14:52,280 --> 00:14:55,540
Por último, hay la unidad
lineal exponencial o ELU.

279
00:14:55,540 --> 00:15:00,265
Es aproximadamente lineal en la parte
no negativa del espacio de entrada

280
00:15:00,265 --> 00:15:02,680
es uniforme, monótona y,
lo más importante

281
00:15:02,680 --> 00:15:05,320
distinta de cero
en la parte negativa de la entrada.

282
00:15:05,320 --> 00:15:09,315
Además está mejor centrada en cero
que las ReLu convencionales

283
00:15:09,315 --> 00:15:10,785
lo que acelera el aprendizaje.

284
00:15:10,785 --> 00:15:14,480
La desventaja principal de las ELU
es que son más caras en la composición

285
00:15:14,480 --> 00:15:17,475
que las ReLu dado que
tienen que calcular el exponente.

286
00:15:17,475 --> 00:15:20,285
Las redes neuronales
pueden ser complejas arbitrariamente

287
00:15:20,285 --> 00:15:21,710
pueden haber muchas capas

288
00:15:21,710 --> 00:15:23,930
neuronas por capa, salidas, entradas

289
00:15:23,930 --> 00:15:26,800
diferentes tipos
de funciones de activación, etc.

290
00:15:26,800 --> 00:15:29,050
¿Cuál es el propósito
de tener varias capas?

291
00:15:29,050 --> 00:15:30,390
Cada capa que agrego

292
00:15:30,390 --> 00:15:32,860
agrega complejidad
a las funciones que puedo crear.

293
00:15:32,860 --> 00:15:36,790
Cada capa subsiguiente es una composición
de las funciones anteriores.

294
00:15:36,790 --> 00:15:40,255
Como usamos funciones de activación
no lineales en las capas ocultas

295
00:15:40,255 --> 00:15:43,570
creo una pila de transformaciones
de datos que rotan

296
00:15:43,570 --> 00:15:45,510
estiran y comprimen mis datos.

297
00:15:45,510 --> 00:15:48,190
El propósito de hacer todo esto

298
00:15:48,190 --> 00:15:50,470
es transferir mis datos de una forma

299
00:15:50,470 --> 00:15:53,540
que puedan ajustar sin problemas
el hiperplano para la regresión

300
00:15:53,540 --> 00:15:56,365
o separar los datos con un hiperplano
para la clasificación.

301
00:15:56,365 --> 00:16:01,580
Asignamos desde un espacio de atributos
originales a un espacio convolucionado.

302
00:16:01,580 --> 00:16:04,530
¿Qué pasa si agrego
neuronas adicionales a una capa?

303
00:16:04,530 --> 00:16:08,740
Cada neurona nueva agrega
una dimensión a mi espacio vectorial.

304
00:16:08,740 --> 00:16:10,925
Si comienzo con tres neuronas de entrada

305
00:16:10,925 --> 00:16:12,830
empiezo en el espacio vectorial R3.

306
00:16:12,830 --> 00:16:15,491
Pero, si mi próxima capa
tiene cuatro neuronas

307
00:16:15,491 --> 00:16:17,490
entonces pasé al espacio vectorial R4

308
00:16:17,490 --> 00:16:20,490
Cuando analizamos los métodos Kernel
en el curso anterior

309
00:16:20,490 --> 00:16:23,490
teníamos un conjunto de datos
que no se podía separar fácilmente

310
00:16:23,490 --> 00:16:26,395
con un hiperplano en el espacio vectorial
de entrada original.

311
00:16:26,395 --> 00:16:29,520
Cuando agregamos la dimensión
y transformamos los datos

312
00:16:29,520 --> 00:16:32,120
para rellenar la dimensión nueva
de la forma correcta

313
00:16:32,120 --> 00:16:37,045
pudimos hacer una división clara
entre las clases de datos.

314
00:16:37,045 --> 00:16:39,570
Esto también funciona
con las redes neuronales.

315
00:16:39,570 --> 00:16:43,100
¿Qué pasa si tengo varios nodos de salida?

316
00:16:43,750 --> 00:16:45,920
Tener varios nodos de salida le permite

317
00:16:45,920 --> 00:16:49,995
comparar varias etiquetas y propagar
las áreas correspondientes hacia atrás.

318
00:16:49,995 --> 00:16:52,850
Imagine hacer una clasificación
de imágenes

319
00:16:52,850 --> 00:16:56,045
con varias entidades o clases
dentro de cada imagen.

320
00:16:56,045 --> 00:16:59,660
No podemos predecir una clase
porque pueden haber muchas

321
00:16:59,660 --> 00:17:02,405
de manera que contar con
esta flexibilidad es genial.

322
00:17:02,505 --> 00:17:05,409
Las redes neuronales pueden
ser complejas de forma arbitraria.

323
00:17:05,409 --> 00:17:06,875
Para crear dimensiones ocultas

324
00:17:06,875 --> 00:17:08,059
puedo agregar [espacio].

325
00:17:08,059 --> 00:17:10,309
Para incrementar
la composición de los atributos

326
00:17:10,309 --> 00:17:11,454
puedo agregar [espacio].

327
00:17:11,454 --> 00:17:13,290
Si tengo varias etiquetas, por ejemplo

328
00:17:13,290 --> 00:17:14,999
puedo agregar [espacio].

329
00:17:15,239 --> 00:17:18,755
La respuesta correcta es
neuronas, capas, salidas.

330
00:17:18,755 --> 00:17:20,680
Para cambiar las dimensiones ocultas

331
00:17:20,680 --> 00:17:22,870
puedo cambiar la cantidad
de capas de neuronas.

332
00:17:22,870 --> 00:17:25,820
Eso determina las dimensiones
del espacio vectorial.

333
00:17:25,850 --> 00:17:27,380
Si una capa
tiene cuatro neuronas

334
00:17:27,380 --> 00:17:29,280
entonces está en el espacio
vectorial R4

335
00:17:29,280 --> 00:17:33,080
y si una capa tiene 500 neuronas
está en el espacio vectorial R500.

336
00:17:33,080 --> 00:17:36,935
Es decir que tiene 500 dimensiones reales.

337
00:17:36,935 --> 00:17:40,417
Agregar una capa
no cambia la dimensión de la capa anterior

338
00:17:40,660 --> 00:17:43,970
incluso puede no cambiar
la dimensión de su capa

339
00:17:43,970 --> 00:17:47,870
a menos que tenga una cantidad distinta
de neuronas en la capa anterior.

340
00:17:47,870 --> 00:17:51,995
Lo que las capas adicionales agregan
es una mayor composición de funciones.

341
00:17:51,995 --> 00:17:53,960
Recuerde "g ∘ f (x)"

342
00:17:53,960 --> 00:17:58,355
es la composición
de las funciones g y f en la entrada x.

343
00:17:58,355 --> 00:18:01,460
Por lo tanto, primero transformo x por f

344
00:18:01,460 --> 00:18:04,630
y luego transformo ese resultado por g.

345
00:18:04,630 --> 00:18:07,840
Cuantas más capas tenga, más profundo
irán las funciones g anidadas.

346
00:18:07,840 --> 00:18:10,140
Esto es ideal para combinar
funciones no lineales

347
00:18:10,140 --> 00:18:13,240
y hacer mapas de atributos
convolucionados difíciles de construir

348
00:18:13,240 --> 00:18:15,550
para las personas
pero ideales para computadoras

349
00:18:15,550 --> 00:18:17,560
y que nos permiten darle una forma

350
00:18:17,560 --> 00:18:21,010
a nuestros datos que para poder
aprender y obtener información valiosa.

351
00:18:21,010 --> 00:18:24,060
Recibimos la información valiosa
a través de las capas de salida

352
00:18:24,060 --> 00:18:27,985
que, durante la inferencia, serán
las respuestas al problema de AA.

353
00:18:27,985 --> 00:18:31,245
Si solo quiere conocer la probabilidad
de que una imagen sea un perro

354
00:18:31,245 --> 00:18:33,440
puede hacer con solo un nodo de salida.

355
00:18:33,440 --> 00:18:36,770
Pero si quiere conocer la probabilidad
de que una imagen sea un perro

356
00:18:36,770 --> 00:18:38,160
un gato, un pájaro o un alce

357
00:18:38,160 --> 00:18:40,800
entonces deberá tener
un nodo para cada uno de ellos.

358
00:18:40,800 --> 00:18:45,960
Las otras respuestas son incorrectas,
pues dos o más palabras no son correctas.