1
00:00:00,000 --> 00:00:04,605
ではNNのIntensibleプレイグラウンドで
遊びましょう

2
00:00:04,605 --> 00:00:06,585
このNNのプレイグラウンドラボでは

3
00:00:06,585 --> 00:00:08,160
Intensiveプレイグラウンドを使って

4
00:00:08,160 --> 00:00:10,450
NNを構築してデータを学習させます

5
00:00:10,450 --> 00:00:13,290
これらの問題を2つの方法で解決します

6
00:00:13,290 --> 00:00:17,640
最初は手動の特徴エンジニアリングを使用して
モデルをトレーニングします

7
00:00:17,640 --> 00:00:19,560
ここでは独自の知識を使用して

8
00:00:19,560 --> 00:00:23,720
データの学習に最適な特徴の組み合わせと
変換を推測します

9
00:00:23,720 --> 00:00:25,260
次に操作権を

10
00:00:25,260 --> 00:00:27,450
ニューラルネットワークに渡し

11
00:00:27,450 --> 00:00:29,790
簡単な入力特徴を使用して

12
00:00:29,790 --> 00:00:34,065
層とニューロンを追加し
特徴エンジニアリングの実施を確認します

13
00:00:34,065 --> 00:00:35,925
Intensibleプレイグラウンドに戻ります

14
00:00:35,925 --> 00:00:38,310
このラボでは特徴エンジニアリングが

15
00:00:38,310 --> 00:00:41,565
NNより優れているかを確認します

16
00:00:41,565 --> 00:00:44,820
私はそうはならないと思います
調べてみましょう

17
00:00:44,820 --> 00:00:48,390
この図表では

18
00:00:48,390 --> 00:00:54,750
青とオレンジの点の分類を行います
これは分類問題です

19
00:00:54,750 --> 00:00:57,840
これらの点は2つの同心円のように見えます

20
00:00:57,840 --> 00:01:00,000
しかしこのケースには多数のノイズがあります

21
00:01:00,000 --> 00:01:03,090
そのため多数の混合が生じています

22
00:01:03,090 --> 00:01:09,340
ここではX1とX2がトレーニングで
どのような動きをするかを確認します

23
00:01:09,740 --> 00:01:14,400
見てわかるようにほとんど学習していません

24
00:01:14,400 --> 00:01:17,400
全体的に不鮮明で ほとんど白です

25
00:01:17,400 --> 00:01:19,640
そのためどちらにもなりません

26
00:01:19,640 --> 00:01:22,060
下のスケールによると-1 0 1です

27
00:01:22,060 --> 00:01:25,220
つまりあまり学習していません
改善できるか確認しましょう

28
00:01:25,220 --> 00:01:28,095
特徴エンジニアリングではこれは円になります

29
00:01:28,095 --> 00:01:31,005
そこでX1の2乗とX2の2乗を行います

30
00:01:31,005 --> 00:01:34,530
ここでやってみます どうですか

31
00:01:34,530 --> 00:01:37,290
見てください 楕円になりました

32
00:01:37,290 --> 00:01:42,295
この関数が何かを
もうすぐ把握できるということです

33
00:01:42,295 --> 00:01:44,470
円であることがわかりますが

34
00:01:44,470 --> 00:01:45,550
多くのノイズがあります

35
00:01:45,550 --> 00:01:47,465
そのため少し外側にシフトします

36
00:01:47,465 --> 00:01:50,820
しかしX1とX2を取り除いて

37
00:01:50,820 --> 00:01:53,900
線形を形成すれば 損失を0.275から

38
00:01:53,900 --> 00:01:57,210
下げることができます
やってみましょう

39
00:01:57,370 --> 00:02:00,060
0.285です

40
00:02:00,060 --> 00:02:02,715
もう少し丸くなりました

41
00:02:02,715 --> 00:02:05,790
しかしテスト損失が少し改善されました

42
00:02:05,790 --> 00:02:09,389
同じことがNNでできるかどうか見てみましょう

43
00:02:09,389 --> 00:02:12,525
X1とX2だけに戻りましょう

44
00:02:12,525 --> 00:02:15,855
すでに見たように 非常にひどい結果しか
出ていません

45
00:02:15,855 --> 00:02:19,570
隠れ層を追加します
ドメインを2つ追加します

46
00:02:21,530 --> 00:02:27,975
ご覧のようにこの関数の把握に非常に
苦労しています

47
00:02:27,975 --> 00:02:31,950
問題はこの2つのニューロンに
十分な容量がないことです

48
00:02:31,950 --> 00:02:35,715
この分布を学習できるほど高次元表現では
ありません

49
00:02:35,715 --> 00:02:37,995
ここで停止して見てみましょう

50
00:02:37,995 --> 00:02:39,150
ニューロンを追加します

51
00:02:39,150 --> 00:02:41,510
この関数の学習にはこの容量で十分でしょう

52
00:02:41,510 --> 00:02:43,250
できました

53
00:02:43,250 --> 00:02:49,635
まだうまくいきません

54
00:02:49,635 --> 00:02:53,985
これを見てみます

55
00:02:53,985 --> 00:02:55,540
時間がかかりましたが

56
00:02:55,540 --> 00:02:58,810
徐々に関数の形状を把握しています

57
00:02:58,810 --> 00:03:02,540
ここではある種の長方形です

58
00:03:02,540 --> 00:03:06,440
つまりこの分布を示せるニューロンの量の

59
00:03:06,440 --> 00:03:10,355
変わり目をさまよっているようです

60
00:03:10,355 --> 00:03:13,540
もう1つニューロンを追加して

61
00:03:13,540 --> 00:03:17,070
どうなるか見てみましょう

62
00:03:17,070 --> 00:03:19,020
はるかに短時間で終わりました

63
00:03:19,020 --> 00:03:21,000
ここではニューロンは4つだけですが

64
00:03:21,000 --> 00:03:25,510
さらに多数のニューロンを追加すると
どうなるでしょうか

65
00:03:25,700 --> 00:03:29,490
それでは4つの型を入れましょう

66
00:03:29,490 --> 00:03:32,260
何が起こるか見てみましょう

67
00:03:32,260 --> 00:03:36,960
トレーニングします

68
00:03:36,960 --> 00:03:38,460
かなり低速です

69
00:03:38,460 --> 00:03:41,765
これらの中間層の処理に多数の計算が
行われています

70
00:03:41,765 --> 00:03:44,005
最終的にはうまくいくと思います

71
00:03:44,005 --> 00:03:47,290
ご覧のように少し過学習かもしれません

72
00:03:47,290 --> 00:03:50,675
もう単純な円ではありません

73
00:03:50,675 --> 00:03:52,480
規則性のない多角形です

74
00:03:52,480 --> 00:03:56,110
つまりデータの過学習で
テスト損失に対処できていません

75
00:03:56,110 --> 00:03:58,055
前より高くなっています

76
00:03:58,055 --> 00:04:01,950
他の分布を見てみましょう

77
00:04:02,030 --> 00:04:05,910
ここに標準の分布Xrがあります

78
00:04:05,910 --> 00:04:09,260
XとYの両方が正または負の場合は青になり

79
00:04:09,260 --> 00:04:14,240
どちらかの場合はオレンジクラスになります

80
00:04:14,240 --> 00:04:17,540
これをX1 X2だけで学習できるか確認します

81
00:04:20,240 --> 00:04:23,069
ご覧のように 前と同じように

82
00:04:23,069 --> 00:04:27,515
IqとX2はこの関数を表せるほど
強力ではありません

83
00:04:27,515 --> 00:04:29,480
ボード全体で基本的に0です

84
00:04:29,480 --> 00:04:33,120
特徴エンジニアリングでこれを把握できるか
確認します

85
00:04:33,120 --> 00:04:35,580
特徴エンジニアリングでは

86
00:04:35,580 --> 00:04:38,475
その見た目がわかっている
X1 X2を導入します

87
00:04:38,475 --> 00:04:42,150
これをトレーニングします
見てください

88
00:04:42,150 --> 00:04:46,205
かなりいいですね テスト損失は0.17です
素晴らしい

89
00:04:46,205 --> 00:04:49,010
非常に簡単でした

90
00:04:49,010 --> 00:04:52,220
これが重みで0.19です 素晴らしい

91
00:04:52,220 --> 00:04:54,800
確かにノイズもあるので間違いもありますが

92
00:04:54,800 --> 00:04:57,635
大半の部分がかなり正確です

93
00:04:57,635 --> 00:05:00,240
ではNNを使った機械学習が

94
00:05:00,240 --> 00:05:03,255
これより優れた処理を行えるか確認します

95
00:05:03,255 --> 00:05:06,330
X1とX2を戻して

96
00:05:06,330 --> 00:05:08,950
隠れ層を追加します

97
00:05:08,950 --> 00:05:10,850
もう一度見てみましょう

98
00:05:10,850 --> 00:05:13,440
できるだけ量は少なくしたいので

99
00:05:13,440 --> 00:05:17,670
これを2つのニューロンにして学習させます

100
00:05:17,670 --> 00:05:19,920
しかしご覧のように

101
00:05:19,920 --> 00:05:21,720
これを把握できていません

102
00:05:21,720 --> 00:05:24,450
このモデルは複雑性も容量も不十分です

103
00:05:24,450 --> 00:05:27,240
ここで停止して3つ目のニューロンを追加します

104
00:05:27,240 --> 00:05:30,910
もう一度トレーニングしましょう

105
00:05:32,690 --> 00:05:35,250
ご覧のように

106
00:05:35,250 --> 00:05:37,605
この関数の学習に非常に苦労しています

107
00:05:37,605 --> 00:05:39,450
おそらくもう少しでしょう

108
00:05:39,450 --> 00:05:42,260
もう少し待たないと学習するかどうか
わかりませんが

109
00:05:42,260 --> 00:05:44,340
スタックしたようです

110
00:05:44,340 --> 00:05:46,875
初期化すれば直るでしょう

111
00:05:46,875 --> 00:05:49,650
どうでしょうか

112
00:05:49,650 --> 00:05:52,065
あらゆることを行い 初期化しました

113
00:05:52,065 --> 00:05:54,020
ある程度関数を学習するでしょう

114
00:05:54,020 --> 00:05:57,930
斜めの砂時計のようにも見えます

115
00:05:57,930 --> 00:06:00,540
しかし関数ではありません

116
00:06:00,540 --> 00:06:02,115
損失も大きく上昇しています

117
00:06:02,115 --> 00:06:03,570
4つに増やしましょう

118
00:06:03,570 --> 00:06:06,570
これでうまくいくかもしれません

119
00:06:06,570 --> 00:06:10,040
まだ砂時計も見えますが

120
00:06:10,040 --> 00:06:12,800
一連の四角のように見え始めています

121
00:06:12,800 --> 00:06:15,470
それがこの関数です よくなっています

122
00:06:15,470 --> 00:06:18,620
ここで大量に追加して

123
00:06:18,620 --> 00:06:23,350
過学習になるかどうか見てみましょう

124
00:06:27,110 --> 00:06:31,575
ご覧のように非常に低速化し
トレーニングの損失も生じています

125
00:06:31,575 --> 00:06:36,425
しかしかなり正方形になっています

126
00:06:36,425 --> 00:06:43,120
いいように見えます

127
00:06:43,940 --> 00:06:48,375
他の分布タイプを試します

128
00:06:48,375 --> 00:06:50,910
ここにらせんがあります

129
00:06:50,910 --> 00:06:53,090
2つのらせんが互いの周りを回っています

130
00:06:53,090 --> 00:06:55,685
銀河の写真のようです

131
00:06:55,685 --> 00:06:58,810
これをX1 X2でトレーニングできるか
確認します

132
00:06:58,810 --> 00:07:01,080
おそらくできないでしょう

133
00:07:01,080 --> 00:07:03,580
ご覧のように

134
00:07:03,580 --> 00:07:05,615
分布をまったく学習していません

135
00:07:05,615 --> 00:07:07,790
ほぼ0に近い状態です

136
00:07:07,790 --> 00:07:09,515
これが何かを判断できていません

137
00:07:09,515 --> 00:07:12,930
次は特徴エンジニアリングを試します

138
00:07:12,930 --> 00:07:14,600
試してみましょう

139
00:07:14,600 --> 00:07:16,335
どうしますか？

140
00:07:16,335 --> 00:07:19,690
円を試しましょうか？

141
00:07:19,730 --> 00:07:22,710
いいえ これを追加しましょう

142
00:07:22,710 --> 00:07:24,120
正弦と余弦

143
00:07:24,120 --> 00:07:28,160
sineX1とsineX2です
実行しています

144
00:07:28,160 --> 00:07:31,570
ここに6つの未加工の特徴が発生して

145
00:07:31,570 --> 00:07:33,045
入り込んでいます

146
00:07:33,045 --> 00:07:34,230
上部を見ると

147
00:07:34,230 --> 00:07:36,390
ここがゆっくりと近づいています

148
00:07:36,390 --> 00:07:39,570
ここに大きな隙間があります

149
00:07:39,570 --> 00:07:43,370
ここは非常に強く外挿しています

150
00:07:43,370 --> 00:07:45,000
あまりうまく動作していません

151
00:07:45,000 --> 00:07:46,830
ご覧のように停止しています

152
00:07:46,830 --> 00:07:50,115
これをNNでうまくできるか確認します

153
00:07:50,115 --> 00:07:52,150
これをすべてオフにし

154
00:07:52,150 --> 00:07:53,990
隠れ層を追加します

155
00:07:53,990 --> 00:07:57,100
2つのニューロンで開始したら
うまくいくか確認します

156
00:07:58,100 --> 00:08:03,515
ご覧のようにX1とX2のときと
まったく変わりません

157
00:08:03,515 --> 00:08:06,050
このモデルを学習する容量がありません

158
00:08:06,050 --> 00:08:09,870
3つに増やして学習できるか見てみます

159
00:08:11,600 --> 00:08:16,615
前回より少しよくなり
ここに外挿がありますが

160
00:08:16,615 --> 00:08:18,880
この活性化した6つの特徴の保存は

161
00:08:18,880 --> 00:08:23,455
それほどうまく行えていません

162
00:08:23,455 --> 00:08:27,835
ではもう1つニューロンを追加できるか
確認します

163
00:08:27,835 --> 00:08:33,220
層を追加します
これでできるか見てみましょう

164
00:08:33,470 --> 00:08:37,340
テスト損失に対して
トレーニング損失が非常低く

165
00:08:37,340 --> 00:08:40,289
うまくいっています

166
00:08:40,289 --> 00:08:45,300
スタックしています

167
00:08:45,300 --> 00:08:48,755
もう少し試します
隠れ層を追加します

168
00:08:48,755 --> 00:08:52,040
すべて4まで増やします

169
00:08:52,040 --> 00:08:53,180
これで十分でしょう

170
00:08:53,180 --> 00:08:56,605
結果を見てみましょう

171
00:08:56,605 --> 00:08:59,170
両方ともかなり下がりました

172
00:08:59,170 --> 00:09:04,035
しかし画面全体が白いので
判断は下せていません

173
00:09:04,035 --> 00:09:07,190
そこに変曲点があります

174
00:09:07,190 --> 00:09:09,630
損失は大きく低下しています

175
00:09:10,490 --> 00:09:16,350
しかしテスト損失は上昇もしています

176
00:09:16,350 --> 00:09:19,660
その後は平坦です
やはり容量が足りません

177
00:09:19,660 --> 00:09:24,350
次は思い切って各層にニューロンを
8つ追加します

178
00:09:24,350 --> 00:09:29,290
これならこの複雑でノイズの多い関数を
学習するのに十分でしょう

179
00:09:29,290 --> 00:09:32,720
できました

180
00:09:32,720 --> 00:09:37,230
トレーニングしてみましょう

181
00:09:37,230 --> 00:09:40,920
このトレーニングをここで結合するのに
非常に低速になっています

182
00:09:40,920 --> 00:09:45,880
最後にはこの関数を動かす方法が
見つかるはずです

183
00:09:46,640 --> 00:09:49,110
トレーニング損失は低下しています

184
00:09:49,110 --> 00:09:51,670
テスト損失は上昇しています

185
00:10:00,220 --> 00:10:04,220
テスト損失はどちらかと言えば平坦です

186
00:10:05,290 --> 00:10:07,480
皆さんがこれを行った場合

187
00:10:07,480 --> 00:10:09,575
ネットワークのランダム初期化により

188
00:10:09,575 --> 00:10:13,170
結果はかなり変動するでしょう
他のものを試しましょう

189
00:10:13,170 --> 00:10:20,250
これはもう少し期待できそうです

190
00:10:25,700 --> 00:10:29,430
これはもう少し期待できそうに見えます

191
00:10:29,430 --> 00:10:36,000
これらをここで学習して埋めているのが
わかります

192
00:10:36,560 --> 00:10:43,630
テスト損失が分岐しているので過学習のようです
これはよくありません

193
00:10:49,340 --> 00:10:51,840
そうです

194
00:10:51,840 --> 00:10:53,565
ご覧のように

195
00:10:53,565 --> 00:10:55,245
この山のようなネットワークでも

196
00:10:55,245 --> 00:10:59,000
この分布をうまく学習できません

197
00:10:59,000 --> 00:11:00,785
こうした外挿や

198
00:11:00,785 --> 00:11:06,470
広範な推測が行われ テスト損失の結果も
あまりよくありません

199
00:11:06,470 --> 00:11:07,880
これを見てください

200
00:11:07,880 --> 00:11:12,030
テスト損失が急に低下しています 
素晴らしい

201
00:11:19,940 --> 00:11:24,155
学習した関数の量が増えています

202
00:11:24,155 --> 00:11:29,170
しかしネットワークが巨大なため非常に低速です

203
00:11:32,000 --> 00:11:34,725
これらの各層の間には

204
00:11:34,725 --> 00:11:37,845
64の重みがあります

205
00:11:37,845 --> 00:11:39,240
6層あるので

206
00:11:39,240 --> 00:11:41,850
6 x 64の重みがあります

207
00:11:41,850 --> 00:11:45,885
特徴層と上層の間は含みません

208
00:11:45,885 --> 00:11:49,000
そこには他の8つがあります

209
00:11:50,810 --> 00:11:54,255
これを見てください 
これは素晴らしい

210
00:11:54,255 --> 00:11:56,530
この関数をかなりよく学習しています

211
00:11:56,530 --> 00:11:59,330
しかしこれらの外挿があり

212
00:11:59,330 --> 00:12:00,670
ここには内挿があり

213
00:12:00,670 --> 00:12:05,300
このオレンジのピークはらせんの中心を
通過しています

214
00:12:07,430 --> 00:12:11,300
時間が経つとさらに少しずつよくなります

215
00:12:11,300 --> 00:12:13,640
テスト損失は低下しています

216
00:12:13,640 --> 00:12:18,210
しかしこの形状は非常に過学習です

217
00:12:23,030 --> 00:12:26,170
準備完了です 
ご覧のようにようやく

218
00:12:26,170 --> 00:12:30,335
NNを使ってこうした形状を見つけることが
できました

219
00:12:30,335 --> 00:12:33,140
らせんの場合はNNの方が

220
00:12:33,140 --> 00:12:36,200
適しているときもあります

221
00:12:36,200 --> 00:12:38,200
NNでこの形状を見つけ出せました