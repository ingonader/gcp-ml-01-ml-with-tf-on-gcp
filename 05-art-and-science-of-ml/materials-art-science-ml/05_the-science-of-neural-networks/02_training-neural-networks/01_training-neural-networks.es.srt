1
00:00:00,000 --> 00:00:03,060
Ahora que conoce mejor
las redes neuronales

2
00:00:03,060 --> 00:00:04,890
veamos cómo las puede entrenar

3
00:00:04,890 --> 00:00:07,140
las dificultades comunes
y algunas técnicas

4
00:00:07,140 --> 00:00:10,060
que pueden mejorar
el entrenamiento y la generalización.

5
00:00:10,060 --> 00:00:14,970
En TensorFlow, con la API de Estimator,
usar un "DNNRegressor"

6
00:00:14,970 --> 00:00:17,280
es muy parecido
a usar un "LinearRegressor"

7
00:00:17,280 --> 00:00:20,050
solo se deben agregar
unos pocos parámetros en el código.

8
00:00:20,050 --> 00:00:22,740
Podemos usar optimizadores
basados en el momento

9
00:00:22,740 --> 00:00:24,260
como el predeterminado,
AdaGrad.

10
00:00:24,260 --> 00:00:26,600
O podemos probar otros,
como Adam.

11
00:00:26,600 --> 00:00:30,150
También debemos agregar un parámetros
llamado "hidden_units"

12
00:00:30,150 --> 00:00:31,425
que es una lista.

13
00:00:31,425 --> 00:00:34,710
La cantidad de elementos de la lista
es la cantidad de capas ocultas

14
00:00:34,710 --> 00:00:38,030
y los valores de cada elementos de
la lista es la cantidad de neuronas

15
00:00:38,030 --> 00:00:39,690
de esa capa oculta en particular.

16
00:00:39,690 --> 00:00:42,625
También hay un parámetro
llamado "dropout".

17
00:00:42,625 --> 00:00:44,430
Lo examinaremos
en unos minutos.

18
00:00:44,430 --> 00:00:47,580
Pero, en breve, se usa para
activar y desactivar

19
00:00:47,580 --> 00:00:51,855
neuronas individuales para cada ejemplo
para mejorar la generalización.

20
00:00:51,855 --> 00:00:54,140
Observe la documentación técnica

21
00:00:54,140 --> 00:00:57,390
para conocer el conjunto completo
de parámetros que puede configurar.

22
00:00:57,390 --> 00:00:59,170
Todos estos elementos

23
00:00:59,170 --> 00:01:01,360
se pueden hiperparametrizar
para poder ajustar

24
00:01:01,360 --> 00:01:04,435
su modelo a fin de obtener
la mejor generalización.

25
00:01:04,435 --> 00:01:09,345
La propagación inversa es un tema clásico
del curso de redes neuronales de AA.

26
00:01:09,345 --> 00:01:10,760
Pero, en cierto modo

27
00:01:10,760 --> 00:01:13,500
es como enseñarles a las personas
a crear un compilador.

28
00:01:13,500 --> 00:01:15,759
Es esencial para
una comprensión profunda

29
00:01:15,759 --> 00:01:18,415
pero no es necesario
para la comprensión inicial.

30
00:01:18,415 --> 00:01:21,280
Lo más importante es saber
que hay un algoritmo eficiente

31
00:01:21,280 --> 00:01:25,360
para calcular los derivados y que
TensorFlow lo hará automáticamente.

32
00:01:25,360 --> 00:01:28,140
Hay algunos casos de errores
interesantes para evaluar

33
00:01:28,140 --> 00:01:29,900
como el desvanecimiento
de gradientes

34
00:01:29,900 --> 00:01:32,740
el crecimiento excesivo de gradientes
y la pérdida de capas.

35
00:01:32,740 --> 00:01:36,629
Durante el proceso de entrenamiento,
en especial en las redes profundas

36
00:01:36,629 --> 00:01:38,429
los gradientes pueden desvanecerse

37
00:01:38,429 --> 00:01:43,610
cada capa adicional de su red puede
reducir la señal y la contaminación.

38
00:01:43,610 --> 00:01:45,370
Por ejemplo,
cuando se usan

39
00:01:45,370 --> 00:01:48,310
las funciones de activación sigmoide
o de tangente hiperbólica

40
00:01:48,310 --> 00:01:49,630
en todas las capas ocultas.

41
00:01:49,630 --> 00:01:52,790
Cuando comienza a saturar
termina en regiones asintóticas

42
00:01:52,790 --> 00:01:54,935
de la función, que comienzan a
ser una meseta

43
00:01:54,935 --> 00:01:58,000
y la pendiente se acerca
cada vez más al cero.

44
00:01:58,000 --> 00:02:00,930
Cuando va hacia atrás en la red
durante la propagación inversa

45
00:02:00,930 --> 00:02:04,150
su gradiente se vuelve más pequeño
debido a que está combinando

46
00:02:04,150 --> 00:02:06,800
todos estos gradientes pequeños
hasta que el gradiente

47
00:02:06,800 --> 00:02:08,070
se desvanece por completo.

48
00:02:08,070 --> 00:02:10,854
Cuando sucede esto,
los pesos no se actualizan

49
00:02:10,854 --> 00:02:13,900
y el entrenamiento
se detiene.

50
00:02:13,900 --> 00:02:17,065
Una forma simple de solucionarlo
es usar funciones de activación

51
00:02:17,065 --> 00:02:21,900
no lineales, que no saturen,
como las ReLu, Elu, etc.

52
00:02:21,900 --> 00:02:26,710
También podemos tener el problema de un
crecimiento excesivo de los gradientes

53
00:02:26,710 --> 00:02:31,315
a tal punto que los pesos son tan grandes
que causan desbordamientos.

54
00:02:31,315 --> 00:02:34,295
Incluso si comenzamos con
gradientes relativamente pequeños

55
00:02:34,295 --> 00:02:36,025
como con un valor de dos

56
00:02:36,025 --> 00:02:39,230
se pueden combinar y agrandarse
con muchas capas.

57
00:02:39,230 --> 00:02:43,280
Eso sucede en especial
en los modelos de secuencias largas.

58
00:02:43,280 --> 00:02:47,120
La tasa de aprendizaje puede ser un factor
porque cuando se actualizan los pesos

59
00:02:47,120 --> 00:02:48,965
multiplicamos el gradiente

60
00:02:48,965 --> 00:02:51,830
con la tasa de aprendizaje y 
restamos eso del peso actual.

61
00:02:51,830 --> 00:02:55,700
Incluso si el gradiente no es tan grande
con una tasa de aprendizaje de más de uno

62
00:02:55,700 --> 00:03:00,725
puede agrandarse mucho y causar
problemas en nuestras redes.

63
00:03:01,285 --> 00:03:04,010
Hay muchas técnicas para minimizar esto

64
00:03:04,010 --> 00:03:07,195
como la organización del peso
y los tamaños de lotes más pequeños.

65
00:03:07,195 --> 00:03:09,380
Otra técnica es el recorte de gradiente

66
00:03:09,380 --> 00:03:12,694
con lo que verificamos si
el gradiente normal excede un umbral

67
00:03:12,694 --> 00:03:15,855
que se puede hiperparametrizar
o ajustar, y de ser así

68
00:03:15,855 --> 00:03:18,525
se pueden volver a escalar
los componentes del gradiente

69
00:03:18,525 --> 00:03:20,485
para que se queden
por debajo del máximo.

70
00:03:20,485 --> 00:03:22,970
Otra técnica útil
es la normalización de los lotes

71
00:03:22,970 --> 00:03:26,105
que resuelve el problema del
cambio de covariables internas.

72
00:03:26,105 --> 00:03:28,990
Es entrenamiento
porque los gradientes fluyen mejor.

73
00:03:28,990 --> 00:03:31,975
También puede usar
una tasa de aprendizaje más alta

74
00:03:31,975 --> 00:03:33,535
y podría deshacerse
del retirado

75
00:03:33,535 --> 00:03:36,780
lo que disminuye la competencia
a su propio de tipo de regularización

76
00:03:36,780 --> 00:03:38,600
debido a la contaminación
del minilote.

77
00:03:38,600 --> 00:03:40,475
Para realizar
la normalización de lotes

78
00:03:40,475 --> 00:03:42,035
busque la media del minilote

79
00:03:42,035 --> 00:03:44,455
su desviación estándar

80
00:03:44,455 --> 00:03:46,840
normalice las entradas en ese nodo

81
00:03:46,840 --> 00:03:52,640
y, luego, ajuste y cambie así:
gamma por x más beta

82
00:03:52,640 --> 00:03:55,360
donde gamma y beta
son los parámetros de aprendizaje.

83
00:03:55,360 --> 00:03:58,290
Si gamma es igual a la variación
de la raíz cuadrada de X

84
00:03:58,290 --> 00:03:59,850
y beta es igual a la media de X,

85
00:03:59,850 --> 00:04:02,110
se restablece la función
de activación original.

86
00:04:02,110 --> 00:04:06,145
Así, puede controlar el rango de entradas
para que no se agranden demasiado.

87
00:04:06,145 --> 00:04:09,720
Idealmente, los gradientes deberían
mantenerse lo más cerca posible

88
00:04:09,720 --> 00:04:12,100
de uno, especialmente
para las redes muy profundas.

89
00:04:12,100 --> 00:04:15,910
Así, no se combinan y no hay
desbordamiento ni subdesbordamiento.

90
00:04:15,910 --> 00:04:18,742
Otro modo de error común de
descenso de gradientes

91
00:04:18,742 --> 00:04:20,750
es que se pueden
perder las capas de ReLu.

92
00:04:20,750 --> 00:04:23,830
Por suerte, con TensorBoard
podemos supervisar

93
00:04:23,830 --> 00:04:26,630
los resúmenes durante
y después del entrenamiento

94
00:04:26,630 --> 00:04:28,380
de nuestros modelos
de red neuronal.

95
00:04:28,380 --> 00:04:33,085
Si se usa un estimador DNN prediseñado
hay un escalar automático resumido

96
00:04:33,085 --> 00:04:36,200
en cada capa oculta
con la fracción con valores cero

97
00:04:36,200 --> 00:04:38,410
en las activaciones de esa capa.

98
00:04:38,410 --> 00:04:41,260
Las ReLu dejan de funcionar
cuando sus entradas

99
00:04:41,260 --> 00:04:45,060
las mantienen en el dominio negativo
y su activación tiene un valor de cero.

100
00:04:45,060 --> 00:04:49,149
No termina allí, porque su contribución
en la próxima capa es de cero

101
00:04:49,149 --> 00:04:52,460
ya que a pesar de los pesos
que las conecten a las próximas neuronas

102
00:04:52,460 --> 00:04:55,340
su activación es de cero,
por lo que la entrada será de cero.

103
00:04:55,340 --> 00:04:58,740
Los ceros que ingresan
en la próxima neurona no ayudan

104
00:04:58,740 --> 00:05:01,890
a que ingrese al dominio positivo
y esas activaciones de neuronas

105
00:05:01,890 --> 00:05:04,960
también serán de cero y el problema
se propaga en cascada.

106
00:05:04,960 --> 00:05:08,495
Luego, realizamos la propagación inversa
y sus gradientes son de cero

107
00:05:08,495 --> 00:05:11,440
por lo que no tenemos los pesos y
el entrenamiento se detiene.

108
00:05:11,440 --> 00:05:12,460
Eso es un problema.

109
00:05:12,460 --> 00:05:15,996
Ya hablamos de usar "Leaky ReLu"
o ReLu paramétricas o las Elu

110
00:05:15,996 --> 00:05:19,535
que son más lentas, pero también
puede disminuir las tasas de aprendizaje

111
00:05:19,535 --> 00:05:22,695
para impedir que las capas de ReLu
no se activen o se pierdan.

112
00:05:22,695 --> 00:05:26,860
Un gradiente grande por una tasa
de aprendizaje muy alta puede actualizar

113
00:05:26,860 --> 00:05:31,735
los pesos de tal forma que no se activará
ningún punto de datos de nuevo.

114
00:05:31,735 --> 00:05:33,639
Como el gradiente es cero

115
00:05:33,639 --> 00:05:36,250
no actualizamos los pesos
a una cantidad más razonable

116
00:05:36,250 --> 00:05:38,980
por lo que el problema
persistirá indefinidamente.

117
00:05:38,980 --> 00:05:41,290
Verifiquemos nuestra intuición

118
00:05:41,290 --> 00:05:42,610
¿Qué le pasará al modelo

119
00:05:42,610 --> 00:05:46,075
si tenemos dos señales útiles
relacionadas de forma independiente

120
00:05:46,075 --> 00:05:49,210
con la etiqueta,
pero con escalas distintas?

121
00:05:49,210 --> 00:05:51,280
Por ejemplo, puedo tener un predictor

122
00:05:51,280 --> 00:05:56,455
del sabor de una sopa cuyos atributos
representen "calidad de los ingredientes".

123
00:05:56,455 --> 00:05:59,260
Si el atributo de caldo de gallina
se mide en litros

124
00:05:59,260 --> 00:06:02,160
pero el caldo de carne
se mide en mililitros

125
00:06:02,160 --> 00:06:06,045
puede ser difícil para el descenso
de gradientes estocástico

126
00:06:06,045 --> 00:06:07,255
hacer la convergencia

127
00:06:07,255 --> 00:06:11,170
ya que la tasa de aprendizaje óptima para
estas dos dimensiones puede ser distinta.

128
00:06:11,170 --> 00:06:13,940
Tener los datos limpios y
en un rango de computación útil

129
00:06:13,940 --> 00:06:17,650
tiene muchos beneficios durante
el entrenamiento de sus modelos de AA.

130
00:06:17,650 --> 00:06:21,315
Que los valores de atributos sean pequeños
y específicamente centrados en cero

131
00:06:21,315 --> 00:06:24,375
ayuda a acelerar el entrenamiento
y a evitar problemas numéricos.

132
00:06:24,375 --> 00:06:27,935
Por eso la normalización de lotes
es útil para el exceso de gradientes

133
00:06:27,935 --> 00:06:31,910
porque garantiza que se conservan
no solo los atributos de entrada iniciales

134
00:06:31,910 --> 00:06:34,490
sino también los atributos intermedios

135
00:06:34,490 --> 00:06:37,950
en un rango útil, de modo
que no causen problemas en las capas.

136
00:06:37,950 --> 00:06:40,960
También nos permite
evitar la trampa de N/A

137
00:06:40,960 --> 00:06:43,930
con la que el modelo puede
crecer mucho si los valores exceden

138
00:06:43,930 --> 00:06:45,480
un rango de precisión numérico.

139
00:06:45,480 --> 00:06:47,580
Una combinación
de la escala de los atributos

140
00:06:47,580 --> 00:06:51,145
y una tasa de aprendizaje más baja
puede ayudar a evitar este problema.

141
00:06:51,145 --> 00:06:55,050
Además, evitar los valores atípicos
ayuda con la generalización.

142
00:06:55,050 --> 00:06:58,590
Descubrirlos con la detección
de anomalías y quitarlos previamente

143
00:06:58,590 --> 00:07:02,365
del conjunto de datos antes
del entrenamiento puede ser de gran ayuda.

144
00:07:02,365 --> 00:07:06,950
Recuerde que no hay un método
que funcione con todos los datos.

145
00:07:06,950 --> 00:07:11,045
Hay casos buenos y deficientes
para cada uno de estos enfoques.

146
00:07:11,045 --> 00:07:14,850
Hay muchos métodos para escalar
los valores de función a números pequeños.

147
00:07:14,850 --> 00:07:20,420
Primero, está el ajuste lineal con
el que busca el mínimo y máximo de datos.

148
00:07:20,420 --> 00:07:21,910
Luego, para cada valor

149
00:07:21,910 --> 00:07:23,700
restamos el mínimo

150
00:07:23,700 --> 00:07:26,855
y lo dividimos por la diferencia
del mínimo y máximo o el rango.

151
00:07:26,855 --> 00:07:29,510
Así, todos los valores
serán de entre cero y uno.

152
00:07:29,510 --> 00:07:31,820
Cero será el mínimo y uno el máximo.

153
00:07:31,820 --> 00:07:34,695
Esto también se llama normalización.

154
00:07:34,695 --> 00:07:37,845
También está el recorte de gradiente
o límite duro

155
00:07:37,845 --> 00:07:40,575
con el que configura
un valor mínimo y uno máximo.

156
00:07:40,575 --> 00:07:43,880
Por ejemplo, si el valor mínimo

157
00:07:43,880 --> 00:07:47,540
puede ser -7 y el máximo es 10

158
00:07:47,540 --> 00:07:50,575
todos los valores menores que -7
se convertirán en -7

159
00:07:50,575 --> 00:07:53,430
y todos los valores mayores que 10
se convertirán en 10.

160
00:07:53,430 --> 00:07:57,570
Otro método es la escala logarítmica
con el que aplica la función logarítmica

161
00:07:57,570 --> 00:07:58,730
a sus datos de entrada.

162
00:07:58,730 --> 00:08:01,990
Es ideal cuando sus datos tienen
rangos enormes y desea condensarlos

163
00:08:01,990 --> 00:08:05,140
para que se acerquen
a la magnitud del valor.

164
00:08:05,140 --> 00:08:10,625
Otro método es la estandarización.

165
00:08:10,625 --> 00:08:14,120
Con este, se calcula la media
de sus datos y la desviación estándar.

166
00:08:14,120 --> 00:08:15,750
Una vez que tiene estos valores

167
00:08:15,750 --> 00:08:19,365
resta la media de cada punto de datos
y la divide por la desviación estándar.

168
00:08:19,365 --> 00:08:22,260
Así, los datos se centrarán en cero

169
00:08:22,260 --> 00:08:26,030
porque la media nueva será cero y
la desviación estándar será uno.

170
00:08:26,030 --> 00:08:29,335
Hay muchas otras formas
de ajustar los datos.

171
00:08:29,335 --> 00:08:33,924
¿Cuál es ideal si mi modelo tiene
un crecimiento excesivo de gradientes?

172
00:08:33,924 --> 00:08:37,470
La respuesta correcta es A, B, C y D.

173
00:08:37,470 --> 00:08:41,245
El problema generalmente se produce
cuando los pesos se agrandan demasiado

174
00:08:41,245 --> 00:08:44,215
lo que puede suceder cuando la tasa
de aprendizaje es muy alta.

175
00:08:44,215 --> 00:08:46,610
Esto genera otros problemas

176
00:08:46,610 --> 00:08:50,225
como la estabilidad numérica,
la divergencia y la pérdida de ReLu.

177
00:08:50,225 --> 00:08:54,300
Por lo tanto, es una buena idea
disminuir la tasa de aprendizaje

178
00:08:54,300 --> 00:08:56,290
hasta encontrar
la zona de habitabilidad.

179
00:08:56,290 --> 00:08:58,840
La regularización de pesos
también puede ayudar

180
00:08:58,840 --> 00:09:02,170
porque penalizará a los pesos muy grandes

181
00:09:02,170 --> 00:09:04,660
lo que dificulta
el exceso de gradientes.

182
00:09:04,660 --> 00:09:07,690
Además, aplicar un recorte
de gradiente puede garantizar

183
00:09:07,690 --> 00:09:10,770
que los gradientes no excedan
un umbral determinado.

184
00:09:10,770 --> 00:09:14,595
Puede ayudar a mitigar
una tasa de aprendizaje más alta.

185
00:09:14,595 --> 00:09:16,920
Sin embargo, con una tasa
lo suficientemente alta

186
00:09:16,920 --> 00:09:19,260
aún puede aumentar los pesos
a valores muy altos.

187
00:09:19,260 --> 00:09:21,140
La normalización de lotes puede ayudar

188
00:09:21,140 --> 00:09:24,975
a conservar a las entradas intermedias
de cada capa en un rango delimitado

189
00:09:24,975 --> 00:09:27,770
lo que reduce la posibilidad
de que los pesos aumenten

190
00:09:27,770 --> 00:09:30,910
fuera de rango con un pequeño costo
de computación adicional.

191
00:09:30,910 --> 00:09:34,505
Hay muchos métodos para solucionar
el crecimiento excesivo de los gradientes

192
00:09:34,505 --> 00:09:36,170
no es necesario
que vea a un doctor.

193
00:09:36,170 --> 00:09:39,270
Experimente con estas herramientas
y descubra qué funciona mejor.

194
00:09:39,270 --> 00:09:41,845
Otra forma de regularización
que ayuda a crear

195
00:09:41,845 --> 00:09:45,770
modelos más generalizados es agregar
capas de retirados a las redes neuronales.

196
00:09:45,770 --> 00:09:49,760
Para usar los retirados, agrego
una unión a una o más capas.

197
00:09:49,800 --> 00:09:53,390
En TensorFlow, el parámetro que pasa
se llama retirado o dropout

198
00:09:53,390 --> 00:09:55,690
que es la probabilidad
de retirar una neurona

199
00:09:55,690 --> 00:09:58,860
de forma temporal de la red
en lugar de que permanezca activa.

200
00:09:58,860 --> 00:10:01,495
Debe ser cuidadoso
cuando configura este número

201
00:10:01,495 --> 00:10:04,110
ya que otras funciones que tienen
mecanismos de retirada

202
00:10:04,110 --> 00:10:06,295
usan la probabilidad de conservación

203
00:10:06,295 --> 00:10:08,380
complementaria
a la probabilidad de retirada

204
00:10:08,380 --> 00:10:11,540
o la probabilidad de conservar
una neurona activada o desactivada.

205
00:10:11,540 --> 00:10:14,530
No debería configurar solo un 10%
de probabilidad de retirada

206
00:10:14,530 --> 00:10:17,640
ya que ahora solo conserva el 10%
de sus nodos de forma aleatoria

207
00:10:17,640 --> 00:10:20,485
eso sería un modelo
muy disperso involuntario.

208
00:10:20,485 --> 00:10:23,035
¿Cuál es el funcionamiento
interno de los retirados?

209
00:10:23,035 --> 00:10:26,415
Supongamos que configuramos
una probabilidad de retirada del 20%.

210
00:10:26,415 --> 00:10:29,555
Esto significa que para cada
propagación hacia adelante en la red

211
00:10:29,555 --> 00:10:33,220
el algoritmo se arriesgará en cada neurona
y en la capa de unión de retirados.

212
00:10:33,220 --> 00:10:36,660
Si el resultado es mayor que 20
y la neurona seguirá activa en la red

213
00:10:36,660 --> 00:10:38,920
con otros valores 
la neurona se retirará

214
00:10:38,920 --> 00:10:41,920
y el resultado será un valor
de cero sin importar sus entradas

215
00:10:41,920 --> 00:10:45,305
y no habrá adición negativa
ni positiva en la red

216
00:10:45,305 --> 00:10:49,730
ya que agregar un valor de cero no cambia
nada y no hay simulación de la neurona.

217
00:10:49,730 --> 00:10:54,145
Para compensar que cada nodo se conserva
durante solo un porcentaje del tiempo

218
00:10:54,145 --> 00:10:56,175
las activaciones se ajustan

219
00:10:56,175 --> 00:10:58,640
en 1 sobre -1
de la probabilidad de retirada

220
00:10:58,640 --> 00:11:02,070
o, en otras palabras,
1 sobre la probabilidad de conservación.

221
00:11:02,070 --> 00:11:05,790
Durante el entrenamiento, para que sea
el valor esperado de la activación.

222
00:11:05,790 --> 00:11:08,900
Cuando no se está entrenando,
sin necesidad de cambiar el código

223
00:11:08,900 --> 00:11:11,900
la unión desaparece y las neuronas

224
00:11:11,900 --> 00:11:13,815
en la antigua capa de unión de retirados

225
00:11:13,815 --> 00:11:16,685
están siempre activas y usan los pesos
que entrenó el modelo.

226
00:11:16,685 --> 00:11:21,580
La idea de los retirados es que
crean un modelo de ensamble

227
00:11:21,580 --> 00:11:24,530
porque para cada
propagación hacia adelante

228
00:11:24,530 --> 00:11:27,990
hay una red distinta y
se observa el minilote de datos.

229
00:11:27,990 --> 00:11:30,640
Cuando se junta en la expectativa

230
00:11:30,640 --> 00:11:33,690
es como si hubiese entrenado
2 redes neuronales elevadas a la n.

231
00:11:33,690 --> 00:11:36,005
n es el número de neuronas retiradas

232
00:11:36,005 --> 00:11:38,735
y hacer que ellas trabajen en un ensamble

233
00:11:38,735 --> 00:11:41,805
similar a un grupo de árboles de decisión
en un bosque aleatorio.

234
00:11:41,805 --> 00:11:44,050
También tiene el efecto de extender

235
00:11:44,050 --> 00:11:46,660
la distribución
de los datos en toda la red

236
00:11:46,660 --> 00:11:48,670
en lugar de que
la mayor parte de la señal

237
00:11:48,670 --> 00:11:50,870
favorezca a una rama de la red.

238
00:11:50,870 --> 00:11:54,850
Me lo imagino como desviar agua de un río
o arrollo con varias derivaciones

239
00:11:54,850 --> 00:11:59,190
o diques para garantizar que todos
los causes tengan agua y no se sequen.

240
00:11:59,190 --> 00:12:02,440
Así, la red usa más de su capacidad

241
00:12:02,440 --> 00:12:06,140
ya que la señal fluye
más uniformemente en toda la red

242
00:12:06,140 --> 00:12:08,615
y logrará un mejor
entrenamiento y generalización

243
00:12:08,615 --> 00:12:12,325
sin el desarrollo de grandes dependencias
de neuronas en las rutas populares.

244
00:12:12,325 --> 00:12:15,900
Los valores típicos de retirados
son de entre 20% y 50%.

245
00:12:15,900 --> 00:12:17,555
Si fueran menores

246
00:12:17,555 --> 00:12:21,205
no causarían mucho efecto en la red,
ya que rara vez se retirará un nodo.

247
00:12:21,205 --> 00:12:22,530
Si fueran mayores

248
00:12:22,530 --> 00:12:25,875
no se realizaría bien el entrenamiento
ya que la red será muy dispersa

249
00:12:25,875 --> 00:12:28,420
para tener la capacidad
de aprender sin distribución.

250
00:12:28,420 --> 00:12:31,100
También puede usarlo en redes grandes

251
00:12:31,100 --> 00:12:35,045
porque brinda más capacidad al modelo para
aprender representaciones independientes.

252
00:12:35,045 --> 00:12:38,310
Es decir, hay más pases posibles
para que pruebe la red.

253
00:12:38,310 --> 00:12:39,980
Cuanto más retire

254
00:12:39,980 --> 00:12:41,440
menos conserva

255
00:12:41,440 --> 00:12:43,290
y la regularización será más potente.

256
00:12:43,290 --> 00:12:45,720
Si configura
la probabilidad de retirada a 1

257
00:12:45,720 --> 00:12:47,810
no conserva nada y cada neurona

258
00:12:47,810 --> 00:12:50,490
en la capa de unión de retirados
se elimina de la neurona

259
00:12:50,490 --> 00:12:52,600
y el resultado es una activación de cero.

260
00:12:52,600 --> 00:12:54,760
En la propagación inversa, esto significa

261
00:12:54,760 --> 00:12:58,015
que los pesos no se actualizarán
y esta capa no aprenderá nada.

262
00:12:58,015 --> 00:13:00,035
Si configura la probabilidad en cero

263
00:13:00,035 --> 00:13:03,865
todas las neuronas permanecen activas
y no se regularizan los retirados.

264
00:13:03,865 --> 00:13:06,460
Es un método
más costoso en computación

265
00:13:06,460 --> 00:13:09,970
sin contar con una capa de unión
de retirados porque aún debe arriesgar.

266
00:13:09,970 --> 00:13:13,505
Lo ideal es tener valores
entre cero y uno.

267
00:13:13,505 --> 00:13:17,065
Con probabilidades de retirada
de entre 10% y 50%

268
00:13:17,065 --> 00:13:20,635
un buen punto de partida es 20%
y agregará más de ser necesario.

269
00:13:20,635 --> 00:13:22,875
No hay una probabilidad de retirada

270
00:13:22,875 --> 00:13:25,785
que funcione para todos los modelos
y distribuciones de datos.

271
00:13:25,785 --> 00:13:28,300
Los retirados actúan como
otra forma de [espacio].

272
00:13:28,300 --> 00:13:31,140
Obliga a los datos a fluir por
[espacio] rutas

273
00:13:31,140 --> 00:13:33,155
para que la distribución
sea más uniforme.

274
00:13:33,155 --> 00:13:35,650
También simula
el aprendizaje [espacio].

275
00:13:35,650 --> 00:13:39,170
No olvide ajustar las activaciones
de retirados a la inversa de [espacio].

276
00:13:39,170 --> 00:13:41,830
Eliminamos los retirados
durante [espacio].

277
00:13:41,830 --> 00:13:45,355
La respuesta correcta es la E.
Los retirados actúan como otra forma

278
00:13:45,355 --> 00:13:48,765
de regularización para que
el modelo pueda generalizar mejor.

279
00:13:48,765 --> 00:13:52,160
Para ello, desactiva los nodos
con una probabilidad de retirada

280
00:13:52,160 --> 00:13:55,245
que obliga a los datos a fluir
por diversas rutas

281
00:13:55,245 --> 00:13:57,215
para que la distribución
sea más uniforme.

282
00:13:57,215 --> 00:14:00,385
De lo contrario, los datos y activaciones
asociados pueden aprender

283
00:14:00,385 --> 00:14:01,785
a tomar rutas preferenciales

284
00:14:01,785 --> 00:14:04,395
que puede generar un
entrenamiento deficiente de la red

285
00:14:04,395 --> 00:14:06,775
y un rendimiento ineficiente
de los datos nuevos.

286
00:14:06,775 --> 00:14:09,638
Los retirados también simulan
el aprendizaje de ensamble

287
00:14:09,638 --> 00:14:11,305
ya que crean una agregación

288
00:14:11,305 --> 00:14:14,220
de 2 modelos elevados a n, debido
a la desactivación aleatoria

289
00:14:14,220 --> 00:14:16,500
de los nodos en cada
propagación hacia adelante.

290
00:14:16,500 --> 00:14:18,645
La n corresponde al número
de nodos retirados.

291
00:14:18,645 --> 00:14:19,980
Cada lote ve
una red distinta

292
00:14:19,980 --> 00:14:23,380
por lo que no se puede sobreajustar
en todo el conjunto de entrenamiento

293
00:14:23,380 --> 00:14:24,600
como un bosque aleatorio.

294
00:14:24,600 --> 00:14:27,425
No olvide ajustar las activaciones
de retirados a la inversa

295
00:14:27,425 --> 00:14:29,025
de la probabilidad
de conservación

296
00:14:29,025 --> 00:14:31,145
que es 1 menos
la probabilidad de retirada.

297
00:14:31,145 --> 00:14:33,625
Hacemos esto para que
el nodo se ajuste correctamente

298
00:14:33,625 --> 00:14:35,750
durante el entrenamiento
ya que la inferencia

299
00:14:35,750 --> 00:14:36,770
estará siempre activa

300
00:14:36,770 --> 00:14:39,020
pues quitamos el retirado
durante la inferencia.