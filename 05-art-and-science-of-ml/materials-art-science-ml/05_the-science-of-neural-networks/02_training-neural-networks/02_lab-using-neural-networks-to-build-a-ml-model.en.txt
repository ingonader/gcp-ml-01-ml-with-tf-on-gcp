Now, to put our new knowledge into practice and use Neural Networks in TensorFlow to build an ML model. It's time for another exciting lab using Neural Networks to build ML model. In this lab, you will use the canned estimator, DNNReggressor class in TensorFlow to predict median housing price based on many different features. The data is based on a 1990 census data from California. This data is at the city block level, so these features reflect the total number of rooms in that block, or the total number of people who live on that block, respectively. Welcome back. We're going to go through some of our code to see how we can make a Neural Network using the DNN class Regressor in TensorFlow. So here we are, we're going to learn how to use a Neural Network. So, we're going to use this housing data based on 1990 census data from California. This data is at the city block level. So, it's going to reflect features, the total number of rooms in that block, the total number of people who live in that block respectively. So, let's use a set of features to rate the house value. So, first let me set up. So in the first cell, we're going to lower unnecessary libraries. We're going to import the math, shutil, numpy, pandas, tensorflow. Make sure your proposity is set to info so you can get a lots of results. Make sure some reformatting for pandas is set. So, now we're going to load our dataset from this URL here, or ml Carlifornia housing train data into a panda's data frame. Next, we examine the data. So, it's a good idea to get to know your data a little bit before you work with it. We'll print out a quick summary of useful statistics on each column. This will include things like mean, standard deviation, max, min and various quantiles. So first, what we're going to do is print off the head of the data frame. What this does is simply print off an example of the first five rows of the dataset; longitude, latitude, housing median age, total rooms, total bedrooms, population, households, median income and median house value; which is my label in this case. It's what I want to predict using these other features. So, actually lets see what the statistics are. This I can do with df.describe. It will show me the count, the means, standard deviation, the minimum, 25th percentile, the 50th percentile, the 75th percentile and the maximum. As you can see here, everything looks pretty clean here. However, it is on a city block level still. So, we're going to have to figure out how to do that on a per house level. So to do that, I take the number of rooms, if I want to find that, I take the total rooms for the entire city block, and divide with the total number of households in that block. This will give me the average number of rooms per house. Same goes for bedrooms, I take the number of bedrooms, I'm then going to use the total number of bedrooms in the whole block, divide with the number of households in that block to get the average number of bedrooms. Now, for persons per house, I'm going to take the total population of that block and divide by the number of households, same with the average number of people in that house. Now, if I do a df.describe, we'll see my original columns here. However, I have new columns added right here. This are my average number of rooms per house, my average number of bedrooms per house, and my average number of a persons per house. Excellent. Now, I can drop those population statistics now, and say block level statistics; like total rooms, total bedrooms, population, households, and I'm going to drop all those columns in place. So, I don't create a new data frame. And now, if I do df.describe, you'll see I have my new features over here, my old features over there. Here's my label, and those things I used before are no longer there. This is now at a house level view. So now, let's build our Neural Network model that will have our feature data in the correct format. All right. So, what we're going to do is create our feature columns. So remember, feature columns are basically getting our data into the right representation for our model to use. So, even if it is already in floating point notation, we still need to decide if it's going to be a floating point in a column or not. So, it comes in here, and I'm looping as you can see here, over all the columns and median housing age, median income, the number of rooms, number of bedrooms, and the persons per house. After that, I want to do a little more feature engineering. So, I'm going to create a new feature column called Longitude. It's going to be a bucketized column of the numerical longitude, with the spacing of linear space from a negative 124.3 to negative 114.3 in steps of five. Then feature columns latitude, I'm going to have the same sort of thing, except now it's going to be from the latitudes 32.5 to 42 with 10 buckets in this. The reason I'm doing this is because California is longer than it is wider. Therefore, our latitude should have a greater number of buckets; 10 buckets versus the five buckets for longitude. Just printing out my feature column names. Here, I can see, I have median income, persons per house, number of rooms, housing median age, longitude, number of bedrooms and latitude. That's great. But first, we need to make sure we split this into a train and evaluation data set. So, that way I'm able to see how my model is progressing as I'm training. To do this, I'm going to create a random mask, where I'm checking for the length of data frame, I'm going to create that many number of random values, from a uniform distribution, and if they're less than 0.8, I'm going to save it into this mask vector. What's going to happen is, this mask vector is actually length of the data frame, but they're all trues and false, it is called a Boolean mask, when I apply this Boolean mask in my data frame. So therefore, for all things were that mask was true, those rows will be put into trained data frame. And for all values that are not true, that's what this tilde is right here, they will be put into evaluation data frame. Therefore, this will give me pretty much an 80 percent split into my train data frame, and the rest of the 20 percent of my data goes in the evaluation data frame. Here, I also have a scale factor, as you can see, I'm 100,000. The reason for this is because I want to scale my labels here. Because they are way too large. As you can see, there are totally different scales. These are in the 100,000, millions range almost, and these are all much smaller like single one or two digit floats. So, I'm going to do that. I'm also going to create my batch size here, and set that, I'm going to set it to 100. Set it to 100 rows at a time under each one of these data frames. I had to then create my training input function. So for this, I'm going to use the nifty estimator pandas input function right here, where X equals my features. Well, this is going to create a dictionary of tensors, will be the output of that. This will turn my train data frame of median house values of that column. It'll read that into Y, which will then become a tensor for my labels. Number of epochs is going to equal one in this case from a batch size and I am going to shuffle. All right. Over here, I have my eval input function. Once again, it's going to use the pandas input function to do its work. And now, we're going to use all perimeter [inaudible] for the input data frame. However, I'm going to have shuffle, equal false because I don't want to shuffle my evaluations set because I want repeatability. I also create another function here called rmse, which is going to print out the rmse of my model. Calling in the name of it and calling the input function associated. So for this, I'm going to create into the metrics. I'm going to model.evaluate of my estimator. Remember, my estimator is set as model. And I'm going to pass it to as input function, where it's going to be the input function that is passed to our print_rmse, and I'm going to do onestep. The right news about this, is that I'm going to be- this metrics is out, it should be dictionary. It's still a regression problem. So, I'm going to end up with loss, average loss, and a global step. Then I'm going to print the rmse on this data set, and the answer will be, I'm going to have to hit the square root because currently the average loss is just the mse. From the rmse, I've check the square root. Also you might notice that I'm multiplying by the scale here. So, I can get back into the correct units of price, the mean the house value. Now, I'm going to equip my Linear Reggressor. I created an output directory, this is where all my files will be saved from the training, like my checkpoints, my event logs, any saved models for instance. I want remove it, to make sure I have a fresh start each time. So, we're going to remove everything in that tree, make sure it's a clear fresh folder. I'm going to create my custom optimizer. This is Linear Regression. So, I'm going to use a follow the regularized leader optimizer, since that's usually a pretty good choice for that. I'm going to have a learning rate of 0.01, then I'm going to create my model. So here, I'm creating my estimator now. It's going to be a linear aggressor, and I'm passing my model directory. So, I'm going to put my stuff, and then feature columns and I'm going to pass my feature columns values. So, these are the tensors for that. And then my optimizer is going to be my custom optimizer here [inaudible] leader. I'm going to train for a number of steps here. For this, I'm going to train for a hundred times unlike to my data frame over my batch size. Essentially, what this means, I may train for 100 epochs. I then call model.train, passing my input function, specifically my training input function, and my number of steps could be this number of steps I created here. It's going to train my model. Then at the very end, I'm going to print the rmse of that model. I'm going to call my evaluation input function, so that way, it will be on my evaluation input functions set. As you can see, when I perform the training, I have the default config here, and changing that I create a checkpoint and I start the training process. I compute the loss at step one. It looks like, and then that's how many steps per second I've done, and as training proceeds, the loss is hopefully going down. We can see that my evaluation final average loss is 0.93, after 137 global steps, and my total loss is 3,141. And my evaluation, by multiplying back by the scale on my evaluations set, the rmse is $96,583. Remember, rmse is essentially the standard deviation of your residuals. Remember, in your residuals, are the difference between your prediction and the actual label. So now, let's see if we can do any better with my DNNRegressor. Everything is the same as before, except this time I'm using the AdamOptimizer, because that's usually pretty great to use on DNNReggressor's rather than the [inaudible] leader. I'm also going to now change from the Linear Regressor to the DNNReggressor. Where I pass it as before everything else. However, I'm going to add in my hidden units and I'm going to have one, two, three layers here, where the first layer has 100 hidden neurons. The second layer has 50 hidden neurons, and the last layer has 20 hidden neurons. I'm also passing the feature columns, the optimizer I created, which is using Adam this time. Then a drop dropout of 10 percent. Remember, this is the drafted probability and not the key probability like it is in some other insulations. I'm also creating the number of steps the same as before, and I'm training as before, and I printed my rmse. Let's see if it can do any better. All right. So, it does everything the same as before, when my default configuration is at training. Let's see what the final steps are. So, my average loss from my training is 0.67. This is already a good sign because it's lower than what I had before, 0.93. But on my rmse on this, it's $81,974. As you can see here, I have a lot smaller standard deviation compared to the last one, which means this model is doing much better. Of course, you can always make this much more complicated and use way more fancy algorithms, which goes to show you, that a Neural Network can very easily get you much better performance than a Linear Regression can. Lastly, what we can do, we call this in TensorBoard, and we can look at how it's progressing.