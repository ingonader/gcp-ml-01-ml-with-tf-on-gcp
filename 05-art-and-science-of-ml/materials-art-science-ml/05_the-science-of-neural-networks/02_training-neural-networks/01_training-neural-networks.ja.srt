1
00:00:00,000 --> 00:00:03,060
NNを少し深く学んだので

2
00:00:03,060 --> 00:00:04,890
次はトレーニング方法や

3
00:00:04,890 --> 00:00:06,200
よくある落とし穴

4
00:00:06,200 --> 00:00:10,060
トレーニングの高速化と優れた一般化に
役立つ方法を学びます

5
00:00:10,060 --> 00:00:14,970
Estimator APIを使用するTensorFlowでは
DNNRegressorの使用は

6
00:00:14,970 --> 00:00:17,280
LinearRegressorと似ていて

7
00:00:17,280 --> 00:00:19,890
追加すべきコードのパラメータはわずかです

8
00:00:19,890 --> 00:00:24,210
既定のAdagradなど
慣性ベースのオプティマイザを使用できます

9
00:00:24,210 --> 00:00:26,600
Adamなどの他の方法も試せます

10
00:00:26,600 --> 00:00:30,150
また隠れユニットというパラメータも必要です

11
00:00:30,150 --> 00:00:31,425
これがリストです

12
00:00:31,425 --> 00:00:34,710
このリストにある項目の数は隠れ層の数で

13
00:00:34,710 --> 00:00:39,390
各リスト項目の値は特定の隠れ層の
ニューロンの数です

14
00:00:39,390 --> 00:00:42,625
ドロップアウトという新しいパラメータも
あります

15
00:00:42,625 --> 00:00:44,430
これも少し説明しますが

16
00:00:44,430 --> 00:00:47,580
ここでは一般化の性能向上を目指して

17
00:00:47,580 --> 00:00:51,855
各例の個々のニューロンのオンオフに
使用されます

18
00:00:51,855 --> 00:00:54,140
設定できるパラメータの詳細は

19
00:00:54,140 --> 00:00:56,430
関連文書を参照してください

20
00:00:56,430 --> 00:01:00,080
これらはすべてハイパーパラメータ化
できるので

21
00:01:00,080 --> 00:01:02,500
一般化の性能が最適になるように

22
00:01:02,500 --> 00:01:04,435
モデルを調整できます

23
00:01:04,435 --> 00:01:09,345
誤差逆伝搬はML NNコースの
従来のトピックの1つです

24
00:01:09,345 --> 00:01:10,760
しかし一部のレベルでは

25
00:01:10,760 --> 00:01:13,270
コンパイラの構築法の学習と同じで

26
00:01:13,270 --> 00:01:15,759
深く理解するには必須ですが

27
00:01:15,759 --> 00:01:18,415
最初の理解には不要です

28
00:01:18,415 --> 00:01:21,280
導関数の計算に有効なアルゴリズムの存在と

29
00:01:21,280 --> 00:01:25,360
TensorFlowがこれを自動で行うことを
知ってください

30
00:01:25,360 --> 00:01:28,330
ただし興味深い失敗例もあります

31
00:01:28,330 --> 00:01:29,710
勾配消失や

32
00:01:29,710 --> 00:01:32,190
勾配爆発や不感層などです

33
00:01:32,190 --> 00:01:38,109
まず特に深いネットワークでの
トレーニング課程で勾配が消失し

34
00:01:38,109 --> 00:01:43,610
ネットワークの各層で相次いで信号対ノイズが
小さくなります

35
00:01:43,610 --> 00:01:44,890
この例としては

36
00:01:44,890 --> 00:01:49,000
シグモイドまたはTanh活性化関数の
隠れ層全体での使用です

37
00:01:49,000 --> 00:01:50,730
飽和を開始すると

38
00:01:50,730 --> 00:01:54,715
関数の漸近領域が水平になり始め

39
00:01:54,715 --> 00:01:58,000
傾斜がほぼ0に近づいていきます

40
00:01:58,000 --> 00:02:00,930
誤差逆伝搬でネットワークを後退するとき

41
00:02:00,930 --> 00:02:04,150
勾配が小さくなります
勾配が完全に消失するまで

42
00:02:04,150 --> 00:02:07,800
小さい勾配をすべて組み合わせるためです

43
00:02:07,800 --> 00:02:10,854
これが起こると重みは更新されなくなり

44
00:02:10,854 --> 00:02:13,900
そのためトレーニングは急停止します

45
00:02:13,900 --> 00:02:17,065
簡単な修正方法はReLUや ELUなどの

46
00:02:17,065 --> 00:02:21,900
飽和しない非線形活性化関数を使用することです

47
00:02:21,900 --> 00:02:26,710
次に勾配爆発という逆の問題が
生じることもあります

48
00:02:26,710 --> 00:02:31,315
重みが大きくなってオーバーフローするまで
勾配が大きくなるものです

49
00:02:31,315 --> 00:02:34,295
比較的小さな勾配で始まっても

50
00:02:34,295 --> 00:02:36,025
たとえば値が2でも

51
00:02:36,025 --> 00:02:39,230
多くの層で組み合わさると非常に大きくなります

52
00:02:39,230 --> 00:02:43,280
これは特に系列長が長い系列モデルに
当てはまります

53
00:02:43,280 --> 00:02:46,400
学習率も重要です
重みの更新では

54
00:02:46,400 --> 00:02:48,965
勾配と学習率を掛けて

55
00:02:48,965 --> 00:02:51,830
その積を現在の重みから引くためです

56
00:02:51,830 --> 00:02:55,700
そのため勾配が大きくなくても
学習率が1より大きいと

57
00:02:55,700 --> 00:03:00,725
重みが大きすぎてネットワークに問題が生じる
可能性があります

58
00:03:00,725 --> 00:03:04,010
これを最小限に抑える方法は多数あります

59
00:03:04,010 --> 00:03:06,905
重みの整理やバッチサイズの縮小などです

60
00:03:06,905 --> 00:03:09,380
グレーディングとクリッピングもあります

61
00:03:09,380 --> 00:03:12,694
この方法では標準勾配が閾値を超えた場合

62
00:03:12,694 --> 00:03:15,855
ハイパーパラメータや調整を行えますが

63
00:03:15,855 --> 00:03:19,965
その場合は勾配要素を変更して
最大値より小さくできます

64
00:03:19,965 --> 00:03:21,910
他の便利な方法は

65
00:03:21,910 --> 00:03:26,105
Internal Covariance Shift問題を
解決するバッチ正規化です

66
00:03:26,105 --> 00:03:28,990
これは勾配を改善するトレーニングの一部です

67
00:03:28,990 --> 00:03:33,295
また高い学習率を使用してドロップアウトを
排除できます

68
00:03:33,295 --> 00:03:37,960
これはミニバッチのノイズにより
競合を独自の正則化へと低速化させます

69
00:03:37,960 --> 00:03:39,975
バッチ正規化を実施するには

70
00:03:39,975 --> 00:03:42,035
まずミニバッチの平均を見つけ

71
00:03:42,035 --> 00:03:44,455
ミニバッチの標準偏差を探します

72
00:03:44,455 --> 00:03:46,840
次に入力をノードに対して正規化し

73
00:03:46,840 --> 00:03:52,570
γ x X + βで段階的にシフトします

74
00:03:52,570 --> 00:03:55,360
ここでγとβは学習済みパラメータです

75
00:03:55,360 --> 00:03:59,290
γがXの分散の平方根でβがXの平均の場合

76
00:03:59,290 --> 00:04:01,810
元の活性化関数が復元されます

77
00:04:01,810 --> 00:04:06,145
このように入力の範囲を制御できるので
大きくなりすぎません

78
00:04:06,145 --> 00:04:10,090
勾配はできるだけ1に近づけることが理想です

79
00:04:10,090 --> 00:04:12,100
ネットが深い場合は特にそうです

80
00:04:12,100 --> 00:04:15,910
結合によるオーバーフローやアンダーフローを
避けます

81
00:04:15,910 --> 00:04:20,750
勾配降下のよくある失敗に
実層の不感化があります

82
00:04:20,750 --> 00:04:23,160
幸いにもTensorBoardを使うと

83
00:04:23,160 --> 00:04:28,000
NNモデルのトレーニング中とトレーニング後の
太陽光線をモニタリングできます

84
00:04:28,000 --> 00:04:33,085
キャンディやEstimatorの使用が自動的に

85
00:04:33,085 --> 00:04:35,470
各GNの隠れ層のスカラサマリになると

86
00:04:35,470 --> 00:04:38,410
その層の活性化のゼロ値の割合を示します

87
00:04:38,410 --> 00:04:41,530
入力がずっと負領域のままで

88
00:04:41,530 --> 00:04:44,870
活性化がゼロ値になる場合
ReLUは動作を停止します

89
00:04:44,870 --> 00:04:49,149
そこで終わりません
次の層の貢献がゼロのだからです

90
00:04:49,149 --> 00:04:52,140
重みが次のニューロンに結合しているのに

91
00:04:52,140 --> 00:04:55,340
その活性化がゼロで
入力がゼロになるためです

92
00:04:55,340 --> 00:04:57,870
大量のゼロが次のニューロンに入り

93
00:04:57,870 --> 00:05:00,060
当然それが正領域に入り込み

94
00:05:00,060 --> 00:05:04,960
これらのニューロンの活性化もゼロになり
この問題は次々と続きます

95
00:05:04,960 --> 00:05:08,495
そこで誤差逆伝搬を行うと勾配はゼロになり

96
00:05:08,495 --> 00:05:12,460
重みがなくなるためトレーニングは停止します
失敗です

97
00:05:12,460 --> 00:05:17,265
Leaky/Parametric ReLUなどの使用を
説明しましたが

98
00:05:17,265 --> 00:05:19,415
学習率を下げて

99
00:05:19,415 --> 00:05:22,375
ReLU層の不活性化を阻止できます

100
00:05:22,375 --> 00:05:26,860
大きな勾配は学習率が高すぎることが原因で

101
00:05:26,860 --> 00:05:31,735
重みが更新されても
データポイントが再度活性化されません

102
00:05:31,735 --> 00:05:33,639
勾配がゼロなので

103
00:05:33,639 --> 00:05:35,450
合理的数値に重みを更新できず

104
00:05:35,450 --> 00:05:38,980
問題はいつまでも続きます

105
00:05:38,980 --> 00:05:41,290
簡単な直観チェックを行いましょう

106
00:05:41,290 --> 00:05:42,610
有用な信号が2つあり

107
00:05:42,610 --> 00:05:45,325
どちらも個々にラベルと相関性があるものの

108
00:05:45,325 --> 00:05:49,210
尺度が異なる場合
モデルはどうなるでしょうか？

109
00:05:49,210 --> 00:05:50,250
たとえば

110
00:05:50,250 --> 00:05:56,285
スープのおいしさの予測因子があり
各特徴が材料投入の質を示しているとします

111
00:05:56,285 --> 00:05:59,260
チキンストックの特徴はリットルで測定され

112
00:05:59,260 --> 00:06:02,160
ビーフはミリリットルで測定されている場合

113
00:06:02,160 --> 00:06:05,955
直感をグレーディングする確率変数が
うまく収束しません

114
00:06:05,955 --> 00:06:10,240
この2次元の最適な学習率が異なるためです

115
00:06:10,240 --> 00:06:13,940
データをクリアにして
計算に役立つ範囲に入れることには

116
00:06:13,940 --> 00:06:17,820
機械学習モデルのトレーニング過程において
多数の利点があります

117
00:06:17,820 --> 00:06:21,375
特徴値を小さくしてゼロ中心にすると

118
00:06:21,375 --> 00:06:24,185
トレーニング速度が上がり
数値問題を回避できます

119
00:06:24,185 --> 00:06:27,935
こうした理由でバッチ正規化は勾配爆発に
役立ちました

120
00:06:27,935 --> 00:06:31,910
最初の入力特徴だけでなく

121
00:06:31,910 --> 00:06:34,490
すべての中間特徴も健全範囲内に収め

122
00:06:34,490 --> 00:06:37,950
各層に問題が起こらないようにしました

123
00:06:37,950 --> 00:06:41,280
これでNaNトラップも回避できます

124
00:06:41,280 --> 00:06:44,790
これでは値が数値精度範囲を超えると
モデルが爆発します

125
00:06:44,790 --> 00:06:48,210
特徴スケーリングと学習率低下の組み合わせで

126
00:06:48,210 --> 00:06:50,685
この厄介な落とし穴を回避できます

127
00:06:50,685 --> 00:06:55,050
また異常値の除外も一般化に役立ちます

128
00:06:55,050 --> 00:06:58,130
異常検出などで異常値を検出し

129
00:06:58,130 --> 00:07:02,365
トレーニング前にデータセットから前処理で
除外できれば非常に役立ちます

130
00:07:02,365 --> 00:07:06,950
どんなデータにも使える汎用的な方法は
ありません

131
00:07:06,950 --> 00:07:11,045
各方法について適切な事例と不適切な事例を
考えてみましょう

132
00:07:11,045 --> 00:07:14,850
特徴値のスケールを小さくする方法は
多数あります

133
00:07:14,850 --> 00:07:20,420
線形スケーリングでは
まずデータの最小値と最大値を見つけます

134
00:07:20,420 --> 00:07:21,910
次に各値について

135
00:07:21,910 --> 00:07:23,130
最小値を減算し

136
00:07:23,130 --> 00:07:26,855
最大値と最小値の差
つまり範囲で除算します

137
00:07:26,855 --> 00:07:29,510
これですべての値が0と1の間になります

138
00:07:29,510 --> 00:07:31,820
0が最小値で1が最大値です

139
00:07:31,820 --> 00:07:34,695
これは正規化とも呼ばれます

140
00:07:34,695 --> 00:07:37,845
ハードキャッピング（クリッピング）もあります

141
00:07:37,845 --> 00:07:40,575
ここでは最小値と最大値を設定します

142
00:07:40,575 --> 00:07:45,330
たとえば最小値を-7と設定し

143
00:07:45,330 --> 00:07:47,540
最大値を10と設定した場合

144
00:07:47,540 --> 00:07:50,575
-7未満の値はすべて-7になり

145
00:07:50,575 --> 00:07:53,430
10を超える値はすべて10になります

146
00:07:53,430 --> 00:07:58,730
ログスケーリングという方法では
対数関数を入力データに適用します

147
00:07:58,730 --> 00:08:01,600
これが役立つのは
データの範囲が広いため

148
00:08:01,600 --> 00:08:05,140
これを凝縮させて
値の大きさだけにしたいときです

149
00:08:05,140 --> 00:08:10,625
先ほど紹介したバッチ正規化のもう1つの方法は
標準化です

150
00:08:10,625 --> 00:08:14,120
ここでデータの平均と標準偏差を計算します

151
00:08:14,120 --> 00:08:15,750
この2つの値がわかったら

152
00:08:15,750 --> 00:08:19,245
平均を各データポイントから引き
標準偏差で割ります

153
00:08:19,245 --> 00:08:21,740
これで新しい平均が0になり

154
00:08:21,740 --> 00:08:25,910
新しい標準偏差が1になるため
データはゼロ中心になります

155
00:08:25,910 --> 00:08:29,335
他にもデータのスケーリング方法は多数あります

156
00:08:29,335 --> 00:08:33,924
モデルに勾配爆発が生じた場合の推奨事項は？

157
00:08:33,924 --> 00:08:37,280
正解はA、B、C、Dです。

158
00:08:37,280 --> 00:08:41,245
問題が生じるのはたいてい
重みが大きすぎるときで

159
00:08:41,245 --> 00:08:44,015
これが生じるのは学習率が高すぎるときです

160
00:08:44,015 --> 00:08:46,610
これは数値安定化や発散など

161
00:08:46,610 --> 00:08:50,225
他にもさまざまな問題を引き起こします

162
00:08:50,225 --> 00:08:56,290
そのため学習率を下げて適切なゴルディロック
ゾーンを見つけることが必要です

163
00:08:56,290 --> 00:08:59,590
この点において重みの正則化も役立ちます

164
00:08:59,590 --> 00:09:02,170
非常に大きな重みにはペナルティを科し

165
00:09:02,170 --> 00:09:04,660
勾配爆発を起こりにくくします

166
00:09:04,660 --> 00:09:07,690
また勾配クリッピングを適用しても

167
00:09:07,690 --> 00:09:10,770
勾配が特定の閾値を超えないようにできます

168
00:09:10,770 --> 00:09:14,595
学習率の上昇をある程度緩和できます

169
00:09:14,595 --> 00:09:16,390
しかし高率のままだと

170
00:09:16,390 --> 00:09:19,070
重みが非常に高くなる可能性があります

171
00:09:19,070 --> 00:09:21,140
バッチ正規化は

172
00:09:21,140 --> 00:09:24,975
各層の中間入力が狭い範囲に留まるため

173
00:09:24,975 --> 00:09:28,420
重みが範囲外まで大きくなる確率が大幅に減り

174
00:09:28,420 --> 00:09:30,910
計算コストも小さくなります

175
00:09:30,910 --> 00:09:33,385
勾配爆発の処理方法は多数あるので

176
00:09:33,385 --> 00:09:35,210
医者の助けは不要です

177
00:09:35,210 --> 00:09:38,860
こうしたツールを試して最適な方法を
探せばいいのです

178
00:09:38,860 --> 00:09:43,245
より一般化されたモデルの構築に役立つ
他の正則化には

179
00:09:43,245 --> 00:09:45,770
NNにドロップアウト層を追加できます

180
00:09:45,770 --> 00:09:49,800
ドロップアウトを使うには層にラッパーを
追加します

181
00:09:49,800 --> 00:09:53,390
TensorFlowでは渡すパラメータが
ドロップアウトで

182
00:09:53,390 --> 00:09:55,410
ネットワークからニューロンを

183
00:09:55,410 --> 00:09:58,860
一時的にドロップアウトさせる確率です

184
00:09:58,860 --> 00:10:01,575
この数値の設定には注意が必要です

185
00:10:01,575 --> 00:10:04,110
ドロップアウトのある他の関数では

186
00:10:04,110 --> 00:10:06,295
キープ確率を使用します

187
00:10:06,295 --> 00:10:08,750
これはドロップ確率を補うもの

188
00:10:08,750 --> 00:10:11,130
ニューロンのオンオフの確率です

189
00:10:11,130 --> 00:10:14,530
確率を10%だけ落とすつもりが

190
00:10:14,530 --> 00:10:17,640
ノードの10%のみをランダムにキープすると

191
00:10:17,640 --> 00:10:20,485
意図的でないスパースモデルになります

192
00:10:20,485 --> 00:10:23,035
ドロップアウトの内部での仕組みは？

193
00:10:23,035 --> 00:10:26,045
ドロップアウト確率を20%に設定した場合

194
00:10:26,045 --> 00:10:28,775
ネットワークに前方パスされたものでは

195
00:10:28,775 --> 00:10:32,340
ニューロンとドロップアウト
ラップ層がサイコロで決まります

196
00:10:32,340 --> 00:10:36,660
サイコロの目が20より大きく
そのニューロンがアクティブの場合

197
00:10:36,660 --> 00:10:38,920
脱落し

198
00:10:38,920 --> 00:10:41,920
入力に関係なくゼロ値が出力されるため

199
00:10:41,920 --> 00:10:45,305
事実上ネットワークには何も追加されません

200
00:10:45,305 --> 00:10:49,730
ゼロを追加しても何も変わらず
ニューロンへの刺激は存在しません

201
00:10:49,730 --> 00:10:54,145
各ノードがわずかな時間しかキープされない
事実の埋め合わせに

202
00:10:54,145 --> 00:10:56,685
活性化のスケーリングを

203
00:10:56,685 --> 00:10:59,440
1÷ (1-ドロップアウト確率)

204
00:10:59,440 --> 00:11:02,070
つまり
（1÷トレーニング中のキープ確率）で行うと

205
00:11:02,070 --> 00:11:05,790
それが活性化の期待値になります

206
00:11:05,790 --> 00:11:08,900
トレーニングを行わず 
コードの変更も不要な場合は

207
00:11:08,900 --> 00:11:10,920
ラッパーは事実上消え

208
00:11:10,920 --> 00:11:14,505
正式なドロップアウトラッパー層の
ニューロンは常にオンで

209
00:11:14,505 --> 00:11:16,685
そのモデルがトレーニングした重みを使います

210
00:11:16,685 --> 00:11:21,580
ドロップアウトの考えでは
基本的にアンサンブルモデルが構築されます

211
00:11:21,580 --> 00:11:24,530
前方パスでは事実上

212
00:11:24,530 --> 00:11:27,990
データミニバッチは異なるネットワークを
参照します

213
00:11:27,990 --> 00:11:30,740
これらをすべて予測に追加すると

214
00:11:30,740 --> 00:11:33,690
2～nのNNをトレーニングすることになります

215
00:11:33,690 --> 00:11:36,005
nはドロップアウトニューロン数です

216
00:11:36,005 --> 00:11:38,735
アンサンブルの作動は多数の
ディシジョンツリーが

217
00:11:38,735 --> 00:11:41,805
ランダムフォレストで協力するのと似ています

218
00:11:41,805 --> 00:11:44,050
また信号の大部分をネットワークの

219
00:11:44,050 --> 00:11:46,440
1本のブランチに向かわせるのではなく

220
00:11:46,440 --> 00:11:48,030
ネットワーク全体の

221
00:11:48,030 --> 00:11:50,870
データ分布に広がる相加的効果もあります

222
00:11:50,870 --> 00:11:54,850
これは複数の分岐やダムを使って川の水を
迂回させ

223
00:11:54,850 --> 00:11:59,190
全水路に水が行き渡って
干上がらないようにするのと同じです

224
00:11:59,190 --> 00:12:02,440
この方法は信号がネットワーク全体に
均等に流れ

225
00:12:02,440 --> 00:12:06,140
ネットワークはその容量の多くを使用します

226
00:12:06,140 --> 00:12:08,615
そのためトレーニングと一般化を適切に行え

227
00:12:08,615 --> 00:12:12,105
ニューロンも人気のパスに大きく依存しません

228
00:12:12,105 --> 00:12:15,900
ドロップアウトの標準値は20～50%です

229
00:12:15,900 --> 00:12:17,555
これを大幅に低くすると

230
00:12:17,555 --> 00:12:21,205
ノードのドロップアウトが滅多に起こらず
効果が低下し

231
00:12:21,205 --> 00:12:24,160
高すぎてもトレーニングは行われません

232
00:12:24,160 --> 00:12:26,545
ネットワークがスパース化しすぎて

233
00:12:26,545 --> 00:12:28,200
分布がなく学習できません

234
00:12:28,200 --> 00:12:31,400
これは大きなネットワークでも使用できます

235
00:12:31,400 --> 00:12:35,045
モデルが独立した表現を学習できる
容量があります

236
00:12:35,045 --> 00:12:38,310
つまりネットワークが試せるパスが多くあります

237
00:12:38,310 --> 00:12:39,980
ドロップアウトを多くして

238
00:12:39,980 --> 00:12:41,440
キープを少なくすると

239
00:12:41,440 --> 00:12:43,290
正則化が強化されます

240
00:12:43,290 --> 00:12:46,940
ドロップアウト確率を1に設定すると
キープは0になり

241
00:12:46,940 --> 00:12:49,340
ラップドロップアウト層の全ニューロンが

242
00:12:49,340 --> 00:12:50,740
ニューロンから除外され

243
00:12:50,740 --> 00:12:52,600
ゼロ活性化が出力されます

244
00:12:52,600 --> 00:12:54,760
誤差逆伝搬では

245
00:12:54,760 --> 00:12:58,015
重みは更新されず
この層は何も学習しません

246
00:12:58,015 --> 00:13:00,035
この確率を0に設定すると

247
00:13:00,035 --> 00:13:03,455
全ニューロンが活性化し
ドロップアウト正則化はありません

248
00:13:03,455 --> 00:13:06,460
ドロップアウトラッパーのないこの方法では

249
00:13:06,460 --> 00:13:09,750
サイコロが必要になるため
計算コストが高くなります

250
00:13:09,750 --> 00:13:13,505
もちろん0と1の中間が理想的な位置です

251
00:13:13,505 --> 00:13:17,065
ドロップアウト確率は10～50%が理想的で

252
00:13:17,065 --> 00:13:20,635
適切なベースラインは20%から始まり
必要なら追加します

253
00:13:20,635 --> 00:13:22,875
全モデルやデータ分布に使える

254
00:13:22,875 --> 00:13:25,785
汎用的なドロップアウト確率はありません

255
00:13:25,785 --> 00:13:28,300
ドロップアウトは＿＿として機能します

256
00:13:28,300 --> 00:13:33,155
データがより均等に広がるように
データを＿＿パスに流します

257
00:13:33,155 --> 00:13:35,650
また＿＿学習をシミュレートします

258
00:13:35,650 --> 00:13:39,290
＿＿の逆数でドロップアウトの活性化を
スケーリングします

259
00:13:39,290 --> 00:13:41,830
＿＿中はドロップアウトを除外します

260
00:13:41,830 --> 00:13:45,355
正解はEです
ドロップアウトは

261
00:13:45,355 --> 00:13:48,765
正則化として機能するため
モデルの一般化が向上します

262
00:13:48,765 --> 00:13:52,160
これを行うには
ドロップアウト確率でノードをオフにし

263
00:13:52,160 --> 00:13:56,545
データがより均等に広がるように
データを複数パスに流します

264
00:13:56,545 --> 00:13:58,605
そうしないとデータと活性化は

265
00:13:58,605 --> 00:14:01,425
優先的なパスを取るようになり

266
00:14:01,425 --> 00:14:03,595
ネットワークのトレーニング不足につながり

267
00:14:03,595 --> 00:14:06,775
新しいデータの性能が低下します

268
00:14:06,775 --> 00:14:11,305
ドロップアウトは2～nのモデルの集約を
作成して

269
00:14:11,305 --> 00:14:15,760
アンサンブル学習をシミュレートし
前方パスのノードを無作為にオフします

270
00:14:15,760 --> 00:14:17,795
nはドロップアウトノード数です

271
00:14:17,795 --> 00:14:20,010
バッチは別のネットワークを見るので

272
00:14:20,010 --> 00:14:24,120
モデルはトレーニングセット全体で
過学習になりません

273
00:14:24,120 --> 00:14:28,105
キープ確率の逆数でドロップアウトの
活性化をスケーリングします

274
00:14:28,105 --> 00:14:30,365
キープ確率=1-ドロップアウト確率

275
00:14:30,365 --> 00:14:34,195
これはノードの予測で行え
トレーニング中にスケーリングされます

276
00:14:34,195 --> 00:14:36,340
推測の場合は常にオンになっており

277
00:14:36,340 --> 00:14:39,020
推測中はドロップアウトを除外するためです