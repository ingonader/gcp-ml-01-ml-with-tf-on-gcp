Now that you have dived a little deeper into neural networks. Let's sort of how we can train them, some common pitfalls and something techniques to help speed up train and provide better journalism. In TensorFlow using the Estimator API, using a DNNRegressor, is very similar to using a LinearRegressor, with only a few parameters for the code that need to be added. We can use momentum based optimizers such as the default Adagrad, or we can try many others such as Adam. Also we now have to add a parameter named hidden units, which is a list. The number of items in this list is the number of hidden layers and the values of each list item is a number of neurons for that particular hidden layer. You will also know there is a new parameter named dropout. We'll cover this and more in a few minutes. But for now this is used to turn individual neurons on and off for each example in hopes of having better generalization performance. Please look at the tentacle documentation for the complete set of parameters you can configure. These are all things that could be hyperparameterized so that you can tune your model to have the best generalization performance. Back propagation is one of the traditional topics in an ML Neural Networks course. But at some level, it's kind of like teaching people how to build a compiler. It's essential for deeper understanding but not necessarily needed for initial understanding. The main thing to know is that there is an efficient algorithm for calculating derivatives and TensorFlow will do it for you automatically. There are some interesting failure cases to talk about though, such as vanishing gradients, exploding gradients and dead layers. First, during the training process especially for deep networks gradients can vanish, each additional layer in your network can successively reduce signal vs noise. An example of this is when using sigmoid or tanh activation functions throughout your hidden layers. As you begin to saturate you end up in the asymptotic regions of the function which begin to plateau, the slope is getting closer and closer to approximately zero. When you go backwards through the network during back prop, your gradient can become smaller and smaller because you're compounding all these small gradients until the gradient completely vanishes. When this happens your weights are no longer updating and therefore training grinds to a halt. A simple way to fix this is to use non saturating non-linear activation functions such as ReLUs, ELUs, et cetera. Next, we can also have the opposite problem where gradients explode, by getting bigger and bigger until our weights gets so large we overflow. Even starting with relatively small gradients, such as a value of two, can compound and become quite large over many layers. This is especially true for sequence models with long sequence lengths, learning rates can be a factor here because in our weight updates, remember we multiplied the gradient with the learning rate and then subtract that from the current weight. So, even if the grading isn't that big with a learning rate greater than one it can now become too big and cause problems for us and our network. There are many techniques to try and minimize this. Such as weight organization and smaller batch sizes. Another technique is grading and clipping, where we check to see if the normal the gradient exceeds some threshold, which you can hyperparameter or tune and if so, then you can re-scale the gradient components to fit below your maximum. Another useful technique is batch normalization which solves the problem called internal co-variance shift. It's piece of training because gradients flow better. It also can often use a higher learning rate and might be able to get rid of dropout which slows competition down to its own kind of regularization due to mini batch noise. To perform batch normalization, first find the mini batch mean, then the mini batch's standard deviation, then normalize the inputs to that node, then scale and shift by gamma times X plus beta, where gamma and beta are learned parameters. If gamma equals the square root variance of X and beta equals the mean of X, the original activation function is restored. This way, you can control the range of your inputs so that they don't become too large. Ideally, you would like to keep your gradients as close to one as possible especially for very deep nets. So you don't compound and eventually underflow or overflow. Another common failure mode of grading descent is that real layers can die. Fortunately, using TensorBoard we can monitor the sun rays during and after training of our Neural Network models. If using a candy and an estimator is automatically a scalar summary said for each GN hidden layer showing the fraction of zero values of the activations for that layer. ReLUs stop working when their inputs keep them in the negative domain giving their activation a value of zero. It doesn't end there because then their contribution in the next layer is zero, because despite what the weights are connecting it to the next neurons it's activation is zero thus the input becomes zero. A bunch of zeros come into the next neuron doesn't help it get into the positive domain and then these neurons activations become zero too and the problem continues to cascade. Then we perform back prop and their gradients are zero, so we don't have the weights and thus training halts. Not good. We've talked about using Leaky or parametric ReLUs or even the slower ELUs, but you can also lower your learning rates to help stop ReLu layers from not activating and not staying. A large gradient possibly due to too high of a learning rate can update the weights in such a way that no data point will ever activate it again. Since the gradient is zero, we won't update the weight to something more reasonable so the problem will persist indefinitely. Let's have a quick intuition check, what will happen to our model, if we have two useful signals both independently correlated with the label but there are at different scales? For example, we might have a soup deliciousness predictor where features represent qualities of giving ingredients. If the feature for chicken stock is measured in liters, but beef stock is measured in milliliters then stochastic grading the scent might have a hard time converging well. Since the optimal learning rate for these two dimensions is likely different. Having your data clean and in a computationally helpful range has many benefits during the training process of your machine learning models. Having feature value small and specifically zero centered helps speed up training and avoids numerical issues. This is why batch normalization was helpful with exploding gradients because it made sure to keep not just the initial input features, but all of the intermediate features within a healthy range as not to cause problems with our layers. This also helps us avoid the NaN trap, where our model can blow up if values exceed numerical precision range. A combination of features scaling and/or lower learning rate can help us avoid this nasty pitfall. Also, avoiding outlier values helps with generalization. So detecting these perhaps the anomaly detection and pre-processing them out of the data set before training can be a great help. Remember that there is no one best one size fits all method for all data. It is possible to think of good and bad cases for each of these approaches. There are many methods to make our future value scale to small numbers. First, there is linear scaling where you first find the minimum and maximum of the data. Then for each value, we subtract the minimum and then divide by the difference of the maximum and minimum or range. This will make all values between zero and one, where zero will be the minimum and one will be the maximum. This is also called normalization. There is also hard caping or clipping, where you set a minimum value and a maximum value. For instance, if my minimum value is allowed to be negative seven and my maximum value is 10, then all values less than negative seven will become negative seven, and all values greater than 10 will become 10. Log scaling is another method where you apply the logarithm function to your input data. This is great when your data has huge range and you want to condense it down to be more about just the magnitude of the value. Another method which we just talked about with batch normalization is standardization. Here, you calculate the mean of your data and the standard deviation. Once you have these two values, you subtract the mean from every data point and then divide with the standard deviation. This way, your data becomes zero centered because your new mean become zero and your new standard deviation becomes one. Of course, there are many other ways to scale your data. Which of these is good advice if my model is experiencing exploding gradients? The correct answer is A, B, C and D. The problem often occurs when weights get too large, which can happen when our learning rate gets too high. This can lead to a whole bunch of other issues like numerical stability, divergence and [inaudible]. Therefore, lowering the learning rate to find that nice Goldilocks zone is a great idea. Weight authorization can also help in this respect because there will be a penalty for very large weights, which should make it harder for gradients to explode. Also, applying gradient clipping can ensure that gradients never get beyond a certain threshold that we set. This can help mitigate somewhat a higher learning rate. However, with a high enough rate, it can still drive the weights to very high values. Batch normalization can help the intermediate inputs at each layer stay within a tighter range, so there will be a much reduced chance of weights growing out of range for a small extra computational cost. There are many methods to treat exploding gradients, so you don't need a doctor to help. All you have to do is experiment with these tools and see what works best. Another form of regularization that helps build more generalizable models is adding dropout layers to our neural networks. To use dropout, I add a wrapper to one or more of my layers. Intenser flow, the parameter you pass is called dropout, which is the probability of dropping a neuron temporarily from the network rather than keeping it turned on. You want to be careful when setting this number because for some other functions that have a dropout mechanism, they use keep probability, which is a complement to drop probability or the probability of keeping a neuron on or off. You wouldn't want to intend only a 10 percent probability to drop, but actually are now only keeping 10 percent in your nodes randomly; that's a very unintentional sparse model. So, how does dropout work under the hood? Let's say we set a dropout probability of 20 percent. This means that for each forward parsed to the network, the algorithm will roll the dice for each neuron and the dropout wrapped layer. If the dice roll is greater than 20 and the neuron will stay active in the network, [inaudible] roll will be dropped, and output a value of zero regardless of its inputs effectively not adding negatively or positively to the network. Since adding zero changes nothing and simulates to the neuron doesn't exist. To make up for the fact that each node is only kept some percentage of the time, the activations are scaled by one over one minus the dropout probability or in other words, one over the keep probability during training so that it is the expectation value of the activation. When not doing training without having to change any code, the wrapper effectively disappears and the neurons in the formally dropout wrapper layer are always on and use whatever weights were trained by the model. The awesome idea of dropout is that it is essentially creating an ensemble model, because for each forward pass there is effectively a different network that the mini batch of data is seen. When all this is added together in expectation, it is like I would train two to the n neural networks, where n is the number of dropout neurons, and have them working in an ensemble similar to a bunch of decision trees working together in a random forest. There is also the added effect of spreading out the data distribution of the entire network, rather than having the majority of the signal favor going along one branch of the network. I usually imagine this as diverting water in a river or stream with multiple shunts or dams to ensure all waterways eventually get some water and don't dry up. This way, your network uses more of its capacity since the signal more evenly flows across the entire network and thus, you'll have better training and generalization without large neuron dependencies being developed in popular paths. Typical values for dropout are between 20 to 50 percent. If you go much lower than that, there is not much effect from the network since you are rarely dropping any nodes. If you go higher, then training doesn't happen as well since the network becomes too sparse to have the capacity to learn without distribution. You also want to use this on larger networks because there is more capacity for the model to learn independent representations. In other words, there are more possible pass for the network to try. The more you drop out, therefore the less you keep, the stronger the regularization. If you set your dropout probability to one, then you keep nothing and every neuron in the wrapped dropout layer is effectively removed from the neuron, and outputs a zero activation. During backprop, this means that the weights will not update and this layer will learn nothing. If you set your probably to zero, then all neurons are kept active and there is no dropout regularization. It's pretty much just a more computationally costly way to not have a dropout wrapper at all because you still have to roll the dice. Of course, somewhere between zero and one is where you want to be. Specifically with dropout probabilities between 10 to 50 percent, where a good baseline is usually starting at 20 percent and then adding more is needed. There is no one-size-fits-all dropout probability for all models and all data distributions. Dropout acts as another form of blank. It forces data to flow down blank paths so that there is a more even spread. It also simulates blank learning. Don't forget to scale the dropout activations by the inverse of the blank. We remove dropout during blank. The correct answer is E. Dropout act is another form of regularization so the model can generalize better. It does this turning off nodes with a dropout probability, which forces data to flow down multiple paths so that there is a more even spread. Otherwise, data and the activations associated with it can learn to take preferential paths, which might lead to under training of the network as a whole and provide poor performance on new data. Dropout also simulates ensemble learning by creating an aggregate of two to the n models due to the random turning off of nodes for each forward pass, where n is the number of dropout nodes. Each batch sees a different network, so the model can't overfit on the entire training set much like a random forest. Don't forget to scale the dropout activations by the inverse of the keep probability, which is one minus the dropout probability. We do this with the expectation on the node will be scaled correctly during training, since for inference, it will always be on; since we remove dropout during inference.