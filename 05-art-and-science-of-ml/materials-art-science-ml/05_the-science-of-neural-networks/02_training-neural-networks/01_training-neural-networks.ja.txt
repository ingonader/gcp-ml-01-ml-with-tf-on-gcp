NNを少し深く学んだので 次はトレーニング方法や よくある落とし穴 トレーニングの高速化と優れた一般化に
役立つ方法を学びます Estimator APIを使用するTensorFlowでは
DNNRegressorの使用は LinearRegressorと似ていて 追加すべきコードのパラメータはわずかです 既定のAdagradなど
慣性ベースのオプティマイザを使用できます Adamなどの他の方法も試せます また隠れユニットというパラメータも必要です これがリストです このリストにある項目の数は隠れ層の数で 各リスト項目の値は特定の隠れ層の
ニューロンの数です ドロップアウトという新しいパラメータも
あります これも少し説明しますが ここでは一般化の性能向上を目指して 各例の個々のニューロンのオンオフに
使用されます 設定できるパラメータの詳細は 関連文書を参照してください これらはすべてハイパーパラメータ化
できるので 一般化の性能が最適になるように モデルを調整できます 誤差逆伝搬はML NNコースの
従来のトピックの1つです しかし一部のレベルでは コンパイラの構築法の学習と同じで 深く理解するには必須ですが 最初の理解には不要です 導関数の計算に有効なアルゴリズムの存在と TensorFlowがこれを自動で行うことを
知ってください ただし興味深い失敗例もあります 勾配消失や 勾配爆発や不感層などです まず特に深いネットワークでの
トレーニング課程で勾配が消失し ネットワークの各層で相次いで信号対ノイズが
小さくなります この例としては シグモイドまたはTanh活性化関数の
隠れ層全体での使用です 飽和を開始すると 関数の漸近領域が水平になり始め 傾斜がほぼ0に近づいていきます 誤差逆伝搬でネットワークを後退するとき 勾配が小さくなります
勾配が完全に消失するまで 小さい勾配をすべて組み合わせるためです これが起こると重みは更新されなくなり そのためトレーニングは急停止します 簡単な修正方法はReLUや ELUなどの 飽和しない非線形活性化関数を使用することです 次に勾配爆発という逆の問題が
生じることもあります 重みが大きくなってオーバーフローするまで
勾配が大きくなるものです 比較的小さな勾配で始まっても たとえば値が2でも 多くの層で組み合わさると非常に大きくなります これは特に系列長が長い系列モデルに
当てはまります 学習率も重要です
重みの更新では 勾配と学習率を掛けて その積を現在の重みから引くためです そのため勾配が大きくなくても
学習率が1より大きいと 重みが大きすぎてネットワークに問題が生じる
可能性があります これを最小限に抑える方法は多数あります 重みの整理やバッチサイズの縮小などです グレーディングとクリッピングもあります この方法では標準勾配が閾値を超えた場合 ハイパーパラメータや調整を行えますが その場合は勾配要素を変更して
最大値より小さくできます 他の便利な方法は Internal Covariance Shift問題を
解決するバッチ正規化です これは勾配を改善するトレーニングの一部です また高い学習率を使用してドロップアウトを
排除できます これはミニバッチのノイズにより
競合を独自の正則化へと低速化させます バッチ正規化を実施するには まずミニバッチの平均を見つけ ミニバッチの標準偏差を探します 次に入力をノードに対して正規化し γ x X + βで段階的にシフトします ここでγとβは学習済みパラメータです γがXの分散の平方根でβがXの平均の場合 元の活性化関数が復元されます このように入力の範囲を制御できるので
大きくなりすぎません 勾配はできるだけ1に近づけることが理想です ネットが深い場合は特にそうです 結合によるオーバーフローやアンダーフローを
避けます 勾配降下のよくある失敗に
実層の不感化があります 幸いにもTensorBoardを使うと NNモデルのトレーニング中とトレーニング後の
太陽光線をモニタリングできます キャンディやEstimatorの使用が自動的に 各GNの隠れ層のスカラサマリになると その層の活性化のゼロ値の割合を示します 入力がずっと負領域のままで 活性化がゼロ値になる場合
ReLUは動作を停止します そこで終わりません
次の層の貢献がゼロのだからです 重みが次のニューロンに結合しているのに その活性化がゼロで
入力がゼロになるためです 大量のゼロが次のニューロンに入り 当然それが正領域に入り込み これらのニューロンの活性化もゼロになり
この問題は次々と続きます そこで誤差逆伝搬を行うと勾配はゼロになり 重みがなくなるためトレーニングは停止します
失敗です Leaky/Parametric ReLUなどの使用を
説明しましたが 学習率を下げて ReLU層の不活性化を阻止できます 大きな勾配は学習率が高すぎることが原因で 重みが更新されても
データポイントが再度活性化されません 勾配がゼロなので 合理的数値に重みを更新できず 問題はいつまでも続きます 簡単な直観チェックを行いましょう 有用な信号が2つあり どちらも個々にラベルと相関性があるものの 尺度が異なる場合
モデルはどうなるでしょうか？ たとえば スープのおいしさの予測因子があり
各特徴が材料投入の質を示しているとします チキンストックの特徴はリットルで測定され ビーフはミリリットルで測定されている場合 直感をグレーディングする確率変数が
うまく収束しません この2次元の最適な学習率が異なるためです データをクリアにして
計算に役立つ範囲に入れることには 機械学習モデルのトレーニング過程において
多数の利点があります 特徴値を小さくしてゼロ中心にすると トレーニング速度が上がり
数値問題を回避できます こうした理由でバッチ正規化は勾配爆発に
役立ちました 最初の入力特徴だけでなく すべての中間特徴も健全範囲内に収め 各層に問題が起こらないようにしました これでNaNトラップも回避できます これでは値が数値精度範囲を超えると
モデルが爆発します 特徴スケーリングと学習率低下の組み合わせで この厄介な落とし穴を回避できます また異常値の除外も一般化に役立ちます 異常検出などで異常値を検出し トレーニング前にデータセットから前処理で
除外できれば非常に役立ちます どんなデータにも使える汎用的な方法は
ありません 各方法について適切な事例と不適切な事例を
考えてみましょう 特徴値のスケールを小さくする方法は
多数あります 線形スケーリングでは
まずデータの最小値と最大値を見つけます 次に各値について 最小値を減算し 最大値と最小値の差
つまり範囲で除算します これですべての値が0と1の間になります 0が最小値で1が最大値です これは正規化とも呼ばれます ハードキャッピング（クリッピング）もあります ここでは最小値と最大値を設定します たとえば最小値を-7と設定し 最大値を10と設定した場合 -7未満の値はすべて-7になり 10を超える値はすべて10になります ログスケーリングという方法では
対数関数を入力データに適用します これが役立つのは
データの範囲が広いため これを凝縮させて
値の大きさだけにしたいときです 先ほど紹介したバッチ正規化のもう1つの方法は
標準化です ここでデータの平均と標準偏差を計算します この2つの値がわかったら 平均を各データポイントから引き
標準偏差で割ります これで新しい平均が0になり 新しい標準偏差が1になるため
データはゼロ中心になります 他にもデータのスケーリング方法は多数あります モデルに勾配爆発が生じた場合の推奨事項は？ 正解はA、B、C、Dです。 問題が生じるのはたいてい
重みが大きすぎるときで これが生じるのは学習率が高すぎるときです これは数値安定化や発散など 他にもさまざまな問題を引き起こします そのため学習率を下げて適切なゴルディロック
ゾーンを見つけることが必要です この点において重みの正則化も役立ちます 非常に大きな重みにはペナルティを科し 勾配爆発を起こりにくくします また勾配クリッピングを適用しても 勾配が特定の閾値を超えないようにできます 学習率の上昇をある程度緩和できます しかし高率のままだと 重みが非常に高くなる可能性があります バッチ正規化は 各層の中間入力が狭い範囲に留まるため 重みが範囲外まで大きくなる確率が大幅に減り 計算コストも小さくなります 勾配爆発の処理方法は多数あるので 医者の助けは不要です こうしたツールを試して最適な方法を
探せばいいのです より一般化されたモデルの構築に役立つ
他の正則化には NNにドロップアウト層を追加できます ドロップアウトを使うには層にラッパーを
追加します TensorFlowでは渡すパラメータが
ドロップアウトで ネットワークからニューロンを 一時的にドロップアウトさせる確率です この数値の設定には注意が必要です ドロップアウトのある他の関数では キープ確率を使用します これはドロップ確率を補うもの ニューロンのオンオフの確率です 確率を10%だけ落とすつもりが ノードの10%のみをランダムにキープすると 意図的でないスパースモデルになります ドロップアウトの内部での仕組みは？ ドロップアウト確率を20%に設定した場合 ネットワークに前方パスされたものでは ニューロンとドロップアウト
ラップ層がサイコロで決まります サイコロの目が20より大きく
そのニューロンがアクティブの場合 脱落し 入力に関係なくゼロ値が出力されるため 事実上ネットワークには何も追加されません ゼロを追加しても何も変わらず
ニューロンへの刺激は存在しません 各ノードがわずかな時間しかキープされない
事実の埋め合わせに 活性化のスケーリングを 1÷ (1-ドロップアウト確率) つまり
（1÷トレーニング中のキープ確率）で行うと それが活性化の期待値になります トレーニングを行わず 
コードの変更も不要な場合は ラッパーは事実上消え 正式なドロップアウトラッパー層の
ニューロンは常にオンで そのモデルがトレーニングした重みを使います ドロップアウトの考えでは
基本的にアンサンブルモデルが構築されます 前方パスでは事実上 データミニバッチは異なるネットワークを
参照します これらをすべて予測に追加すると 2～nのNNをトレーニングすることになります nはドロップアウトニューロン数です アンサンブルの作動は多数の
ディシジョンツリーが ランダムフォレストで協力するのと似ています また信号の大部分をネットワークの 1本のブランチに向かわせるのではなく ネットワーク全体の データ分布に広がる相加的効果もあります これは複数の分岐やダムを使って川の水を
迂回させ 全水路に水が行き渡って
干上がらないようにするのと同じです この方法は信号がネットワーク全体に
均等に流れ ネットワークはその容量の多くを使用します そのためトレーニングと一般化を適切に行え ニューロンも人気のパスに大きく依存しません ドロップアウトの標準値は20～50%です これを大幅に低くすると ノードのドロップアウトが滅多に起こらず
効果が低下し 高すぎてもトレーニングは行われません ネットワークがスパース化しすぎて 分布がなく学習できません これは大きなネットワークでも使用できます モデルが独立した表現を学習できる
容量があります つまりネットワークが試せるパスが多くあります ドロップアウトを多くして キープを少なくすると 正則化が強化されます ドロップアウト確率を1に設定すると
キープは0になり ラップドロップアウト層の全ニューロンが ニューロンから除外され ゼロ活性化が出力されます 誤差逆伝搬では 重みは更新されず
この層は何も学習しません この確率を0に設定すると 全ニューロンが活性化し
ドロップアウト正則化はありません ドロップアウトラッパーのないこの方法では サイコロが必要になるため
計算コストが高くなります もちろん0と1の中間が理想的な位置です ドロップアウト確率は10～50%が理想的で 適切なベースラインは20%から始まり
必要なら追加します 全モデルやデータ分布に使える 汎用的なドロップアウト確率はありません ドロップアウトは＿＿として機能します データがより均等に広がるように
データを＿＿パスに流します また＿＿学習をシミュレートします ＿＿の逆数でドロップアウトの活性化を
スケーリングします ＿＿中はドロップアウトを除外します 正解はEです
ドロップアウトは 正則化として機能するため
モデルの一般化が向上します これを行うには
ドロップアウト確率でノードをオフにし データがより均等に広がるように
データを複数パスに流します そうしないとデータと活性化は 優先的なパスを取るようになり ネットワークのトレーニング不足につながり 新しいデータの性能が低下します ドロップアウトは2～nのモデルの集約を
作成して アンサンブル学習をシミュレートし
前方パスのノードを無作為にオフします nはドロップアウトノード数です バッチは別のネットワークを見るので モデルはトレーニングセット全体で
過学習になりません キープ確率の逆数でドロップアウトの
活性化をスケーリングします キープ確率=1-ドロップアウト確率 これはノードの予測で行え
トレーニング中にスケーリングされます 推測の場合は常にオンになっており 推測中はドロップアウトを除外するためです