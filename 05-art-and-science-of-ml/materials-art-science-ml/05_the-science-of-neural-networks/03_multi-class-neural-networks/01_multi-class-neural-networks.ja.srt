1
00:00:00,000 --> 00:00:02,250
NNの詳細やその効率的な

2
00:00:02,250 --> 00:00:03,480
トレーニング方法を学びました

3
00:00:03,480 --> 00:00:05,340
最適な一般化も行いました

4
00:00:05,340 --> 00:00:07,860
次はマルチクラスのNNを説明し

5
00:00:07,860 --> 00:00:10,545
マルチクラス分類の問題を扱います

6
00:00:10,545 --> 00:00:13,005
またシグモイド関数です

7
00:00:13,005 --> 00:00:14,895
調整済み確率を得られます

8
00:00:14,895 --> 00:00:18,620
これは2項分類問題の正規回帰で役立ちます

9
00:00:18,620 --> 00:00:21,450
または正クラスの確率が見つかります

10
00:00:21,450 --> 00:00:22,875
その確率を1から引くと

11
00:00:22,875 --> 00:00:24,690
負クラスに入ります

12
00:00:24,690 --> 00:00:27,900
クラスが3つ以上ある場合はどうしますか？

13
00:00:27,900 --> 00:00:30,345
多数のマルチクラス問題があります

14
00:00:30,345 --> 00:00:33,465
この例はオペラ劇場のチケットの種類です

15
00:00:33,465 --> 00:00:36,715
モデルは推奨する座席の種類です

16
00:00:36,715 --> 00:00:39,045
座席は4種類あるとします

17
00:00:39,045 --> 00:00:40,935
ピット、ストール

18
00:00:40,935 --> 00:00:42,855
サークル、スイートです

19
00:00:42,855 --> 00:00:45,780
各座席の種類の確率を得たいなら

20
00:00:45,780 --> 00:00:50,940
クラスが多すぎるので
通常の2項分類は使えません

21
00:00:50,940 --> 00:00:53,070
ピットが正クラスなら

22
00:00:53,070 --> 00:00:54,915
負クラスは？

23
00:00:54,915 --> 00:00:57,285
残りのクラスはどうしますか？

24
00:00:57,285 --> 00:01:01,170
考えられるのはマルチクラス分類問題から

25
00:01:01,170 --> 00:01:03,390
複数の2項分類問題への変換です

26
00:01:03,390 --> 00:01:08,510
これは「1対全部」または「1対残り」手法です

27
00:01:08,510 --> 00:01:11,525
この手法では各クラスを繰り返し処理します

28
00:01:11,525 --> 00:01:14,555
各繰り返しで
あるクラスを正クラスにし

29
00:01:14,555 --> 00:01:18,615
残りの全クラスをまとめて負クラスに入れます

30
00:01:18,615 --> 00:01:23,010
このようにして正クラスに入る確率を予測し

31
00:01:23,010 --> 00:01:27,015
反対に他のクラスに入らない確率を予測します

32
00:01:27,015 --> 00:01:29,880
重要なことは
出力するのは確率であり

33
00:01:29,880 --> 00:01:32,175
クラスレベルではないことです

34
00:01:32,175 --> 00:01:34,410
そのため1つのサンプルに対して

35
00:01:34,410 --> 00:01:36,960
マルチクラスの予測では
曖昧度が作成されません

36
00:01:36,960 --> 00:01:40,940
正クラスに選択した各クラスを
モデルでトレーニングしたら

37
00:01:40,940 --> 00:01:44,340
機械学習の最も重要な部分に移動しました
予測です

38
00:01:44,340 --> 00:01:46,680
予測でトレーニング済みの2項分類モデルにより

39
00:01:46,680 --> 00:01:49,335
予測サンプルを送ると

40
00:01:49,335 --> 00:01:52,980
モデルが最高の確率または信頼スコアを生成し

41
00:01:52,980 --> 00:01:55,770
私たちが全体的な予測済みクラスを選択します

42
00:01:55,770 --> 00:01:58,050
これは素晴らしい解決策に思えますが

43
00:01:58,050 --> 00:01:59,775
複数の問題が伴います

44
00:01:59,775 --> 00:02:02,700
まず信頼スコアというスキルは

45
00:02:02,700 --> 00:02:05,460
各2項分類モデルによって異なるため

46
00:02:05,460 --> 00:02:07,500
全体的な予測に偏りが生じます

47
00:02:07,500 --> 00:02:10,350
ただしそうでない場合でも

48
00:02:10,350 --> 00:02:12,570
2項分類モデルでは

49
00:02:12,570 --> 00:02:16,910
データ分布が非常に不均衡になります
各クラスの負クラスは

50
00:02:16,910 --> 00:02:19,110
正クラスとして現在マークされている

51
00:02:19,110 --> 00:02:23,575
クラスを除く他の全クラスの合計になるためです

52
00:02:23,575 --> 00:02:28,160
この不均衡問題を修正できるのは
「1対1」方法です

53
00:02:28,160 --> 00:02:31,070
ここにあるのは各クラスのモデルではなく

54
00:02:31,070 --> 00:02:33,830
クラスの二元組み合わせモデルです

55
00:02:33,830 --> 00:02:35,320
任意のクラスがある場合

56
00:02:35,320 --> 00:02:38,300
n x (n - 1)を2で割った数だけ

57
00:02:38,300 --> 00:02:41,620
モデルが存在します
n位の2乗になります

58
00:02:41,620 --> 00:02:45,665
この例の4クラスではモデルは6つですが

59
00:02:45,665 --> 00:02:47,855
画像のコンペティションのように

60
00:02:47,855 --> 00:02:49,805
クラスが1000あったら

61
00:02:49,805 --> 00:02:53,440
モデルは499,500になります

62
00:02:53,440 --> 00:02:57,540
各モデルが予測済みラベルに対して1票出力し

63
00:02:57,540 --> 00:03:02,345
各モデルの正クラスラベルに+1または+0します

64
00:03:02,345 --> 00:03:06,640
すべての票が累計され
得票数が最大のクラスが勝ちになります

65
00:03:06,640 --> 00:03:09,950
しかしこれでは曖昧性問題が解決しません

66
00:03:09,950 --> 00:03:11,610
入力分布に基づくと

67
00:03:11,610 --> 00:03:15,095
異なるクラスの得票数が同じになりうるためです

68
00:03:15,095 --> 00:03:20,605
こうした大きな欠点のないマルチクラス分類の
実行方法はあるのでしょうか？

69
00:03:20,605 --> 00:03:24,970
考えられるのはNNを使用した
「1対全部」方法で

70
00:03:24,970 --> 00:03:27,810
各クラスに複数のモデルを作成する代わりに

71
00:03:27,810 --> 00:03:31,750
考えうる各クラスに固有の出力を持つ
モデルを1つ作成します

72
00:03:31,750 --> 00:03:35,770
各例に対してこのモデルを
「特定のクラス」対「他の全クラス」の

73
00:03:35,770 --> 00:03:38,395
信号でトレーニングできます

74
00:03:38,395 --> 00:03:41,975
そのためラベルの設計には注意が必要です

75
00:03:41,975 --> 00:03:44,470
trueクラスに1つだけ作成せずに

76
00:03:44,470 --> 00:03:47,455
クラスの数の長さのベクトルを作成します

77
00:03:47,455 --> 00:03:50,110
ここでtrueクラスに当たるのは1

78
00:03:50,110 --> 00:03:51,845
残りは0になります

79
00:03:51,845 --> 00:03:56,400
このようにtrueクラスに当たる
シグモイドニューロンが1に近づいたら

80
00:03:56,400 --> 00:03:58,030
報酬を与え

81
00:03:58,030 --> 00:04:01,240
他のニューロンが1に近づいたら罰を与えます

82
00:04:01,240 --> 00:04:05,845
誤差が大きければ重みのネットワークを通じて
誤差逆伝送を行います

83
00:04:05,845 --> 00:04:09,740
ただし新しい数百万のクラスには
問題が生じる場合があります

84
00:04:09,740 --> 00:04:11,815
数百万の出力ニューロンです

85
00:04:11,815 --> 00:04:13,765
数百万の損失計算の後に

86
00:04:13,765 --> 00:04:16,839
数百万の誤差が生じ誤差逆伝搬が行われます

87
00:04:16,839 --> 00:04:18,745
計算コストが高額になります

88
00:04:18,745 --> 00:04:20,200
もっといい方法は？

89
00:04:20,200 --> 00:04:22,840
単に制約を追加すると

90
00:04:22,840 --> 00:04:25,025
出力の合計が1になります

91
00:04:25,025 --> 00:04:28,945
これで出力が確率として解釈されます

92
00:04:28,945 --> 00:04:32,405
この正規化関数をソフトマックスと呼びます

93
00:04:32,405 --> 00:04:36,825
各ノードには
W x X + bを

94
00:04:36,825 --> 00:04:40,720
全ノードの合計で割った指数関数があります

95
00:04:40,720 --> 00:04:44,000
これで全ノードが0と1の間になり

96
00:04:44,000 --> 00:04:47,290
全確率が1になります

97
00:04:47,290 --> 00:04:49,000
このように各例で

98
00:04:49,000 --> 00:04:51,700
各クラスの正規化確率を得られます

99
00:04:51,700 --> 00:04:54,130
次にargmaxでクラスを探します

100
00:04:54,130 --> 00:04:57,005
予測したラベルより高い確率です

101
00:04:57,005 --> 00:04:59,820
TensorFlowではWとXの適用メッシュ

102
00:04:59,820 --> 00:05:02,730
偏りノードとして最終層のロジットを計算し

103
00:05:02,730 --> 00:05:05,850
1がある場合は結果に追加します

104
00:05:05,850 --> 00:05:09,520
これでクラスの数のバッチサイズが
テンソル形状になります

105
00:05:09,520 --> 00:05:12,670
ラベルがワンホットエンコーディングされ

106
00:05:12,670 --> 00:05:14,380
各例のtrueクラスが1

107
00:05:14,380 --> 00:05:17,215
他のクラスが0を取得します

108
00:05:17,215 --> 00:05:20,065
そのためクラスの数ごとの

109
00:05:20,065 --> 00:05:22,820
バッチサイズのテンソル形状にもなります

110
00:05:22,820 --> 00:05:24,330
TensorFlowの

111
00:05:24,330 --> 00:05:28,870
softmax_cross_entropy_with_logits関数
で

112
00:05:28,870 --> 00:05:30,690
ラベルはソフトになります

113
00:05:30,690 --> 00:05:32,450
つまり

114
00:05:32,450 --> 00:05:34,950
クラスが相互に排他的であっても

115
00:05:34,950 --> 00:05:37,035
確率はその必要がありません

116
00:05:37,035 --> 00:05:39,660
たとえばクラスが3つあったら

117
00:05:39,660 --> 00:05:42,665
ミニバッチはそのラベルが

118
00:05:42,665 --> 00:05:47,415
0.15、0.8、0.05になっていたかも
しれません

119
00:05:47,415 --> 00:05:49,710
ワンホットエンコーディングでなくても

120
00:05:49,710 --> 00:05:53,520
合計すると1になるため
これらは有効な確率分布です

121
00:05:53,520 --> 00:05:58,800
softmax_cross_entropy_with_logits
でロジットをラベルと比べます

122
00:05:58,800 --> 00:06:01,875
結果は形状のテンソル、バッチサイズになります

123
00:06:01,875 --> 00:06:06,030
TensorFlow 1.5+では
この関数のv2が作成され

124
00:06:06,030 --> 00:06:08,630
v1の関数セットが非推奨となりました

125
00:06:08,630 --> 00:06:11,010
ミニバッチの平均損失を得るには

126
00:06:11,010 --> 00:06:13,715
出力でreduce_meanを使用します

127
00:06:13,715 --> 00:06:16,120
便宜のためTensorFlowでは

128
00:06:16,120 --> 00:06:22,200
sparse_softmax_cross_entropy_with_logits
を使用できます

129
00:06:22,200 --> 00:06:25,080
この場合ワンハイエンコーディングまたは

130
00:06:25,080 --> 00:06:26,850
ソフトエンコーディングを捨て

131
00:06:26,850 --> 00:06:29,250
ゼロとクラス数から1を引いた数の間の

132
00:06:29,250 --> 00:06:32,785
trueクラスのインデックスを渡します

133
00:06:32,785 --> 00:06:36,870
つまりラベルは形状のテンソル、バッチサイズに
なります

134
00:06:36,870 --> 00:06:41,550
関数の出力はまだ以前の形状のテンソル
バッチサイズと同じです

135
00:06:41,550 --> 00:06:46,845
それでもそのテンソルのreduce_meanで
ミニバッチの平均損失を求めます

136
00:06:46,845 --> 00:06:49,365
両方のソフトマックス関数では

137
00:06:49,365 --> 00:06:52,935
クラスが相互に排他的なため
これらのみを使用します

138
00:06:52,935 --> 00:06:56,700
たとえば画像1は犬のみの写真で

139
00:06:56,700 --> 00:06:58,785
画像2は猫のみの写真ですが

140
00:06:58,785 --> 00:07:03,420
画像3が犬と猫の両方の写真だとしたら
どうしますか？

141
00:07:03,420 --> 00:07:04,665
そしてML問題では

142
00:07:04,665 --> 00:07:06,075
それを知りたいと思います

143
00:07:06,075 --> 00:07:09,105
ソフトマックスを使って各確率を得ますが

144
00:07:09,105 --> 00:07:11,670
そのargmaxをラベルとします

145
00:07:11,670 --> 00:07:14,295
そのためモデルの画像に応じて

146
00:07:14,295 --> 00:07:15,780
ラベルが犬になったり

147
00:07:15,780 --> 00:07:17,340
猫になったりします

148
00:07:17,340 --> 00:07:21,210
これは使えません 知りたいのは
両方が写っているかどうかと

149
00:07:21,210 --> 00:07:23,985
他のクラスがあるかどうかです

150
00:07:23,985 --> 00:07:28,570
これはマルチクラスマルチラベル分類問題です

151
00:07:28,570 --> 00:07:32,390
ここでは
0から1の各クラスの確率が必要です

152
00:07:32,390 --> 00:07:36,415
TensorFlowはこれを行うのに
適した関数です

153
00:07:36,415 --> 00:07:39,280
sigmoid_cross_entropy_with_logitsは

154
00:07:39,280 --> 00:07:42,595
クラス数のテンソルごとにバッチサイズを
返します

155
00:07:42,595 --> 00:07:46,325
各例の各出力ノードを評価する必要があります

156
00:07:46,325 --> 00:07:50,080
各出力ノードはそこにつながる重みも意味します

157
00:07:50,080 --> 00:07:52,860
1ステップの出力ノードネットワーク100個は

158
00:07:52,860 --> 00:07:55,765
100ステップの出力ネットワーク1つと同じです

159
00:07:55,765 --> 00:07:59,780
非常に高価で クラスが非常に多い場合
スケーリングが困難です

160
00:07:59,780 --> 00:08:02,340
このソフトマックスの近似値を求め

161
00:08:02,340 --> 00:08:07,575
巨大なマルチクラス問題の計算コストを
削減できる方法が必要です

162
00:08:07,575 --> 00:08:11,460
ソフトマックスの近似値バージョンが存在します

163
00:08:11,460 --> 00:08:15,760
Candidate Sampingは正ラベルをすべて
計算しますが

164
00:08:15,760 --> 00:08:18,330
負レベルではすべてを計算せずに

165
00:08:18,330 --> 00:08:20,400
ランダムサンプリングします

166
00:08:20,400 --> 00:08:22,095
計算を大幅に減らします

167
00:08:22,095 --> 00:08:23,920
負のサンプル数は

168
00:08:23,920 --> 00:08:27,230
Candidate Samplingの重要な
ハイパーパラメータです

169
00:08:27,230 --> 00:08:30,695
明白な理由により
常に低く見積もられます

170
00:08:30,695 --> 00:08:35,049
TensorFlowではsample softmax loss
関数を使えます

171
00:08:35,049 --> 00:08:37,335
ソフトマックスの近似値を求めるには

172
00:08:37,335 --> 00:08:39,945
Noise-Contrastive Estimationで

173
00:08:39,945 --> 00:08:43,750
出力分布のモデリングを使って

174
00:08:43,750 --> 00:08:46,435
ソフトマックスの分母の近似値を求めます

175
00:08:46,435 --> 00:08:49,000
ロジットの指数関数の合計が含まれます

176
00:08:49,000 --> 00:08:52,620
これにより計算コストが安い近似値算出法で

177
00:08:52,620 --> 00:08:54,290
損失が見つかります

178
00:08:54,290 --> 00:08:58,030
分母の合計で全クラスを評価する必要は
ありません

179
00:08:58,030 --> 00:09:00,190
Candidate Samplingは
より直感的であり

180
00:09:00,190 --> 00:09:01,950
良好なモデルは不要です

181
00:09:01,950 --> 00:09:04,600
Noise-Contrastiveには良好な
モデルが必要です

182
00:09:04,600 --> 00:09:07,505
出力のモデリング分布に依存しているためです

183
00:09:07,505 --> 00:09:11,110
トレーニング中にはこうした関数を使用しますが

184
00:09:11,110 --> 00:09:12,820
推論の評価では

185
00:09:12,820 --> 00:09:15,960
精度向上のため
完全なソフトマックスを使用します

186
00:09:15,960 --> 00:09:19,375
そのためには既定のパーティション化戦略を

187
00:09:19,375 --> 00:09:20,905
modeからdivに変更し

188
00:09:20,905 --> 00:09:24,715
トレーニング、評価、予測間で損失が
一定になるようにします

189
00:09:24,715 --> 00:09:26,935
この分類出力では

190
00:09:26,935 --> 00:09:29,880
ラベルと確率の両方が相互排他的な場合

191
00:09:29,880 --> 00:09:31,490
＿＿を使用します

192
00:09:31,490 --> 00:09:33,605
ラベルが相互排他的で

193
00:09:33,605 --> 00:09:36,890
確率はそうでない場合
＿＿を使用します

194
00:09:36,890 --> 00:09:39,340
ラベルが相互排他的でない場合

195
00:09:39,340 --> 00:09:41,710
＿＿を使用します

196
00:09:41,710 --> 00:09:44,855
正解はAです

197
00:09:44,855 --> 00:09:46,710
この分類出力では

198
00:09:46,710 --> 00:09:50,435
相互排他的なラベルと確率の両方がある場合

199
00:09:50,435 --> 00:09:53,690
softmax_cross_entropy_with_logits_v2
を使います

200
00:09:53,690 --> 00:09:57,260
この場合
各例にtrueクラスは1つしかなく

201
00:09:57,260 --> 00:09:59,480
trueクラスにソフトラベルを与え

202
00:09:59,480 --> 00:10:02,030
trueクラスでのワンホットは不要だが

203
00:10:02,030 --> 00:10:06,380
0と1の間の各クラスの値の組み合わせは
その合計が1である限り

204
00:10:06,380 --> 00:10:08,335
どの組み合わせにもなり得ます

205
00:10:08,335 --> 00:10:10,760
ラベルが相互排他的で

206
00:10:10,760 --> 00:10:15,820
確率が違う場合に使うのは
sparse_softmax_cross_entropy_with_logitsです

207
00:10:15,820 --> 00:10:17,980
ソフトラベルは使えませんが

208
00:10:17,980 --> 00:10:20,435
モデルのデータサイズを生成できます

209
00:10:20,435 --> 00:10:24,560
各例のクラス数のベクトルではなく
ラベルを圧縮して

210
00:10:24,560 --> 00:10:27,710
trueクラスのインデックスにできます

211
00:10:27,710 --> 00:10:30,330
ラベルが相互排他的でない場合は

212
00:10:30,330 --> 00:10:32,925
softmax_cross_entropy_with_logits
を使います

213
00:10:32,925 --> 00:10:36,470
このように考えうる各クラスの確率を得て

214
00:10:36,470 --> 00:10:38,810
各クラスの信頼スコアを得られます

215
00:10:38,810 --> 00:10:42,940
これは複数のクラスが入っている画像や

216
00:10:42,940 --> 00:10:46,070
各クラスの存在を知りたい場合に役立ちます