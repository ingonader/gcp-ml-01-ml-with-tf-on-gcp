Now that we've learned more about neural networks, and how to train them efficiently, and get the most generalization out of them, let's now talk about multi-class neural networks when working with multi-class classification problems. Here again, is the sigmoid function. Which gives us calibrated probabilities. It's useful in the legitimate regression for binary class problems, or I can find the probability in the positive class, where one minus that, it's probably to be in the negative class. What then do we do when we have more than two classes? There are lots of multi-class problems. This example is of ticket types in an Opera hall. Perhaps the model is for the seat type to recommend. Let's say there are four places to sit, in the pit, in stalls, in a circle, or in a suite. If I want a probability for each of these seat types, I can't just use a normal binary classification because I have too too many classes. If pit is my positive class, then what is this negative class? What do I do with the remaining classes? One idea is to transform the problem from multi-class classification, to many binary classification problems. A method to do this is the one verse all or one verse rest approach. In this approach, we'll iterate through each class. For each iteration, that class with the positive class, and then where all the remaining classes will be lumped together into the negative class. In this way, I am predicting the probability of being in the positive class, and conversely the probability of not being in the other classes. It is important that we output our pretty probability, and not just the class level. So that we don't create ambiguities if multiple classes are predicted for a single sample. Once the model trained for each class being selected as the positive one, we moved to the most valuable part of Machine Learning; predictions. To make a prediction, you send your predictions sample through each of the trained binary classification models. Then the model that produces the highest probability or confidence score, we would choose as the overall predicted class. Although this seems like a great solution, it comes with several problems. First, the skill that the confidence score is might be different for each of the binary classification models, which biases our overall prediction. However, even if that isn't the case, each of the binary classification models, see very unbalanced data distributions since for each one, the negative class is the sum of all the other classes, besides the one that is currently marked for the positive class. A possible fix for this imbalance problem is the one verse one method. Here, instead of having a model for each class, there is a model for each binary combination of the classes. If there are any classes, this means that there would be n times n minus one, over two models so of order n squared. Already for four classes in our example that is six models, but if I had a thousand classes, like image that competition, there would be 499,500 models. Wow! Each model essentially outputs a vote for its predicted label, plus one or plus zero for the positive class label of each model. Then all the votes are accumulated and the class that has the most wins. However, this doesn't fix the ambiguity problem, because based on the input distribution, it could end up having the same number of votes for different classes. So, is there any way to do multi-class classification without these major drawbacks? An idea could be to use the one verse all approach with Neural Networks, where instead of having multiple models for each class, there is one model with a unique output for each possible class. We can train this model on a signal of ''my class'' verse ''all other classes'' for each example that it sees. Therefore, we need to be careful in how we design our labels. Instead of having just a one for our true class, we will have a vector of length of the number of classes, where our true classes correspond it'll be one, and the rest will be zero. This way, you will reward this corresponding sigmoid neuron for the true class, if it gets too close to one, it will punish the other sigmoid neurons if they also get close to one. With a higher error to be back propagated back through the network of their weights. However, we may have problems with millions of new classes, since we will have millions of output neurons. Thus millions of loss calculations, followed by millions of errors being back-propagated through the network. Very computationally expensive. Is there a better way? If we simply add an additional constraint, the sum of outputs equals one. Then it allows the output to be interpreted as probabilities. This normalizing function is called Softmax. At each node, we find the exponential of W times X, plus b and then divide by the sum of all the nodes. This ensures all nodes are between zero and one, and that the total probability equals one as it should. This way, for each example, you will get a normalized probability for each class, where you can then take that augmax to find the class, the higher probability as your predicted label. In TenserFlow, we calculate our logits in our final layer as a mesh of application of W and X, the biased node, add to the result if one exists. This will give us a tensor shape of batch size for the number of classes. Our labels are one hot encoded as we talked about previously, where the true class gets a one, and the other classes get zero for each example. Therefore, also having the shape of tenser, of batch size, by the number of classes. Note, because we are using TensorFlow, Softmax cross entropy, with logits function, the labels can actually be soft. What I mean by this is, even though the classes are still mutually exclusive, the probabilities need not be. If you had three classes, for example, your mini batch could have been one with its labels be 0.15, 0.8, and 0.05, as your label. They are not one hot encoded, however, there are still a valid probability distribution since they sum to one. Finally, we compare our logits with our labels using softmax cross entropy with logits. It will get a result in tensor of shape, batch size. In TensorFlow, 1.5 plus a version two of the function was created with the version one function set to be deprecated. To get the average loss for that mini batch, just use reduced mean on the output. For convenience, TensorFlow has another function you can use instead to calculate the softmax called sparse softmax cross entropy with logits. In this case, we do away with the one high encoding, or soft encoding of our labels, and instead just provide the index of the true class between zero and the number of classes minus one. This means that our labels are now a tensor of shape, batch size. The output of the function is still the same as before as a tensor of shape, batch size. I will still just reduce mean that tensor to get the average loss of the mini batch. Remember, for both softmax functions, we are only using them because our classes are mutually exclusive. For instance, image one is only a picture of a dog, and image two is only a picture of a cat. However, what if image three is a picture of both a dog and a cat? And for my ML problem, I want to know that. Using softmax, I will get a probability for each one, but I will take the augmax of it as my label. Therefore, depending on my image in my model, it might label as a dog, or it might label as a cat. This is no good, because I want to know if both are in there, and if there are any other classes in there as well. This is a multi-class multi-label classification problem. In this case, I want the probability of each class from zero to one. Fortunately, TensorFlow is a nifty function that does just that, called sigmoid cross entropy with logits, which returns a batch size by number of classes tensor. We need to evaluate every output node for every example. Of course, every output node means also the weights that lead up to it. So, a single step of 100 output node network, is like a hundred steps of a single output network. Hugely expensive and hard to scale for a very large number of classes. We need some way to approximate this softmax, that we can reduce some of the competition costs for very large multi-class problems. Fortunately, aproximate versions of softmax exist. Candidate sampling calculates for all the positive labels, but rather than also reform the computation on all the negative labels, it randomly samples some negatives. We should greatly reduce computation. The number of negative sampled is an important hyper parameter to a candidate sampling model. It is always, for obvious reasons, in underestimate. In TensorFlow, we can use the function sample softmax loss. Another way to approximate the softmax, is to use noise-contrastive estimation. Noise-contrastive estimation approximates the denominator the softmax, which contains the sum of all the exponentials of the logits, by modeling the distribution of outputs instead. This can provide an approximate less competitionally expensive means, to find our softmax loss, without having to evaluate every class in the sum of the denominator. Candidate sampling is more intuitive, and doesn't require a really good model. Noise contrastive requires a really good model, since it relies on modeling distribution of the outputs. Typically, we will use these functions during training, but for evaluation in inference, for better accuracy, we usually use the full softmax. To do this, make sure to change the default partition strategy, from mode to div, for the losses to be consistent between training, evaluation and prediction. For our classification output, if we have both mutually exclusive labels and probabilities, we should use blank. If the labels are mutually exclusive, the probabilities aren't, then we should use blank. If our labels aren't mutually exclusive, we should use blank. The correct answer is A. For our classification output, if we have both mutually exclusive labels and probabilities, we should use softmax cross entropy with logits version two. This means that there is only one true class for each example, and we allow for soft labels with the true class, does not need to be one hotted for the true class, but can be any combination of values between zero and one for each class, as long as they all sum up to one. If the labels are mutually exclusive, the probabilities aren't, then we should use sparse softmax cross entropy with logits. This doesn't allow for soft labels, but does help produce the model data size, since you can compress your labels and are just being the index of the true class, rather than a vector of the number of classes for each example. If our labels aren't mutually exclusive, we should use sigmoid cross entropy with logits. This way, we will get a probability for each possible class, which can give us confidence scores of each class being represented in the output such as an image with multiple classes in it, or we want to know the existence of each class.