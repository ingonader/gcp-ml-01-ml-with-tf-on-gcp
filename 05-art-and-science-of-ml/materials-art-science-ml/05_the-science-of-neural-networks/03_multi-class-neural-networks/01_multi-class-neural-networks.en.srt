1
00:00:00,000 --> 00:00:02,250
Now that we've learned more about neural networks,

2
00:00:02,250 --> 00:00:03,480
and how to train them efficiently,

3
00:00:03,480 --> 00:00:05,340
and get the most generalization out of them,

4
00:00:05,340 --> 00:00:07,860
let's now talk about multi-class neural networks

5
00:00:07,860 --> 00:00:10,545
when working with multi-class classification problems.

6
00:00:10,545 --> 00:00:13,005
Here again, is the sigmoid function.

7
00:00:13,005 --> 00:00:14,895
Which gives us calibrated probabilities.

8
00:00:14,895 --> 00:00:18,620
It's useful in the legitimate regression for binary class problems,

9
00:00:18,620 --> 00:00:21,450
or I can find the probability in the positive class,

10
00:00:21,450 --> 00:00:22,875
where one minus that,

11
00:00:22,875 --> 00:00:24,690
it's probably to be in the negative class.

12
00:00:24,690 --> 00:00:27,900
What then do we do when we have more than two classes?

13
00:00:27,900 --> 00:00:30,345
There are lots of multi-class problems.

14
00:00:30,345 --> 00:00:33,465
This example is of ticket types in an Opera hall.

15
00:00:33,465 --> 00:00:36,715
Perhaps the model is for the seat type to recommend.

16
00:00:36,715 --> 00:00:39,045
Let's say there are four places to sit,

17
00:00:39,045 --> 00:00:40,935
in the pit, in stalls,

18
00:00:40,935 --> 00:00:42,855
in a circle, or in a suite.

19
00:00:42,855 --> 00:00:45,780
If I want a probability for each of these seat types,

20
00:00:45,780 --> 00:00:50,940
I can't just use a normal binary classification because I have too too many classes.

21
00:00:50,940 --> 00:00:53,070
If pit is my positive class,

22
00:00:53,070 --> 00:00:54,915
then what is this negative class?

23
00:00:54,915 --> 00:00:57,285
What do I do with the remaining classes?

24
00:00:57,285 --> 00:01:01,170
One idea is to transform the problem from multi-class classification,

25
00:01:01,170 --> 00:01:03,390
to many binary classification problems.

26
00:01:03,390 --> 00:01:08,510
A method to do this is the one verse all or one verse rest approach.

27
00:01:08,510 --> 00:01:11,525
In this approach, we'll iterate through each class.

28
00:01:11,525 --> 00:01:14,555
For each iteration, that class with the positive class,

29
00:01:14,555 --> 00:01:18,615
and then where all the remaining classes will be lumped together into the negative class.

30
00:01:18,615 --> 00:01:23,010
In this way, I am predicting the probability of being in the positive class,

31
00:01:23,010 --> 00:01:27,015
and conversely the probability of not being in the other classes.

32
00:01:27,015 --> 00:01:29,880
It is important that we output our pretty probability,

33
00:01:29,880 --> 00:01:32,175
and not just the class level.

34
00:01:32,175 --> 00:01:34,410
So that we don't create ambiguities if

35
00:01:34,410 --> 00:01:36,960
multiple classes are predicted for a single sample.

36
00:01:36,960 --> 00:01:40,940
Once the model trained for each class being selected as the positive one,

37
00:01:40,940 --> 00:01:44,340
we moved to the most valuable part of Machine Learning; predictions.

38
00:01:44,340 --> 00:01:46,680
To make a prediction, you send your predictions sample

39
00:01:46,680 --> 00:01:49,335
through each of the trained binary classification models.

40
00:01:49,335 --> 00:01:52,980
Then the model that produces the highest probability or confidence score,

41
00:01:52,980 --> 00:01:55,770
we would choose as the overall predicted class.

42
00:01:55,770 --> 00:01:58,050
Although this seems like a great solution,

43
00:01:58,050 --> 00:01:59,775
it comes with several problems.

44
00:01:59,775 --> 00:02:02,700
First, the skill that the confidence score is might

45
00:02:02,700 --> 00:02:05,460
be different for each of the binary classification models,

46
00:02:05,460 --> 00:02:07,500
which biases our overall prediction.

47
00:02:07,500 --> 00:02:10,350
However, even if that isn't the case,

48
00:02:10,350 --> 00:02:12,570
each of the binary classification models,

49
00:02:12,570 --> 00:02:16,020
see very unbalanced data distributions since for each one,

50
00:02:16,020 --> 00:02:19,110
the negative class is the sum of all the other classes,

51
00:02:19,110 --> 00:02:23,575
besides the one that is currently marked for the positive class.

52
00:02:23,575 --> 00:02:28,160
A possible fix for this imbalance problem is the one verse one method.

53
00:02:28,160 --> 00:02:31,070
Here, instead of having a model for each class,

54
00:02:31,070 --> 00:02:33,830
there is a model for each binary combination of the classes.

55
00:02:33,830 --> 00:02:35,320
If there are any classes,

56
00:02:35,320 --> 00:02:38,300
this means that there would be n times n minus one,

57
00:02:38,300 --> 00:02:41,620
over two models so of order n squared.

58
00:02:41,620 --> 00:02:45,665
Already for four classes in our example that is six models,

59
00:02:45,665 --> 00:02:47,855
but if I had a thousand classes,

60
00:02:47,855 --> 00:02:49,805
like image that competition,

61
00:02:49,805 --> 00:02:53,440
there would be 499,500 models.

62
00:02:53,440 --> 00:02:57,540
Wow! Each model essentially outputs a vote for its predicted label,

63
00:02:57,540 --> 00:03:02,345
plus one or plus zero for the positive class label of each model.

64
00:03:02,345 --> 00:03:06,640
Then all the votes are accumulated and the class that has the most wins.

65
00:03:06,640 --> 00:03:09,950
However, this doesn't fix the ambiguity problem,

66
00:03:09,950 --> 00:03:11,610
because based on the input distribution,

67
00:03:11,610 --> 00:03:15,095
it could end up having the same number of votes for different classes.

68
00:03:15,095 --> 00:03:20,605
So, is there any way to do multi-class classification without these major drawbacks?

69
00:03:20,605 --> 00:03:24,970
An idea could be to use the one verse all approach with Neural Networks,

70
00:03:24,970 --> 00:03:27,810
where instead of having multiple models for each class,

71
00:03:27,810 --> 00:03:31,750
there is one model with a unique output for each possible class.

72
00:03:31,750 --> 00:03:35,020
We can train this model on a signal of ''my class'' verse ''all

73
00:03:35,020 --> 00:03:38,395
other classes'' for each example that it sees.

74
00:03:38,395 --> 00:03:41,975
Therefore, we need to be careful in how we design our labels.

75
00:03:41,975 --> 00:03:44,470
Instead of having just a one for our true class,

76
00:03:44,470 --> 00:03:47,455
we will have a vector of length of the number of classes,

77
00:03:47,455 --> 00:03:50,110
where our true classes correspond it'll be one,

78
00:03:50,110 --> 00:03:51,845
and the rest will be zero.

79
00:03:51,845 --> 00:03:56,400
This way, you will reward this corresponding sigmoid neuron for the true class,

80
00:03:56,400 --> 00:03:58,030
if it gets too close to one,

81
00:03:58,030 --> 00:04:01,240
it will punish the other sigmoid neurons if they also get close to one.

82
00:04:01,240 --> 00:04:05,845
With a higher error to be back propagated back through the network of their weights.

83
00:04:05,845 --> 00:04:09,740
However, we may have problems with millions of new classes,

84
00:04:09,740 --> 00:04:11,815
since we will have millions of output neurons.

85
00:04:11,815 --> 00:04:13,765
Thus millions of loss calculations,

86
00:04:13,765 --> 00:04:16,840
followed by millions of errors being back-propagated through the network.

87
00:04:16,840 --> 00:04:18,745
Very computationally expensive.

88
00:04:18,745 --> 00:04:20,200
Is there a better way?

89
00:04:20,200 --> 00:04:22,840
If we simply add an additional constraint,

90
00:04:22,840 --> 00:04:25,025
the sum of outputs equals one.

91
00:04:25,025 --> 00:04:28,945
Then it allows the output to be interpreted as probabilities.

92
00:04:28,945 --> 00:04:32,405
This normalizing function is called Softmax.

93
00:04:32,405 --> 00:04:36,825
At each node, we find the exponential of W times X,

94
00:04:36,825 --> 00:04:40,720
plus b and then divide by the sum of all the nodes.

95
00:04:40,720 --> 00:04:44,000
This ensures all nodes are between zero and one,

96
00:04:44,000 --> 00:04:47,290
and that the total probability equals one as it should.

97
00:04:47,290 --> 00:04:49,000
This way, for each example,

98
00:04:49,000 --> 00:04:51,700
you will get a normalized probability for each class,

99
00:04:51,700 --> 00:04:54,130
where you can then take that augmax to find the class,

100
00:04:54,130 --> 00:04:57,005
the higher probability as your predicted label.

101
00:04:57,005 --> 00:04:59,820
In TenserFlow, we calculate our logits in

102
00:04:59,820 --> 00:05:02,730
our final layer as a mesh of application of W and X,

103
00:05:02,730 --> 00:05:05,850
the biased node, add to the result if one exists.

104
00:05:05,850 --> 00:05:09,520
This will give us a tensor shape of batch size for the number of classes.

105
00:05:09,520 --> 00:05:12,670
Our labels are one hot encoded as we talked about previously,

106
00:05:12,670 --> 00:05:14,380
where the true class gets a one,

107
00:05:14,380 --> 00:05:17,215
and the other classes get zero for each example.

108
00:05:17,215 --> 00:05:20,065
Therefore, also having the shape of tenser,

109
00:05:20,065 --> 00:05:22,820
of batch size, by the number of classes.

110
00:05:22,820 --> 00:05:25,730
Note, because we are using TensorFlow,

111
00:05:25,730 --> 00:05:28,460
Softmax cross entropy, with logits function,

112
00:05:28,460 --> 00:05:30,690
the labels can actually be soft.

113
00:05:30,690 --> 00:05:32,450
What I mean by this is,

114
00:05:32,450 --> 00:05:34,950
even though the classes are still mutually exclusive,

115
00:05:34,950 --> 00:05:37,035
the probabilities need not be.

116
00:05:37,035 --> 00:05:39,660
If you had three classes, for example,

117
00:05:39,660 --> 00:05:43,665
your mini batch could have been one with its labels be 0.15,

118
00:05:43,665 --> 00:05:47,415
0.8, and 0.05, as your label.

119
00:05:47,415 --> 00:05:49,710
They are not one hot encoded, however,

120
00:05:49,710 --> 00:05:53,520
there are still a valid probability distribution since they sum to one.

121
00:05:53,520 --> 00:05:58,800
Finally, we compare our logits with our labels using softmax cross entropy with logits.

122
00:05:58,800 --> 00:06:01,875
It will get a result in tensor of shape, batch size.

123
00:06:01,875 --> 00:06:05,220
In TensorFlow, 1.5 plus a version two of

124
00:06:05,220 --> 00:06:08,630
the function was created with the version one function set to be deprecated.

125
00:06:08,630 --> 00:06:11,010
To get the average loss for that mini batch,

126
00:06:11,010 --> 00:06:13,715
just use reduced mean on the output.

127
00:06:13,715 --> 00:06:18,030
For convenience, TensorFlow has another function you can use instead to

128
00:06:18,030 --> 00:06:22,200
calculate the softmax called sparse softmax cross entropy with logits.

129
00:06:22,200 --> 00:06:25,170
In this case, we do away with the one high encoding,

130
00:06:25,170 --> 00:06:26,850
or soft encoding of our labels,

131
00:06:26,850 --> 00:06:29,250
and instead just provide the index of

132
00:06:29,250 --> 00:06:32,785
the true class between zero and the number of classes minus one.

133
00:06:32,785 --> 00:06:36,870
This means that our labels are now a tensor of shape, batch size.

134
00:06:36,870 --> 00:06:41,550
The output of the function is still the same as before as a tensor of shape, batch size.

135
00:06:41,550 --> 00:06:46,845
I will still just reduce mean that tensor to get the average loss of the mini batch.

136
00:06:46,845 --> 00:06:49,365
Remember, for both softmax functions,

137
00:06:49,365 --> 00:06:52,935
we are only using them because our classes are mutually exclusive.

138
00:06:52,935 --> 00:06:56,700
For instance, image one is only a picture of a dog,

139
00:06:56,700 --> 00:06:58,785
and image two is only a picture of a cat.

140
00:06:58,785 --> 00:07:03,420
However, what if image three is a picture of both a dog and a cat?

141
00:07:03,420 --> 00:07:04,665
And for my ML problem,

142
00:07:04,665 --> 00:07:06,075
I want to know that.

143
00:07:06,075 --> 00:07:09,105
Using softmax, I will get a probability for each one,

144
00:07:09,105 --> 00:07:11,670
but I will take the augmax of it as my label.

145
00:07:11,670 --> 00:07:14,295
Therefore, depending on my image in my model,

146
00:07:14,295 --> 00:07:15,780
it might label as a dog,

147
00:07:15,780 --> 00:07:17,340
or it might label as a cat.

148
00:07:17,340 --> 00:07:21,210
This is no good, because I want to know if both are in there,

149
00:07:21,210 --> 00:07:23,985
and if there are any other classes in there as well.

150
00:07:23,985 --> 00:07:28,570
This is a multi-class multi-label classification problem.

151
00:07:28,570 --> 00:07:32,390
In this case, I want the probability of each class from zero to one.

152
00:07:32,390 --> 00:07:36,415
Fortunately, TensorFlow is a nifty function that does just that,

153
00:07:36,415 --> 00:07:39,280
called sigmoid cross entropy with logits,

154
00:07:39,280 --> 00:07:42,595
which returns a batch size by number of classes tensor.

155
00:07:42,595 --> 00:07:46,325
We need to evaluate every output node for every example.

156
00:07:46,325 --> 00:07:50,080
Of course, every output node means also the weights that lead up to it.

157
00:07:50,080 --> 00:07:53,130
So, a single step of 100 output node network,

158
00:07:53,130 --> 00:07:55,765
is like a hundred steps of a single output network.

159
00:07:55,765 --> 00:07:59,780
Hugely expensive and hard to scale for a very large number of classes.

160
00:07:59,780 --> 00:08:02,340
We need some way to approximate this softmax,

161
00:08:02,340 --> 00:08:07,575
that we can reduce some of the competition costs for very large multi-class problems.

162
00:08:07,575 --> 00:08:11,460
Fortunately, aproximate versions of softmax exist.

163
00:08:11,460 --> 00:08:14,810
Candidate sampling calculates for all the positive labels,

164
00:08:14,810 --> 00:08:18,330
but rather than also reform the computation on all the negative labels,

165
00:08:18,330 --> 00:08:20,400
it randomly samples some negatives.

166
00:08:20,400 --> 00:08:22,095
We should greatly reduce computation.

167
00:08:22,095 --> 00:08:23,920
The number of negative sampled is

168
00:08:23,920 --> 00:08:27,230
an important hyper parameter to a candidate sampling model.

169
00:08:27,230 --> 00:08:30,695
It is always, for obvious reasons, in underestimate.

170
00:08:30,695 --> 00:08:35,050
In TensorFlow, we can use the function sample softmax loss.

171
00:08:35,050 --> 00:08:37,335
Another way to approximate the softmax,

172
00:08:37,335 --> 00:08:39,945
is to use noise-contrastive estimation.

173
00:08:39,945 --> 00:08:43,750
Noise-contrastive estimation approximates the denominator the softmax,

174
00:08:43,750 --> 00:08:46,435
which contains the sum of all the exponentials of the logits,

175
00:08:46,435 --> 00:08:49,000
by modeling the distribution of outputs instead.

176
00:08:49,000 --> 00:08:52,620
This can provide an approximate less competitionally expensive means,

177
00:08:52,620 --> 00:08:54,290
to find our softmax loss,

178
00:08:54,290 --> 00:08:58,030
without having to evaluate every class in the sum of the denominator.

179
00:08:58,030 --> 00:09:00,190
Candidate sampling is more intuitive,

180
00:09:00,190 --> 00:09:01,950
and doesn't require a really good model.

181
00:09:01,950 --> 00:09:04,600
Noise contrastive requires a really good model,

182
00:09:04,600 --> 00:09:07,505
since it relies on modeling distribution of the outputs.

183
00:09:07,505 --> 00:09:11,110
Typically, we will use these functions during training,

184
00:09:11,110 --> 00:09:12,820
but for evaluation in inference,

185
00:09:12,820 --> 00:09:15,960
for better accuracy, we usually use the full softmax.

186
00:09:15,960 --> 00:09:19,375
To do this, make sure to change the default partition strategy,

187
00:09:19,375 --> 00:09:20,905
from mode to div,

188
00:09:20,905 --> 00:09:24,715
for the losses to be consistent between training, evaluation and prediction.

189
00:09:24,715 --> 00:09:26,935
For our classification output,

190
00:09:26,935 --> 00:09:29,880
if we have both mutually exclusive labels and probabilities,

191
00:09:29,880 --> 00:09:31,490
we should use blank.

192
00:09:31,490 --> 00:09:33,605
If the labels are mutually exclusive,

193
00:09:33,605 --> 00:09:36,890
the probabilities aren't, then we should use blank.

194
00:09:36,890 --> 00:09:39,340
If our labels aren't mutually exclusive,

195
00:09:39,340 --> 00:09:41,710
we should use blank.

196
00:09:41,710 --> 00:09:44,855
The correct answer is A.

197
00:09:44,855 --> 00:09:46,710
For our classification output,

198
00:09:46,710 --> 00:09:50,435
if we have both mutually exclusive labels and probabilities,

199
00:09:50,435 --> 00:09:53,690
we should use softmax cross entropy with logits version two.

200
00:09:53,690 --> 00:09:57,260
This means that there is only one true class for each example,

201
00:09:57,260 --> 00:09:59,480
and we allow for soft labels with the true class,

202
00:09:59,480 --> 00:10:02,030
does not need to be one hotted for the true class,

203
00:10:02,030 --> 00:10:06,380
but can be any combination of values between zero and one for each class,

204
00:10:06,380 --> 00:10:08,335
as long as they all sum up to one.

205
00:10:08,335 --> 00:10:10,760
If the labels are mutually exclusive,

206
00:10:10,760 --> 00:10:15,820
the probabilities aren't, then we should use sparse softmax cross entropy with logits.

207
00:10:15,820 --> 00:10:17,980
This doesn't allow for soft labels,

208
00:10:17,980 --> 00:10:20,435
but does help produce the model data size,

209
00:10:20,435 --> 00:10:24,560
since you can compress your labels and are just being the index of the true class,

210
00:10:24,560 --> 00:10:27,710
rather than a vector of the number of classes for each example.

211
00:10:27,710 --> 00:10:30,330
If our labels aren't mutually exclusive,

212
00:10:30,330 --> 00:10:32,925
we should use sigmoid cross entropy with logits.

213
00:10:32,925 --> 00:10:36,470
This way, we will get a probability for each possible class,

214
00:10:36,470 --> 00:10:38,810
which can give us confidence scores of each class being

215
00:10:38,810 --> 00:10:42,940
represented in the output such as an image with multiple classes in it,

216
00:10:42,940 --> 00:10:46,070
or we want to know the existence of each class.