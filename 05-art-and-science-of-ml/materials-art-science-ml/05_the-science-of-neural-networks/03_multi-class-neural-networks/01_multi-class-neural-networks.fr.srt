1
00:00:00,000 --> 00:00:02,810
Maintenant que nous avons découvert
les réseaux de neurones,

2
00:00:02,810 --> 00:00:06,190
et comment les entraîner efficacement
et optimiser leur généralisation,

3
00:00:06,190 --> 00:00:09,150
nous allons aborder les réseaux
de neurones à classes multiples

4
00:00:09,150 --> 00:00:11,730
et les problèmes de classification
à classes multiples.

5
00:00:11,730 --> 00:00:15,655
Voici à nouveau la fonction sigmoïde,
qui nous fournit des probabilités calibrées.

6
00:00:15,655 --> 00:00:19,145
Elle sert à la régression logistique
pour les problèmes de classe binaire,

7
00:00:19,145 --> 00:00:21,570
où on trouve la probabilité
dans la classe positive,

8
00:00:21,570 --> 00:00:24,910
et où 1 moins la probabilité
correspond à la classe négative.

9
00:00:24,910 --> 00:00:28,575
Que faire alors lorsque nous avons
plus de deux classes ?

10
00:00:28,575 --> 00:00:30,955
Les problèmes de classes multiples
ne manquent pas.

11
00:00:30,955 --> 00:00:34,015
Prenons en exemple les types de billets
d'une salle de concert,

12
00:00:34,015 --> 00:00:37,175
et un modèle qui servirait
à recommander le type de place.

13
00:00:37,175 --> 00:00:42,685
Imaginons quatre types de place :
orchestre, corbeille, balcon ou loge.

14
00:00:42,685 --> 00:00:45,545
Si je veux obtenir une probabilité
pour chaque type de place,

15
00:00:45,545 --> 00:00:49,020
je ne peux pas me contenter d'utiliser
une classification binaire normale,

16
00:00:49,020 --> 00:00:51,470
car j'ai trop de classes.

17
00:00:51,470 --> 00:00:54,910
Si orchestre est ma classe positive,
qu'en est-il de la classe négative ?

18
00:00:54,910 --> 00:00:57,835
Que faire des classes restantes ?

19
00:00:57,835 --> 00:01:01,495
Il est possible de convertir ce problème
de classification à classes multiples

20
00:01:01,495 --> 00:01:04,000
en plusieurs problèmes
de classification binaire.

21
00:01:04,000 --> 00:01:08,820
On appliquerait alors la méthode dite
"un contre tous" ou "un contre reste".

22
00:01:08,820 --> 00:01:11,680
Dans ce cas, il faut créer des itérations
pour chaque classe.

23
00:01:11,680 --> 00:01:14,835
Pour chaque itération, cette classe
représente la classe positive,

24
00:01:14,835 --> 00:01:18,245
et les classes restantes sont regroupées
dans la classe négative.

25
00:01:18,245 --> 00:01:22,755
Je peux prédire ainsi la probabilité
d'appartenir à la classe positive,

26
00:01:22,755 --> 00:01:26,480
ou inversement, la probabilité
de ne pas appartenir aux autres classes.

27
00:01:27,210 --> 00:01:30,355
Il est important de générer une sortie
pour notre probabilité,

28
00:01:30,355 --> 00:01:34,180
et pas pour l'étiquette de classe seulement,
afin de ne pas créer d'ambiguïtés

29
00:01:34,180 --> 00:01:37,175
si plusieurs classes sont prédites
pour un seul échantillon.

30
00:01:37,175 --> 00:01:41,080
Une fois que chaque modèle a été entraîné
pour la classe considérée comme positive,

31
00:01:41,080 --> 00:01:44,160
nous passons à l'étape la plus utile
du ML : les prédictions.

32
00:01:44,160 --> 00:01:47,300
Pour faire une prédiction,
vous envoyez l'échantillon de prédiction

33
00:01:47,300 --> 00:01:50,490
à chacun des modèles
de classification binaire entraînés.

34
00:01:50,490 --> 00:01:53,685
Le modèle qui génère la probabilité
ou le score de confiance maximal

35
00:01:53,685 --> 00:01:56,240
est alors sélectionné
comme étant la classe prédite.

36
00:01:56,240 --> 00:02:00,060
Bien que cette approche semble idéale,
elle présente quelques problèmes.

37
00:02:00,060 --> 00:02:03,075
Tout d'abord, il se peut
que l'échelle des valeurs de confiance

38
00:02:03,075 --> 00:02:05,790
diffère pour chacun des modèles
de classification binaire,

39
00:02:05,790 --> 00:02:08,140
ce qui a une incidence
sur la prédiction globale.

40
00:02:08,140 --> 00:02:12,770
Cependant, même si ce n'est pas le cas,
chaque modèle de classification binaire

41
00:02:12,770 --> 00:02:16,490
voit des données très déséquilibrées,
étant donné que pour chacun d'entre eux

42
00:02:16,490 --> 00:02:19,350
la classe négative est la somme
de toutes les autres classes,

43
00:02:19,350 --> 00:02:24,520
outre la classe actuellement considérée
comme positive.

44
00:02:24,520 --> 00:02:26,810
Pour résoudre ce déséquilibre,
il serait possible

45
00:02:26,810 --> 00:02:30,995
d'utiliser la méthode "un à un".
Au lieu d'un modèle pour chaque classe,

46
00:02:30,995 --> 00:02:33,970
on a un modèle pour chaque combinaison
binaire des classes.

47
00:02:33,970 --> 00:02:36,980
En présence de n classes,
cela reviendrait à avoir

48
00:02:36,980 --> 00:02:41,590
n multiplié par n moins 1,
divisé par 2 modèles d'ordre n au carré.

49
00:02:41,590 --> 00:02:46,070
Pour les quatre classes de notre exemple,
on obtient ainsi six modèles,

50
00:02:46,070 --> 00:02:50,055
mais si j'ai 1 000 classes
comme pour la compétition ImageNet,

51
00:02:50,055 --> 00:02:54,075
j'aurai alors 499 500 modèles !

52
00:02:54,675 --> 00:02:57,615
Chaque modèle génère 1 vote
pour son étiquette prédite,

53
00:02:57,615 --> 00:03:02,430
plus 1 ou plus 0 pour l'étiquette
de classe positive de chaque modèle.

54
00:03:02,430 --> 00:03:07,150
Tous les votes sont totalisés, et
la classe qui en a le plus l'emporte.

55
00:03:07,150 --> 00:03:10,315
Toutefois, cela ne règle pas
le problème d'ambiguïté,

56
00:03:10,315 --> 00:03:12,980
car d'après la distribution d'entrée,
on pourrait obtenir

57
00:03:12,980 --> 00:03:15,810
le même nombre de votes
pour différentes classes.

58
00:03:15,810 --> 00:03:18,980
Est-il possible d'effectuer
une classification à classes multiples

59
00:03:18,980 --> 00:03:20,655
sans ces inconvénients majeurs ?

60
00:03:21,825 --> 00:03:25,495
On pourrait utiliser l'approche
"un contre tous" avec les réseaux de neurones,

61
00:03:25,495 --> 00:03:28,370
où, plutôt que d'avoir plusieurs modèles
pour chaque classe,

62
00:03:28,370 --> 00:03:32,150
on a un modèle avec une sortie unique
pour chaque classe possible.

63
00:03:32,150 --> 00:03:36,330
On entraîne ce modèle avec un signal
"ma classe" contre "toutes les autres classes"

64
00:03:36,330 --> 00:03:38,730
pour chaque exemple qu'il voit.

65
00:03:38,730 --> 00:03:42,065
On doit donc créer nos étiquettes avec soin.

66
00:03:42,065 --> 00:03:44,795
Plutôt que de n'avoir
qu'un 1 pour la vraie classe,

67
00:03:44,795 --> 00:03:47,700
on aura une longueur de vecteur
égale au nombre de classes,

68
00:03:47,700 --> 00:03:50,605
où la correspondance aux vraies classes
est représentée par 1,

69
00:03:50,605 --> 00:03:52,320
et le reste est représenté par 0.

70
00:03:52,320 --> 00:03:56,425
Vous récompensez ainsi le neurone à fonction
sigmoïde correspondant à la vraie classe.

71
00:03:56,425 --> 00:04:00,470
Si la valeur est trop près de 1, cela pénalise
les autres neurones à fonction sigmoïde

72
00:04:00,470 --> 00:04:02,130
si eux aussi se rapprochent de 1.

73
00:04:02,130 --> 00:04:05,870
L'erreur plus importante est calculée
par rétropropagation via le réseau

74
00:04:05,870 --> 00:04:08,915
pour mettre à jour les pondérations.
Cela pourrait poser problème

75
00:04:08,915 --> 00:04:12,350
en présence de millions de classes,
et de millions de neurones de sortie.

76
00:04:12,350 --> 00:04:16,215
On aurait des millions de calculs de perte,
suivis de millions d'erreurs calculées

77
00:04:16,215 --> 00:04:19,405
dans une rétropropagation via le réseau,
ce qui serait très coûteux.

78
00:04:19,405 --> 00:04:21,019
Y a-t-il une meilleure solution ?

79
00:04:21,019 --> 00:04:25,235
Il suffit d'ajouter une contrainte
pour que la somme des sorties soit égale à 1,

80
00:04:25,235 --> 00:04:29,435
ce qui permet d'interpréter
la sortie comme une probabilité.

81
00:04:29,435 --> 00:04:32,685
Cette fonction de normalisation
s'appelle softmax.

82
00:04:32,685 --> 00:04:37,445
Pour chaque nœud, on calcule
l'exponentielle de w multiplié par x plus b,

83
00:04:37,445 --> 00:04:40,465
qu'on divise par la somme totale des nœuds.

84
00:04:41,105 --> 00:04:44,120
Ainsi, tous les nœuds
ont une valeur comprise entre 0 et 1,

85
00:04:44,120 --> 00:04:47,030
et la probabilité totale
est égale à 1, comme il se doit.

86
00:04:47,370 --> 00:04:51,620
Pour chaque exemple, vous obtenez
une probabilité normalisée pour chaque classe,

87
00:04:51,620 --> 00:04:54,470
où vous pouvez prendre cet argmax
pour trouver la classe,

88
00:04:54,470 --> 00:04:57,790
la probabilité supérieure
pour l'étiquette prédite.

89
00:04:57,790 --> 00:04:59,980
Dans TensorFlow,
on calcule les fonctions logit

90
00:04:59,980 --> 00:05:03,055
dans la couche finale
en appliquant la matrice de w et x,

91
00:05:03,055 --> 00:05:05,630
le nœud biaisé étant ajouté au résultat
le cas échéant.

92
00:05:05,630 --> 00:05:10,000
On obtient ainsi la forme du Tensor, à savoir
la taille du lot pour le nombre de classes.

93
00:05:10,000 --> 00:05:13,470
Nos étiquettes ont un encodage one-hot
comme mentionné, où la vraie classe

94
00:05:13,470 --> 00:05:17,750
a la valeur 1 et les autres classes
ont la valeur 0 pour chaque exemple.

95
00:05:17,750 --> 00:05:22,585
La forme du Tensor est donc égale
à la taille du lot pour le nombre de classes.

96
00:05:23,075 --> 00:05:27,935
Comme on utilise
sotfmax_cross_entropy_with_logits

97
00:05:27,935 --> 00:05:30,960
dans TensorFlow,
les étiquettes peuvent être floues.

98
00:05:30,960 --> 00:05:35,130
Autrement dit, bien que les classes
soient mutuellement exclusives,

99
00:05:35,130 --> 00:05:37,650
les probabilités ne doivent pas
l'être nécessairement.

100
00:05:37,650 --> 00:05:41,840
Si vous avez trois classes, par exemple,
le mini-lot pourrait avoir la valeur 1

101
00:05:41,840 --> 00:05:47,165
et ses étiquettes 
les valeurs 0,15, 0,8 et 0,05.

102
00:05:47,775 --> 00:05:51,400
Elles n'ont pas un encodage one-hot,
mais elles représentent une distribution

103
00:05:51,400 --> 00:05:53,890
de probabilités valide,
car leur somme est égale à 1.

104
00:05:53,890 --> 00:05:56,420
Enfin, on compare nos fonctions logit
à nos étiquettes

105
00:05:56,420 --> 00:05:59,485
à l'aide de
softmax_cross_entroy_with_logits.

106
00:05:59,485 --> 00:06:02,080
La forme du Tensor est la taille du lot.

107
00:06:02,080 --> 00:06:06,210
TensorFlow 1.5 et ses versions ultérieures
proposent une deuxième version de la fonction,

108
00:06:06,210 --> 00:06:09,170
tandis que la première version est obsolète.

109
00:06:09,170 --> 00:06:12,260
Pour obtenir la perte moyenne
de ce mini-lot, il suffit d'utiliser

110
00:06:12,260 --> 00:06:14,575
reduce_mean pour la sortie.

111
00:06:14,575 --> 00:06:18,030
Pour plus de facilité, TensorFlow
compte une autre fonction appelée

112
00:06:18,030 --> 00:06:21,780
sparse_softmax_cross_entropy_with_logits.

113
00:06:22,370 --> 00:06:25,170
Dans ce cas, on n'a pas besoin
de l'encodage one-hot,

114
00:06:25,170 --> 00:06:27,310
ni de l'encodage flou des étiquettes,

115
00:06:27,310 --> 00:06:29,250
et on se contente de fournir l'index

116
00:06:29,250 --> 00:06:32,785
de la vraie classe entre 0
et le nombre de classes moins 1.

117
00:06:32,785 --> 00:06:36,870
Cela signifie que pour nos étiquettes,
la forme du Tensor est la taille de lot.

118
00:06:36,870 --> 00:06:41,550
La sortie de la fonction ne change pas :
la forme du Tensor est la taille de lot.

119
00:06:41,550 --> 00:06:46,845
J'applique toujours reduce_mean au Tensor
pour obtenir la perte moyenne du mini-lot.

120
00:06:46,845 --> 00:06:49,365
N'oubliez pas qu'on utilise
ces deux fonctions softmax

121
00:06:49,365 --> 00:06:52,935
uniquement si les classes
sont mutuellement exclusives.

122
00:06:52,935 --> 00:06:59,110
Par exemple, l'image 1 ne représente
qu'un chien et l'image 2 qu'un chat.

123
00:06:59,110 --> 00:07:03,555
Cependant, qu'en est-il si l'image 3
représente un chien et un chat ?

124
00:07:03,555 --> 00:07:06,470
Pour résoudre mon problème de ML,
j'ai besoin de le savoir.

125
00:07:06,470 --> 00:07:09,235
Avec softmax, j'obtiens une probabilité
pour chaque cas,

126
00:07:09,235 --> 00:07:11,625
mais j'utilise argmax en tant qu'étiquette.

127
00:07:11,625 --> 00:07:16,065
Selon l'image associée à mon modèle,
je pourrais avoir une étiquette "chien",

128
00:07:16,065 --> 00:07:17,820
ou une étiquette "chat".

129
00:07:17,820 --> 00:07:21,555
Ce n'est pas satisfaisant, car je veux savoir
si les deux figurent sur l'image,

130
00:07:21,555 --> 00:07:24,500
et s'il y a d'autres classes présentes.

131
00:07:24,500 --> 00:07:28,615
Il s'agit d'un problème de classification
à plusieurs libellés et à classes multiples.

132
00:07:28,615 --> 00:07:32,560
Dans ce cas, je veux que la probabilité
de chaque classe soit comprise entre 0 et 1.

133
00:07:32,560 --> 00:07:36,740
Heureusement, c'est précisément
le but de la fonction de TensorFlow

134
00:07:36,740 --> 00:07:39,725
nommée sigmoid_cross_entropy_with_logits,

135
00:07:39,725 --> 00:07:43,540
qui renvoie un Tensor de type
taille de lot par nombre de classes.

136
00:07:43,540 --> 00:07:46,265
On doit évaluer chaque nœud de sortie
pour chaque exemple.

137
00:07:46,265 --> 00:07:50,745
Évidemment, chaque nœud de sortie
inclut également les pondérations associées.

138
00:07:50,745 --> 00:07:53,700
Un seul pas pour un réseau
avec 100 nœuds de sortie correspond

139
00:07:53,700 --> 00:07:56,430
à 100 pas pour un réseau
avec un seul nœud de sortie.

140
00:07:56,430 --> 00:08:00,330
La mise à l'échelle est coûteuse et difficile
pour un très grand nombre de classes.

141
00:08:00,330 --> 00:08:03,800
On doit parvenir à se rapprocher
de la fonction softmax, afin de réduire

142
00:08:03,800 --> 00:08:08,070
les coûts de calcul dans le cas
d'un très grand nombre de classes.

143
00:08:08,070 --> 00:08:11,375
Heureusement, il existe
des versions se rapprochant de softmax.

144
00:08:11,375 --> 00:08:14,120
L'échantillonnage de candidats
calcule une prédiction

145
00:08:14,120 --> 00:08:17,460
pour toutes les étiquettes positives,
mais plutôt que d'en faire autant

146
00:08:17,460 --> 00:08:20,950
pour toutes les étiquettes négatives,
il utilise un échantillon aléatoire.

147
00:08:20,950 --> 00:08:23,000
Le calcul est donc plus efficace.

148
00:08:23,000 --> 00:08:26,475
Le nombre d'étiquettes négatives
échantillonnées est un hyperparamètre clé

149
00:08:26,475 --> 00:08:28,630
dans un modèle
d'échantillonnage de candidats.

150
00:08:28,630 --> 00:08:32,040
Il est toujours,
pour des raisons évidentes, sous-estimé.

151
00:08:32,040 --> 00:08:35,515
Dans TensorFlow, on peut utiliser
la fonction sampled_softmax_loss.

152
00:08:35,515 --> 00:08:37,319
Pour se rapprocher de softmax,

153
00:08:37,319 --> 00:08:40,115
on peut aussi utiliser l'estimation
contraste/bruit (NCE).

154
00:08:40,115 --> 00:08:44,095
Elle permet de se rapprocher
du dénominateur de softmax,

155
00:08:44,095 --> 00:08:46,445
qui contient la somme
de toutes les exponentielles

156
00:08:46,445 --> 00:08:49,730
des fonctions logit, en modélisant
plutôt la distribution des sorties.

157
00:08:49,730 --> 00:08:52,825
C'est un moyen moins coûteux
en termes de calculs d'obtenir

158
00:08:52,825 --> 00:08:56,390
la perte de softmax,
sans avoir à évaluer chaque classe

159
00:08:56,390 --> 00:08:59,610
dans la somme du dénominateur.
L'échantillonnage des candidats

160
00:08:59,610 --> 00:09:02,420
est plus intuitif
et ne nécessite pas un modèle bien conçu.

161
00:09:02,420 --> 00:09:05,180
L'estimation contraste/bruit
requière un modèle bien conçu,

162
00:09:05,180 --> 00:09:08,330
car elle repose sur la modélisation
de la distribution des sorties.

163
00:09:08,330 --> 00:09:11,695
En général, on utilise ces fonctions
durant l'apprentissage,

164
00:09:11,695 --> 00:09:14,700
mais durant l'évaluation et l'inférence,
pour plus de précision,

165
00:09:14,700 --> 00:09:18,270
on utilise plutôt softmax complet.
Pour cela, veillez à changer la stratégie

166
00:09:18,270 --> 00:09:22,300
de partitionnement par défaut, mode, par div,
pour que les pertes soient cohérentes

167
00:09:22,300 --> 00:09:24,775
entre l'apprentissage, l'évaluation
et la prédiction.

168
00:09:24,775 --> 00:09:28,785
Pour la sortie de notre classification,
si on dispose d'étiquettes et de probabilités

169
00:09:28,785 --> 00:09:31,655
mutuellement exclusives,
on devrait utiliser [blanc].

170
00:09:31,655 --> 00:09:33,610
Si les étiquettes
sont mutuellement exclusives,

171
00:09:33,610 --> 00:09:37,145
mais que les probabilités ne le sont pas,
nous devrions utiliser [blanc].

172
00:09:37,145 --> 00:09:39,560
Si nos étiquettes ne sont pas
mutuellement exclusives,

173
00:09:39,560 --> 00:09:41,660
nous devrions utiliser [blanc].

174
00:09:42,340 --> 00:09:44,560
La bonne réponse est A.

175
00:09:45,420 --> 00:09:49,420
Pour la sortie de notre classification,
si on dispose d'étiquettes et de probabilités

176
00:09:49,420 --> 00:09:54,185
mutuellement exclusives, on devrait utiliser
softmax_cross_entropy_with_logits_v2.

177
00:09:54,185 --> 00:09:57,290
Cela signifie qu'il n'y a
qu'une vraie classe pour chaque exemple

178
00:09:57,290 --> 00:10:00,150
et que les étiquettes sont floues
pour la vraie classe.

179
00:10:00,150 --> 00:10:03,160
L'encodage one-hot
n'est pas requis pour la vraie classe,

180
00:10:03,160 --> 00:10:07,200
et les étiquettes peuvent avoir des valeurs
comprises entre 0 et 1 pour chaque classe,

181
00:10:07,200 --> 00:10:09,220
pourvu que leur somme soit égale à 1.

182
00:10:09,220 --> 00:10:12,700
Si les étiquettes sont mutuellement
exclusives, mais pas les probabilités,

183
00:10:12,700 --> 00:10:16,150
nous devrions utiliser
sparse_softmax_cross_entropy_with_logits.

184
00:10:16,150 --> 00:10:19,540
Cette fonction n'autorise pas
les étiquettes floues, mais aide à réduire

185
00:10:19,540 --> 00:10:23,065
la taille des données du modèle,
car les étiquettes compressées constituent

186
00:10:23,065 --> 00:10:26,740
l'index de la classe vraie,
plutôt qu'un vecteur du nombre de classes

187
00:10:26,740 --> 00:10:29,120
pour chaque exemple.
Si nos étiquettes ne sont pas

188
00:10:29,120 --> 00:10:32,830
mutuellement exclusives, on devrait utiliser
sigmoid_cross_entropy_with_logits.

189
00:10:32,830 --> 00:10:34,915
Ainsi, on obtient une probabilité

190
00:10:34,915 --> 00:10:38,200
pour chaque classe possible,
ce qui nous donne les scores de confiance

191
00:10:38,200 --> 00:10:42,010
pour chaque classe représentée en sortie,
telle qu'une image à classes multiples,

192
00:10:42,010 --> 00:10:46,050
ou si on veut vérifier
l'existence de chaque classe.