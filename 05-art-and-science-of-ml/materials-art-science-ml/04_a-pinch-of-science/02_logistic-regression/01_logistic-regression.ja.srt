1
00:00:00,000 --> 00:00:01,925
ではL1正則化の次に

2
00:00:01,925 --> 00:00:04,575
ロジスティック回帰について掘り下げ

3
00:00:04,575 --> 00:00:08,135
なぜ正則化の使用が重要なのか
考えてみましょう

4
00:00:08,135 --> 00:00:11,035
たとえばコイン投げの結果を
予測するとします

5
00:00:11,035 --> 00:00:13,030
普通のコインなら
予想値は必ず

6
00:00:13,030 --> 00:00:16,305
表が50％で裏が50％だと
皆知っています

7
00:00:16,305 --> 00:00:21,195
では代わりに普通ではない
折れ曲がったコインならどうでしょう

8
00:00:21,195 --> 00:00:25,750
ここではコイン投げ結果の予測を
普通のコインと異常なコインのほか

9
00:00:25,750 --> 00:00:30,150
さまざまなサイズや重さなど
あらゆるコインについて考えてみます

10
00:00:30,150 --> 00:00:35,455
コインの表裏を予測するには
どのような特徴を利用すればよいでしょうか

11
00:00:35,455 --> 00:00:38,360
おそらく曲がりの角度を
利用できるでしょう

12
00:00:38,360 --> 00:00:41,440
それによって質量のX％が
他方の面に分布したり

13
00:00:41,440 --> 00:00:45,045
空気抵抗や重心によって
回転に違いが出たりするからです

14
00:00:45,045 --> 00:00:47,655
コインの質量も
役に立つかもしれませんし

15
00:00:47,655 --> 00:00:51,690
サイズのほか直径や厚みなどの
特性もそうです

16
00:00:51,690 --> 00:00:54,240
特徴エンジニアリングを行って

17
00:00:54,240 --> 00:00:57,260
コインの体積や密度も
知ることができるでしょう

18
00:00:57,260 --> 00:01:01,620
またコインの素材なども
有用な情報かもしれません

19
00:01:01,620 --> 00:01:03,915
こうした特徴は
測定が非常に容易です

20
00:01:03,915 --> 00:01:07,245
しかしこれらはあくまで
一面に過ぎません

21
00:01:07,245 --> 00:01:10,440
それ以外はコインの投げ方自体に
依存します

22
00:01:10,440 --> 00:01:13,565
コインにどれだけの線速度や
角速度が与えられたか

23
00:01:13,565 --> 00:01:15,520
投げ始めの角度はどうか

24
00:01:15,520 --> 00:01:18,345
落下角度や
風速なども影響します

25
00:01:18,345 --> 00:01:21,665
こうしたものは測定が
やや難しくなります

26
00:01:21,665 --> 00:01:23,690
ではこうした特徴をもとに

27
00:01:23,690 --> 00:01:26,950
表裏を予想できる
最も単純なモデルは何でしょうか

28
00:01:26,950 --> 00:01:29,035
もちろん線形回帰ですね

29
00:01:29,035 --> 00:01:31,325
何か問題はあるでしょうか

30
00:01:31,325 --> 00:01:33,895
ここでのラベルは表と裏です

31
00:01:33,895 --> 00:01:36,935
言い換えれば表と
表でないほうですね

32
00:01:36,935 --> 00:01:42,040
これはワンホットエンコーディングなら
一方を表、一方を裏で表せます

33
00:01:42,040 --> 00:01:46,180
しかし標準的な平均二乗誤差損失関数による
線形回帰を利用すると

34
00:01:46,180 --> 00:01:49,520
予測値は0～1の範囲外に
なってしまうかもしれません

35
00:01:49,520 --> 00:01:53,075
コイン投げの予測結果が
2.75と出たらどうでしょうか

36
00:01:53,075 --> 00:01:54,775
無意味ですよね

37
00:01:54,775 --> 00:01:57,350
二乗誤差を最小化するモデルは

38
00:01:57,350 --> 00:02:00,180
確率を0～1にするという
制約は受けません

39
00:02:00,180 --> 00:02:02,485
しかしここではそれが必要です

40
00:02:02,485 --> 00:02:04,920
たとえば新しい例として特に

41
00:02:04,920 --> 00:02:08,450
ゼロ未満や1超えの値を予測する
モデルを考えてみましょう

42
00:02:08,450 --> 00:02:11,480
この場合このモデルを
確率には使用できません

43
00:02:11,480 --> 00:02:16,050
予測値をゼロや1で制限するといった
単純な方策ではバイアスが生じるため

44
00:02:16,050 --> 00:02:17,910
他の方法が必要です

45
00:02:17,910 --> 00:02:20,530
具体的には新しい損失関数です

46
00:02:20,530 --> 00:02:24,990
これを線形回帰からロジスティック回帰に
変換すればジレンマを解決できます

47
00:02:24,990 --> 00:02:27,130
前のコースで
機械学習の歴史を学び

48
00:02:27,130 --> 00:02:29,765
シグモイド活性化関数を
使用してみましたが

49
00:02:29,765 --> 00:02:32,660
ここでそれを掘り下げたいと思います

50
00:02:32,660 --> 00:02:36,040
シグモイド活性化関数では
基本的に線形回帰から

51
00:02:36,040 --> 00:02:39,310
加重和のwTxに
線形回帰のBを加えましたが

52
00:02:39,310 --> 00:02:43,580
単にそれを出力して
平均二乗誤差損失を計算する代わりに

53
00:02:43,580 --> 00:02:47,630
活性化関数を直線から
シグモイドに変換すると

54
00:02:47,630 --> 00:02:52,070
それが引数となりゼロと1の間に
滑らかに並べられます

55
00:02:52,070 --> 00:02:55,400
シグモイドへの入力は通常
線形回帰の出力ですが

56
00:02:55,400 --> 00:02:57,680
これがロジットと呼ばれます

57
00:02:57,680 --> 00:03:01,760
これが線形モデルの
非線形変換です

58
00:03:01,760 --> 00:03:05,790
ロジットが負の無限大になると
確率はゼロに漸近し

59
00:03:05,790 --> 00:03:09,480
正の無限大になると1に漸近することに
注意してください

60
00:03:09,480 --> 00:03:12,200
これはトレーニングに
どんな意味を持つでしょうか

61
00:03:12,200 --> 00:03:18,510
平均二乗誤差と違いシグモイドでは
1.0や0.0の確率は推測されません

62
00:03:18,510 --> 00:03:23,860
つまり勾配降下法では損失が
限りなくゼロに近付けられるため

63
00:03:23,860 --> 00:03:28,610
正則化を行わなければ
重みは限りなく正または負の無限大に近付き

64
00:03:28,610 --> 00:03:31,500
それによって問題が
生じることになります

65
00:03:31,500 --> 00:03:34,940
まずシグモイドの出力は
どう解釈できるでしょうか

66
00:03:34,940 --> 00:03:37,710
単なる0～1までの
数多くの関数でしょうか

67
00:03:37,710 --> 00:03:40,725
または何かそれ以上のものでしょうか

68
00:03:40,725 --> 00:03:42,990
幸いこれは
それ以上のものです

69
00:03:42,990 --> 00:03:45,360
これは校正された
確率の推定値です

70
00:03:45,360 --> 00:03:48,580
シグモイド関数は単なる範囲ではなく

71
00:03:48,580 --> 00:03:52,100
ロジスティック確率分布の
累積分布関数なので

72
00:03:52,100 --> 00:03:56,592
分位関数はロジットの逆数になり
これが対数オッズをモデリングします

73
00:03:56,592 --> 00:04:01,250
そのため数学的にはシグモイドの逆を
確率と考えることができます

74
00:04:01,250 --> 00:04:05,230
このように校正は
出力が確率のような実数値であることを

75
00:04:05,230 --> 00:04:07,550
示す事実だと考えられます

76
00:04:07,550 --> 00:04:11,370
これは埋め込みベクトルのような
校正されていない出力が

77
00:04:11,370 --> 00:04:15,485
内部的には参考になるが
値に真の相関関係はないのと対照的です

78
00:04:15,485 --> 00:04:19,409
0～1の数値を提示できる
出力活性化関数は無数にありますが

79
00:04:19,409 --> 00:04:25,260
訓練データセットの発生率の
校正済みの推定値だと実証されているのは

80
00:04:25,260 --> 00:04:27,920
このシグモイドだけです

81
00:04:27,920 --> 00:04:31,560
シグモイド活性化関数についての
この事実を利用すれば

82
00:04:31,560 --> 00:04:35,050
バイナリ分類の問題を
確率の問題に変換できます

83
00:04:35,050 --> 00:04:38,525
たとえば顧客がある商品を
購入するかどうかといった

84
00:04:38,525 --> 00:04:40,890
Yes/Noを予測する代わりに

85
00:04:40,890 --> 00:04:44,090
顧客がその商品を購入する確率を
予測できるのです

86
00:04:44,090 --> 00:04:46,280
これをしきい値と組み合わせれば

87
00:04:46,280 --> 00:04:49,925
単なるバイナリの回答よりも
はるかに高い予測力が得られます

88
00:04:49,925 --> 00:04:55,610
これでゼロから1の校正済みの確率に対する
ロジスティック回帰の出力を計算できました

89
00:04:55,610 --> 00:04:58,955
では誤差を検出し
それを利用して誤差逆伝搬により

90
00:04:58,955 --> 00:05:01,715
重みを更新するには
どうすればいいでしょうか

91
00:05:01,715 --> 00:05:04,850
交差エントロピーという
損失関数を使用するのです

92
00:05:04,850 --> 00:05:06,920
これは対数損失でもあります

93
00:05:06,920 --> 00:05:10,235
平均二乗誤差と違い
誤差はあまり重視されません

94
00:05:10,235 --> 00:05:13,835
出力は比較的ラベルに近く
二次式と比べるとほぼ直線です

95
00:05:13,835 --> 00:05:17,120
しかし交差エントロピーでは
平均二乗誤差と違い

96
00:05:17,120 --> 00:05:21,570
予測がラベルの逆に近い場合
値が指数関数的に増加します

97
00:05:21,570 --> 00:05:26,480
言い換えればモデルが誤っているだけでなく
高い信頼度をもって誤っている場合

98
00:05:26,480 --> 00:05:28,800
ペナルティが非常に
高くなります

99
00:05:28,800 --> 00:05:33,060
さらに平均二乗誤差の導関数はトレーニングの
問題を引き起こすことがあります

100
00:05:33,060 --> 00:05:36,390
出力をできるだけゼロか1に
近付けていくと

101
00:05:36,390 --> 00:05:40,935
「出力×1－出力」の勾配が
どんどん小さくなり

102
00:05:40,935 --> 00:05:43,535
重みがどんどん少なくなって

103
00:05:43,535 --> 00:05:46,470
トレーニングは完全に滞ってしまいます

104
00:05:46,470 --> 00:05:48,850
しかしエントロピーの交差は

105
00:05:48,850 --> 00:05:52,170
ロジスティック関数×1－ロジスティック
関数なので

106
00:05:52,170 --> 00:05:54,765
誤差逆伝搬の際にちょうど
相殺されます

107
00:05:54,765 --> 00:05:56,975
そのため問題は起こりません

108
00:05:56,975 --> 00:06:00,015
しかしロジスティック回帰では
正則化が重要です

109
00:06:00,015 --> 00:06:03,390
損失をゼロに近付けることは
困難かつ危険だからです

110
00:06:03,390 --> 00:06:06,840
まず勾配降下法では
交差エントロピーを最小化するため

111
00:06:06,840 --> 00:06:12,070
正のラベルについては出力値が1に
負のラベルについてはゼロに近付けられます

112
00:06:12,070 --> 00:06:13,740
シグモイドの式により

113
00:06:13,740 --> 00:06:16,800
ロジットが負の無限大の場合は
関数がゼロに漸近し

114
00:06:16,800 --> 00:06:19,620
正の無限大であれば
関数は1に漸近します

115
00:06:19,620 --> 00:06:22,205
ロジットを負または正の
無限大にするには

116
00:06:22,205 --> 00:06:25,805
たとえば重みが増えた結果
数値安定性の問題やオーバーフロー

117
00:06:25,805 --> 00:06:28,765
アンダーフローなどが起こる状況を
考えてください

118
00:06:28,765 --> 00:06:31,465
これは危険で
訓練が無効になりかねません

119
00:06:31,465 --> 00:06:34,440
また漸近線の近くでは
ここに見られるように

120
00:06:34,440 --> 00:06:37,200
シグモイド関数は
どんどん平坦になります

121
00:06:37,200 --> 00:06:40,635
これは導関数がどんどんゼロに
近付くことを意味します

122
00:06:40,635 --> 00:06:44,020
重みの更新には導関数と
誤差逆伝搬を使用するため

123
00:06:44,020 --> 00:06:46,995
勾配はゼロにならないことが重要です

124
00:06:46,995 --> 00:06:49,140
そうでないとトレーニングが停止します

125
00:06:49,140 --> 00:06:51,065
これは飽和と呼ばれます

126
00:06:51,065 --> 00:06:53,910
すべての活性化がこのプラトーに入り

127
00:06:53,910 --> 00:06:57,620
勾配消失問題が発生し
トレーニングが困難になる状態のことです

128
00:06:57,620 --> 00:07:00,410
ここにも有用なヒントがあります

129
00:07:00,410 --> 00:07:05,325
たとえば個々の例に一意のIDを割り当て
各IDをその特徴に割り当てるとします

130
00:07:05,325 --> 00:07:08,690
ここで正則化していない
ロジスティック回帰を使用すると

131
00:07:08,690 --> 00:07:10,680
完全な過学習につながります

132
00:07:10,680 --> 00:07:15,330
モデルがすべての例で損失を
ゼロにしようと試み それが失敗すると

133
00:07:15,330 --> 00:07:19,605
指標となるそれぞれの特徴の重みは
正または負の無限大にされます

134
00:07:19,605 --> 00:07:22,445
これは現実には
特徴交差をともなう

135
00:07:22,445 --> 00:07:24,265
高次元データで起こりえます

136
00:07:24,265 --> 00:07:28,645
1つの例でしか発生しないまれなクロスが
多数あることが多いからです

137
00:07:28,645 --> 00:07:32,435
では過学習を避けるには
どうすればよいでしょうか

138
00:07:32,435 --> 00:07:36,180
ロジスティック回帰を行う際に
重要なのはどれでしょうか

139
00:07:36,180 --> 00:07:38,940
正解はAとBの両方です

140
00:07:38,940 --> 00:07:41,440
ロジスティック回帰に
正則化を加えると

141
00:07:41,440 --> 00:07:44,740
パラメータの重みを小さくして
モデルを単純に保てます

142
00:07:44,740 --> 00:07:47,400
このペナルティ項を
損失関数に加えることで

143
00:07:47,400 --> 00:07:52,620
勾配降下法で交差エントロピーにより
重みが正または負の無限大になり

144
00:07:52,620 --> 00:07:56,010
数値問題が発生することを
防止できます

145
00:07:56,010 --> 00:07:58,500
また[inaudible]ロジットにより

146
00:07:58,500 --> 00:08:01,780
シグモイド関数の
より平坦でない部分に留まって

147
00:08:01,780 --> 00:08:04,910
勾配をゼロに近付きにくく
することができるため

148
00:08:04,910 --> 00:08:07,835
重みの更新と
訓練の継続が可能になります

149
00:08:07,835 --> 00:08:10,750
ですからCは誤りで
Eもそうです

150
00:08:10,750 --> 00:08:15,250
正則化では校正済みの確率推定値の出力は
変換されません

151
00:08:15,250 --> 00:08:17,700
ロジスティック回帰の優れた点は

152
00:08:17,700 --> 00:08:21,340
すでに校正済みの特性の推定値を
出力できるところです

153
00:08:21,340 --> 00:08:25,395
シグモイド関数はロジスティック確率分布の
累積分布関数だからです

154
00:08:25,395 --> 00:08:29,910
このため単なるYes/No、正誤、
売り買いなどのバイナリ回答ではなく

155
00:08:29,910 --> 00:08:32,330
実際に確率を予測できます

156
00:08:32,330 --> 00:08:37,544
過学習への対策として
正則化と早期終了の両方がよく行われます

157
00:08:37,544 --> 00:08:41,174
正則化モデルでは重みが大きいと
複雑性が高まるため

158
00:08:41,174 --> 00:08:45,265
調整によって重みが大きくなり
シナリオがどんどんまれになると

159
00:08:45,265 --> 00:08:47,990
損失が増加するため
そこで終了するのです

160
00:08:47,990 --> 00:08:51,090
L2正則化では重み値が
より小さく保たれ

161
00:08:51,090 --> 00:08:55,665
L1正則化ではより多くの特徴が削除されるため
モデルをよりスパースに保てます

162
00:08:55,665 --> 00:08:59,870
ハイパーパラメータ調整では
L1とL2の最適な選択肢を見つけるため

163
00:08:59,870 --> 00:09:04,525
検証内のポイントの損失関数を探し
最低値を取得します

164
00:09:04,525 --> 00:09:08,320
そのポイントでは正則化が少ないと
分散が増大し

165
00:09:08,320 --> 00:09:10,920
過学習が始まり
一般化が損なわれます

166
00:09:10,920 --> 00:09:13,840
正則化が多いとバイアスが増え

167
00:09:13,840 --> 00:09:17,315
未学習が始まって
一般化が損なわれます

168
00:09:17,315 --> 00:09:21,370
早期終了は過学習が始まったときに
訓練を終了するものです

169
00:09:21,370 --> 00:09:24,870
モデルを訓練するときは
検証データセットのモデルを

170
00:09:24,870 --> 00:09:28,535
個々のステップ、エピック、
分などの単位で評価します

171
00:09:28,535 --> 00:09:31,465
訓練を続けるにつれ
訓練誤差と検証誤差の両方が

172
00:09:31,465 --> 00:09:33,805
減少していく必要がありますが

173
00:09:33,805 --> 00:09:37,560
実際にはある時点で
検証誤差が増加し始めることがあります

174
00:09:37,560 --> 00:09:41,360
この時点でモデルは
訓練データセットを記憶し始め

175
00:09:41,360 --> 00:09:44,870
検証データセットに一般化する
能力を失い出しています

176
00:09:44,870 --> 00:09:49,595
さらに重要なのは将来このモデルを
新しいデータに一般化できなくなることです

177
00:09:49,595 --> 00:09:53,370
早期終了を利用すると
モデルをこの時点で停止してバックアップし

178
00:09:53,370 --> 00:09:55,845
前のステップの重みを使用して
検証誤差や

179
00:09:55,845 --> 00:09:58,665
ファンクションポイントへの
到達を防止できます

180
00:09:58,665 --> 00:10:01,615
ここでは損失はただ
L(w, D)となっており

181
00:10:01,615 --> 00:10:03,265
正則化の項はありません

182
00:10:03,265 --> 00:10:07,450
興味深いことに早期終了は
L2正則化とほぼ同等で

183
00:10:07,450 --> 00:10:11,910
そちらのほうが安価なため
よく代わりに利用されます

184
00:10:11,910 --> 00:10:16,600
幸い実際には常にこの両方が
エポックの正則化L1およびL2として

185
00:10:16,600 --> 00:10:21,235
さらに一定の早期終了正則化としても
利用されています

186
00:10:21,235 --> 00:10:25,000
L2正則化と早期終了は
やや冗長に見えますが

187
00:10:25,000 --> 00:10:29,860
リベラルなシステムではハイパーパラメータの
選択が最適でないこともあるため

188
00:10:29,860 --> 00:10:32,630
早期終了はその選択の
修正にも役立ちます

189
00:10:32,630 --> 00:10:37,005
ロジスティック回帰モデルから
確率を得られるのは素晴らしいことですが

190
00:10:37,005 --> 00:10:39,945
場合によっては単に
現実的な問題についての

191
00:10:39,945 --> 00:10:43,170
単純な決定を行いたい
だけのこともあります

192
00:10:43,170 --> 00:10:46,480
電子メールを迷惑メールフォルダに
振り分けるべきか

193
00:10:46,480 --> 00:10:48,430
ローンを承認すべきか

194
00:10:48,430 --> 00:10:51,645
ユーザーをどの道路に
導くべきかといったことです

195
00:10:51,645 --> 00:10:54,230
確率推定値をどのように利用すれば

196
00:10:54,230 --> 00:10:57,580
このモデルを利用して
決定を行うのに役立つでしょうか

197
00:10:57,580 --> 00:10:59,535
しきい値を選択するのです

198
00:10:59,535 --> 00:11:02,460
バイナリ分類問題の
単純なしきい値としては

199
00:11:02,460 --> 00:11:08,790
50％以下のすべての確率をNoとし
50％以上のすべての確率をYesとします

200
00:11:08,790 --> 00:11:11,285
しかし特定の
現実的な問題については

201
00:11:11,285 --> 00:11:13,565
この分類の重み付けを変更して

202
00:11:13,565 --> 00:11:16,985
60/40、20/80、19/91などと
することもあります

203
00:11:16,985 --> 00:11:21,180
これは第一種過誤と第二種過誤の
バランスをどうするかによります

204
00:11:21,180 --> 00:11:24,955
言い換えれば偽陽性と
偽陰性のバランスです

205
00:11:24,955 --> 00:11:29,190
バイナリ分類では
可能な結果は4種類になります

206
00:11:29,190 --> 00:11:33,585
真陽性、真陰性、
偽陽性、偽陰性の4つです

207
00:11:33,585 --> 00:11:37,810
これらの値の組み合わせによって
精度などの評価尺度が得られます

208
00:11:37,810 --> 00:11:41,480
精度は真陽性の数を
陽性の総数で除算して求め

209
00:11:41,480 --> 00:11:46,310
再現率は真陽性の数を
真陽性と偽陰性の総数で除算して求めます

210
00:11:46,310 --> 00:11:49,485
再現率は感度または真陽性率を示します

211
00:11:49,485 --> 00:11:53,480
しきい値の選択を調整すれば
任意の尺度を最適化できます

212
00:11:53,480 --> 00:11:56,540
これを簡単にする方法はあるでしょうか

213
00:11:56,540 --> 00:12:00,320
受信者操作特性曲線は
ROC曲線ともいいますが

214
00:12:00,320 --> 00:12:04,295
特定のモデルによる予測で
異なる決定しきい値を使用したときに

215
00:12:04,295 --> 00:12:08,410
どのように異なる偽陽性率と真陽性率が
生成されるかを示します

216
00:12:08,410 --> 00:12:12,350
しきい値を下げると偽陽性の数が
増えやすくなりますが

217
00:12:12,350 --> 00:12:15,465
同時に真陽性の数も増えます

218
00:12:15,465 --> 00:12:19,975
理想的なモデルとは偽陽性も
偽陰性もゼロになるもので

219
00:12:19,975 --> 00:12:25,295
それを等式に加えると真陽性率は1
偽陽性率はゼロになります

220
00:12:25,295 --> 00:12:30,440
曲線を生成するには
可能な個々の決定しきい値を再評価します

221
00:12:30,440 --> 00:12:33,430
それぞれのしきい値が
1つの点を生成し

222
00:12:33,430 --> 00:12:37,025
多数のしきい値を評価することで
次第に曲線が形成されます

223
00:12:37,025 --> 00:12:40,545
幸いこれには効率的な
分類ベースのアルゴリズムがあります

224
00:12:40,545 --> 00:12:43,775
それぞれのモデルは
異なるROC曲線を生成します

225
00:12:43,775 --> 00:12:47,060
ではどの決定しきい値を
使用すべきかわからない場合

226
00:12:47,060 --> 00:12:51,710
どうすればこれらの曲線を利用して
モデルの相対的性能を比較できるでしょうか

227
00:12:51,710 --> 00:12:54,140
この場合は曲線の下の領域（AUC）を

228
00:12:54,140 --> 00:12:57,990
可能な分類しきい値における
性能の集約的尺度として利用します

229
00:12:57,990 --> 00:13:02,090
AUCは究極的にどのシステムしきい値が
使用されるか不明なときに

230
00:13:02,090 --> 00:13:04,360
モデルを選択するのに役立ちます

231
00:13:04,360 --> 00:13:07,540
これはランダムな正の値と
負の値を使用する場合に

232
00:13:07,540 --> 00:13:12,840
モデルがそれらを正しい相対的順序で
スコア付けする確率を尋ねるようなものです

233
00:13:12,840 --> 00:13:15,560
AUCの利点は
スケール不変であると同時に

234
00:13:15,560 --> 00:13:18,400
分類しきい値についても
不変であることです

235
00:13:18,400 --> 00:13:20,905
AUCはこうした理由で
よく利用されます

236
00:13:20,905 --> 00:13:23,970
また精度/再現率曲線に
利用されることもあります

237
00:13:23,970 --> 00:13:27,050
最近では
精度/再現率/利得曲線ともいいますが

238
00:13:27,050 --> 00:13:31,810
単に4種類の結果の異なる組み合わせを
軸のメトリクスとして利用します

239
00:13:31,810 --> 00:13:36,810
しかしこれをただ集合尺度として扱うと
一部の効果が見落とされることがあります

240
00:13:36,810 --> 00:13:42,755
たとえば非常に可能性の低い陰性値を
さらにまれなものとしてランク付けすれば

241
00:13:42,755 --> 00:13:46,765
AUCのわずかな改善を
実現できることがあります

242
00:13:46,765 --> 00:13:50,405
これは良いことですが
実質的に有益ではない可能性もあります

243
00:13:50,405 --> 00:13:53,020
ロジスティック回帰モデルを
評価する場合

244
00:13:53,020 --> 00:13:56,250
予測にバイアスがないことを
確認しなければなりません

245
00:13:56,250 --> 00:13:57,835
ここでいうバイアスとは

246
00:13:57,835 --> 00:14:01,505
モデルの線形方程式における
バイアス項のことではありません

247
00:14:01,505 --> 00:14:06,130
そうではなく正または負の方向への
全体的なシフトが必要だということです

248
00:14:06,130 --> 00:14:08,400
予測バイアスを確認する
単純な方法は

249
00:14:08,400 --> 00:14:11,240
モデルがデータセットに作成した
予測の平均値を

250
00:14:11,240 --> 00:14:14,640
そのデータセットのラベルの平均値と
比較することです

251
00:14:14,640 --> 00:14:17,870
値が比較的近くなければ
問題があるかもしれません

252
00:14:17,870 --> 00:14:20,455
バイアスは坑道のカナリアの
ようなもので

253
00:14:20,455 --> 00:14:23,100
何かが誤っている
指標として利用できます

254
00:14:23,100 --> 00:14:25,955
バイアスがあれば
間違いなく問題があります

255
00:14:25,955 --> 00:14:27,730
ただしバイアスがゼロでも

256
00:14:27,730 --> 00:14:30,430
それだけでシステムが
完璧とはいえませんが

257
00:14:30,430 --> 00:14:32,665
いずれにしても
有効な目安になります

258
00:14:32,665 --> 00:14:35,725
バイアスがあれば
特徴セットが不完全だったり

259
00:14:35,725 --> 00:14:39,465
パイプラインのバグやトレーニングサンプルの
バイアスがあったりします

260
00:14:39,465 --> 00:14:42,000
データのスライスで
バイアスを確認すると

261
00:14:42,000 --> 00:14:45,760
バイアスを除去してモデルを改良するための
ガイドになります

262
00:14:45,760 --> 00:14:48,050
少し例を見てみましょう

263
00:14:48,050 --> 00:14:51,465
これは単純な試験的ブラウザの
校正プロットです

264
00:14:51,465 --> 00:14:54,025
見てのとおり
これは両対数グラフではなく

265
00:14:54,025 --> 00:14:58,490
予測によるバケット化ログ対数を
実際に観察されたものと比較しています

266
00:14:58,490 --> 00:15:02,130
中程度の範囲では校正が
非常にうまくいっていますが

267
00:15:02,130 --> 00:15:04,890
最下端ではかなり悪くなっています

268
00:15:04,890 --> 00:15:07,930
これはデータの基盤が
良好に表されていない部分や

269
00:15:07,930 --> 00:15:11,500
ノイズがあったり
留保が強すぎたりする場合も発生します

270
00:15:11,500 --> 00:15:14,080
バケット化は2種類の方法で
行えます

271
00:15:14,080 --> 00:15:16,980
文字通りターゲットの予測を分割するか

272
00:15:16,980 --> 00:15:19,515
または分位点でバケット化します

273
00:15:19,515 --> 00:15:21,990
なぜ確率の予測で
予測をバケット化して

274
00:15:21,990 --> 00:15:24,900
校正プロットを作成する
必要があるのでしょうか

275
00:15:24,900 --> 00:15:28,970
どのようなイベントでも
真のラベルはゼロか1になります

276
00:15:28,970 --> 00:15:31,665
たとえばクリックがあったか否かなどです

277
00:15:31,665 --> 00:15:34,540
しかし予測値は常に
確率的な推測なので

278
00:15:34,540 --> 00:15:38,085
中間的な0.1や0.33といった
値になります

279
00:15:38,085 --> 00:15:41,155
個々の例では
いつも予測が外れます

280
00:15:41,155 --> 00:15:44,045
しかし十分な数の例を組み合わせれば

281
00:15:44,045 --> 00:15:50,420
真のゼロおよび1の合計の平均は
予測した平均確率とほぼ同じになります

282
00:15:50,430 --> 00:15:55,215
ロジスティック回帰を行う際に
重要なものは次のうちどれでしょうか

283
00:15:55,215 --> 00:15:57,690
正解は上記すべてです

284
00:15:57,690 --> 00:16:02,905
モデルの一般化可能性は 最良の予測と
新しいデータを得るために極めて重要で

285
00:16:02,905 --> 00:16:05,590
それがそもそもモデルを作成する理由です

286
00:16:05,590 --> 00:16:09,200
そのためにはデータを
入れすぎないことが重要です

287
00:16:09,200 --> 00:16:14,425
ですからL1正則化ではスパース性のため
L2正則化ではモデル幅を小さく保つために

288
00:16:14,425 --> 00:16:17,270
目的関数にペナルティ項を追加したり

289
00:16:17,270 --> 00:16:20,015
早期終了を加えたりすることも
役に立ちます

290
00:16:20,015 --> 00:16:24,170
また確率推定値が出力されたときに
どんな決定を行うかを決めるための

291
00:16:24,170 --> 00:16:26,790
調整されたしきい値を
選ぶことも重要です

292
00:16:26,790 --> 00:16:30,650
ビジネスの尺度を最小化または
最大化することは重要ですので

293
00:16:30,650 --> 00:16:34,465
それがよく定義されていない場合は
より統計的な手法を利用し

294
00:16:34,465 --> 00:16:38,340
たとえば真と偽の陽性/陰性の数を計算し
それを組み合わせると

295
00:16:38,340 --> 00:16:41,790
真陽性率と偽陽性率などの
さまざまな尺度を利用できます

296
00:16:41,790 --> 00:16:45,680
それからこのプロセスを
他の多くのしきい値にもついて繰り返し

297
00:16:45,680 --> 00:16:51,375
曲線の下の領域（AUC）をプロットすれば
モデル性能の相対的な集約的尺度が得られます

298
00:16:51,375 --> 00:16:54,785
最後に
予測にはバイアスがないことが重要で

299
00:16:54,785 --> 00:16:58,440
バイアスがない場合でも
モデルが良好に機能していることを

300
00:16:58,440 --> 00:17:00,560
常に確認する必要があります

301
00:17:00,560 --> 00:17:03,030
バイアスを確認するにはまず

302
00:17:03,030 --> 00:17:06,765
予測の平均が観測誤差と
非常に近いことを確認します

303
00:17:06,765 --> 00:17:10,230
バイアスが隠れている場所を
特定するのに役立つ方法は

304
00:17:10,230 --> 00:17:13,459
データのスライスを確認し
校正プロットなどを用いて

305
00:17:13,459 --> 00:17:16,400
問題領域を隔離し
さらに調整を行うことです