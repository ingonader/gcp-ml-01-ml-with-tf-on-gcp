ではL1正則化の次に ロジスティック回帰について掘り下げ なぜ正則化の使用が重要なのか
考えてみましょう たとえばコイン投げの結果を
予測するとします 普通のコインなら
予想値は必ず 表が50％で裏が50％だと
皆知っています では代わりに普通ではない
折れ曲がったコインならどうでしょう ここではコイン投げ結果の予測を
普通のコインと異常なコインのほか さまざまなサイズや重さなど
あらゆるコインについて考えてみます コインの表裏を予測するには
どのような特徴を利用すればよいでしょうか おそらく曲がりの角度を
利用できるでしょう それによって質量のX％が
他方の面に分布したり 空気抵抗や重心によって
回転に違いが出たりするからです コインの質量も
役に立つかもしれませんし サイズのほか直径や厚みなどの
特性もそうです 特徴エンジニアリングを行って コインの体積や密度も
知ることができるでしょう またコインの素材なども
有用な情報かもしれません こうした特徴は
測定が非常に容易です しかしこれらはあくまで
一面に過ぎません それ以外はコインの投げ方自体に
依存します コインにどれだけの線速度や
角速度が与えられたか 投げ始めの角度はどうか 落下角度や
風速なども影響します こうしたものは測定が
やや難しくなります ではこうした特徴をもとに 表裏を予想できる
最も単純なモデルは何でしょうか もちろん線形回帰ですね 何か問題はあるでしょうか ここでのラベルは表と裏です 言い換えれば表と
表でないほうですね これはワンホットエンコーディングなら
一方を表、一方を裏で表せます しかし標準的な平均二乗誤差損失関数による
線形回帰を利用すると 予測値は0～1の範囲外に
なってしまうかもしれません コイン投げの予測結果が
2.75と出たらどうでしょうか 無意味ですよね 二乗誤差を最小化するモデルは 確率を0～1にするという
制約は受けません しかしここではそれが必要です たとえば新しい例として特に ゼロ未満や1超えの値を予測する
モデルを考えてみましょう この場合このモデルを
確率には使用できません 予測値をゼロや1で制限するといった
単純な方策ではバイアスが生じるため 他の方法が必要です 具体的には新しい損失関数です これを線形回帰からロジスティック回帰に
変換すればジレンマを解決できます 前のコースで
機械学習の歴史を学び シグモイド活性化関数を
使用してみましたが ここでそれを掘り下げたいと思います シグモイド活性化関数では
基本的に線形回帰から 加重和のwTxに
線形回帰のBを加えましたが 単にそれを出力して
平均二乗誤差損失を計算する代わりに 活性化関数を直線から
シグモイドに変換すると それが引数となりゼロと1の間に
滑らかに並べられます シグモイドへの入力は通常
線形回帰の出力ですが これがロジットと呼ばれます これが線形モデルの
非線形変換です ロジットが負の無限大になると
確率はゼロに漸近し 正の無限大になると1に漸近することに
注意してください これはトレーニングに
どんな意味を持つでしょうか 平均二乗誤差と違いシグモイドでは
1.0や0.0の確率は推測されません つまり勾配降下法では損失が
限りなくゼロに近付けられるため 正則化を行わなければ
重みは限りなく正または負の無限大に近付き それによって問題が
生じることになります まずシグモイドの出力は
どう解釈できるでしょうか 単なる0～1までの
数多くの関数でしょうか または何かそれ以上のものでしょうか 幸いこれは
それ以上のものです これは校正された
確率の推定値です シグモイド関数は単なる範囲ではなく ロジスティック確率分布の
累積分布関数なので 分位関数はロジットの逆数になり
これが対数オッズをモデリングします そのため数学的にはシグモイドの逆を
確率と考えることができます このように校正は
出力が確率のような実数値であることを 示す事実だと考えられます これは埋め込みベクトルのような
校正されていない出力が 内部的には参考になるが
値に真の相関関係はないのと対照的です 0～1の数値を提示できる
出力活性化関数は無数にありますが 訓練データセットの発生率の
校正済みの推定値だと実証されているのは このシグモイドだけです シグモイド活性化関数についての
この事実を利用すれば バイナリ分類の問題を
確率の問題に変換できます たとえば顧客がある商品を
購入するかどうかといった Yes/Noを予測する代わりに 顧客がその商品を購入する確率を
予測できるのです これをしきい値と組み合わせれば 単なるバイナリの回答よりも
はるかに高い予測力が得られます これでゼロから1の校正済みの確率に対する
ロジスティック回帰の出力を計算できました では誤差を検出し
それを利用して誤差逆伝搬により 重みを更新するには
どうすればいいでしょうか 交差エントロピーという
損失関数を使用するのです これは対数損失でもあります 平均二乗誤差と違い
誤差はあまり重視されません 出力は比較的ラベルに近く
二次式と比べるとほぼ直線です しかし交差エントロピーでは
平均二乗誤差と違い 予測がラベルの逆に近い場合
値が指数関数的に増加します 言い換えればモデルが誤っているだけでなく
高い信頼度をもって誤っている場合 ペナルティが非常に
高くなります さらに平均二乗誤差の導関数はトレーニングの
問題を引き起こすことがあります 出力をできるだけゼロか1に
近付けていくと 「出力×1－出力」の勾配が
どんどん小さくなり 重みがどんどん少なくなって トレーニングは完全に滞ってしまいます しかしエントロピーの交差は ロジスティック関数×1－ロジスティック
関数なので 誤差逆伝搬の際にちょうど
相殺されます そのため問題は起こりません しかしロジスティック回帰では
正則化が重要です 損失をゼロに近付けることは
困難かつ危険だからです まず勾配降下法では
交差エントロピーを最小化するため 正のラベルについては出力値が1に
負のラベルについてはゼロに近付けられます シグモイドの式により ロジットが負の無限大の場合は
関数がゼロに漸近し 正の無限大であれば
関数は1に漸近します ロジットを負または正の
無限大にするには たとえば重みが増えた結果
数値安定性の問題やオーバーフロー アンダーフローなどが起こる状況を
考えてください これは危険で
訓練が無効になりかねません また漸近線の近くでは
ここに見られるように シグモイド関数は
どんどん平坦になります これは導関数がどんどんゼロに
近付くことを意味します 重みの更新には導関数と
誤差逆伝搬を使用するため 勾配はゼロにならないことが重要です そうでないとトレーニングが停止します これは飽和と呼ばれます すべての活性化がこのプラトーに入り 勾配消失問題が発生し
トレーニングが困難になる状態のことです ここにも有用なヒントがあります たとえば個々の例に一意のIDを割り当て
各IDをその特徴に割り当てるとします ここで正則化していない
ロジスティック回帰を使用すると 完全な過学習につながります モデルがすべての例で損失を
ゼロにしようと試み それが失敗すると 指標となるそれぞれの特徴の重みは
正または負の無限大にされます これは現実には
特徴交差をともなう 高次元データで起こりえます 1つの例でしか発生しないまれなクロスが
多数あることが多いからです では過学習を避けるには
どうすればよいでしょうか ロジスティック回帰を行う際に
重要なのはどれでしょうか 正解はAとBの両方です ロジスティック回帰に
正則化を加えると パラメータの重みを小さくして
モデルを単純に保てます このペナルティ項を
損失関数に加えることで 勾配降下法で交差エントロピーにより
重みが正または負の無限大になり 数値問題が発生することを
防止できます また[inaudible]ロジットにより シグモイド関数の
より平坦でない部分に留まって 勾配をゼロに近付きにくく
することができるため 重みの更新と
訓練の継続が可能になります ですからCは誤りで
Eもそうです 正則化では校正済みの確率推定値の出力は
変換されません ロジスティック回帰の優れた点は すでに校正済みの特性の推定値を
出力できるところです シグモイド関数はロジスティック確率分布の
累積分布関数だからです このため単なるYes/No、正誤、
売り買いなどのバイナリ回答ではなく 実際に確率を予測できます 過学習への対策として
正則化と早期終了の両方がよく行われます 正則化モデルでは重みが大きいと
複雑性が高まるため 調整によって重みが大きくなり
シナリオがどんどんまれになると 損失が増加するため
そこで終了するのです L2正則化では重み値が
より小さく保たれ L1正則化ではより多くの特徴が削除されるため
モデルをよりスパースに保てます ハイパーパラメータ調整では
L1とL2の最適な選択肢を見つけるため 検証内のポイントの損失関数を探し
最低値を取得します そのポイントでは正則化が少ないと
分散が増大し 過学習が始まり
一般化が損なわれます 正則化が多いとバイアスが増え 未学習が始まって
一般化が損なわれます 早期終了は過学習が始まったときに
訓練を終了するものです モデルを訓練するときは
検証データセットのモデルを 個々のステップ、エピック、
分などの単位で評価します 訓練を続けるにつれ
訓練誤差と検証誤差の両方が 減少していく必要がありますが 実際にはある時点で
検証誤差が増加し始めることがあります この時点でモデルは
訓練データセットを記憶し始め 検証データセットに一般化する
能力を失い出しています さらに重要なのは将来このモデルを
新しいデータに一般化できなくなることです 早期終了を利用すると
モデルをこの時点で停止してバックアップし 前のステップの重みを使用して
検証誤差や ファンクションポイントへの
到達を防止できます ここでは損失はただ
L(w, D)となっており 正則化の項はありません 興味深いことに早期終了は
L2正則化とほぼ同等で そちらのほうが安価なため
よく代わりに利用されます 幸い実際には常にこの両方が
エポックの正則化L1およびL2として さらに一定の早期終了正則化としても
利用されています L2正則化と早期終了は
やや冗長に見えますが リベラルなシステムではハイパーパラメータの
選択が最適でないこともあるため 早期終了はその選択の
修正にも役立ちます ロジスティック回帰モデルから
確率を得られるのは素晴らしいことですが 場合によっては単に
現実的な問題についての 単純な決定を行いたい
だけのこともあります 電子メールを迷惑メールフォルダに
振り分けるべきか ローンを承認すべきか ユーザーをどの道路に
導くべきかといったことです 確率推定値をどのように利用すれば このモデルを利用して
決定を行うのに役立つでしょうか しきい値を選択するのです バイナリ分類問題の
単純なしきい値としては 50％以下のすべての確率をNoとし
50％以上のすべての確率をYesとします しかし特定の
現実的な問題については この分類の重み付けを変更して 60/40、20/80、19/91などと
することもあります これは第一種過誤と第二種過誤の
バランスをどうするかによります 言い換えれば偽陽性と
偽陰性のバランスです バイナリ分類では
可能な結果は4種類になります 真陽性、真陰性、
偽陽性、偽陰性の4つです これらの値の組み合わせによって
精度などの評価尺度が得られます 精度は真陽性の数を
陽性の総数で除算して求め 再現率は真陽性の数を
真陽性と偽陰性の総数で除算して求めます 再現率は感度または真陽性率を示します しきい値の選択を調整すれば
任意の尺度を最適化できます これを簡単にする方法はあるでしょうか 受信者操作特性曲線は
ROC曲線ともいいますが 特定のモデルによる予測で
異なる決定しきい値を使用したときに どのように異なる偽陽性率と真陽性率が
生成されるかを示します しきい値を下げると偽陽性の数が
増えやすくなりますが 同時に真陽性の数も増えます 理想的なモデルとは偽陽性も
偽陰性もゼロになるもので それを等式に加えると真陽性率は1
偽陽性率はゼロになります 曲線を生成するには
可能な個々の決定しきい値を再評価します それぞれのしきい値が
1つの点を生成し 多数のしきい値を評価することで
次第に曲線が形成されます 幸いこれには効率的な
分類ベースのアルゴリズムがあります それぞれのモデルは
異なるROC曲線を生成します ではどの決定しきい値を
使用すべきかわからない場合 どうすればこれらの曲線を利用して
モデルの相対的性能を比較できるでしょうか この場合は曲線の下の領域（AUC）を 可能な分類しきい値における
性能の集約的尺度として利用します AUCは究極的にどのシステムしきい値が
使用されるか不明なときに モデルを選択するのに役立ちます これはランダムな正の値と
負の値を使用する場合に モデルがそれらを正しい相対的順序で
スコア付けする確率を尋ねるようなものです AUCの利点は
スケール不変であると同時に 分類しきい値についても
不変であることです AUCはこうした理由で
よく利用されます また精度/再現率曲線に
利用されることもあります 最近では
精度/再現率/利得曲線ともいいますが 単に4種類の結果の異なる組み合わせを
軸のメトリクスとして利用します しかしこれをただ集合尺度として扱うと
一部の効果が見落とされることがあります たとえば非常に可能性の低い陰性値を
さらにまれなものとしてランク付けすれば AUCのわずかな改善を
実現できることがあります これは良いことですが
実質的に有益ではない可能性もあります ロジスティック回帰モデルを
評価する場合 予測にバイアスがないことを
確認しなければなりません ここでいうバイアスとは モデルの線形方程式における
バイアス項のことではありません そうではなく正または負の方向への
全体的なシフトが必要だということです 予測バイアスを確認する
単純な方法は モデルがデータセットに作成した
予測の平均値を そのデータセットのラベルの平均値と
比較することです 値が比較的近くなければ
問題があるかもしれません バイアスは坑道のカナリアの
ようなもので 何かが誤っている
指標として利用できます バイアスがあれば
間違いなく問題があります ただしバイアスがゼロでも それだけでシステムが
完璧とはいえませんが いずれにしても
有効な目安になります バイアスがあれば
特徴セットが不完全だったり パイプラインのバグやトレーニングサンプルの
バイアスがあったりします データのスライスで
バイアスを確認すると バイアスを除去してモデルを改良するための
ガイドになります 少し例を見てみましょう これは単純な試験的ブラウザの
校正プロットです 見てのとおり
これは両対数グラフではなく 予測によるバケット化ログ対数を
実際に観察されたものと比較しています 中程度の範囲では校正が
非常にうまくいっていますが 最下端ではかなり悪くなっています これはデータの基盤が
良好に表されていない部分や ノイズがあったり
留保が強すぎたりする場合も発生します バケット化は2種類の方法で
行えます 文字通りターゲットの予測を分割するか または分位点でバケット化します なぜ確率の予測で
予測をバケット化して 校正プロットを作成する
必要があるのでしょうか どのようなイベントでも
真のラベルはゼロか1になります たとえばクリックがあったか否かなどです しかし予測値は常に
確率的な推測なので 中間的な0.1や0.33といった
値になります 個々の例では
いつも予測が外れます しかし十分な数の例を組み合わせれば 真のゼロおよび1の合計の平均は
予測した平均確率とほぼ同じになります ロジスティック回帰を行う際に
重要なものは次のうちどれでしょうか 正解は上記すべてです モデルの一般化可能性は 最良の予測と
新しいデータを得るために極めて重要で それがそもそもモデルを作成する理由です そのためにはデータを
入れすぎないことが重要です ですからL1正則化ではスパース性のため
L2正則化ではモデル幅を小さく保つために 目的関数にペナルティ項を追加したり 早期終了を加えたりすることも
役に立ちます また確率推定値が出力されたときに
どんな決定を行うかを決めるための 調整されたしきい値を
選ぶことも重要です ビジネスの尺度を最小化または
最大化することは重要ですので それがよく定義されていない場合は
より統計的な手法を利用し たとえば真と偽の陽性/陰性の数を計算し
それを組み合わせると 真陽性率と偽陽性率などの
さまざまな尺度を利用できます それからこのプロセスを
他の多くのしきい値にもついて繰り返し 曲線の下の領域（AUC）をプロットすれば
モデル性能の相対的な集約的尺度が得られます 最後に
予測にはバイアスがないことが重要で バイアスがない場合でも
モデルが良好に機能していることを 常に確認する必要があります バイアスを確認するにはまず 予測の平均が観測誤差と
非常に近いことを確認します バイアスが隠れている場所を
特定するのに役立つ方法は データのスライスを確認し
校正プロットなどを用いて 問題領域を隔離し
さらに調整を行うことです