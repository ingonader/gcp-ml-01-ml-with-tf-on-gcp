1
00:00:00,190 --> 00:00:03,560
Vaya, la regularización L1
realmente nos ayudó a reducir

2
00:00:03,560 --> 00:00:06,930
nuestro complejo modelo
a uno más pequeño y generalizable.

3
00:00:07,330 --> 00:00:10,450
Seleccionamos todos los atributos
y dos capas ocultas en medio

4
00:00:10,610 --> 00:00:13,785
que crearon muchas conexiones,
representadas por las líneas intermedias.

5
00:00:13,845 --> 00:00:14,795
Cuando lo entrenamos

6
00:00:14,885 --> 00:00:17,405
todos los pesos estaban activos,
pero eran débiles.

7
00:00:17,575 --> 00:00:20,240
Había muchos atributos
con poco poder de predicción.

8
00:00:20,630 --> 00:00:24,320
Además, en vez de ver un lindo círculo 
que nos dice que los datos calzan

9
00:00:24,660 --> 00:00:26,870
tenemos esta forma oblonga

10
00:00:27,090 --> 00:00:29,000
que quizá no se generalizó bien.

11
00:00:29,560 --> 00:00:33,090
Tras la regularización,
los atributos inútiles se fueron a cero

12
00:00:33,390 --> 00:00:35,720
y sus líneas
son delgadas y se inhabilitaron.

13
00:00:36,200 --> 00:00:39,670
Los únicos atributos que
sobrevivieron fueron x1 y x2 al cuadrado

14
00:00:39,880 --> 00:00:43,250
lo que tiene sentido, ya que sumadas
forman la ecuación de un círculo

15
00:00:43,530 --> 00:00:45,540
que es una forma que aprende.

16
00:00:46,190 --> 00:00:48,310
Como sabemos
que es la distribución verdadera

17
00:00:48,340 --> 00:00:50,998
podemos estar seguros
de que el modelo generalizará bien.