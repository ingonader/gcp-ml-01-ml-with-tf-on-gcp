Vamos ver agora como a regularização de L1 pode ser importante na
criação de modelos concisos e esparsos. Neste laboratório de regularização de L1, adicionei muitos atributos espúrios e
criei um modelo razoavelmente complexo. Primeiro, vamos treinar o modelo
sem a regularização de L1, e depois veremos se ela ajuda a deixar o modelo em uma forma
muito mais esparsa, concisa e generalizável. Olá, seja bem-vindo de volta
ao TensorFlow Playground. Neste laboratório, vamos ver se a regularização de L1 pode tornar
os modelos mais esparsos e concisos. Como você pode ver,
este é um problema de classificação, em que vamos tentar classificar
essas duas formas. Temos aqui dois círculos concêntricos, o círculo azul no meio e
o círculo laranja no lado de fora. A boa notícia é que
não há ruído definido. Portanto, será fácil de forçar. O que você também pode notar
é que há atributos. Todos eles estão ativados. Isso significa que será
um modelo bastante complicado. Sabemos intuitivamente que este é
um X ao quadrado por Y ao quadrado, ou X1 ao quadrado pelo tipo de equação
de X2 ao quadrado porque há círculos. No entanto, temos outros
atributos extras adicionados a isso. Também temos camadas extras aqui
com seis neurônios cada. Isso é muito complexo. Vamos ver como isso treina
sem regularização de L1, definida como nenhuma neste caso. Isso foi bem rápido. Como você vê, a regularização de L1
encontrou a distribuição de nossos dados. No entanto, você pode notar que há
algumas inconsistências aqui, algumas quedas e protuberâncias,
não é exatamente um círculo. A razão disso é porque
talvez esteja sobreajustado. Temos muitos atributos,
e muitas camadas ocultas estavam encontrando uma função complexa
sobreajustada para esses dados. Há uma maneira para encontrar
um modelo mais simples? Sem engenharia de atributos, temos a regularização de L1
para poder usar isso. Vamos ver se funciona. Vou definir minha regularização
para L1, certo? Vou começar uma nova inicialização
e vamos ver como isso funciona. Olhe só, está muito melhor. Vamos investigar um pouco mais. Como você vê, ela aprendeu
um círculo muito mais suave, o que é ótimo, porque isso é intuitivo
com o que vemos nos dados. No entanto, na vida real, geralmente não temos distribuições
tão agradáveis ​​quanto essa. Portanto, talvez precisaremos usar isso
para muitos outros processos. E há atributos aqui, você vê que temos X1 ao quadrado
e X2 ao quadrado, e há ponderações. São praticamente as únicas ponderações
que têm mais alguma magnitude. Todos as outras estão esmaecidas com um valor zero. Isso vai para as camadas ocultas internas
onde você pode ver, X1 e X2 ao quadrado são praticamente
os únicos que se propagam. E todos vão para esse neurônio
na última camada, e, finalmente, para a saída. Então, é como se estivéssemos usando
apenas X1 e X2 ao quadrado porque eles são muito mais preditivos
no modelo do que os outros atributos. E devido à natureza de L1
e à provável distribuição, é capaz de reduzi-lo.