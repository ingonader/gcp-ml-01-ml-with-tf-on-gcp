1
00:00:00,000 --> 00:00:03,300
Wow! L1 Regularization really helped

2
00:00:03,300 --> 00:00:07,110
prune our complex model down into a much smaller generalizable model.

3
00:00:07,110 --> 00:00:08,870
We set out with all features selected,

4
00:00:08,870 --> 00:00:10,650
and two hidden layers between,

5
00:00:10,650 --> 00:00:13,615
which created a lot of connections represented by the lines in between.

6
00:00:13,615 --> 00:00:17,465
When we trained it, each of the weights were active, but pretty weak.

7
00:00:17,465 --> 00:00:20,330
We know there are a lot of features with very low parts of fower.

8
00:00:20,330 --> 00:00:24,280
Also, instead of seeing a nice circle like our eyes know that the data fits,

9
00:00:24,280 --> 00:00:29,180
we've got this sort of oblong misshapen circle that probably isn't generalized very well.

10
00:00:29,180 --> 00:00:33,360
Adding regularization, we saw the useless features all go to zero,

11
00:00:33,360 --> 00:00:35,750
with the lines becoming thin and greyed out.

12
00:00:35,750 --> 00:00:39,720
The only features that survived were x1 squared and x2 squared,

13
00:00:39,720 --> 00:00:43,530
which makes sense since those added together make the equation for a circle,

14
00:00:43,530 --> 00:00:45,900
which unsurprisingly is a shape it learns.

15
00:00:45,900 --> 00:00:48,380
Since we know this is the true distribution,

16
00:00:48,380 --> 00:00:51,110
we can be confident that our model will generalize well.