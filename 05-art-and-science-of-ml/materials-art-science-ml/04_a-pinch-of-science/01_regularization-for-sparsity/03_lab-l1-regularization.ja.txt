スパースで簡潔なモデルに含まれるL1正則化が
いかに重要かを見ていきましょう このL1正則化ラボでは 多数の疑似特徴を追加した結果
非常に複雑なモデルができています まずこのモデルを
L1正則化なしでトレーニングし 次に
もしL1正則化を行えば モデルがもっとスパースで簡潔かつ
より一般化しやすいものになるかどうか 見ていきます ではTensorFlowの画面に戻ります このラボではL1正則化によって モデルがよりスパースで簡潔に
なるかどうかを見ていきます ご覧のとおり
これは分類の問題であり この2つの形状を
分類することにします ここには2つの同心円があり 中央に青い円が
外側にはオレンジの円があります 幸いなことに
ノイズは設定されていないので トレーニングはさほど
難しくないはずです またここではすべての特徴が
オンになっています つまり非常に複雑な
モデルになるということです 円があるため
これは直感的にX二乗×Y二乗か X1二乗×X2二乗の類の
等式だとわかります しかしこれには追加の特徴を
多数加えています またここには追加の層があり
各々が6つのニューロンを含んでいます ですから極めて複雑です ではこれをL1正則化なしで
トレーニングしてみましょう 正則化はなしに設定します もうできました ご覧のとおりL1正則化で
データの分布がほぼ発見されました ただいくらかの
不整合が見られますね ここに少しくぼみがあり
こっちは少し膨らんでいて 厳密な円ではありません この理由はおそらく
過学習でしょう 特徴も隠れ層も
数が多すぎるため このデータの複雑な関数が
過学習されています これよりずっと単純なモデルを
見つける方法はあるでしょうか 特徴エンジニアリングを
行わなければ これをL1正則化で
利用することはできません ではそれを試してみましょう ここでは正則化を
L1に設定します では新たな初期化を開始して
どうなるか見てみましょう このように
かなり良くなりました もう少し
よく見てみましょう ご覧のとおり
ずっと滑らかな円が学習されました このほうが直感的に
データに適合しています しかし現実には このような良好な分布が
得られることは普通ありません ですからこれを他の多くのプロセスでも
利用する必要があります ここには特徴があり ここにX1二乗とX2二乗
そしてそれらの重みがあります これらはまだ大きさが残っている
ほぼ唯一の重みです 他の重みはこのとおり
値ゼロでグレー表示されています これが次にこの隠れ層に入ります ご覧のとおりほぼ
X1二乗とX2二乗のみが伝播しており すべてこの最後の層の
ニューロンに そして出力に到達しています まるでX1二乗とX2二乗だけを
使っているようです モデルの他の特徴よりも
はるかに予測性が高いからです そしてこれは
L1と確率分布の性質により 縮小が可能になっています