1
00:00:00,000 --> 00:00:05,975
スパースで簡潔なモデルに含まれるL1正則化が
いかに重要かを見ていきましょう

2
00:00:05,980 --> 00:00:07,740
このL1正則化ラボでは

3
00:00:07,740 --> 00:00:12,275
多数の疑似特徴を追加した結果
非常に複雑なモデルができています

4
00:00:12,275 --> 00:00:15,360
まずこのモデルを
L1正則化なしでトレーニングし

5
00:00:15,360 --> 00:00:18,015
次に
もしL1正則化を行えば

6
00:00:18,015 --> 00:00:23,150
モデルがもっとスパースで簡潔かつ
より一般化しやすいものになるかどうか

7
00:00:23,150 --> 00:00:24,345
見ていきます

8
00:00:24,345 --> 00:00:26,970
ではTensorFlowの画面に戻ります

9
00:00:26,970 --> 00:00:29,335
このラボではL1正則化によって

10
00:00:29,335 --> 00:00:32,715
モデルがよりスパースで簡潔に
なるかどうかを見ていきます

11
00:00:32,715 --> 00:00:35,235
ご覧のとおり
これは分類の問題であり

12
00:00:35,235 --> 00:00:37,770
この2つの形状を
分類することにします

13
00:00:37,770 --> 00:00:40,290
ここには2つの同心円があり

14
00:00:40,290 --> 00:00:45,330
中央に青い円が
外側にはオレンジの円があります

15
00:00:45,330 --> 00:00:48,250
幸いなことに
ノイズは設定されていないので

16
00:00:48,250 --> 00:00:51,465
トレーニングはさほど
難しくないはずです

17
00:00:51,465 --> 00:00:55,155
またここではすべての特徴が
オンになっています

18
00:00:55,155 --> 00:00:58,525
つまり非常に複雑な
モデルになるということです

19
00:00:58,525 --> 00:01:02,695
円があるため
これは直感的にX二乗×Y二乗か

20
00:01:02,695 --> 00:01:06,940
X1二乗×X2二乗の類の
等式だとわかります

21
00:01:06,940 --> 00:01:11,095
しかしこれには追加の特徴を
多数加えています

22
00:01:11,095 --> 00:01:15,680
またここには追加の層があり
各々が6つのニューロンを含んでいます

23
00:01:15,680 --> 00:01:17,930
ですから極めて複雑です

24
00:01:17,930 --> 00:01:21,860
ではこれをL1正則化なしで
トレーニングしてみましょう

25
00:01:21,860 --> 00:01:24,450
正則化はなしに設定します

26
00:01:26,010 --> 00:01:27,825
もうできました

27
00:01:27,825 --> 00:01:33,225
ご覧のとおりL1正則化で
データの分布がほぼ発見されました

28
00:01:33,225 --> 00:01:36,720
ただいくらかの
不整合が見られますね

29
00:01:36,720 --> 00:01:40,570
ここに少しくぼみがあり
こっちは少し膨らんでいて

30
00:01:40,570 --> 00:01:42,890
厳密な円ではありません

31
00:01:42,890 --> 00:01:45,640
この理由はおそらく
過学習でしょう

32
00:01:45,640 --> 00:01:49,250
特徴も隠れ層も
数が多すぎるため

33
00:01:49,250 --> 00:01:53,030
このデータの複雑な関数が
過学習されています

34
00:01:53,030 --> 00:01:56,800
これよりずっと単純なモデルを
見つける方法はあるでしょうか

35
00:01:56,800 --> 00:01:59,070
特徴エンジニアリングを
行わなければ

36
00:01:59,070 --> 00:02:01,565
これをL1正則化で
利用することはできません

37
00:02:01,565 --> 00:02:04,840
ではそれを試してみましょう

38
00:02:04,840 --> 00:02:08,490
ここでは正則化を
L1に設定します

39
00:02:08,490 --> 00:02:13,810
では新たな初期化を開始して
どうなるか見てみましょう

40
00:02:15,590 --> 00:02:19,440
このように
かなり良くなりました

41
00:02:19,440 --> 00:02:22,365
もう少し
よく見てみましょう

42
00:02:22,365 --> 00:02:25,770
ご覧のとおり
ずっと滑らかな円が学習されました

43
00:02:25,770 --> 00:02:28,800
このほうが直感的に
データに適合しています

44
00:02:28,800 --> 00:02:30,520
しかし現実には

45
00:02:30,520 --> 00:02:33,880
このような良好な分布が
得られることは普通ありません

46
00:02:33,880 --> 00:02:38,250
ですからこれを他の多くのプロセスでも
利用する必要があります

47
00:02:38,250 --> 00:02:39,870
ここには特徴があり

48
00:02:39,870 --> 00:02:44,010
ここにX1二乗とX2二乗
そしてそれらの重みがあります

49
00:02:44,010 --> 00:02:47,480
これらはまだ大きさが残っている
ほぼ唯一の重みです

50
00:02:47,480 --> 00:02:50,940
他の重みはこのとおり
値ゼロでグレー表示されています

51
00:02:50,940 --> 00:02:56,300
これが次にこの隠れ層に入ります

52
00:02:56,300 --> 00:03:01,380
ご覧のとおりほぼ
X1二乗とX2二乗のみが伝播しており

53
00:03:01,380 --> 00:03:03,990
すべてこの最後の層の
ニューロンに

54
00:03:03,990 --> 00:03:06,380
そして出力に到達しています

55
00:03:06,380 --> 00:03:10,360
まるでX1二乗とX2二乗だけを
使っているようです

56
00:03:10,360 --> 00:03:14,080
モデルの他の特徴よりも
はるかに予測性が高いからです

57
00:03:14,080 --> 00:03:16,995
そしてこれは
L1と確率分布の性質により

58
00:03:16,995 --> 00:03:19,090
縮小が可能になっています