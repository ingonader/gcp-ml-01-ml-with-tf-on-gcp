Wow! L1-Regularisierung hat wirklich geholfen, unser komplexes Modell
zu einem viel kleineren, verallgemeinerbareren Modell zu kürzen. Alle Eigenschaften und zwei versteckte Ebenen dazwischen haben viele Verbindungen
kreiert, dargestellt durch die Linien. Als wir es trainiert haben, waren alle
Gewichte aktiv, aber ziemlich schwach. Wir wissen, dass viele
Eigenschaften wenig Ausprägung haben. Statt eines schönen Kreises, den
unsere Augen von den Daten erwarten, ist da ein länglicher, unförmiger Kreis,
der wohl nicht sehr verallgemeinerbar ist. Durch Regularisierung fielen
alle unnützen Eigenschaften auf null und die Linien wurden dünn und ausgegraut. Die einzigen verbliebenen
Eigenschaften waren x1² und x2², was Sinn macht, weil diese zusammen
die Gleichung für einen Kreis ergeben. Diese Form kann es natürlich lernen. Da wir wissen, dass dies
die wahre Distribution ist, lässt sich unser Modell
sicherlich gut verallgemeinern.