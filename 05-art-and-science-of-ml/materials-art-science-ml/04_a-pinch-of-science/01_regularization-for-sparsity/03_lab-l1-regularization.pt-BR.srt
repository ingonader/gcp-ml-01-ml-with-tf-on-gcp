1
00:00:00,000 --> 00:00:01,795
Vamos ver agora como a

2
00:00:01,795 --> 00:00:05,760
regularização de L1 pode ser importante na
criação de modelos concisos e esparsos.

3
00:00:05,760 --> 00:00:07,740
Neste laboratório de regularização de L1,

4
00:00:07,740 --> 00:00:12,275
adicionei muitos atributos espúrios e
criei um modelo razoavelmente complexo.

5
00:00:12,275 --> 00:00:15,360
Primeiro, vamos treinar o modelo
sem a regularização de L1,

6
00:00:15,360 --> 00:00:18,015
e depois veremos se ela

7
00:00:18,015 --> 00:00:21,210
ajuda a deixar o modelo em uma forma
muito mais esparsa,

8
00:00:21,210 --> 00:00:24,345
concisa e generalizável.

9
00:00:24,345 --> 00:00:26,970
Olá, seja bem-vindo de volta
ao TensorFlow Playground.

10
00:00:26,970 --> 00:00:29,085
Neste laboratório, vamos ver

11
00:00:29,085 --> 00:00:32,595
se a regularização de L1 pode tornar
os modelos mais esparsos e concisos.

12
00:00:32,595 --> 00:00:35,235
Como você pode ver,
este é um problema de classificação,

13
00:00:35,235 --> 00:00:37,770
em que vamos tentar classificar
essas duas formas.

14
00:00:37,770 --> 00:00:40,290
Temos aqui dois círculos concêntricos,

15
00:00:40,290 --> 00:00:45,330
o círculo azul no meio e
o círculo laranja no lado de fora.

16
00:00:45,330 --> 00:00:48,000
A boa notícia é que
não há ruído definido.

17
00:00:48,000 --> 00:00:51,215
Portanto, será fácil de forçar.

18
00:00:51,215 --> 00:00:53,550
O que você também pode notar
é que há atributos.

19
00:00:53,550 --> 00:00:55,205
Todos eles estão ativados.

20
00:00:55,205 --> 00:00:57,735
Isso significa que será
um modelo bastante complicado.

21
00:00:58,595 --> 00:01:02,695
Sabemos intuitivamente que este é
um X ao quadrado por Y ao quadrado,

22
00:01:02,695 --> 00:01:06,940
ou X1 ao quadrado pelo tipo de equação
de X2 ao quadrado porque há círculos.

23
00:01:06,940 --> 00:01:11,095
No entanto, temos outros
atributos extras adicionados a isso.

24
00:01:11,095 --> 00:01:15,680
Também temos camadas extras aqui
com seis neurônios cada.

25
00:01:15,680 --> 00:01:17,930
Isso é muito complexo.

26
00:01:17,930 --> 00:01:21,860
Vamos ver como isso treina
sem regularização de L1,

27
00:01:21,860 --> 00:01:23,930
definida como nenhuma neste caso.

28
00:01:26,180 --> 00:01:27,615
Isso foi bem rápido.

29
00:01:27,615 --> 00:01:33,225
Como você vê, a regularização de L1
encontrou a distribuição de nossos dados.

30
00:01:33,225 --> 00:01:38,060
No entanto, você pode notar que há
algumas inconsistências aqui,

31
00:01:38,060 --> 00:01:42,800
algumas quedas e protuberâncias,
não é exatamente um círculo.

32
00:01:42,800 --> 00:01:45,640
A razão disso é porque
talvez esteja sobreajustado.

33
00:01:45,640 --> 00:01:49,250
Temos muitos atributos,
e muitas camadas ocultas

34
00:01:49,250 --> 00:01:53,030
estavam encontrando uma função complexa
sobreajustada para esses dados.

35
00:01:53,030 --> 00:01:56,800
Há uma maneira para encontrar
um modelo mais simples?

36
00:01:56,800 --> 00:01:59,010
Sem engenharia de atributos,

37
00:01:59,010 --> 00:02:01,605
temos a regularização de L1
para poder usar isso.

38
00:02:01,605 --> 00:02:03,110
Vamos ver se funciona.

39
00:02:04,210 --> 00:02:08,490
Vou definir minha regularização
para L1, certo?

40
00:02:08,490 --> 00:02:13,570
Vou começar uma nova inicialização
e vamos ver como isso funciona.

41
00:02:16,370 --> 00:02:19,140
Olhe só, está muito melhor.

42
00:02:20,420 --> 00:02:22,365
Vamos investigar um pouco mais.

43
00:02:22,615 --> 00:02:25,770
Como você vê, ela aprendeu
um círculo muito mais suave,

44
00:02:25,770 --> 00:02:28,820
o que é ótimo, porque isso é intuitivo
com o que vemos nos dados.

45
00:02:29,390 --> 00:02:31,050
No entanto, na vida real,

46
00:02:31,050 --> 00:02:34,240
geralmente não temos distribuições
tão agradáveis ​​quanto essa.

47
00:02:34,240 --> 00:02:38,000
Portanto, talvez precisaremos usar isso
para muitos outros processos.

48
00:02:38,590 --> 00:02:39,870
E há atributos aqui,

49
00:02:39,870 --> 00:02:43,990
você vê que temos X1 ao quadrado
e X2 ao quadrado, e há ponderações.

50
00:02:43,990 --> 00:02:47,260
São praticamente as únicas ponderações
que têm mais alguma magnitude.

51
00:02:47,260 --> 00:02:48,500
Todos as outras

52
00:02:48,500 --> 00:02:50,505
estão esmaecidas com um valor zero.

53
00:02:51,335 --> 00:02:56,940
Isso vai para as camadas ocultas internas
onde você pode ver,

54
00:02:56,940 --> 00:03:01,940
X1 e X2 ao quadrado são praticamente
os únicos que se propagam.

55
00:03:01,940 --> 00:03:04,230
E todos vão para esse neurônio
na última camada,

56
00:03:04,230 --> 00:03:06,000
e, finalmente, para a saída.

57
00:03:06,000 --> 00:03:10,120
Então, é como se estivéssemos usando
apenas X1 e X2 ao quadrado

58
00:03:10,120 --> 00:03:14,080
porque eles são muito mais preditivos
no modelo do que os outros atributos.

59
00:03:14,080 --> 00:03:17,205
E devido à natureza de L1
e à provável distribuição,

60
00:03:17,202 --> 00:03:18,887
é capaz de reduzi-lo.