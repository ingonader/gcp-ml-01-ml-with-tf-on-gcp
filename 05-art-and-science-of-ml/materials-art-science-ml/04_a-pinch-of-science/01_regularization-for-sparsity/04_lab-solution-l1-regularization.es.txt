Vaya, la regularización L1
realmente nos ayudó a reducir nuestro complejo modelo
a uno más pequeño y generalizable. Seleccionamos todos los atributos
y dos capas ocultas en medio que crearon muchas conexiones,
representadas por las líneas intermedias. Cuando lo entrenamos todos los pesos estaban activos,
pero eran débiles. Había muchos atributos
con poco poder de predicción. Además, en vez de ver un lindo círculo 
que nos dice que los datos calzan tenemos esta forma oblonga que quizá no se generalizó bien. Tras la regularización,
los atributos inútiles se fueron a cero y sus líneas
son delgadas y se inhabilitaron. Los únicos atributos que
sobrevivieron fueron x1 y x2 al cuadrado lo que tiene sentido, ya que sumadas
forman la ecuación de un círculo que es una forma que aprende. Como sabemos
que es la distribución verdadera podemos estar seguros
de que el modelo generalizará bien.