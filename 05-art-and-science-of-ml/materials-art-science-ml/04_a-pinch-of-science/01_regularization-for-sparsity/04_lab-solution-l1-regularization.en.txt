Wow! L1 Regularization really helped prune our complex model down into a much smaller generalizable model. We set out with all features selected, and two hidden layers between, which created a lot of connections represented by the lines in between. When we trained it, each of the weights were active, but pretty weak. We know there are a lot of features with very low parts of fower. Also, instead of seeing a nice circle like our eyes know that the data fits, we've got this sort of oblong misshapen circle that probably isn't generalized very well. Adding regularization, we saw the useless features all go to zero, with the lines becoming thin and greyed out. The only features that survived were x1 squared and x2 squared, which makes sense since those added together make the equation for a circle, which unsurprisingly is a shape it learns. Since we know this is the true distribution, we can be confident that our model will generalize well.