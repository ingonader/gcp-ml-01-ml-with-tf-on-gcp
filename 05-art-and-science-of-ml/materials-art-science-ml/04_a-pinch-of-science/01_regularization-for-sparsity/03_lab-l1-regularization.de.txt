Lassen Sie uns jetzt selbst sehen,
wie wichtig L1-Regularisierung sein kann, um dünnbesetzte,
präzise Modelle zu erschaffen. In dieses L1-Regularisierungslab habe ich viele
störende Eigenschaften aufgenommen, und es ist ein ziemlich 
komplexes Modell entstanden. Zuerst trainieren wir
das Modell ohne L1-Regularisierung und danach 
sehen wir, ob L1-Regularisierung dabei hilft, das Modell 
in eine viel dünner besetzte, präzisere und hoffentlich
verallgemeinerbarere Form zu stutzen. Hi, willkommen zurück
in TensorFlow Playground. In diesem Lab werden wir sehen, ob L1-Regularisierung hilft, 
unsere Modelle dünner besetzt und präziser zu machen. Wie Sie sehen 
ist dies ein Klassifizierungsproblem, bei dem wir versuchen,
diese zwei Formen zu klassifizieren Wir haben hier zwei konzentrische Kreise, der blaue Kreis in 
der Mitte und der orange Kreis außen. Die gute Nachricht ist,
es ist kein Noise gesetzt. Die Darstellung 
sollte also relativ leicht gehen. Sie können auch die Eigenschaften sehen. Alle Eigenschaften sind eingeschaltet. Das heißt, es wäre
ein ziemlich kompliziertes Modell. Wir wissen intuitiv, dass 
diese Gleichung x² geteilt durch y² oder x1² geteilt durch x2²
lauten wird, weil Kreise auftreten. Es gibt hier aber noch 
eine ganze Reihe anderer Eigenschaften. Wir haben hier auch
zusätzliche Ebenen mit je sechs Neuronen. Es ist also hochkomplex. Schauen wir, wie es
ohne L1-Regularisierung trainiert, die wir in diesem Fall abschalten. Das ging ja schnell. Sie sehen hier, 
dass die L1-Regularisierung die Distribution unserer
Daten größtenteils gefunden hat. Trotzdem können Sie hier
einige Inkonsistenzen erkennen, ein paar kleine Senkungen hier
und Beulen dort, also kein echter Kreis. Der Grund ist vielleicht,
dass es überangepasst ist. Wir haben also viel zu viele
Eigenschaften und versteckte Ebenen und erhalten für diese Daten eine
überangepasste, komplexe Funktion. Haben wir die Möglichkeit,
ein viel einfacheres Modell zu finden? Nun, ohne selbst
Feature Engineering anzuwenden, können wir es mit
L1-Regularisierung versuchen. Schauen wir, ob es funktioniert. Ich setze hier 
meine Regularisierung auf L1, starte eine neue Initialisierung und
wir werden sehen, wie sie sich schlägt. Schauen Sie, das ist viel besser. Lassen Sie uns das
ein bisschen mehr untersuchen. Wie Sie hier sehen, hat es
einen viel gleichmäßigeren Kreis gelernt, was gut zu unserer intuitiven
Einschätzung der Daten passt. Im echten Leben haben wir allerdings meist nicht so eine
schöne Distribution wie hier. Darum könnten wir dies
auf viele andere Prozesse anwenden müssen. Was unsere Eigenschaften betrifft, können Sie sehen, dass wir
x1² und x2² haben, sowie ihre Gewichte, eigentlich die einzigen
Gewichte die noch einen Betrag haben. Alle anderen Gewichte
sind ausgegraut mit einem Wert von null. Dies geht dann hier zu den
versteckten Zwischenebenen, wo Sie sehen, x1 und x2 sind letztlich
die einzigen, die sich weiterverbreiten. Und sie gehen alle zu
diesem Neuron der letzten Ebene und schließlich zur Ausgabe. Es ist also, als würden wir
nur x1² und x2² verwenden, weil sie in unserem
Modell viel prognostischer sind als andere Eigenschaften. und durch ihre Besonderheit
in der Wahrscheinlichkeitsverteilung kann L1 sie verkleinern.