1
00:00:00,000 --> 00:00:00,990
Wow!

2
00:00:00,990 --> 00:00:03,500
L1-Regularisierung hat wirklich geholfen,

3
00:00:03,500 --> 00:00:05,695
unser komplexes Modell
zu einem viel kleineren,

4
00:00:05,695 --> 00:00:07,550
verallgemeinerbareren Modell zu kürzen.

5
00:00:07,550 --> 00:00:08,870
Alle Eigenschaften

6
00:00:08,870 --> 00:00:10,650
und zwei versteckte Ebenen dazwischen

7
00:00:10,650 --> 00:00:13,615
haben viele Verbindungen
kreiert, dargestellt durch die Linien.

8
00:00:13,615 --> 00:00:17,465
Als wir es trainiert haben, waren alle
Gewichte aktiv, aber ziemlich schwach.

9
00:00:17,465 --> 00:00:20,530
Wir wissen, dass viele
Eigenschaften wenig Ausprägung haben.

10
00:00:20,530 --> 00:00:24,280
Statt eines schönen Kreises, den
unsere Augen von den Daten erwarten,

11
00:00:24,280 --> 00:00:29,410
ist da ein länglicher, unförmiger Kreis,
der wohl nicht sehr verallgemeinerbar ist.

12
00:00:29,410 --> 00:00:33,360
Durch Regularisierung fielen
alle unnützen Eigenschaften auf null

13
00:00:33,360 --> 00:00:35,950
und die Linien wurden dünn und ausgegraut.

14
00:00:35,950 --> 00:00:39,720
Die einzigen verbliebenen
Eigenschaften waren x1² und x2²,

15
00:00:39,720 --> 00:00:43,530
was Sinn macht, weil diese zusammen
die Gleichung für einen Kreis ergeben.

16
00:00:43,530 --> 00:00:45,900
Diese Form kann es natürlich lernen.

17
00:00:45,900 --> 00:00:48,380
Da wir wissen, dass dies
die wahre Distribution ist,

18
00:00:48,380 --> 00:00:51,110
lässt sich unser Modell
sicherlich gut verallgemeinern.