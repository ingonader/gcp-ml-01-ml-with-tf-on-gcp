正則化を利用して よりスパースでシンプルな
モデルを作成する方法を見ていきましょう コースの初めにL2回帰について
紹介しましたが これは最後の関数の
パラメータ重みの2次の項の合計に加えます これは重みを小さく保ち
不安定性と一意解を得るのに有効ですが モデルが不要に大きく
複雑になる場合があります すべての特徴の重みがやや
小さいままの場合があるからです 代わりにL1正則化と
呼ばれるものを使用して パラメータ重みの絶対値の合計を
最後の関数に加えると あまり保護されていない特徴の重みは
多くの場合ゼロになります これはビルトインの特徴選択機能となり 不良な特徴を排除して
最も強力なものだけを残します このスパースモデルには
多くの利点があります まず保管してロードする
係数の数が少ないため モデルサイズが大幅に縮小し
必要なストレージとメモリが減ります これは埋め込みモデルでは
特に重要です また特徴の数が少ないため マルチ広告の数が大幅に減り
トレーニングの速度が上がるだけでなく さらに重要なのは
予測の速度が上がることです 多くの機械学習モデルはすでに
多数の特徴を備えています たとえば受注した注文の日時を含む
データがあるとして 1つ目の注文モデルにはおそらく 曜日ごとに7つの特徴と
時間ごとに24の特徴があり さらにその他多くの
特徴も含まれています そのため曜日と時間だけで
すでに31の入力情報があります ではここで
曜日の2番目の注文の効果を 時刻とクロスさせると
どうなるでしょうか もともとの31プラスその他に加え 168の入力が加わるため
その1つの日時フィールドだけでも 特徴の合計はほぼ200に達し
さらにその他の特徴も加わります これをたとえば米国の州に関する
HUDエンコードにクロスさせると 特徴のデカルト三重積は
8400となり その多くは非常にスパースで
ゼロばかり含むことになります これでL1正則化による
ビルトインの特徴選択が 有益である理由が
わかると思います L1正則化のほかには
どんな戦略を使用すれば 有用でない特徴の係数を
削除できるでしょうか たとえば単純にゼロ以外の値を含む
特徴の数を使用できます L0ノルムは単に
ゼロでない重みの数であり このノルムを最適化することは
NP困難な非凸最適化問題です この図は非凸最適化の面誤差の
一例を示しています 局所的な山と谷が
多く見られますが これは単純な1次元の例です ここでは非常に多くの
開始点を検討しなければならず これは完全に解決するべき
NP困難問題となっています 幸いL1ノルムは
L2ノルムと同様に凸関数ですが モデルではスパースが
推奨されています この図はL1ノルムとL2ノルムの
確率分布を示しています ゼロではL2ノルムのほうが
ピークがはるかに滑らかなので 重みの大きさが
よりゼロに近くなっています しかしL1ノルムのほうが
より明確にゼロを中心としているため 厳密にゼロになる確率は
L2ノルムよりもずっと高くなります Pノルムによって一般化される
ノルムの数は無限です 他の一部のノルムや
すでにお話しした ベクトルに含まれる非ゼロ値の数を示す
L0ノルムもそうですし ベクトル内のあらゆる値の
最大絶対値であるL無限ノルムもそうです ただし実際には
L2ノルムは通常 L1ノルムよりも
一般化可能なモデルを提供します ただしL1でなくL2を使用すると
モデルははるかに複雑で重くなります これは特徴同士に高い相関性が
あることが多いからです L1正則化ではその一方が使用され
他方は破棄されますが L2正則化では両方の特徴が維持され
それらの重みは小さく保たれます そのためL1ではモデルは小さくなりますが
予測性は下がることがあります この両方の利点を得る
方法はないでしょうか Elastic Netは単にL1とL2の正則化の
ペナルティを線形結合したものです これにより予測性が非常に低い特徴は
スパース性の利点が得られると同時に 重みが小さい有効な特徴は
良好な一般化が可能になります 唯一のトレードオフは 調整すべきハイパーパラメータが
1つでなく2つで 異なる2つのLambda
正則化パラメータがあることです モデルの予測性が低い特徴の
パラメータ重みは L1正則化ではどうなることが
多いでしょうか 正解は値がゼロになることです 正則化手法を用いるときは必ず 最後の関数 一般には目的関数に
ペナルティ項を加えて 決定変数やパラメータの重みが
最適化されすぎないようにします ペナルティ項は事前の知識や
関数の形状などに基づいて選択します L1正則化はモデルに
スパース性をもたらすとともに 確率分布ではゼロのピークが
高いことが示されており 非常に予測性の高い重み以外は ほぼ非正規化値からゼロにシフトされます L2正則化は大きさを
小さくするために使用され その負数はそれを大きくするために
使用されますが これは両方とも不正確です すべての値を正にするのは 最適化の問題に追加の制約を
多く加えて すべての決定変数を
ゼロ超えにするようなものであり これもL1正則化ではありません