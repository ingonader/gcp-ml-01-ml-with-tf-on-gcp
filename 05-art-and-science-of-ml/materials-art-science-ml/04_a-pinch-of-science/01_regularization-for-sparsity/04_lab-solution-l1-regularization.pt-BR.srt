1
00:00:00,000 --> 00:00:03,300
Uau! A regularização L1 ajudou mesmo

2
00:00:03,300 --> 00:00:06,960
a reduzir nosso modelo complexo
para um modelo generalizável muito menor.

3
00:00:06,960 --> 00:00:09,010
Partimos com todos
os atributos selecionados

4
00:00:09,010 --> 00:00:10,650
e duas camadas ocultas entre elas,

5
00:00:10,650 --> 00:00:13,935
o que criou muitas conexões representadas
pelas linhas intermediárias.

6
00:00:13,935 --> 00:00:17,465
Quando treinamos, cada uma das
ponderações estava ativa, mas muito fraca.

7
00:00:17,465 --> 00:00:20,520
Sabemos que há muitos atributos
com partes muito baixas de fower.

8
00:00:20,520 --> 00:00:24,280
Além disso, em vez de ver um belo círculo
como sabemos que os dados se encaixam,

9
00:00:24,280 --> 00:00:26,800
temos esse tipo de círculo
disforme e oblongo

10
00:00:26,800 --> 00:00:29,180
que provavelmente
não é generalizado muito bem.

11
00:00:29,180 --> 00:00:33,360
Adicionando regularização, vimos todos
os atributos inúteis chegarem a zero,

12
00:00:33,360 --> 00:00:35,750
com as linhas ficando finas e esmaecidas.

13
00:00:35,750 --> 00:00:39,720
Os únicos atributos que sobreviveram
foram x1 ao quadrado e x2 ao quadrado,

14
00:00:39,720 --> 00:00:43,530
o que faz sentido, já que eles somados
formam a equação de um círculo,

15
00:00:43,530 --> 00:00:45,900
o que, sem surpresa,
é uma forma que ele aprende.

16
00:00:45,900 --> 00:00:48,380
Como sabemos
que essa é a verdadeira distribuição,

17
00:00:48,368 --> 00:00:51,088
podemos ter certeza de que
o modelo será bem generalizado.