1
00:00:00,000 --> 00:00:02,550
Primeiro, vamos falar sobre
como podemos usar

2
00:00:02,550 --> 00:00:06,150
a regularização para criar modelos
mais esparsos e mais simples.

3
00:00:06,150 --> 00:00:09,215
No início do curso, aprendemos
sobre a regularização de L2,

4
00:00:09,215 --> 00:00:11,590
que é adicionada à soma
dos termos das ponderações

5
00:00:11,590 --> 00:00:13,830
dos parâmetros quadrados
para a função de perda.

6
00:00:13,830 --> 00:00:16,235
Isso foi ótimo para manter
as ponderações pequenas,

7
00:00:16,235 --> 00:00:18,485
ter estabilidade e uma solução única,

8
00:00:18,485 --> 00:00:21,665
mas pode deixar o modelo
desnecessariamente grande e complexo,

9
00:00:21,665 --> 00:00:25,500
já que todos os atributos ainda podem
permanecer com ponderações pequenas.

10
00:00:26,640 --> 00:00:29,805
Usar algo chamado de regularização L1

11
00:00:29,805 --> 00:00:33,965
adiciona a soma do valor absoluto que o
parâmetro pondera à função de perda,

12
00:00:33,965 --> 00:00:37,990
o que tende a forçar as ponderações de
atributos não muito previstos para zero.

13
00:00:37,990 --> 00:00:41,100
Isso funciona como um seletor
de atributos integrado,

14
00:00:41,100 --> 00:00:45,005
eliminando todos os atributos inválidos e
deixando apenas os mais fortes no modelo.

15
00:00:45,005 --> 00:00:47,585
Este modelo esparso tem muitos benefícios.

16
00:00:47,585 --> 00:00:50,760
Primeiro, com menos coeficiente
para armazenar e carregar,

17
00:00:50,760 --> 00:00:53,345
há uma redução no armazenamento
e na memória necessária

18
00:00:53,345 --> 00:00:54,925
com um tamanho de modelo menor,

19
00:00:54,925 --> 00:00:57,730
o que é especialmente importante
para modelos incorporados.

20
00:00:57,730 --> 00:00:59,830
Além disso, com menos atributos,

21
00:00:59,830 --> 00:01:03,755
há muito menos anúncios múltiplos,
o que leva a mais velocidade de treino,

22
00:01:03,755 --> 00:01:06,640
e também aumenta a velocidade de previsão.

23
00:01:06,640 --> 00:01:10,430
Muitos modelos de aprendizado de máquina
já tem atributos suficientes.

24
00:01:10,430 --> 00:01:12,300
Por exemplo, digamos que
eu tenha dados

25
00:01:12,300 --> 00:01:15,365
que contenham a data/hora dos
pedidos que estão sendo colocados.

26
00:01:15,365 --> 00:01:16,840
Nosso primeiro modelo de pedido

27
00:01:16,840 --> 00:01:19,550
provavelmente incluiria 7 atributos
para os dias da semana

28
00:01:19,550 --> 00:01:21,480
e 24 para as horas do dia,

29
00:01:21,480 --> 00:01:23,340
além de outros possíveis.

30
00:01:23,340 --> 00:01:28,975
Então, apenas o dia da semana mais a hora
já somam 31 entradas.

31
00:01:30,335 --> 00:01:31,970
E se quisermos analisar

32
00:01:31,970 --> 00:01:35,670
os efeitos de segunda ordem do dia
da semana e cruzar com a hora do dia?

33
00:01:35,670 --> 00:01:38,440
Há outras 168 entradas

34
00:01:38,440 --> 00:01:43,210
além das nossas 31, mais outras para
um total de quase 200 atributos,

35
00:01:43,210 --> 00:01:47,410
apenas para o campo de data/hora, além
dos outros atributos que estamos usando.

36
00:01:48,130 --> 00:01:51,600
Se cruzarmos isso com uma codificação
one-hot para os EUA, por exemplo,

37
00:01:51,600 --> 00:01:53,840
o produto cartesiano triplo já está

38
00:01:53,840 --> 00:01:58,885
em 8.400 atributos, e muitos estão
provavelmente muito esparsos e com zeros.

39
00:01:58,885 --> 00:02:02,340
Isso deixa claro por que
a seleção de atributos integrados

40
00:02:02,340 --> 00:02:05,390
por meio da regularização de L1
pode ser algo muito bom.

41
00:02:05,930 --> 00:02:09,169
Quais estratégias podemos usar para
remover coeficientes de atributos

42
00:02:09,169 --> 00:02:12,095
que não são úteis além
da regularização de L1, talvez?

43
00:02:13,605 --> 00:02:16,630
Poderíamos incluir contagens simples
de quais atributos ocorrem

44
00:02:16,630 --> 00:02:18,480
com valores diferentes de zero.

45
00:02:19,350 --> 00:02:22,760
A norma-L0 é a contagem
das ponderações diferentes de zero,

46
00:02:22,760 --> 00:02:27,770
e a otimização para ela é um problema
de otimização NP-hard não convexo.

47
00:02:27,770 --> 00:02:32,735
Este diagrama ilustra como uma superfície
de erro de otimização não convexa parece.

48
00:02:32,735 --> 00:02:35,870
Como você vê, há muitos
picos e vales locais,

49
00:02:35,870 --> 00:02:38,500
e este é um exemplo simples
de uma dimensão.

50
00:02:38,500 --> 00:02:42,510
Você teve que explorar muitos pontos
de partida com o gradiente descendente,

51
00:02:42,510 --> 00:02:45,715
tornando este um problema NP-hard
para resolver completamente.

52
00:02:45,715 --> 00:02:50,805
Felizmente, a norma L1, assim como
a norma L2, é convexa,

53
00:02:50,805 --> 00:02:53,500
mas também encoraja a dispersão no modelo.

54
00:02:53,500 --> 00:02:55,960
Na imagem,
as distribuições de probabilidade

55
00:02:55,960 --> 00:02:58,560
das normas L1 e L2 são plotadas.

56
00:02:58,560 --> 00:03:02,445
Observe como a norma L2 tem um pico
muito mais suave em zero,

57
00:03:02,445 --> 00:03:05,595
o que resulta em magnitudes
das ponderações mais próximas de zero.

58
00:03:05,595 --> 00:03:10,060
Porém, você notará que a norma L1 é
mais uma cúspide centrada em zero.

59
00:03:10,060 --> 00:03:15,330
Portanto, maior a probabilidade de ser
exatamente no zero do que a norma L2.

60
00:03:15,330 --> 00:03:19,315
Há um número infinito de normas
que são generalizadas pela norma P.

61
00:03:19,315 --> 00:03:22,260
Algumas outras, como a norma L0
que já abordamos,

62
00:03:22,260 --> 00:03:26,635
que é a contagem dos valores diferentes
de zero em um vetor, e a norma L infinito,

63
00:03:26,635 --> 00:03:30,135
que é o valor absoluto máximo
de qualquer valor em um vetor.

64
00:03:30,135 --> 00:03:32,740
Na prática, porém, normalmente a norma L2

65
00:03:32,740 --> 00:03:35,905
fornece modelos mais generalizáveis
qu​e a norma L1.

66
00:03:35,905 --> 00:03:41,590
Mas acabaremos com modelos pesados ​​mais
complexos se usarmos L2 em vez de L1.

67
00:03:41,590 --> 00:03:45,710
Isso acontece porque, muitas vezes,
os atributos têm alta correlação entre si

68
00:03:45,710 --> 00:03:53,095
e a regularização de L1 usa um deles e
descarta o outro, enquanto a L2

69
00:03:53,095 --> 00:03:56,320
mantém os dois atributos e as magnitudes
de ponderações pequenas.

70
00:03:56,320 --> 00:04:01,120
Com L1, você pode acabar com um modelo
menor, mas pode ser menos preditivo.

71
00:04:01,120 --> 00:04:04,015
Há alguma maneira de conseguir
o melhor das duas?

72
00:04:04,015 --> 00:04:09,870
A rede elástica é só uma combinação linear
das penalidades de regularização L1 e L2.

73
00:04:09,870 --> 00:04:14,330
Assim, temos os benefícios da esparsidade
para atributos preditivos incorretos

74
00:04:14,330 --> 00:04:16,879
e, ao mesmo tempo, mantemos
atributos corretos

75
00:04:16,879 --> 00:04:19,554
com ponderações menores para
uma boa generalização.

76
00:04:19,554 --> 00:04:21,804
A única contrapartida agora
é que há dois

77
00:04:21,804 --> 00:04:24,380
em vez de um hiperparâmetro
para ajustar,

78
00:04:24,380 --> 00:04:27,430
com os dois parâmetros de regularização
do Lambda diferentes.

79
00:04:27,430 --> 00:04:30,630
O que a regularização de L1 tende a fazer

80
00:04:30,630 --> 00:04:34,270
com as ponderações de parâmetros de
atributos preditivos baixos de um modelo?

81
00:04:35,100 --> 00:04:37,780
A resposta correta é ter valores zero.

82
00:04:37,780 --> 00:04:39,950
Sempre que fazemos 
técnicas de regularização,

83
00:04:39,950 --> 00:04:44,495
estamos adicionando um termo de penalidade
à função de perda ou à função objetiva,

84
00:04:44,495 --> 00:04:48,310
para não otimizar demais as variáveis de
decisão ou as ponderações de parâmetros.

85
00:04:48,310 --> 00:04:51,510
Escolhemos os termos de penalidade com
base no conhecimento prévio,

86
00:04:51,510 --> 00:04:53,000
função, forma etc.

87
00:04:53,000 --> 00:04:55,500
A regularização de L1 mostrou induzir

88
00:04:55,500 --> 00:04:58,510
esparsidade ao modelo e, devido
à distribuição de probabilidade,

89
00:04:58,510 --> 00:04:59,925
ter um pico alto em zero.

90
00:04:59,925 --> 00:05:02,710
A maioria das ponderações, exceto
as altamente preditivas,

91
00:05:02,710 --> 00:05:05,380
serão deslocadas dos valores
não regularizados para zero.

92
00:05:05,920 --> 00:05:09,659
A regularização de L2 será usada para ter
pequenas magnitudes,

93
00:05:09,659 --> 00:05:14,555
e o negativo seria usado para ter grandes
magnitudes, que são ambas incorretas.

94
00:05:14,555 --> 00:05:16,725
Ter todos os valores positivos seria como

95
00:05:16,725 --> 00:05:19,810
adicionar muitas restrições
adicionais ao problema de otimização,

96
00:05:19,810 --> 00:05:22,100
limitando todas as
variáveis ​​de decisão a serem

97
00:05:22,100 --> 00:05:25,730
maiores que zero, o que também
não é a regularização de L1.