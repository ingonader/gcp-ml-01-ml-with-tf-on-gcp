Uau! A regularização L1 ajudou mesmo a reduzir nosso modelo complexo
para um modelo generalizável muito menor. Partimos com todos
os atributos selecionados e duas camadas ocultas entre elas, o que criou muitas conexões representadas
pelas linhas intermediárias. Quando treinamos, cada uma das
ponderações estava ativa, mas muito fraca. Sabemos que há muitos atributos
com partes muito baixas de fower. Além disso, em vez de ver um belo círculo
como sabemos que os dados se encaixam, temos esse tipo de círculo
disforme e oblongo que provavelmente
não é generalizado muito bem. Adicionando regularização, vimos todos
os atributos inúteis chegarem a zero, com as linhas ficando finas e esmaecidas. Os únicos atributos que sobreviveram
foram x1 ao quadrado e x2 ao quadrado, o que faz sentido, já que eles somados
formam a equação de um círculo, o que, sem surpresa,
é uma forma que ele aprende. Como sabemos
que essa é a verdadeira distribuição, podemos ter certeza de que
o modelo será bem generalizado.