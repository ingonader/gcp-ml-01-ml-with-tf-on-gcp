1
00:00:00,270 --> 00:00:03,300
Incroyable !
La régularisation L1 nous a bien aidés

2
00:00:03,300 --> 00:00:07,110
à réduire notre modèle complexe
en un modèle plus petit et généralisable.

3
00:00:07,110 --> 00:00:11,090
Toutes les caractéristiques sélectionnées
et deux couches cachées ont créé

4
00:00:11,090 --> 00:00:14,045
de nombreuses connexions
représentées par les lignes du milieu.

5
00:00:14,045 --> 00:00:17,465
À l'entraînement, les pondérations
étaient actives, mais plutôt faibles.

6
00:00:17,465 --> 00:00:20,660
Il y a donc plusieurs caractéristiques
avec des parties très basses.

7
00:00:20,660 --> 00:00:24,280
Au lieu de voir un beau cercle
qui correspondrait à nos données,

8
00:00:24,280 --> 00:00:29,180
nous avons une sorte de cercle/rectangle
biscornu, probablement peu généralisé.

9
00:00:29,180 --> 00:00:33,360
Avec la régularisation, les caractéristiques
inutiles sont toutes tombées à zéro,

10
00:00:33,360 --> 00:00:35,750
et les lignes se sont réduites et grisées.

11
00:00:35,750 --> 00:00:39,720
Les seules caractéristiques
à avoir survécu sont X1² et X2².

12
00:00:39,720 --> 00:00:43,050
Logique : ce sont celles qui, ajoutées,
forment l'équation d'un cercle,

13
00:00:43,050 --> 00:00:46,030
qui sans surprise est une forme
que le modèle apprend.

14
00:00:46,030 --> 00:00:48,220
Puisqu'il s'agit d'une vraie distribution,

15
00:00:48,220 --> 00:00:51,110
nous pouvons être sûrs
que notre modèle se généralisera bien.