Ahora, veamos directamente
la importancia de la regularización L1 al crear
modelos dispersos y concisos. En este lab de regularización L1 agregué muchos atributos inútiles
y creé un modelo bastante complejo. Primero, entrenaremos el modelo
sin una regularización L1 y, luego, veremos si la regularización L1 ayuda a podar el modelo
en uno más disperso conciso y, ojalá, más generalizable. Hola, bienvenido
a TensorFlow Playground. En este lab, veremos si la regularización L1 ayuda a que los
modelos sean más dispersos y concisos. Como puede ver,
es un problema de clasificación. Trataremos de clasificar
estas dos formas. Tenemos dos círculos concéntricos el círculo azul en medio
y el círculo anaranjado por fuera. Lo bueno es que no hay ruido por lo que debería ser fácil de entrenar. También puede ver que hay atributos. Todos los atributos están habilitados. Es decir, será un modelo
bastante complicado. Sabemos por intuición que es una ecuación
x cuadrada por y cuadrada o x1 cuadrada por x2 cuadrada
porque hay círculos. Sin embargo, también hay un montón
de atributos adicionales. También tenemos capas adicionales,
con seis neuronas en cada una. La complejidad es bastante alta. Veamos cómo entrena
sin una regularización L1. Aquí se establece en None. Eso fue muy rápido. Como puede ver la regularización L1
encontró la distribución de los datos. Sin embargo,
hay algunas incongruencias por aquí unas caídas por acá, abultamientos aquí. No es un gran círculo. El motivo puede ser el sobreajuste. Tenemos demasiados atributos
y demasiadas capas ocultas. Hay una función compleja de sobreajuste
en estos datos. ¿Podemos encontrar un modelo
mucho más simple? Sin hacer ingeniería de atributos lograr que la regularización L1
pueda usar esto. Veamos si funciona. Mi regularización será L1. Haré una nueva inicialización,
y veamos cómo le va. Mire eso. Mucho mejor. Investiguemos un poco más. Como puede ver, aprendió
un círculo mucho más suave lo que está muy bien, pues coincide
con lo que vemos en los datos. Sin embargo, en la vida real es raro tener distribuciones
bonitas como esta. Por lo tanto, probablemente debamos
usar esto para muchos otros procesos. Y hay atributos aquí. Tenemos x1 cuadrada
y x2 cuadrada y sus pesos. Son los únicos pesos
a los que les queda magnitud. Todos los otros pesos
están inhabilitados y su valor es cero. Esto se va a las capas ocultas donde podemos ver que x1 y x2 al cuadrado
son las únicas que se propagan. Todas van a esta neurona en la última capa y, finalmente, a la salida. Es como si solo usáramos
x1 y x2 al cuadrado porque son mucho más predictivos en el modelo
que los otros atributos. Debido a la naturaleza de L1
y la distribución de probabilidades puede reducir el modelo.