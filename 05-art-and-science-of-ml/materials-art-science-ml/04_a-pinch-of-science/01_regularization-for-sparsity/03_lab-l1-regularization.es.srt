1
00:00:00,220 --> 00:00:02,505
Ahora, veamos directamente
la importancia

2
00:00:02,505 --> 00:00:05,580
de la regularización L1 al crear
modelos dispersos y concisos.

3
00:00:05,720 --> 00:00:07,630
En este lab de regularización L1

4
00:00:07,680 --> 00:00:12,185
agregué muchos atributos inútiles
y creé un modelo bastante complejo.

5
00:00:12,385 --> 00:00:15,300
Primero, entrenaremos el modelo
sin una regularización L1

6
00:00:15,390 --> 00:00:17,865
y, luego, veremos si la regularización L1

7
00:00:18,065 --> 00:00:21,130
ayuda a podar el modelo
en uno más disperso

8
00:00:21,300 --> 00:00:24,105
conciso y, ojalá, más generalizable.

9
00:00:24,675 --> 00:00:26,740
Hola, bienvenido
a TensorFlow Playground.

10
00:00:27,230 --> 00:00:28,785
En este lab, veremos

11
00:00:28,835 --> 00:00:32,545
si la regularización L1 ayuda a que los
modelos sean más dispersos y concisos.

12
00:00:32,835 --> 00:00:35,105
Como puede ver,
es un problema de clasificación.

13
00:00:35,425 --> 00:00:37,680
Trataremos de clasificar
estas dos formas.

14
00:00:37,960 --> 00:00:40,190
Tenemos dos círculos concéntricos

15
00:00:40,670 --> 00:00:45,070
el círculo azul en medio
y el círculo anaranjado por fuera.

16
00:00:45,750 --> 00:00:47,870
Lo bueno es que no hay ruido

17
00:00:47,870 --> 00:00:50,345
por lo que debería ser fácil de entrenar.

18
00:00:51,665 --> 00:00:53,400
También puede ver que hay atributos.

19
00:00:53,400 --> 00:00:55,175
Todos los atributos están habilitados.

20
00:00:55,375 --> 00:00:57,555
Es decir, será un modelo
bastante complicado.

21
00:00:58,675 --> 00:01:02,625
Sabemos por intuición que es una ecuación
x cuadrada por y cuadrada

22
00:01:02,765 --> 00:01:06,790
o x1 cuadrada por x2 cuadrada
porque hay círculos.

23
00:01:07,160 --> 00:01:10,635
Sin embargo, también hay un montón
de atributos adicionales.

24
00:01:11,725 --> 00:01:15,680
También tenemos capas adicionales,
con seis neuronas en cada una.

25
00:01:15,830 --> 00:01:17,580
La complejidad es bastante alta.

26
00:01:18,340 --> 00:01:21,600
Veamos cómo entrena
sin una regularización L1.

27
00:01:21,800 --> 00:01:23,420
Aquí se establece en None.

28
00:01:26,240 --> 00:01:27,475
Eso fue muy rápido.

29
00:01:27,875 --> 00:01:28,875
Como puede ver

30
00:01:29,125 --> 00:01:32,775
la regularización L1
encontró la distribución de los datos.

31
00:01:33,665 --> 00:01:37,880
Sin embargo,
hay algunas incongruencias por aquí

32
00:01:38,060 --> 00:01:41,180
unas caídas por acá, abultamientos aquí.

33
00:01:41,550 --> 00:01:42,590
No es un gran círculo.

34
00:01:43,300 --> 00:01:45,580
El motivo puede ser el sobreajuste.

35
00:01:45,770 --> 00:01:49,210
Tenemos demasiados atributos
y demasiadas capas ocultas.

36
00:01:49,250 --> 00:01:52,920
Hay una función compleja de sobreajuste
en estos datos.

37
00:01:53,290 --> 00:01:56,620
¿Podemos encontrar un modelo
mucho más simple?

38
00:01:56,970 --> 00:01:59,010
Sin hacer ingeniería de atributos

39
00:01:59,010 --> 00:02:01,315
lograr que la regularización L1
pueda usar esto.

40
00:02:01,555 --> 00:02:02,710
Veamos si funciona.

41
00:02:04,320 --> 00:02:06,570
Mi regularización será L1.

42
00:02:08,680 --> 00:02:12,790
Haré una nueva inicialización,
y veamos cómo le va.

43
00:02:16,600 --> 00:02:18,960
Mire eso. Mucho mejor.

44
00:02:20,520 --> 00:02:21,995
Investiguemos un poco más.

45
00:02:22,745 --> 00:02:25,450
Como puede ver, aprendió
un círculo mucho más suave

46
00:02:25,690 --> 00:02:28,860
lo que está muy bien, pues coincide
con lo que vemos en los datos.

47
00:02:29,530 --> 00:02:31,320
Sin embargo, en la vida real

48
00:02:31,440 --> 00:02:33,630
es raro tener distribuciones
bonitas como esta.

49
00:02:34,130 --> 00:02:37,830
Por lo tanto, probablemente debamos
usar esto para muchos otros procesos.

50
00:02:38,790 --> 00:02:39,850
Y hay atributos aquí.

51
00:02:39,970 --> 00:02:43,065
Tenemos x1 cuadrada
y x2 cuadrada

52
00:02:43,635 --> 00:02:44,400
y sus pesos.

53
00:02:44,440 --> 00:02:46,930
Son los únicos pesos
a los que les queda magnitud.

54
00:02:47,200 --> 00:02:50,470
Todos los otros pesos
están inhabilitados y su valor es cero.

55
00:02:51,365 --> 00:02:55,600
Esto se va a las capas ocultas

56
00:02:55,750 --> 00:03:01,830
donde podemos ver que x1 y x2 al cuadrado
son las únicas que se propagan.

57
00:03:02,050 --> 00:03:04,020
Todas van a esta neurona en la última capa

58
00:03:04,370 --> 00:03:05,630
y, finalmente, a la salida.

59
00:03:06,490 --> 00:03:10,350
Es como si solo usáramos
x1 y x2 al cuadrado

60
00:03:10,440 --> 00:03:13,910
porque son mucho más predictivos en el modelo
que los otros atributos.

61
00:03:14,140 --> 00:03:17,205
Debido a la naturaleza de L1
y la distribución de probabilidades

62
00:03:17,285 --> 00:03:18,680
puede reducir el modelo.