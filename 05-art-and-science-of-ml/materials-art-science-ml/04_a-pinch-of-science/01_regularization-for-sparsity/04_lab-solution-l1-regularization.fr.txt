Incroyable !
La régularisation L1 nous a bien aidés à réduire notre modèle complexe
en un modèle plus petit et généralisable. Toutes les caractéristiques sélectionnées
et deux couches cachées ont créé de nombreuses connexions
représentées par les lignes du milieu. À l'entraînement, les pondérations
étaient actives, mais plutôt faibles. Il y a donc plusieurs caractéristiques
avec des parties très basses. Au lieu de voir un beau cercle
qui correspondrait à nos données, nous avons une sorte de cercle/rectangle
biscornu, probablement peu généralisé. Avec la régularisation, les caractéristiques
inutiles sont toutes tombées à zéro, et les lignes se sont réduites et grisées. Les seules caractéristiques
à avoir survécu sont X1² et X2². Logique : ce sont celles qui, ajoutées,
forment l'équation d'un cercle, qui sans surprise est une forme
que le modèle apprend. Puisqu'il s'agit d'une vraie distribution, nous pouvons être sûrs
que notre modèle se généralisera bien.