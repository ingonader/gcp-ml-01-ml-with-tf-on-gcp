1
00:00:00,000 --> 00:00:06,160
正則化を利用して よりスパースでシンプルな
モデルを作成する方法を見ていきましょう

2
00:00:06,160 --> 00:00:09,365
コースの初めにL2回帰について
紹介しましたが

3
00:00:09,365 --> 00:00:13,790
これは最後の関数の
パラメータ重みの2次の項の合計に加えます

4
00:00:13,790 --> 00:00:18,515
これは重みを小さく保ち
不安定性と一意解を得るのに有効ですが

5
00:00:18,515 --> 00:00:21,665
モデルが不要に大きく
複雑になる場合があります

6
00:00:21,665 --> 00:00:25,880
すべての特徴の重みがやや
小さいままの場合があるからです

7
00:00:25,880 --> 00:00:29,805
代わりにL1正則化と
呼ばれるものを使用して

8
00:00:29,805 --> 00:00:33,965
パラメータ重みの絶対値の合計を
最後の関数に加えると

9
00:00:33,965 --> 00:00:37,990
あまり保護されていない特徴の重みは
多くの場合ゼロになります

10
00:00:37,990 --> 00:00:41,100
これはビルトインの特徴選択機能となり

11
00:00:41,100 --> 00:00:44,495
不良な特徴を排除して
最も強力なものだけを残します

12
00:00:44,495 --> 00:00:47,585
このスパースモデルには
多くの利点があります

13
00:00:47,585 --> 00:00:50,760
まず保管してロードする
係数の数が少ないため

14
00:00:50,760 --> 00:00:54,925
モデルサイズが大幅に縮小し
必要なストレージとメモリが減ります

15
00:00:54,925 --> 00:00:57,710
これは埋め込みモデルでは
特に重要です

16
00:00:57,710 --> 00:00:59,860
また特徴の数が少ないため

17
00:00:59,860 --> 00:01:03,655
マルチ広告の数が大幅に減り
トレーニングの速度が上がるだけでなく

18
00:01:03,655 --> 00:01:06,640
さらに重要なのは
予測の速度が上がることです

19
00:01:06,640 --> 00:01:10,430
多くの機械学習モデルはすでに
多数の特徴を備えています

20
00:01:10,430 --> 00:01:14,915
たとえば受注した注文の日時を含む
データがあるとして

21
00:01:14,915 --> 00:01:17,490
1つ目の注文モデルにはおそらく

22
00:01:17,490 --> 00:01:20,920
曜日ごとに7つの特徴と
時間ごとに24の特徴があり

23
00:01:20,920 --> 00:01:23,660
さらにその他多くの
特徴も含まれています

24
00:01:23,660 --> 00:01:29,535
そのため曜日と時間だけで
すでに31の入力情報があります

25
00:01:29,535 --> 00:01:32,985
ではここで
曜日の2番目の注文の効果を

26
00:01:32,985 --> 00:01:35,945
時刻とクロスさせると
どうなるでしょうか

27
00:01:35,945 --> 00:01:39,070
もともとの31プラスその他に加え

28
00:01:39,070 --> 00:01:43,210
168の入力が加わるため
その1つの日時フィールドだけでも

29
00:01:43,210 --> 00:01:47,590
特徴の合計はほぼ200に達し
さらにその他の特徴も加わります

30
00:01:47,590 --> 00:01:51,600
これをたとえば米国の州に関する
HUDエンコードにクロスさせると

31
00:01:51,600 --> 00:01:55,300
特徴のデカルト三重積は
8400となり

32
00:01:55,300 --> 00:01:59,355
その多くは非常にスパースで
ゼロばかり含むことになります

33
00:01:59,355 --> 00:02:02,850
これでL1正則化による
ビルトインの特徴選択が

34
00:02:02,850 --> 00:02:05,670
有益である理由が
わかると思います

35
00:02:05,670 --> 00:02:08,899
L1正則化のほかには
どんな戦略を使用すれば

36
00:02:08,899 --> 00:02:12,715
有用でない特徴の係数を
削除できるでしょうか

37
00:02:12,715 --> 00:02:18,600
たとえば単純にゼロ以外の値を含む
特徴の数を使用できます

38
00:02:18,600 --> 00:02:22,760
L0ノルムは単に
ゼロでない重みの数であり

39
00:02:22,760 --> 00:02:27,770
このノルムを最適化することは
NP困難な非凸最適化問題です

40
00:02:27,770 --> 00:02:32,735
この図は非凸最適化の面誤差の
一例を示しています

41
00:02:32,735 --> 00:02:35,870
局所的な山と谷が
多く見られますが

42
00:02:35,870 --> 00:02:38,500
これは単純な1次元の例です

43
00:02:38,500 --> 00:02:41,630
ここでは非常に多くの
開始点を検討しなければならず

44
00:02:41,630 --> 00:02:45,715
これは完全に解決するべき
NP困難問題となっています

45
00:02:45,715 --> 00:02:50,585
幸いL1ノルムは
L2ノルムと同様に凸関数ですが

46
00:02:50,585 --> 00:02:53,500
モデルではスパースが
推奨されています

47
00:02:53,500 --> 00:02:58,560
この図はL1ノルムとL2ノルムの
確率分布を示しています

48
00:02:58,560 --> 00:03:02,775
ゼロではL2ノルムのほうが
ピークがはるかに滑らかなので

49
00:03:02,775 --> 00:03:05,595
重みの大きさが
よりゼロに近くなっています

50
00:03:05,595 --> 00:03:10,060
しかしL1ノルムのほうが
より明確にゼロを中心としているため

51
00:03:10,060 --> 00:03:15,330
厳密にゼロになる確率は
L2ノルムよりもずっと高くなります

52
00:03:15,330 --> 00:03:19,355
Pノルムによって一般化される
ノルムの数は無限です

53
00:03:19,355 --> 00:03:21,830
他の一部のノルムや
すでにお話しした

54
00:03:21,830 --> 00:03:26,095
ベクトルに含まれる非ゼロ値の数を示す
L0ノルムもそうですし

55
00:03:26,095 --> 00:03:30,135
ベクトル内のあらゆる値の
最大絶対値であるL無限ノルムもそうです

56
00:03:30,135 --> 00:03:32,740
ただし実際には
L2ノルムは通常

57
00:03:32,740 --> 00:03:35,905
L1ノルムよりも
一般化可能なモデルを提供します

58
00:03:35,905 --> 00:03:41,590
ただしL1でなくL2を使用すると
モデルははるかに複雑で重くなります

59
00:03:41,590 --> 00:03:45,710
これは特徴同士に高い相関性が
あることが多いからです

60
00:03:45,710 --> 00:03:51,095
L1正則化ではその一方が使用され
他方は破棄されますが

61
00:03:51,095 --> 00:03:56,320
L2正則化では両方の特徴が維持され
それらの重みは小さく保たれます

62
00:03:56,320 --> 00:04:01,120
そのためL1ではモデルは小さくなりますが
予測性は下がることがあります

63
00:04:01,120 --> 00:04:04,595
この両方の利点を得る
方法はないでしょうか

64
00:04:04,595 --> 00:04:09,870
Elastic Netは単にL1とL2の正則化の
ペナルティを線形結合したものです

65
00:04:09,870 --> 00:04:14,460
これにより予測性が非常に低い特徴は
スパース性の利点が得られると同時に

66
00:04:14,460 --> 00:04:19,189
重みが小さい有効な特徴は
良好な一般化が可能になります

67
00:04:19,189 --> 00:04:21,154
唯一のトレードオフは

68
00:04:21,154 --> 00:04:24,270
調整すべきハイパーパラメータが
1つでなく2つで

69
00:04:24,270 --> 00:04:27,880
異なる2つのLambda
正則化パラメータがあることです

70
00:04:27,880 --> 00:04:31,530
モデルの予測性が低い特徴の
パラメータ重みは

71
00:04:31,530 --> 00:04:35,250
L1正則化ではどうなることが
多いでしょうか

72
00:04:35,250 --> 00:04:38,170
正解は値がゼロになることです

73
00:04:38,170 --> 00:04:41,210
正則化手法を用いるときは必ず

74
00:04:41,210 --> 00:04:44,540
最後の関数 一般には目的関数に
ペナルティ項を加えて

75
00:04:44,540 --> 00:04:48,310
決定変数やパラメータの重みが
最適化されすぎないようにします

76
00:04:48,310 --> 00:04:53,020
ペナルティ項は事前の知識や
関数の形状などに基づいて選択します

77
00:04:53,030 --> 00:04:56,990
L1正則化はモデルに
スパース性をもたらすとともに

78
00:04:56,990 --> 00:05:00,317
確率分布ではゼロのピークが
高いことが示されており

79
00:05:00,317 --> 00:05:02,935
非常に予測性の高い重み以外は

80
00:05:02,935 --> 00:05:06,020
ほぼ非正規化値からゼロにシフトされます

81
00:05:06,020 --> 00:05:09,530
L2正則化は大きさを
小さくするために使用され

82
00:05:09,530 --> 00:05:12,629
その負数はそれを大きくするために
使用されますが

83
00:05:12,629 --> 00:05:15,245
これは両方とも不正確です

84
00:05:15,245 --> 00:05:17,165
すべての値を正にするのは

85
00:05:17,165 --> 00:05:20,110
最適化の問題に追加の制約を
多く加えて

86
00:05:20,110 --> 00:05:23,360
すべての決定変数を
ゼロ超えにするようなものであり

87
00:05:23,360 --> 00:05:26,050
これもL1正則化ではありません