1
00:00:00,000 --> 00:00:01,440
このラボでは

2
00:00:01,440 --> 00:00:05,790
Datalabから機械学習APIを
呼び出す方法を見てみます

3
00:00:05,790 --> 00:00:07,410
では始めましょう

4
00:00:07,410 --> 00:00:10,990
今回は ゼロからNotebookを作る
代わりに

5
00:00:10,990 --> 00:00:14,930
すでにGitHubリポジトリにある
Notebookで作業を始めます

6
00:00:14,930 --> 00:00:17,135
まずチェックアウトが必要です

7
00:00:17,135 --> 00:00:19,960
Notebookをチェックアウトします

8
00:00:19,960 --> 00:00:22,930
そのためには
リポジトリをクローンする必要があります

9
00:00:22,930 --> 00:00:25,450
さてDatalabを開いて

10
00:00:25,450 --> 00:00:30,115
次にDatalabでBashコマンドを実行します

11
00:00:30,115 --> 00:00:34,230
ここでの要点は
新しいNotebookを開始できることです

12
00:00:35,640 --> 00:00:38,165
このNotebookに好きな名前を付けます

13
00:00:38,165 --> 00:00:41,105
ここではcheckoutとしましょう

14
00:00:43,305 --> 00:00:47,925
ここまでDatalab内でPythonコードを
実行する方法を見ましたが

15
00:00:47,925 --> 00:00:51,215
%bashと入力すると

16
00:00:51,215 --> 00:00:56,475
DatalabはBashを使って
セルにあるすべてのものを実行します

17
00:00:56,475 --> 00:00:59,125
Jupyterの「他のすべて」に似ています

18
00:00:59,125 --> 00:01:04,195
ここでリポジトリのGitクローンを実行します

19
00:01:04,825 --> 00:01:06,985
やってみましょう

20
00:01:10,055 --> 00:01:13,505
ここでは「!ls」が可能です

21
00:01:13,505 --> 00:01:15,465
Bashを実行する別の方法です

22
00:01:15,465 --> 00:01:20,340
training-data-analystという
フォルダがありますね

23
00:01:20,340 --> 00:01:25,615
次にNotebookを読み込んで
実行を始めることができます

24
00:01:27,015 --> 00:01:29,975
ここにtraining-data-analystがあります

25
00:01:30,735 --> 00:01:41,637
ここではcourses（コース）、
machine_learning、deepdiveと進みます

26
00:01:43,867 --> 00:01:47,625
そしてIPython Notebookである
「mlapis」を開きます

27
00:01:47,625 --> 00:01:50,270
これが今回のNotebookです

28
00:01:55,490 --> 00:02:00,175
まずAPIとサービスを
有効にする必要があります

29
00:02:00,175 --> 00:02:03,930
こうするとVision APIを実行でき

30
00:02:03,930 --> 00:02:07,280
Translate API、Speech APIなどを実行できます

31
00:02:07,280 --> 00:02:09,460
ここで下に進んで...

32
00:02:22,740 --> 00:02:24,955
visionと入力します

33
00:02:24,955 --> 00:02:27,220
Vision APIがあります

34
00:02:31,130 --> 00:02:33,695
そしてAPIが有効です

35
00:02:36,825 --> 00:02:41,810
次にTranslate（翻訳）と
Speechでも同じことをします

36
00:02:48,900 --> 00:02:52,215
Google Translation APIです

37
00:02:52,215 --> 00:02:55,445
これもすでに有効です

38
00:02:56,105 --> 00:02:59,240
Natural Language APIです

39
00:03:06,040 --> 00:03:08,940
これも有効です

40
00:03:09,770 --> 00:03:13,815
次にSpeech APIです
これも有効になっているか確認します

41
00:03:13,815 --> 00:03:15,210
これも有効です

42
00:03:15,210 --> 00:03:18,180
すべてのAPIが有効になっています

43
00:03:18,180 --> 00:03:21,380
では 認証情報を取得します

44
00:03:21,380 --> 00:03:24,815
下に移動してAPIとサービスのところで

45
00:03:24,815 --> 00:03:30,580
Credentials（認証情報）を取得します
...これです

46
00:03:38,350 --> 00:03:40,345
すでにAPIキーがあるので

47
00:03:40,345 --> 00:03:42,168
そのキーを使いました

48
00:03:42,168 --> 00:03:43,350
あるいは たとえば

49
00:03:43,350 --> 00:03:45,630
APIキーで認証情報を作成できます

50
00:03:45,630 --> 00:03:47,901
そしてまったく新しいキーを作成し

51
00:03:47,901 --> 00:03:49,800
それをコピーできます

52
00:03:52,130 --> 00:03:53,870
これがAPIキーです

53
00:03:53,870 --> 00:03:55,345
このとおりです

54
00:03:57,025 --> 00:04:00,900
これでML APIに進む準備ができました

55
00:04:00,900 --> 00:04:05,185
Notebookの中の「APIKEY」という場所で

56
00:04:05,185 --> 00:04:10,225
新しいAPIキーに置き換えて 実行します

57
00:04:10,225 --> 00:04:12,270
[Run]ボタンをクリックするか

58
00:04:12,270 --> 00:04:14,295
Shift + Enterも可能です

59
00:04:14,295 --> 00:04:16,100
さらに進んで

60
00:04:16,100 --> 00:04:20,033
Pythonクライアントをインストールします

61
00:04:35,343 --> 00:04:37,640
インストールが終わったら 次に

62
00:04:37,640 --> 00:04:39,995
Translate APIを実行します

63
00:04:39,995 --> 00:04:42,591
ここに入力があります

64
00:04:42,591 --> 00:04:44,671
「is it really this easy?」です

65
00:04:44,671 --> 00:04:51,640
フランス語の翻訳が表示されます
ターゲットをフランス語にしたからです

66
00:04:51,640 --> 00:04:53,838
対象言語をESにしてみましょう

67
00:04:53,838 --> 00:04:56,140
つまりスペイン語です 実行すると

68
00:04:56,140 --> 00:04:58,840
これでスペイン語になりました

69
00:04:58,840 --> 00:05:00,540
どのような仕組みでしょうか？

70
00:05:00,540 --> 00:05:04,780
入力を 文字列の配列として指定しました

71
00:05:04,780 --> 00:05:09,730
そして英語から いずれかの言語に翻訳するよう
サービスに指示して

72
00:05:09,730 --> 00:05:11,727
これらの入力を渡しました

73
00:05:11,727 --> 00:05:16,135
すると出力 つまり
翻訳された文字列が返されました

74
00:05:16,135 --> 00:05:20,188
同様にVision APIを呼び出すこともできます

75
00:05:20,188 --> 00:05:22,950
Vision APIを呼び出すには画像が必要です

76
00:05:22,950 --> 00:05:26,310
この場合は道路標識の画像です

77
00:05:26,310 --> 00:05:29,490
私は中国語が読めないので
正確な意味が分かりません

78
00:05:29,490 --> 00:05:30,815
どんな意味ですか

79
00:05:30,815 --> 00:05:34,235
これをCloud Storageに入力します

80
00:05:34,235 --> 00:05:37,965
これは公開情報なので
何も変更する必要がありません

81
00:05:37,965 --> 00:05:39,860
このまま読むことができます

82
00:05:39,860 --> 00:05:44,015
Vision APIに
その画像を読み取るよう命令すると

83
00:05:44,015 --> 00:05:46,680
その中のテキストを教えてくれます

84
00:05:46,680 --> 00:05:48,605
これを実行しましょう

85
00:05:48,605 --> 00:05:52,585
こうしてJSON出力が返ってきます

86
00:05:52,585 --> 00:05:59,630
さらに もう一度
Vision APIバージョン1を呼び出して

87
00:05:59,630 --> 00:06:02,890
GCS画像のURIを渡します

88
00:06:02,890 --> 00:06:06,080
GCSとはGoogle Cloud Storageです

89
00:06:06,080 --> 00:06:08,325
この画像がCloud Storageにあります

90
00:06:08,325 --> 00:06:11,060
画像を要求に含めて渡すこともできますが

91
00:06:11,060 --> 00:06:13,460
Cloud Storageの方が処理が早いです

92
00:06:13,460 --> 00:06:17,900
画像データを要求と一緒に
アップロードしなくて済むからです

93
00:06:17,900 --> 00:06:20,435
そしてテキストを検出するよう命令すると

94
00:06:20,435 --> 00:06:23,895
この画像にあるテキストがすべて返され

95
00:06:23,895 --> 00:06:28,075
それと一緒に 言語「ZH」つまり中国語と

96
00:06:28,075 --> 00:06:32,910
テキストの各部分の
境界（bounding）ポリゴンも返されます

97
00:06:32,910 --> 00:06:38,235
もちろん さらに操作を進めて
最初の部分を取り出し

98
00:06:38,235 --> 00:06:41,045
テキスト アノテーションを付け

99
00:06:41,045 --> 00:06:44,802
言語ロケールつまりZHを得ることができます

100
00:06:44,802 --> 00:06:47,885
そして 取得したデータをプリントアウトすると

101
00:06:47,885 --> 00:06:51,025
返ってくるのは外国語ZHと

102
00:06:51,025 --> 00:06:54,275
このとおり 外国語のテキストです

103
00:06:54,275 --> 00:06:57,340
次に さらに実行を続けます

104
00:06:57,340 --> 00:07:00,120
もちろん すでに結果が得られたので

105
00:07:00,120 --> 00:07:03,290
このセルをクリックして これを消去すると

106
00:07:03,290 --> 00:07:05,260
もう一度実行できます

107
00:07:05,260 --> 00:07:08,085
実行する内容をよく確認して

108
00:07:08,085 --> 00:07:13,705
このとおり 中国語のテキストが
英語に翻訳されました

109
00:07:13,705 --> 00:07:16,615
もう1つLanguage APIもあります

110
00:07:16,615 --> 00:07:20,435
ここに いくつかの引用文があります

111
00:07:20,435 --> 00:07:24,400
これらの引用に含まれる
感情を調べたいと思います

112
00:07:24,400 --> 00:07:25,935
先程と同様に

113
00:07:25,935 --> 00:07:28,910
セルを消去してから実行します

114
00:07:28,910 --> 00:07:30,130
この場合は

115
00:07:30,130 --> 00:07:33,920
極性（polarity）と
強度（magnitude）を出力します

116
00:07:33,920 --> 00:07:36,285
引用文と関連した特性です

117
00:07:36,285 --> 00:07:38,070
正の極性は

118
00:07:38,070 --> 00:07:39,870
肯定的な感情です

119
00:07:39,870 --> 00:07:42,290
負は否定的な感情です

120
00:07:42,290 --> 00:07:43,750
意味が通りますね

121
00:07:43,750 --> 00:07:47,080
たとえば「成功するには
多くの忍耐が必要だ」という文は

122
00:07:47,080 --> 00:07:48,505
とても肯定的ですが

123
00:07:48,505 --> 00:07:53,075
たとえば「愛する人が
もし死んだら ...」という引用文は

124
00:07:53,075 --> 00:07:55,115
とても否定的な感情ですから

125
00:07:55,115 --> 00:07:56,805
極性は負になります

126
00:07:56,805 --> 00:08:00,210
強度（magnitude）という指標は
テキストの中で

127
00:08:00,210 --> 00:08:04,310
非常に強い感情の言葉が
どれほど頻繁に使われるかを示します

128
00:08:05,090 --> 00:08:08,475
最後にここに表示しているのはSpeech API です

129
00:08:08,475 --> 00:08:09,770
先ほどと同じく

130
00:08:09,770 --> 00:08:12,830
Cloud Storageに音声ファイルがあり

131
00:08:12,830 --> 00:08:17,815
そのスピーチの結果をテキストに
変換するよう命令します

132
00:08:17,815 --> 00:08:20,409
では実行しましょう

133
00:08:20,409 --> 00:08:23,080
JSONレスポンスが得られます

134
00:08:23,080 --> 00:08:27,207
JSONはとても高い信頼率でレスポンスします

135
00:08:27,207 --> 00:08:31,177
それによると その音声ファイルのスピーチは

136
00:08:31,177 --> 00:08:35,589
「How old is the Brooklyn Bridge?」
（ブルックリン橋は建設されて何年か）です

137
00:08:37,189 --> 00:08:43,539
このラボではDatalabを使用し

138
00:08:43,539 --> 00:08:50,240
Python APIを使って
機械学習モデルを呼び出しました

139
00:08:50,240 --> 00:08:53,695
これらのモデルを自分で作る
必要はありませんでした

140
00:08:53,695 --> 00:08:56,600
そのまま使用できる機械学習モデルです

141
00:08:56,600 --> 00:09:01,550
これらのモデルを ご自分の
アプリケーションに組み込むことができます

142
00:09:01,550 --> 00:09:04,790
覚えていただきたい点ですが

143
00:09:04,790 --> 00:09:06,940
機械学習を操作するとき

144
00:09:06,940 --> 00:09:09,795
すべてをゼロから行う必要はありません

145
00:09:09,795 --> 00:09:16,795
画像の中のテキストを認識するには
Vision APIを使用するだけでよいのです