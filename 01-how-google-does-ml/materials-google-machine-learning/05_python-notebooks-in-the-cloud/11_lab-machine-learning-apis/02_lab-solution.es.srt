1
00:00:00,340 --> 00:00:05,510
En este lab, veremos cómo invocar las API
de aprendizaje automático en Datalab.

2
00:00:06,160 --> 00:00:07,290
Comencemos.

3
00:00:07,290 --> 00:00:11,130
Ahora, 
en lugar de crear un cuaderno de cero

4
00:00:11,130 --> 00:00:14,910
recuperamos un cuaderno que ya
está listo en el repositorio de GitHub.

5
00:00:14,910 --> 00:00:16,715
Tenemos que verificarlo.

6
00:00:16,715 --> 00:00:19,910
Veamos el cuaderno.

7
00:00:19,910 --> 00:00:22,380
Para hacerlo,
debemos clonar el repositorio.

8
00:00:22,380 --> 00:00:26,280
Abrimos un Datalab

9
00:00:26,280 --> 00:00:30,125
y ejecutamos un comando bash
desde Datalab.

10
00:00:30,125 --> 00:00:34,670
La idea es que podemos
comenzar un nuevo cuaderno.

11
00:00:34,670 --> 00:00:37,605
Le ponemos el nombre que deseemos.

12
00:00:37,605 --> 00:00:40,605
Llamémoslo "checkout".

13
00:00:43,415 --> 00:00:48,915
Ya vimos cómo ejecutar código
de Python en Datalab

14
00:00:48,915 --> 00:00:56,655
pero si usamos %bash, esto hace
que Datalab ejecute todo en esa celda.

15
00:00:56,655 --> 00:00:59,145
Es como lo que pasa en Jupyter.

16
00:00:59,145 --> 00:01:03,895
Usaremos el comando git clone
de nuestro repositorio.

17
00:01:03,895 --> 00:01:06,745
Hagamos eso.

18
00:01:11,865 --> 00:01:13,521
Puedo usar !ls

19
00:01:13,521 --> 00:01:15,415
que es otra forma de ejecutar Bash.

20
00:01:15,415 --> 00:01:20,380
Y notarán que hay una carpeta
que se llama training-data-analyst.

21
00:01:20,380 --> 00:01:25,315
Podemos cargar el cuaderno
y comenzar a ejecutarlo.

22
00:01:27,015 --> 00:01:29,805
Ahí está training-data-analyst.

23
00:01:31,385 --> 00:01:38,927
Ahora, vamos a
courses/machine_learning/deepdive

24
00:01:43,707 --> 00:01:47,465
y abrimos las API de ML,
el cuaderno IPython.

25
00:01:47,465 --> 00:01:50,010
Ahí está nuestro cuaderno.

26
00:01:55,850 --> 00:01:59,895
Lo primero que debemos hacer
es habilitar "API & services".

27
00:01:59,895 --> 00:02:03,840
De modo que
podamos ejecutar la API de Vision

28
00:02:03,840 --> 00:02:05,550
la API de Translate

29
00:02:05,550 --> 00:02:07,090
la API de Speech, etcétera.

30
00:02:07,090 --> 00:02:09,090
Vamos aquí abajo

31
00:02:22,870 --> 00:02:24,905
y escribimos "vision".

32
00:02:24,905 --> 00:02:27,990
Ahí está la API de Vision

33
00:02:31,700 --> 00:02:33,905
y está habilitada.

34
00:02:36,995 --> 00:02:40,800
Hagamos lo mismo
con las API de Translate y Speech.

35
00:02:48,710 --> 00:02:54,515
Ahí está la API de Google Translation,
que ya está habilitada.

36
00:02:56,005 --> 00:02:58,760
Y la API de Natural Language

37
00:03:06,900 --> 00:03:09,040
también está habilitada.

38
00:03:10,360 --> 00:03:14,000
Asegurémonos de que la API de Speech
esté habilitada.

39
00:03:14,000 --> 00:03:15,925
Lo está.

40
00:03:15,925 --> 00:03:18,350
Muy bien. Todas las API están habilitadas.

41
00:03:18,910 --> 00:03:21,210
Obtengamos las credenciales.

42
00:03:21,210 --> 00:03:24,300
Vamos a "APIs & services"

43
00:03:24,300 --> 00:03:26,335
para obtener las credenciales.

44
00:03:38,135 --> 00:03:40,810
Ya tenemos la clave de API.

45
00:03:40,810 --> 00:03:42,145
La usaremos.

46
00:03:42,268 --> 00:03:45,120
O podemos ir a "Create credentials"

47
00:03:45,120 --> 00:03:48,160
con un clave de API
y creamos una nueva clave.

48
00:03:48,160 --> 00:03:52,009
Copiamos eso. Y ya está.

49
00:03:52,430 --> 00:03:55,580
Esta es nuestra clave de API.

50
00:03:57,495 --> 00:04:01,440
Ahora, estamos listos
para ir a las API de ML.

51
00:04:01,440 --> 00:04:05,255
En nuestro cuaderno, donde dice
clave de API

52
00:04:05,255 --> 00:04:10,655
reemplazaremos con la nueva clave de API
y la ejecutaremos.

53
00:04:10,655 --> 00:04:12,910
Podemos hacer clic en el botón "Run"

54
00:04:12,910 --> 00:04:15,165
o presionar Mayús + Intro.

55
00:04:16,160 --> 00:04:19,823
Instalemos el cliente de Python.

56
00:04:35,303 --> 00:04:37,130
Ya está.

57
00:04:37,130 --> 00:04:40,495
Ahora, ejecutemos la API de Translate.

58
00:04:40,495 --> 00:04:43,241
Notarán que ahí están las entradas

59
00:04:43,241 --> 00:04:45,431
¿es realmente tan sencillo?

60
00:04:45,431 --> 00:04:51,750
Pueden ver la traducción en francés,
porque pedimos que ese sea el objetivo.

61
00:04:51,750 --> 00:04:53,798
Cambiemos el objetivo a ES

62
00:04:53,798 --> 00:04:56,590
que es español, y ejecutemos.

63
00:04:56,590 --> 00:04:59,310
Ahora, obtenemos un texto en español.

64
00:04:59,310 --> 00:05:00,870
¿Cómo funciona?

65
00:05:00,870 --> 00:05:04,940
Especificamos las entradas
como una matriz de strings

66
00:05:04,940 --> 00:05:08,170
y le pedimos al servicio
que hiciera una traducción del inglés

67
00:05:08,170 --> 00:05:12,077
al idioma que queremos,
entregándole esas entradas.

68
00:05:12,077 --> 00:05:16,425
Y lo que obtuvimos son las salidas,
la string traducida.

69
00:05:16,425 --> 00:05:20,508
De manera similar,
invocaremos la API de Vision.

70
00:05:20,508 --> 00:05:23,120
Para hacerlo, necesitamos una imagen.

71
00:05:23,120 --> 00:05:26,822
En este caso,
la imagen es la de un cartel de la calle.

72
00:05:26,822 --> 00:05:29,532
No hablo chino,
por lo que no sé exactamente lo que dice.

73
00:05:29,532 --> 00:05:34,047
Veamos qué dice.
Lo subiremos a Cloud Storage.

74
00:05:34,047 --> 00:05:38,027
Ya es público,
entonces no tenemos que cambiar nada.

75
00:05:38,115 --> 00:05:39,890
Podemos leer…

76
00:05:39,890 --> 00:05:44,415
podemos pedirle a la API de Vision
que lea la imagen

77
00:05:44,415 --> 00:05:47,210
y nos diga qué dice el texto.

78
00:05:47,210 --> 00:05:49,325
Ejecutemos eso.

79
00:05:49,325 --> 00:05:52,925
Y obtenemos la salida JSON.

80
00:05:52,925 --> 00:05:59,490
Lo que hacemos es invocar
la versión 1 de la API de Vision

81
00:05:59,490 --> 00:06:03,310
pasamos el URI de la imagen en GCS.

82
00:06:03,310 --> 00:06:06,440
GCS quiere decir Google Cloud Storage.

83
00:06:06,440 --> 00:06:08,555
La imagen está en Cloud Storage.

84
00:06:08,555 --> 00:06:10,170
También podemos pasar la imagen

85
00:06:10,170 --> 00:06:12,690
como parte de nuestra solicitud.
Pero si está en Cloud Storage

86
00:06:12,690 --> 00:06:17,970
es más rápido, pues no tenemos
que subirla con nuestra solicitud.

87
00:06:17,970 --> 00:06:20,695
Le estamos pidiendo
que haga detección de texto

88
00:06:20,695 --> 00:06:24,235
y lo que nos devuelve
es todo el texto de la imagen

89
00:06:24,235 --> 00:06:28,425
junto con el código del idioma,
que es ZH para chino

90
00:06:28,425 --> 00:06:32,170
y un polígono que rodea
las porciones de texto.

91
00:06:32,980 --> 00:06:38,535
Podríamos obtener la primera parte

92
00:06:38,535 --> 00:06:42,005
y tomar esa anotación de texto

93
00:06:42,005 --> 00:06:45,092
en el idioma
de la configuración regional, que es ZH

94
00:06:45,092 --> 00:06:48,225
y podemos imprimir lo que obtuvimos

95
00:06:48,225 --> 00:06:54,005
que es el texto en el idioma extranjero,
ZH, que es todo esto.

96
00:06:54,425 --> 00:06:57,510
Ahora podemos ejecutarlo.

97
00:06:57,510 --> 00:06:59,800
El resultado ya está aquí

98
00:07:00,270 --> 00:07:03,450
y podemos hacer clic en esta celda
y borrarla.

99
00:07:03,450 --> 00:07:05,730
Y ahora podemos ejecutarlo de nuevo.

100
00:07:05,730 --> 00:07:08,315
Asegúrense de que
lo que ejecuten sea suyo.

101
00:07:08,315 --> 00:07:13,425
Y vemos que el texto en chino
se tradujo al inglés.

102
00:07:13,425 --> 00:07:16,845
Otra API que podemos usar es
la de Language.

103
00:07:16,845 --> 00:07:20,505
Aquí tenemos un conjunto de citas.

104
00:07:20,505 --> 00:07:24,370
Lo que queremos es analizar
la opinión asociada con estas citas.

105
00:07:24,580 --> 00:07:26,195
Igual que antes

106
00:07:26,195 --> 00:07:29,370
borramos la celda y ejecutamos.

107
00:07:29,370 --> 00:07:33,520
En este caso,
imprimiremos la polaridad y la magnitud

108
00:07:33,520 --> 00:07:36,500
asociadas con estas citas.

109
00:07:36,500 --> 00:07:38,515
La polaridad es positiva

110
00:07:38,515 --> 00:07:40,200
si es una opinión positiva

111
00:07:40,200 --> 00:07:42,650
y es negativa si es una opinión negativa.

112
00:07:42,650 --> 00:07:44,140
Tiene sentido.

113
00:07:44,140 --> 00:07:46,550
Si decimos: "Para tener éxito 
se debe tener mucha perseverancia"

114
00:07:46,550 --> 00:07:48,120
es algo muy positivo.

115
00:07:48,120 --> 00:07:51,395
Pero si decimos

116
00:07:51,395 --> 00:07:53,835
"Cuando alguien que amas fallece"

117
00:07:53,835 --> 00:07:56,745
es algo muy negativo.
Entonces, la polaridad es negativa.

118
00:07:56,745 --> 00:07:59,485
Y la magnitud es un indicador

119
00:07:59,485 --> 00:08:04,160
de la frecuencia con la que se usa
lenguaje fuerte en este texto.

120
00:08:04,920 --> 00:08:08,710
Lo último que mostramos
es la API de Speech.

121
00:08:08,710 --> 00:08:12,895
Igual que antes, tenemos un archivo
de audio en Cloud Storage

122
00:08:12,895 --> 00:08:17,610
y le pedimos que lo convierta en texto.

123
00:08:17,610 --> 00:08:20,495
Ejecutamos eso

124
00:08:20,495 --> 00:08:23,409
y obtenemos una respuesta JSON.

125
00:08:23,409 --> 00:08:31,260
JSON responde con mucha confianza
que el texto en ese archivo de audio es

126
00:08:31,260 --> 00:08:33,327
"¿Cuántos años tiene
el puente de Brooklyn?"

127
00:08:36,947 --> 00:08:42,349
Lo que hicimos en este lab es usar

128
00:08:42,349 --> 00:08:50,179
Datalab para usar las API de Python
para invocar los modelos de AA.

129
00:08:50,179 --> 00:08:53,720
Recuerden que estos no son modelos
de AA que desarrollamos.

130
00:08:53,720 --> 00:08:56,595
Son modelos que podemos usar.

131
00:08:56,595 --> 00:09:01,680
Podemos incorporar estos modelos de AA
en nuestras aplicaciones.

132
00:09:01,680 --> 00:09:06,040
Deben tomar en cuenta
que no todas las herramientas de AA

133
00:09:06,040 --> 00:09:10,080
que necesiten deben crearse de cero.

134
00:09:10,080 --> 00:09:14,375
Si lo que quieren
es reconocer imágenes y texto

135
00:09:14,375 --> 00:09:17,075
simplemente usen la API de Vision.