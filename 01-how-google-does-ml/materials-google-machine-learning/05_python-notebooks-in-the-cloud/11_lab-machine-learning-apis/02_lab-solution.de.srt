1
00:00:00,000 --> 00:00:02,550
In diesem Lab sehen wir uns an,

2
00:00:02,550 --> 00:00:05,780
wie wir ML-APIs
aus Datalab heraus aufrufen.

3
00:00:05,780 --> 00:00:07,290
Fangen wir an.

4
00:00:07,290 --> 00:00:10,850
Dieses Mal starten wir mit einem Notebook,

5
00:00:10,850 --> 00:00:14,910
das bereits in unserem
GitHub-Repository vorhanden ist.

6
00:00:14,910 --> 00:00:16,715
Zuerst müssen wir es auschecken.

7
00:00:16,715 --> 00:00:19,910
Checken wir also das Notebook aus.

8
00:00:19,910 --> 00:00:22,380
Dazu müssen Sie das Repository klonen.

9
00:00:22,380 --> 00:00:25,270
Dann öffnen wir ein Datalab

10
00:00:25,270 --> 00:00:30,115
und führen in Datalab
einen Bash-Befehl aus.

11
00:00:30,125 --> 00:00:34,670
Wir möchten ein neues Notebook beginnen.

12
00:00:35,390 --> 00:00:37,905
Das Notebook
kann einen beliebigen Namen haben.

13
00:00:37,905 --> 00:00:40,085
Wir nennen es "checkout".

14
00:00:43,815 --> 00:00:48,165
Bisher haben wir in Datalab
nur Python-Code ausgeführt.

15
00:00:48,165 --> 00:00:55,675
Durch Eingabe von "%bash" 
führt Datalab nun alles in dieser Zelle

16
00:00:55,675 --> 00:00:59,145
mit Bash aus,
wie alles andere in Jupyter auch.

17
00:00:59,145 --> 00:01:06,555
Jetzt erstellen wir einen Git-Klon
von unserem Repository.

18
00:01:10,125 --> 00:01:13,505
An dieser Stelle
führe ich den Befehl "!ls" aus,

19
00:01:13,521 --> 00:01:15,415
ein weiterer Weg, Bash zu nutzen.

20
00:01:15,415 --> 00:01:20,380
Wir sehen hier einen Ordner
namens "training-data-analyst".

21
00:01:20,380 --> 00:01:23,440
Wir können jetzt das Notebook laden

22
00:01:23,440 --> 00:01:25,970
und dann ausführen.

23
00:01:27,015 --> 00:01:29,745
Hier haben wir "training-data-analyst".

24
00:01:30,925 --> 00:01:39,577
Jetzt öffnen wir "courses",
"machine_learning", "deepdive"

25
00:01:44,017 --> 00:01:47,515
und dann "mlapis",
unser IPython-Notebook.

26
00:01:47,515 --> 00:01:50,122
und hier ist unser Notebook

27
00:01:55,840 --> 00:01:59,895
Zuerst aktivieren wir APIs und Dienste,

28
00:01:59,895 --> 00:02:03,840
damit wir die Vision API,

29
00:02:03,840 --> 00:02:05,320
die Translate API,

30
00:02:05,320 --> 00:02:07,090
die Speech API usw. verwenden können.

31
00:02:07,090 --> 00:02:09,870
Wir öffnen die Suche in der API-Bibliothek

32
00:02:22,720 --> 00:02:24,765
und geben "Vision" ein.

33
00:02:24,765 --> 00:02:26,460
Da ist die Vision API.

34
00:02:30,680 --> 00:02:33,935
Die API ist aktiviert.

35
00:02:36,995 --> 00:02:40,850
Wir wiederholen das jetzt
für Translate und Speech.

36
00:02:49,030 --> 00:02:52,725
Da ist die Google Translation API,

37
00:02:52,725 --> 00:02:55,945
die auch bereits aktiviert ist,

38
00:02:55,945 --> 00:02:58,237
und die Natural Language API,

39
00:03:04,467 --> 00:03:05,637
hier haben wir sie,

40
00:03:06,857 --> 00:03:09,350
ist auch aktiviert.

41
00:03:09,990 --> 00:03:13,695
Jetzt noch die Speech API,
die auch aktiviert sein soll.

42
00:03:13,695 --> 00:03:15,020
Sie ist auch aktiviert.

43
00:03:15,020 --> 00:03:18,210
Alle APIs sind also aktiviert.

44
00:03:18,210 --> 00:03:21,300
Jetzt holen wir uns die Anmeldedaten.

45
00:03:21,300 --> 00:03:24,185
Wir öffnen "APIs und Dienste"

46
00:03:24,185 --> 00:03:29,595
und besorgen uns die Anmeldedaten.

47
00:03:38,370 --> 00:03:42,115
Wir haben ja einen API-Schlüssel,
den ich dafür verwenden konnte.

48
00:03:42,115 --> 00:03:46,960
Alternativ können wir auch
einen neuen API-Schlüssel erstellen.

49
00:03:47,621 --> 00:03:49,060
Wir kopieren ihn.

50
00:03:52,040 --> 00:03:55,080
Das ist unser API-Schlüssel.

51
00:03:57,045 --> 00:04:00,650
Jetzt können wir wieder "mlapis" aufrufen,

52
00:04:00,650 --> 00:04:05,015
im Notebook an der Stelle "APIKEY"

53
00:04:05,015 --> 00:04:10,245
den API-Schlüssel
mit dem neuen ersetzen und es ausführen.

54
00:04:10,245 --> 00:04:12,200
Wir können auf "Ausführen" klicken

55
00:04:12,200 --> 00:04:13,845
oder UMSCHALT+Eingabe verwenden.

56
00:04:13,845 --> 00:04:19,060
Wir installieren jetzt den Python-Client.

57
00:04:35,233 --> 00:04:40,060
Danach führen wir die Translate API aus.

58
00:04:40,060 --> 00:04:42,881
Sie sehen dort die Eingaben:

59
00:04:42,881 --> 00:04:44,541
"Is it really this easy?"

60
00:04:44,541 --> 00:04:48,120
Die Übersetzung ist Französisch,

61
00:04:48,120 --> 00:04:51,700
unser aktuelles Ziel.

62
00:04:51,700 --> 00:04:54,394
Ändern wir das zu ES, Spanisch,

63
00:04:54,394 --> 00:04:55,908
und führen sie aus.

64
00:04:55,908 --> 00:04:58,600
Jetzt erhalten wir Spanisch als Ausgabe.

65
00:04:58,600 --> 00:05:00,160
Wie funktioniert das?

66
00:05:00,160 --> 00:05:04,780
Wir haben die Eingaben
als String-Array angegeben

67
00:05:04,780 --> 00:05:07,560
und vom Dienst
eine Übersetzung der Eingaben

68
00:05:07,560 --> 00:05:11,767
von Englisch
in eine gewünschte Sprache angefordert.

69
00:05:11,767 --> 00:05:16,115
Wir erhalten die Ausgaben,
also den übersetzten String.

70
00:05:16,115 --> 00:05:20,198
Auf ähnliche Weise
führen wir die Vision API aus.

71
00:05:20,198 --> 00:05:23,080
Dazu benötigen wir ein Bild.

72
00:05:23,080 --> 00:05:26,600
Wir nehmen ein Bild
mit einem Straßenschild.

73
00:05:26,600 --> 00:05:29,490
Ich spreche kein Chinesich,
daher weiß ich nicht, was dort steht.

74
00:05:29,490 --> 00:05:31,190
Finden wir es heraus.

75
00:05:31,190 --> 00:05:34,235
Wir laden das Bild in Cloud Storage hoch.

76
00:05:34,235 --> 00:05:37,965
Dieser Ordner ist öffentlich,
daher müssen wir nichts ändern.

77
00:05:37,980 --> 00:05:44,425
Wir weisen dann
die Vision API an, das Bild zu lesen

78
00:05:44,425 --> 00:05:46,680
und den Text für uns zu übersetzen.

79
00:05:46,680 --> 00:05:48,555
Führen wir dies aus.

80
00:05:48,555 --> 00:05:52,585
An dieser Stelle
erhalten wir die JSON-Ausgabe.

81
00:05:52,585 --> 00:05:59,660
Wir rufen Version 1 der Vision API auf

82
00:05:59,660 --> 00:06:02,970
und übergeben "gcs-image-uri".

83
00:06:02,970 --> 00:06:06,100
"gcs" bedeutet "Google Cloud Storage",

84
00:06:06,100 --> 00:06:08,215
wo sich unser Bild befindet.

85
00:06:08,215 --> 00:06:11,060
Wir könnten auch ein Bild
als Teil der Anfrage übergeben,

86
00:06:11,060 --> 00:06:13,430
aber der Vorgang
ist über Cloud Storage schneller,

87
00:06:13,430 --> 00:06:17,900
da wir keine Bilddaten 
mit unserer Anfrage hochladen müssen.

88
00:06:17,900 --> 00:06:20,345
Wir fragen eine Texterkennung an

89
00:06:20,345 --> 00:06:26,375
und erhalten sämtlichen Text
in diesem Bild zusammen mit der Sprache.

90
00:06:26,375 --> 00:06:28,075
"zh" bedeutet Chinesisch.

91
00:06:28,075 --> 00:06:31,530
Wir erhalten auch
zu jedem Textteil ein Begrenzungspolygon.

92
00:06:32,910 --> 00:06:38,705
Wir können jetzt den ersten Teil abrufen,

93
00:06:38,715 --> 00:06:44,722
den Texthinweis
mit der Sprache "zh" nehmen

94
00:06:44,722 --> 00:06:47,635
und dann das Ergebnis ausgeben lassen.

95
00:06:47,635 --> 00:06:51,075
Wir erhalten als Fremdsprache "zh"

96
00:06:51,075 --> 00:06:54,275
und den fremdsprachigen Text.

97
00:06:54,275 --> 00:06:57,360
Diesen können wir jetzt
als Eingabe verwenden.

98
00:06:57,360 --> 00:07:00,120
Das Ergebnis wird bereits dargestellt,

99
00:07:00,120 --> 00:07:03,300
daher klicke ich
in diese Zelle und lösche den Inhalt.

100
00:07:03,300 --> 00:07:05,730
Jetzt können wir
die Ausführung noch einmal starten

101
00:07:05,730 --> 00:07:08,145
und sicherstellen,
dass unser Text verwendet wird.

102
00:07:08,145 --> 00:07:13,265
Der chinesische Text
wurde ins Englische übersetzt.

103
00:07:13,265 --> 00:07:16,685
Wir können außerdem
die Language API verwenden.

104
00:07:16,685 --> 00:07:20,345
Hier haben wir einige Zitate.

105
00:07:20,345 --> 00:07:21,770
Wir möchten uns ansehen,

106
00:07:21,770 --> 00:07:24,210
welche Stimmungen
mit diesen Zitaten verbunden sind.

107
00:07:24,210 --> 00:07:27,515
Wie zuvor löschen wir die Zelle

108
00:07:27,515 --> 00:07:29,005
und starten die Ausführung.

109
00:07:29,005 --> 00:07:33,510
In diesem Fall lassen wir
die Polarität und die Größe ausgeben,

110
00:07:33,510 --> 00:07:36,245
die mit den 
einzelnen Zitaten verbunden sind.

111
00:07:36,245 --> 00:07:39,970
Die Polarität ist positiv
bei einer positiven Stimmung.

112
00:07:39,970 --> 00:07:42,430
Sie ist negativ bei negativer Stimmung.

113
00:07:42,430 --> 00:07:43,615
Und das macht Sinn.

114
00:07:43,620 --> 00:07:48,060
"Um Erfolg zu haben, musst du eine
ungeheure Ausdauer besitzen" ist positiv.

115
00:07:48,065 --> 00:07:55,045
Das Zitat "Wenn eine Person,
die du liebst, stirbt" ist jedoch negativ.

116
00:07:55,045 --> 00:07:56,805
Die Polarität ist negativ.

117
00:07:56,805 --> 00:08:04,070
Die Größe zeigt an, wie viele
sehr starke Formulierungen im Text sind.

118
00:08:05,240 --> 00:08:08,715
Als Letztes zeigen wir die Speech API.

119
00:08:08,715 --> 00:08:13,110
Wir haben schon eine Audiodatei
in Cloud Storage hochgeladen.

120
00:08:13,110 --> 00:08:17,835
Wir möchten die Sprache
als Text ausgeben lassen.

121
00:08:17,835 --> 00:08:20,409
Wir führen das aus

122
00:08:20,409 --> 00:08:23,040
und erhalten eine JSON-Antwort.

123
00:08:23,040 --> 00:08:26,907
Die JSON-Antwort gibt
einen hohen Konfidenzwert dafür an,

124
00:08:26,907 --> 00:08:33,409
dass der Text lautet:
"How old is the Brooklyn Bridge?"

125
00:08:37,149 --> 00:08:43,649
Wir haben in diesem Lab Datalab verwendet,

126
00:08:43,649 --> 00:08:50,240
um über Python-APIs
Modelle für maschinelles Lernen zu nutzen.

127
00:08:50,240 --> 00:08:53,595
Wir mussten diese Modelle nicht erstellen,

128
00:08:53,595 --> 00:08:56,780
sondern konnten sie direkt verwenden

129
00:08:56,780 --> 00:09:01,700
und in unsere 
eigenen Anwendungen integrieren.

130
00:09:01,700 --> 00:09:10,050
Sie sehen also, dass Sie nicht jedes 
ML-Modell von Grund auf entwickeln müssen.

131
00:09:10,055 --> 00:09:14,075
Wenn Sie Text und Bilder
erkennen lassen möchten,

132
00:09:14,075 --> 00:09:17,360
können Sie auch einfach
die Vision API verwenden.