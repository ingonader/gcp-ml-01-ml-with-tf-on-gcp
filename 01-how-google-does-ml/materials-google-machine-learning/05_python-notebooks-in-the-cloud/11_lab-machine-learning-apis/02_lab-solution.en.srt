1
00:00:00,000 --> 00:00:02,550
So in this lab, we're going to look at how to invoke

2
00:00:02,550 --> 00:00:05,780
machine learning APIs from within Datalab.

3
00:00:05,780 --> 00:00:07,290
So let's go ahead.

4
00:00:07,290 --> 00:00:11,130
And this time, instead of going ahead and doing this Notebook from scratch,

5
00:00:11,130 --> 00:00:14,910
we will start from a Notebook that's already in our GitHub repository.

6
00:00:14,910 --> 00:00:16,715
So first, we need to check it out.

7
00:00:16,715 --> 00:00:19,910
So let's go ahead and check out the Notebook.

8
00:00:19,910 --> 00:00:22,380
And in order to do that you have to clone the repo.

9
00:00:22,380 --> 00:00:26,280
So, we'll go ahead and open up a Datalab,

10
00:00:26,280 --> 00:00:30,125
and then run a Bash command from within Datalab.

11
00:00:30,125 --> 00:00:34,670
So, the idea here is that we can go ahead and start a new Notebook.

12
00:00:34,670 --> 00:00:37,605
Call this Notebook whatever we want.

13
00:00:37,605 --> 00:00:41,485
Let's call it, Checkout.

14
00:00:41,485 --> 00:00:49,265
Now, so far we've basically looked at running Python code within Datalab but by putting

15
00:00:49,265 --> 00:00:56,655
in percent Bash this makes Datalab run everything in that cell using Bash.

16
00:00:56,655 --> 00:00:59,145
So, this is like everything else in Jupyter.

17
00:00:59,145 --> 00:01:03,895
So here I'm basically going ahead and doing a Git clone of our repo

18
00:01:03,895 --> 00:01:10,125
and let's go ahead and do that.

19
00:01:10,125 --> 00:01:11,865
So at this point,

20
00:01:11,865 --> 00:01:13,521
I can do bangle S,

21
00:01:13,521 --> 00:01:15,415
that's another way to run Bash.

22
00:01:15,415 --> 00:01:20,380
And you will notice that there is a folder called training data analyst.

23
00:01:20,380 --> 00:01:27,015
And, we can now go ahead and load up that Notebook and start executing it.

24
00:01:27,015 --> 00:01:29,805
So there is training data analyst.

25
00:01:29,805 --> 00:01:38,927
And this time, what you want to do is to go into courses, machine learning,

26
00:01:38,927 --> 00:01:47,465
deepdive, and open up the ML APIs that IPython Notebook,

27
00:01:47,465 --> 00:01:55,840
and there is our Notebook.

28
00:01:55,840 --> 00:01:59,895
So, the first thing to do is to go ahead and enable APIs and Services.

29
00:01:59,895 --> 00:02:03,840
And so that we can run the Vision API,

30
00:02:03,840 --> 00:02:05,550
and the Translate API,

31
00:02:05,550 --> 00:02:07,090
and the Speech API, et cetera.

32
00:02:07,090 --> 00:02:18,040
So, we go down here

33
00:02:18,040 --> 00:02:24,765
and type in vision.

34
00:02:24,765 --> 00:02:31,700
And, there is the Vision API

35
00:02:31,700 --> 00:02:36,995
and the API is enabled.

36
00:02:36,995 --> 00:02:42,170
Let's go ahead and do the same thing for Translate and Speech.

37
00:02:47,670 --> 00:02:55,335
There's the Google Translation API that's also enabled already.

38
00:02:55,335 --> 00:03:01,160
And the Natural language API,

39
00:03:01,160 --> 00:03:08,250
there it is, that's enabled as well.

40
00:03:08,250 --> 00:03:13,695
And, the Speech API, let's just make sure it's also enabled.

41
00:03:13,695 --> 00:03:15,020
And that's also enabled.

42
00:03:15,020 --> 00:03:18,210
So great. So all of the APIS are enabled.

43
00:03:18,210 --> 00:03:21,300
So, let's go ahead and get the Credentials.

44
00:03:21,300 --> 00:03:24,185
So, we'll go down to the APIs and Services,

45
00:03:24,185 --> 00:03:38,780
and get the Credentials.

46
00:03:38,780 --> 00:03:40,115
So, we already have the API key.

47
00:03:40,115 --> 00:03:42,128
So, I went ahead and used it.

48
00:03:42,128 --> 00:03:43,330
Or we can go ahead and say,

49
00:03:43,330 --> 00:03:45,680
create Credentials with an API key,

50
00:03:45,680 --> 00:03:47,621
and create a brand new key,

51
00:03:47,621 --> 00:03:52,040
copy that and there we go.

52
00:03:52,040 --> 00:03:53,780
So, that's our API key.

53
00:03:53,780 --> 00:03:55,815
So, here it is.

54
00:03:55,815 --> 00:04:00,650
And now, we're ready to go into the ML APIs.

55
00:04:00,650 --> 00:04:05,015
And in our Notebook where it says API key,

56
00:04:05,015 --> 00:04:10,245
I will replace by the new API key that we have, and run it.

57
00:04:10,245 --> 00:04:12,200
So, I can either click the run button,

58
00:04:12,200 --> 00:04:13,845
or I can do shift enter.

59
00:04:13,845 --> 00:04:15,910
So, let's go ahead and

60
00:04:15,910 --> 00:04:31,803
install the Python client.

61
00:04:31,803 --> 00:04:36,560
So having done that,

62
00:04:36,560 --> 00:04:40,045
let's go ahead, and run the Translate API.

63
00:04:40,045 --> 00:04:43,031
And you notice that there is the inputs,

64
00:04:43,031 --> 00:04:44,541
is it really this easy?

65
00:04:44,541 --> 00:04:51,700
And you see the translation in French because we asked for the target to be French.

66
00:04:51,700 --> 00:04:53,388
Let's change the target to be ES,

67
00:04:53,388 --> 00:04:55,920
that's Espaniol, and run it.

68
00:04:55,920 --> 00:04:58,600
And now, what we get back is Spanish.

69
00:04:58,600 --> 00:05:00,160
So, how does this work?

70
00:05:00,160 --> 00:05:04,780
We went ahead and specified the inputs as an array of strings,

71
00:05:04,780 --> 00:05:08,230
and asked the service to go ahead and do a translation from English

72
00:05:08,230 --> 00:05:11,767
to whichever language we want passing in those inputs.

73
00:05:11,767 --> 00:05:16,115
And what we got back is the outputs, the translated string.

74
00:05:16,115 --> 00:05:20,198
Similarly, what you want to do is to go ahead and invoke the Vision API.

75
00:05:20,198 --> 00:05:21,600
And to invoke the Vision API,

76
00:05:21,600 --> 00:05:23,058
we need an image.

77
00:05:23,058 --> 00:05:26,600
And in this case, the image is the image of a street sign.

78
00:05:26,600 --> 00:05:29,470
I don't know Chinese so I don't know exactly what it says.

79
00:05:29,470 --> 00:05:34,235
Let's see what it says. So we'll go ahead and put this on Cloud storage.

80
00:05:34,235 --> 00:05:37,965
So this is actually been made public so we don't have to change anything here.

81
00:05:37,965 --> 00:05:39,610
We can go ahead and read,

82
00:05:39,610 --> 00:05:44,425
we can ask the Vision API to read that image,

83
00:05:44,425 --> 00:05:46,680
and tell us what text is in it.

84
00:05:46,680 --> 00:05:48,555
So we can go ahead and run that.

85
00:05:48,555 --> 00:05:52,585
And at this point, we get back the JSON output.

86
00:05:52,585 --> 00:05:59,150
So again, what we're doing here is that we're invoking the version one of the Vision API,

87
00:05:59,150 --> 00:06:02,970
passing in the GCS image URI.

88
00:06:02,970 --> 00:06:06,100
GCS meaning, again, Google Cloud Storage.

89
00:06:06,100 --> 00:06:08,215
We have this image on cloud storage.

90
00:06:08,215 --> 00:06:10,210
We could also pass an image as part of

91
00:06:10,210 --> 00:06:13,430
our request but having it on cloud storage makes it faster.

92
00:06:13,430 --> 00:06:17,900
Because we don't have to upload all of that image data along with our request.

93
00:06:17,900 --> 00:06:20,345
And we are asking it to do text detection,

94
00:06:20,345 --> 00:06:23,885
and what comes back is all of the text in this image,

95
00:06:23,885 --> 00:06:28,075
along with the language ZH meaning Chinese,

96
00:06:28,075 --> 00:06:32,180
and a bounding polygon of each of those pieces of text.

97
00:06:32,180 --> 00:06:38,055
We could of course go ahead and get the first piece of it,

98
00:06:38,055 --> 00:06:40,935
and take the text annotation,

99
00:06:40,935 --> 00:06:44,722
get the language, the locale which we said was ZH.

100
00:06:44,722 --> 00:06:47,635
And then, we could go ahead and print out what we got,

101
00:06:47,635 --> 00:06:51,075
and we got back the foreign language to ZH,

102
00:06:51,075 --> 00:06:54,275
and the foreign text which is all of this.

103
00:06:54,275 --> 00:06:57,360
So now, what we can do is to go ahead and run it.

104
00:06:57,360 --> 00:07:00,120
Of course, the result of it having been drawn is already

105
00:07:00,120 --> 00:07:03,300
here so I can click on this cell, clear it.

106
00:07:03,300 --> 00:07:05,070
And then now, you can run it again,

107
00:07:05,070 --> 00:07:08,145
and you can make sure that what you are being run is yours,

108
00:07:08,145 --> 00:07:13,265
and we see that the Chinese text has now been translated into English.

109
00:07:13,265 --> 00:07:16,685
The other thing that we can do is the Language API.

110
00:07:16,685 --> 00:07:20,345
So here, we have a set of quotes.

111
00:07:20,345 --> 00:07:24,210
And what we want to do is to look at the sentiment associated with these quotes.

112
00:07:24,210 --> 00:07:25,825
So again as before,

113
00:07:25,825 --> 00:07:29,000
let's go ahead and clear the cell and run it.

114
00:07:29,000 --> 00:07:30,260
So in this case,

115
00:07:30,260 --> 00:07:33,500
we are printing out the polarity and the magnitude,

116
00:07:33,500 --> 00:07:36,245
all associated with each of these codes.

117
00:07:36,245 --> 00:07:38,570
So, the polarity is positive,

118
00:07:38,570 --> 00:07:39,980
if it's a positive sentiment,

119
00:07:39,980 --> 00:07:42,270
it's negative if it's a negative sentiment.

120
00:07:42,270 --> 00:07:43,670
And that makes sense.

121
00:07:43,670 --> 00:07:46,420
If you say, to succeed you must have tremendous perseverance,

122
00:07:46,420 --> 00:07:48,065
that's a very positive thing.

123
00:07:48,065 --> 00:07:51,435
But, if you say for example,

124
00:07:51,435 --> 00:07:53,195
when someone you love dies.

125
00:07:53,195 --> 00:07:55,025
Well, that's a pretty negative thing.

126
00:07:55,025 --> 00:07:56,805
So polarity is negative.

127
00:07:56,805 --> 00:07:59,520
And the magnitude is an indicator of how

128
00:07:59,520 --> 00:08:04,460
often very strongly worded language occurs in the text.

129
00:08:04,460 --> 00:08:08,715
The final piece that we're showing up here is a Speech API.

130
00:08:08,715 --> 00:08:13,190
And as before, we have an audio file loaded into cloud storage and we

131
00:08:13,190 --> 00:08:17,835
are asking for the result of that speech to be made into text.

132
00:08:17,835 --> 00:08:20,409
So, we can go ahead and run that,

133
00:08:20,409 --> 00:08:23,040
and we get back a JSON response.

134
00:08:23,040 --> 00:08:31,447
And the JSON responds at a very high confidence is that the speech in that audio file is,

135
00:08:31,447 --> 00:08:36,299
"How old is the Brooklyn Bridge?"

136
00:08:36,299 --> 00:08:42,410
So what we have done in this lab is that we've used

137
00:08:42,410 --> 00:08:50,240
Datalab to use Python APIs to essentially invoke the machine learning models.

138
00:08:50,240 --> 00:08:53,595
So remember that these are not machine learning models that we had to build.

139
00:08:53,595 --> 00:08:56,780
These are machine learning models that we could just go ahead and use.

140
00:08:56,780 --> 00:09:01,700
We could incorporate this machine learning models into our own applications.

141
00:09:01,700 --> 00:09:04,430
This is something that you want to recognize that

142
00:09:04,430 --> 00:09:10,055
not every ML thing that you need to do has to be done from scratch.

143
00:09:10,055 --> 00:09:14,075
If what you want to do is to recognize text and images,

144
00:09:14,075 --> 00:09:17,000
you might just use the Vision API.