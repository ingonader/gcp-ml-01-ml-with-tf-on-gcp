1
00:00:00,000 --> 00:00:02,550
Dans cet atelier, nous allons voir
comment invoquer

2
00:00:02,550 --> 00:00:05,660
des API de machine learning
depuis Datalab.

3
00:00:05,660 --> 00:00:07,290
Allons-y !

4
00:00:07,290 --> 00:00:11,130
Cette fois, au lieu de concevoir
un bloc-notes à partir de zéro,

5
00:00:11,130 --> 00:00:14,810
nous allons utiliser un bloc-notes
déjà présent dans notre dépôt GitHub.

6
00:00:14,810 --> 00:00:16,715
Nous devons tout d'abord le vérifier.

7
00:00:16,715 --> 00:00:19,910
Faisons cela, vérifions le bloc-notes.

8
00:00:19,910 --> 00:00:22,380
Pour ce faire, il est nécessaire
de cloner le dépôt.

9
00:00:22,380 --> 00:00:26,280
Nous allons donc ouvrir Datalab,

10
00:00:26,280 --> 00:00:30,125
puis exécuter une commande Bash
depuis Datalab.

11
00:00:30,125 --> 00:00:33,800
L'idée est de démarrer
un nouveau bloc-notes.

12
00:00:34,670 --> 00:00:37,605
Vous pouvez lui attribuer
le nom de votre choix.

13
00:00:37,605 --> 00:00:40,485
Appelons-le "checkout".

14
00:00:43,235 --> 00:00:46,815
Jusqu'à présent, nous avons
essentiellement étudié l'exécution

15
00:00:46,815 --> 00:00:50,965
du code Python dans Datalab, mais si
on ajoute "%bash" ici,

16
00:00:50,965 --> 00:00:56,355
Datalab exécutera tout le code présent dans
cette cellule à l'aide de Bash.

17
00:00:56,355 --> 00:00:59,145
Cela fonctionne comme tous
les autres éléments de Jupyter.

18
00:00:59,145 --> 00:01:03,895
Ici, je vais créer un clone
git de notre dépôt.

19
00:01:10,125 --> 00:01:11,865
À ce stade,

20
00:01:11,865 --> 00:01:13,561
je peux saisir "!ls",

21
00:01:13,561 --> 00:01:15,715
il s'agit d'une autre manière
d'exécuter Bash.

22
00:01:15,715 --> 00:01:20,380
Vous pouvez remarquer la présence d'un
dossier appelé "training data analyst".

23
00:01:20,380 --> 00:01:25,315
Nous pouvons maintenant charger
ce bloc-notes et commencer à l'exécuter.

24
00:01:27,015 --> 00:01:30,065
On retrouve le dossier
"training data analyst" sur cette vue.

25
00:01:30,065 --> 00:01:39,897
Cette fois, vous allez cliquer sur "courses",
"machine-learning", puis "deepdive".

26
00:01:43,217 --> 00:01:47,465
Ensuite, vous allez ouvrir "mlapis.ipynb",
notre bloc-notes python.

27
00:01:47,465 --> 00:01:50,350
Le voici.

28
00:01:55,840 --> 00:01:59,895
Vous devez dans un premier temps
activer les API et les services.

29
00:01:59,895 --> 00:02:03,840
Cela vous permettra d'exécuter
l'API Vision,

30
00:02:03,840 --> 00:02:05,550
l'API Translate,

31
00:02:05,550 --> 00:02:07,090
l'API Speech, etc.

32
00:02:07,090 --> 00:02:09,150
Pour cela, procédez comme suit.

33
00:02:22,010 --> 00:02:24,765
Ensuite, saisissez "vision" dans ce champ.

34
00:02:24,765 --> 00:02:27,070
Et voici l'API Vision.

35
00:02:30,350 --> 00:02:33,365
Notez que l'API est activée.

36
00:02:36,995 --> 00:02:40,490
Faisons la même chose pour
l'API Translate et l'API Speech.

37
00:02:48,880 --> 00:02:55,125
Voici l'API Google Translation,
elle est également activée.

38
00:02:55,415 --> 00:02:58,800
Maintenant, l'API Natural Language.

39
00:03:04,030 --> 00:03:08,250
Elle est ici. Elle aussi est activée.

40
00:03:09,460 --> 00:03:13,695
Enfin, l'API Speech.
Vérifions qu'elle est activée.

41
00:03:13,695 --> 00:03:15,020
Elle est bien activée.

42
00:03:15,020 --> 00:03:18,210
Parfait, toutes les API sont activées.

43
00:03:18,210 --> 00:03:21,340
Maintenant, nous devons récupérer
les identifiants de connexion.

44
00:03:21,340 --> 00:03:24,535
Nous revenons dans la section
"API et services" du menu latéral

45
00:03:24,535 --> 00:03:26,550
pour obtenir ces identifiants.

46
00:03:27,915 --> 00:03:30,005
C'est ici...

47
00:03:38,300 --> 00:03:40,465
Nous avons déjà la clé d'API,

48
00:03:40,465 --> 00:03:42,278
je vais donc l'utiliser pour la suite.

49
00:03:42,278 --> 00:03:45,790
J'aurais également pu créer
des identifiants avec une clé d'API,

50
00:03:45,790 --> 00:03:47,621
et générer une nouvelle clé.

51
00:03:47,621 --> 00:03:52,040
Je copie l'identifiant ici. Voilà.

52
00:03:52,040 --> 00:03:53,840
Voici donc notre clé d'API.

53
00:03:53,840 --> 00:03:55,095
Elle est ici.

54
00:03:56,665 --> 00:04:00,650
Nous sommes maintenant prêts
à utiliser les API ML.

55
00:04:00,650 --> 00:04:05,015
Dans la cellule "APIKEY" de notre bloc-notes,

56
00:04:05,015 --> 00:04:08,165
je vais remplacer la valeur existante par
notre nouvelle clé d'API,

57
00:04:08,165 --> 00:04:10,125
puis lancer l'exécution.

58
00:04:10,125 --> 00:04:12,280
Je peux soit cliquer
sur le bouton "Exécuter",

59
00:04:12,280 --> 00:04:14,515
soit appuyer simultanément sur
"Maj" + "Entrée".

60
00:04:14,515 --> 00:04:19,580
Nous allons ensuite
installer le client Python.

61
00:04:35,030 --> 00:04:40,045
Une fois que le client est installé,
nous pouvons exécuter l'API Translate.

62
00:04:40,045 --> 00:04:43,031
Vous pouvez remarquer que
les entrées sont bien présentes.

63
00:04:43,031 --> 00:04:44,541
Est-ce vraiment aussi simple ?

64
00:04:44,541 --> 00:04:51,380
La traduction s'affiche en français car nous
avons défini cette langue comme cible.

65
00:04:51,380 --> 00:04:53,768
Choisissons maintenant ES (espagnol)

66
00:04:53,768 --> 00:04:55,990
comme langue cible,
puis relançons l'API.

67
00:04:55,990 --> 00:04:58,600
Désormais, la sortie est en espagnol.

68
00:04:58,600 --> 00:05:00,160
Comment cela fonctionne-t-il ?

69
00:05:00,160 --> 00:05:04,780
Nous avons défini comme entrées
un groupe de chaînes de texte,

70
00:05:04,780 --> 00:05:08,470
puis demandé au service d'effectuer
une traduction de ces entrées de l'anglais

71
00:05:08,470 --> 00:05:11,767
vers une autre langue,
définie par la variable "target".

72
00:05:11,767 --> 00:05:16,115
En sortie, nous obtenons donc
les chaînes traduites.

73
00:05:16,115 --> 00:05:20,198
De la même façon, nous allons maintenant
invoquer l'API Vision.

74
00:05:20,198 --> 00:05:21,600
Pour invoquer l'API Vision,

75
00:05:21,600 --> 00:05:23,058
nous avons besoin d'une image.

76
00:05:23,058 --> 00:05:26,530
Dans le cas présent, il s'agit d'une image
représentant une plaque de rue.

77
00:05:26,530 --> 00:05:29,680
Je ne parle pas chinois, donc je ne sais
pas ce que dit ce panneau.

78
00:05:29,680 --> 00:05:34,235
Voyons ce qu'il signifie. Nous allons
l'envoyer vers Cloud Storage.

79
00:05:34,235 --> 00:05:37,965
Il est déjà accessible publiquement,
nous n'avons donc rien à modifier ici.

80
00:05:37,965 --> 00:05:39,610
Nous allons maintenant le lire.

81
00:05:39,610 --> 00:05:44,425
Nous pouvons demander à l'API Vision
de lire cette image,

82
00:05:44,425 --> 00:05:46,680
et de traduire ce texte pour nous.

83
00:05:46,680 --> 00:05:48,555
J'exécute l'API.

84
00:05:48,555 --> 00:05:52,585
Nous obtenons alors la sortie JSON.

85
00:05:52,585 --> 00:05:59,150
Ici, nous invoquons donc
la version 1 de l'API Vision,

86
00:05:59,150 --> 00:06:02,970
en transmettant l'URI de l'image GCS.

87
00:06:02,970 --> 00:06:06,100
Je vous rappelle que GCS signifie
Google Cloud Storage.

88
00:06:06,100 --> 00:06:08,215
Notre image est stockée sur Cloud Storage.

89
00:06:08,215 --> 00:06:10,450
Nous pourrions également
transmettre cette image

90
00:06:10,450 --> 00:06:14,470
dans le corps de la requête, mais la stocker
sur Cloud Storage accélère le processus,

91
00:06:14,470 --> 00:06:17,900
car nous n'avons pas besoin de charger
cette image avec notre requête.

92
00:06:17,900 --> 00:06:20,635
Nous demandons à l'API d'effectuer
une détection de texte.

93
00:06:20,635 --> 00:06:23,885
En sortie, nous obtenons le texte contenu
dans cette image,

94
00:06:23,885 --> 00:06:28,075
la mention "ZH", qui signifie que la
langue source est le chinois,

95
00:06:28,075 --> 00:06:32,180
ainsi qu'un polygone de délimitation
pour chaque portion de texte.

96
00:06:32,180 --> 00:06:38,055
Nous pouvons bien sûr récupérer la
première portion de texte,

97
00:06:38,055 --> 00:06:40,935
prendre l'annotation de texte,

98
00:06:40,935 --> 00:06:44,722
et obtenir la langue source,
ou le code de langue, ZH en l'occurrence.

99
00:06:44,722 --> 00:06:47,635
Nous pouvons ensuite "imprimer"
ce que nous avons obtenu :

100
00:06:47,635 --> 00:06:51,075
la langue étrangère concernée
(le chinois, ZH),

101
00:06:51,075 --> 00:06:54,275
ainsi que le texte en langue étrangère,
soit tout ceci.

102
00:06:54,275 --> 00:06:57,180
Maintenant, nous pouvons
exécuter la requête.

103
00:06:57,180 --> 00:07:00,400
Bien sûr, le résultat de l'exécution
précédente est encore présent.

104
00:07:00,400 --> 00:07:03,300
Je vais donc cliquer sur cette cellule
et l'effacer.

105
00:07:03,300 --> 00:07:05,600
Maintenant, je peux
relancer l'exécution

106
00:07:05,600 --> 00:07:08,145
et m'assurer qu'il s'agit bien
de ma propre requête.

107
00:07:08,145 --> 00:07:13,265
Vous pouvez constater que le texte
chinois a été traduit en anglais.

108
00:07:13,265 --> 00:07:16,685
Nous pouvons également
faire appel à l'API Language.

109
00:07:16,685 --> 00:07:20,345
Ici, nous avons une série de citations.

110
00:07:20,345 --> 00:07:24,210
Imaginez que nous souhaitions identifier
les sentiments associés à ces citations.

111
00:07:24,210 --> 00:07:25,825
Comme précédemment,

112
00:07:25,825 --> 00:07:29,300
nous allons effacer le contenu de la
cellule et exécuter la requête d'API.

113
00:07:29,300 --> 00:07:30,440
Dans ce cas,

114
00:07:30,440 --> 00:07:33,320
nous imprimons la polarité et la magnitude

115
00:07:33,320 --> 00:07:36,245
associées à chacune de ces citations.

116
00:07:36,245 --> 00:07:38,570
Donc, la polarité est positive

117
00:07:38,570 --> 00:07:39,980
pour un sentiment positif.

118
00:07:39,980 --> 00:07:42,270
Elle est négative pour un
sentiment négatif.

119
00:07:42,270 --> 00:07:43,670
C'est assez logique.

120
00:07:43,670 --> 00:07:46,820
La citation "to succeed you must have
tremendous perseverance"

121
00:07:46,820 --> 00:07:49,735
(la persévérance est la clé de la réussite)
est très positive.

122
00:07:49,735 --> 00:07:51,455
En revanche, celle qui commence par

123
00:07:51,455 --> 00:07:53,995
"when someone you love dies"
(la perte d'un être cher)

124
00:07:53,995 --> 00:07:55,025
est assez négative.

125
00:07:55,025 --> 00:07:56,805
Donc sa polarité est négative.

126
00:07:56,805 --> 00:07:59,670
La magnitude indique quant à elle

127
00:07:59,670 --> 00:08:04,120
la fréquence d'utilisation d'un
langage très fort dans le texte.

128
00:08:04,670 --> 00:08:08,715
Le dernier élément que je souhaite
vous montrer est l'API Speech.

129
00:08:08,715 --> 00:08:12,190
Comme précédemment, nous avons importé
un fichier (audio, cette fois)

130
00:08:12,190 --> 00:08:13,190
dans Cloud Storage,

131
00:08:13,190 --> 00:08:17,835
et nous demandons la transcription
du discours en texte.

132
00:08:17,835 --> 00:08:20,409
Exécutons cette requête.

133
00:08:20,409 --> 00:08:23,040
Nous obtenons en retour une réponse JSON.

134
00:08:23,040 --> 00:08:27,187
La sortie JSON indique, avec
un haut degré de certitude,

135
00:08:27,187 --> 00:08:31,447
que les paroles prononcées dans ce
fichier audio sont les suivantes :

136
00:08:31,447 --> 00:08:34,979
"How old is the Brooklyn Bridge?"
(De quand date le Pont de Brooklyn ?)

137
00:08:37,399 --> 00:08:39,759
Pour résumer, dans cet atelier,

138
00:08:39,949 --> 00:08:44,040
nous avons utilisé Datalab pour envoyer
des requêtes aux API Python,

139
00:08:44,040 --> 00:08:50,240
principalement dans l'optique d'invoquer
des modèles de machine learning.

140
00:08:50,240 --> 00:08:53,595
Souvenez-vous que nous n'avons pas
eu besoin de concevoir ces modèles.

141
00:08:53,595 --> 00:08:56,780
Ils existent déjà, il vous
suffit de les utiliser.

142
00:08:56,780 --> 00:09:01,700
Vous pouvez incorporer ces modèles de
machine learning dans vos applications.

143
00:09:01,700 --> 00:09:06,507
Notez bien que vous n'aurez pas besoin
de créer tous vos outils de ML

144
00:09:06,507 --> 00:09:08,997
à partir de zéro.

145
00:09:10,055 --> 00:09:14,075
Par exemple, pour effectuer une
reconnaissance de texte ou d'image,

146
00:09:14,075 --> 00:09:17,000
il vous suffit d'utiliser l'API Vision.