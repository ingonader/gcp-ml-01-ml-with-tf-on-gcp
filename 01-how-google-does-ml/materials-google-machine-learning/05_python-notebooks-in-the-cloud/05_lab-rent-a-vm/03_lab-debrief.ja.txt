このラボでは 必要なアクセスと
セキュリティを備えた Compute Engineインスタンスの
作成方法について見ました そしてCloud Storageでデータを交換し 簡単なタスクを行いました つまり ウェブページで現在の
地震データを表示することです GCP Consoleにログインした後 
まず最初に Compute Engineインスタンスを
作成しました Compute Engineインスタンスは
左側にあります ここでCompute Engineと
VMインスタンスがあります ここで インスタンスを作成できます インスタンスには好きな名前を付けられます 「earthquake」という名前にしましょう us-central1-bを選びます vCPUは1つで十分でしょう すべてのCloud APIへのアクセスを許可して インスタンスを作成します 現在 インスタンスを作成中です ラベルは必要ありませんので 情報パネルを非表示にします earthquakeというインスタンスを
us-central1-bゾーンに作成中です まだIPアドレスがありません ... はい インスタンスができました
外部IPもあります SSHメニュー項目に移り SSHをクリックして先に進みます ポップアップを許可しさらにSSHします 作成したばかりの仮想マシンに
SSHでアクセスしました 作成したばかりの この仮想マシンには 必要なソフトウェアがありません 重要なソフトウェアとして たとえば
gitというソフトが必要でしょう これを使うと
リポジトリからソースコードを入手できます しかしgitと入力しても gitコマンドが見つかりません まずgitをインストールする必要があります sudo apt-get install gitで
インストールできます ここで 別のセルで続行します gitがインストール中です Sudoではルートとして操作できるので VMにソフトウェアをインストールする
ためのアクセス権が得られます ここでgitと入力すると gitが存在します gitがインストール済みです 次に このコースに対応するコードの gitクローンを作成します これはgithub.com Google Cloud Platform
training-data-analystにあります この時点でリポジトリ
training data analystがあります ここでQwiklabに戻ると 先ほどCompute Engineインスタンスを
すでに作成し それにSSH接続して gitソフトウェアをインストールしました 次は どうしますか USGSから地震データを取り込みましょう 幸い データを入手可能にする
スクリプトが作成済みで そのスクリプトはリポジトリの中にあります training data analystに移動して ... どこを見ましょうか コース/機械学習/
deepdive 01 googlemlを調べる必要があります やってみましょう cdに続いてcourses machine learning deepdive タブの入力補助を使っているので すべてを入力しなくて済みます cd 01 googlemlと入力すると 必要なスクリプトがここにあります 次にingest.shを調べることができます
見てみましょう earthquakes（地震）の中です less ingest.shと入力します これで既存のearthquakes.csvが
すべて削除され download wgetつまりweb getにより
csvファイルをダウンロードします その名前はearthquakes.csvです 実行するにはbash ingest.shと
入力します これでearthquakes.csvが
存在するようになりました 最初の数行を見るためにheadできます ファイルの最初の10行です ここ数週間に発生したすべての地震の
時刻 緯度 経度が表示されます これを入手した後 データを変換できます データの変換が終わりました これはDatalab Notebookの一例です これクリックして概観を見てみましょう earthquakes.iphython Notebookがあり 実行中のコードが表示されます このコードは地震のデータを入手し matplotlibを使ってそれをプロットします 次にそれをpng画像ファイルとして保存します 操作を進めます 指示に戻り 足りないソフトウェアを
すべてインストールする必要があります 他にどんなソフトウェアが必要か ちょっと調べてみましょう install missing.shがあります
その中のインストール指示によると basemapをインストールする必要があります basemapはPythonパッケージです
これで地図を描くことができます Python numpyは数値処理用ライブラリです matplotlibは
基礎プロッティング用ライブラリです 必要な3つのPythonパッケージだけを
インストールします bash install missing.shと入力すると これらのソフトウェア
パッケージをすべて入手して この仮想マシンにインストールします これで ソフトウェアが
すべてインストールされました 次に このディレクトリで
transform.pyを実行できます transform.pyの中には IPython Notebookのすべての
pythonコードが含まれています プロットして画像ファイルを
作成するコードです ではpython transfrom.pyを実行しましょう この時点で ls minus lrtを実行できるならば 作成したばかりの内容を表示できます ご覧のように新しいファイル
earthquakes.pngがあります 画像ファイルです この画像ファイルを取り出して
クラウドに置く必要があるとします クラウド上のストレージ つまり Google Cloud Storageに置いてみましょう そのために バケットを作成します GCPウェブサービスメニューに戻ります 左側にあるバーをクリックして
Storageまでスクロールダウンし バケットを作成します このバケットの名前は
一意である必要があります 世界で他にない名前です 世界中で一意のものと言えば
プロジェクト名です よほど運が悪くない限り 私たちのプロジェクト名を
誰もバケット名に使っていないでしょう ですからプロジェクトの名前をコピーします それをバケットの名前として使います 複数リージョンまたは特定リージョンの
バケットにすることができます ここでは特定リージョンバケットを作成します us-central1です Compute Engineインスタンスの作成に
使ったのと同じリージョンですね これで データ取得にかかる待ち時間が減ります こうして作成してみましょう これでバケットが作成されました ここで 作成したばかりのearthquake.pngを
このバケットにコピーします その方法は？ gsutilを使うことができます コピーするには まずgsutilと入力し バケットの名前を入力します バケット名は 先程のように
プロジェクトの名前です 幸いにもこのプロジェクト名を使えます プロジェクト名 さらにたとえばearthquakes これで すべてのファイル earthquakes.htm earthquakes.png
earthquakes.csvが3つとも クラウドにコピーされ
バケットの中に存在します 実際にバケットに戻って更新表示してみます これはearthquakesです
中には3つのファイルがあります 次に3つのファイルをすべて取り出して
パブリックに共有してみましょう パブリックに共有すると パブリックリンクを入手できます このearthquakes.htmの
パブリックリンクをクリックできます クリックして表示されるのは storage.googleapis.com/プロジェクト名 /earthquake/earthquake.htmです これは基本的にフォルダ構造です そして完全に公開されたウェブページが
見えます 公開ウェブページである理由は
すでにパブリックにしたからです 公開共有を設定しました そうしないと
ウェブでこれにアクセスできません こうして このラボではCompute Engineを使用し さらにCloud Storageを基本的に
コンピューティングリソースとして 従来型の方法で使いました もちろんこの方法でリソースを
使うわけではありません VMを作成してソフトウェアを
インストールするわけではありません 代わりに このコースの大半では いわゆるマネージド サービスを使用します これを使えば 必要なコードを実行するときに 自分でVMをプロビジョニングしたり ソフトウェアをインストールしたり
しなくて済みます 代わりに「ここにあるコードを
実行しなさい」と指示し 最終結果として
そのコードの影響がなくなります このような高いレベルの より抽象的な形で クラウドサービスを使う方法を この後のラボで見ていきます 今回のラボで皆さまにお見せしたかったことは どんな基礎の上に
すべての機能が構築されているか つまり
コンピューティング用のCompute Engineと 永続的保存のためのCloud Storageの基礎です