So in this lab, we looked at how to create a Compute Engine Instance provided with the necessary access and security association to it, and then interact with data on Cloud storage×ª and carry out a simple task which was to make a web page display current earthquake data. So first thing that we did was at once we were in the GCP console, we went ahead and wanted to create a Compute Engine Instance. And the compute engine instance is on the left hand side. There it is, compute engine, VM instances. And at this point, we can go ahead and create the instance. And we can call the instance whatever we want, so let me go ahead and call it, maybe earthquake. And, let me pick it to be in US Central 1-b and 1 vCPU should be enough, and we will allow access to all the Cloud APIs, and go ahead and create the instance. So at this point, this instance is being created. You don't need the label. So let me hide the info panel. So the instance called earthquake is getting created in the zone US central 1-b. And at this point, there is no IP address and at this point the instance has been created and in now we have an external IP. I can go to the SSH menu item, click on SSH and go ahead and allow pop-ups and SSH. So there we are, we are now SSH in into the virtual machine that we have just created. Now this virtual machine that we've just created doesn't have the software that we need. So for example, one of the key piece of software that will want is software called git, which allows me to get the source code from the repository. But if I type in git, you notice that git command is not found. So the first thing I need to do is to go ahead and install git. So I can do that by sudo apt-get install git. And at that point, another cell, want to continue. Git is getting installed. Sudo let's me run an operation as a route which I need access to in order to install software on the VM. So at this point now if I type in git, there is git. Git has been installed and what I want to do is to go ahead and do a git clone of the code corresponding to this course. That's in github.com, Google Cloud platform, training-data-analyst. And at this point, I have the repository training data analyst. And if we now go back to the qwiklabs, we've just gone ahead and created the compute engine instance. We've SSH into it. We've installed the software git. So now what is it that we want to do? We want to basically go ahead and ingest earthquake data from USGS. Fortunately, a script has already been written that allows us to get it and that script. So that script is present inside the repository cd to training data analyst. And, let's see where we need to go. And now, we need to go into a courses machine learning deepdive 01, googleml. So let's do that. So cd to courses machine learning deepdive. And by the way, I'm typing in tabs to do the completion so I don't have to type in everything. So cd 01 googleml. And in there are the scripts that we will need. So we can go ahead and look at the ingest.sh. Let's go ahead and look at ingest.sh. So that is in earthquakes and less ingest.sh. And all that this is doing is removing any earthquakes.csv that already exists and doing a download wget or web get is a way to download a csv file and I'm calling it earthquakes.csv. So, we can go ahead and run that by typing in bash ingest.sh. And at this point, there is an earthquakes.csv that exists. We can do a head of it to look at the first few lines. So there is the first ten lines of the file and as you noticed there is a time, there's a latitude, longitude of all of the earthquakes that have happened in the recent week. And now having gotten that, we can now go ahead and transform the data. The transforming the data is done. And this is an example of a data lab notebook. So let me go ahead and click on this to show you what that looks like. So there is an earthquakes.ipython notebook and it shows you the code that is being done and what the code is being done as it goes ahead and gets the earthquake data and plots it using matplotlib. And then, what we will do is to basically go ahead and save it as an image as a png file. So let's go ahead and do that. Go back to the instructions and we need to install all of the missing software. So what other software do we need? Let's go ahead and look at what other software we need. So there is the install missing.sh that contains installation instructions to install basemap. Basemap is a Python package that allows us to draw geographic maps. Python numpy is a numeric processing library and matplotlib which is the basic plotting library. So we're just installing the three python packages that we need. So I'll do bash install missing.sh. At this point, it's going to go ahead and get all of the software packages and install them on this virtual machine. So now that all of the software has been installed, I can go ahead and run transform.py In this directory. Transform.py contains all of the python code that was present in the IPython notebook, the ones to go ahead and plot it and create an image file. So let's go ahead and do a python transform.py. And at this point, if you can do ls minus lrt, it shows us the latest things that have been created, and as you see there is a new file called earthquakes.png. That is the image file. So now, we want to take this image file and we want to put it onto the Cloud, and the way we do that, the storage on the Cloud, is in Google Cloud storage. So in order to do that, let's go ahead and create a bucket. Let's go back to the GCP web services menu, click on the left hand side bar and scroll down to storage and create a bucket. And the name of the bucket I need it to be unique. It needs to be globally unique and one globally unique thing that we do have is a project name. And unless we're extremely unlucky, nobody has used our project name as the name of their bucket. So let me go ahead and copy the name of the project. And use that as the name of the bucket. And, we can have this be a multi-regional bucket, our regional bucket. Let me go and make it a regional bucket. And it'll be in the US central one which is the same region that I used to create my Compute Engine Instance. This reduces the amount of latency involved in getting at the data. So go ahead and create it. And at this point the bucket has been created. And now, what we want to do is to copy the earthquake.png that we just created into this bucket. How do we do that? We can do that using gsutil. So, let's go ahead and we will copy this. So I'm going to type in gsutil and the name of your bucket. The name of your bucket again is the name of the project. So fortunately, I can just use that. Name of project and maybe earthquakes. And at this point, all of the files, all earthquakes.htm and earthquakes.png and earthquakes.csv. All three of them get copied over to the Cloud and they are all present in the bucket. In fact if we go back to the bucket and we refresh, we are now seeing earthquakes and inside the earthquakes we see the three files. Let's go ahead and take all of these three files and share them publicly. And by sharing it publicly, we basically get a public link. So I can now click on this public link to earthquakes.htm. And when I click on the public's link, you basically see that it's called storage.googleapis.com slash the name of my project, slash earthquake slash earthquake.htm which is basically the folder structure. And we see that now we have a completely public webpage. And the reason that we have a public webpage is because we went ahead and made it public. We set share publicly. If we did not have share publicly, we would not be able to access this on the web. So in this lab, what we've done is that we've used Compute Engine and we use Cloud storage as essentially computing resources, the traditional way. Of course, this is not the way we are going to work with resources. We're not going to be going ahead and creating a VM and installing software to work with it. Instead, for the most part in this course, we'll be using what are called Managed Services. Managed services allow us to run the code that we want to run, without having to provision VM's ourselves. Install software ourselves. Instead, you say, here's same code, run it for me and the end result would be the effect of that code having been dropped. So we will see this higher level way, the more abstract way, of using Cloud services in later labs. But, what we wanted to show you in this particular lab was the fundamentals of what all of this is built on and the fundamentals that everything is built on, our Compute Engine for compute and Cloud storage for storing things in a persistent way.