1
00:00:00,000 --> 00:00:02,120
So, we are now in Lab Two,

2
00:00:02,120 --> 00:00:07,345
where we're going to show how to use Datalab to run a BigQuery query.

3
00:00:07,345 --> 00:00:11,140
So here, we are going to be analyzing a dataset that's relatively large.

4
00:00:11,140 --> 00:00:14,230
70 million rows, eight Gigabytes of data,

5
00:00:14,230 --> 00:00:17,730
and we'll analyze it using BigQuery and using Cloud Datalab.

6
00:00:17,730 --> 00:00:20,940
So first thing to do is to go ahead launch Cloud Datalab,

7
00:00:20,940 --> 00:00:23,350
and we can do this from within Cloud Shell.

8
00:00:23,350 --> 00:00:26,370
So, first thing to do is to basically figure out,

9
00:00:26,370 --> 00:00:28,710
what our compute zones are?

10
00:00:28,710 --> 00:00:32,325
So that we can run Datalab in one of those compute zones.

11
00:00:32,325 --> 00:00:34,900
I happen to know that U.S. Central one is a compute zone,

12
00:00:34,900 --> 00:00:36,370
so I'm going to skip that part.

13
00:00:36,370 --> 00:00:39,205
And then, go ahead and create Datalab.

14
00:00:39,205 --> 00:00:43,750
So, that's Datalab create the name of the vm and sum zone.

15
00:00:43,750 --> 00:00:45,510
So, let's go ahead and do that.

16
00:00:45,510 --> 00:00:48,455
So, we'll go back to Cloud Shell.

17
00:00:48,455 --> 00:00:50,580
So I'm hearing Cloud Shell,

18
00:00:50,580 --> 00:00:53,765
I will say "datalab create mydatalabvm."

19
00:00:53,765 --> 00:01:00,295
And the zone is going to be "us-central1-b."

20
00:01:00,295 --> 00:01:06,795
Now, this command takes probably about five minutes to complete.

21
00:01:06,795 --> 00:01:08,810
So we're going to fast forward the video, and then,

22
00:01:08,810 --> 00:01:12,300
get to the point where you see this message that

23
00:01:12,300 --> 00:01:16,450
says that we can now connect on localhost 8081,

24
00:01:16,450 --> 00:01:19,485
or do a Web preview on 8081.

25
00:01:19,485 --> 00:01:23,580
So, let's wait for that to show up, and then we will continue.

26
00:01:23,580 --> 00:01:26,405
So while Datalab is starting,

27
00:01:26,405 --> 00:01:28,965
let's go ahead, and try the BigQuery.

28
00:01:28,965 --> 00:01:31,125
So this is a query that I want to run,

29
00:01:31,125 --> 00:01:34,755
the comment here is very important because we want to run standard SQL,

30
00:01:34,755 --> 00:01:41,965
and by default, the BigQuery user interface at least now at the time that I'm recording,

31
00:01:41,965 --> 00:01:44,480
it defaults to what is called Legacy SQL.

32
00:01:44,480 --> 00:01:46,950
So, we will go to the BigQuery Console,

33
00:01:46,950 --> 00:01:51,400
and we can get to the BigQuery Console from the GCP Menu,

34
00:01:51,400 --> 00:01:58,590
by selecting this, and going down, and selecting BigQuery.

35
00:01:58,590 --> 00:02:02,860
And we get asked to sign in to the password,

36
00:02:02,860 --> 00:02:05,255
the password again is from QwikLabs.

37
00:02:05,255 --> 00:02:09,475
So, take the QwikLabs password,

38
00:02:09,475 --> 00:02:19,390
put that in, and we are in BigQuery.

39
00:02:19,390 --> 00:02:22,500
Make sure that the project is your project.

40
00:02:22,500 --> 00:02:27,455
So in this case, not this and not QwikLabs resources or anything else.

41
00:02:27,455 --> 00:02:30,640
So we are here, we can click on the "Compose Query" and again,

42
00:02:30,640 --> 00:02:33,025
your user interface might be slightly different,

43
00:02:33,025 --> 00:02:35,175
user interfaces change all the time.

44
00:02:35,175 --> 00:02:38,480
But in general, many of these buttons,

45
00:02:38,480 --> 00:02:42,835
et cetera, should be recognizable.

46
00:02:42,835 --> 00:02:48,445
So we'll go here, and we will go ahead and run this particular query.

47
00:02:48,445 --> 00:02:51,025
So let's go ahead and run the query.

48
00:02:51,025 --> 00:02:53,115
So in another way, if you didn't want to say

49
00:02:53,115 --> 00:02:55,760
#standardSQL here is that I could gone ahead,

50
00:02:55,760 --> 00:02:57,900
and done show options.

51
00:02:57,900 --> 00:03:00,080
And turned off the Legacy SQL.

52
00:03:00,080 --> 00:03:01,725
So that's another way to do it.

53
00:03:01,725 --> 00:03:04,865
But either way, we want to run standardSQL.

54
00:03:04,865 --> 00:03:08,430
So we're going to run standardSQL, and then to run the Query.

55
00:03:08,430 --> 00:03:11,560
So this is being done on a dataset called

56
00:03:11,560 --> 00:03:15,355
"bigquery-samples" are project by BigQuery samples,

57
00:03:15,355 --> 00:03:18,090
and a dataset airline_ontime_data,

58
00:03:18,090 --> 00:03:20,320
and the name of the table is flights.

59
00:03:20,320 --> 00:03:24,135
So we can see that here, there is bigquery-samples.

60
00:03:24,135 --> 00:03:26,855
We don't actually see that here.

61
00:03:26,855 --> 00:03:31,470
So how would you get to see a project that is not on the left-hand side menu?

62
00:03:31,470 --> 00:03:34,530
What you would do is to go ahead and click on this down menu,

63
00:03:34,530 --> 00:03:36,375
and say "Switch to project",

64
00:03:36,375 --> 00:03:40,250
and "Display Project," and go and put that project in.

65
00:03:40,250 --> 00:03:43,700
And at this point, bigquery-samples shows up.

66
00:03:43,700 --> 00:03:47,290
And in bigquery_samples, there is the airline_ontime_data,

67
00:03:47,290 --> 00:03:50,410
and in there is the table called flights.

68
00:03:50,410 --> 00:03:53,070
So I can go ahead and look at the flights,

69
00:03:53,070 --> 00:03:56,325
and we see that in the preview,

70
00:03:56,325 --> 00:03:59,130
that these are the columns,

71
00:03:59,130 --> 00:04:02,665
and some example data values in the table.

72
00:04:02,665 --> 00:04:04,395
And look at the details,

73
00:04:04,395 --> 00:04:07,880
and it turns out that this table is nearly eight Gigabytes.

74
00:04:07,880 --> 00:04:10,425
It has over 70 million rows.

75
00:04:10,425 --> 00:04:12,165
And it is this.

76
00:04:12,165 --> 00:04:16,220
So, let's go ahead, and run the query.

77
00:04:21,630 --> 00:04:24,040
So it is

78
00:04:24,040 --> 00:04:27,300
this that we're basically queried against.

79
00:04:27,300 --> 00:04:30,915
What have you done here? We said go ahead and select the departure_dalay,

80
00:04:30,915 --> 00:04:33,000
and count the number of flights.

81
00:04:33,000 --> 00:04:34,350
So this is the number of flights out of

82
00:04:34,350 --> 00:04:38,055
a specific departure_delay because you are grouping by departure_delay.

83
00:04:38,055 --> 00:04:41,990
So for example, if the departure_delay's negative 37.

84
00:04:41,990 --> 00:04:47,040
In other words, that the flight departed 37 minutes early,

85
00:04:47,040 --> 00:04:48,505
how many flights were there?

86
00:04:48,505 --> 00:04:52,145
There are 107 such flights in the dataset,

87
00:04:52,145 --> 00:04:55,720
and these are the quantiles.

88
00:04:55,720 --> 00:05:00,195
So, this is each 28 percentile, right?

89
00:05:00,195 --> 00:05:02,555
Because it's divide by five.

90
00:05:02,555 --> 00:05:06,025
Like 80 percent of those flights,

91
00:05:06,025 --> 00:05:09,880
arrive 66 minutes or more early,

92
00:05:09,880 --> 00:05:17,975
and 60 to 80 percent of flights arrived between 41 minutes and 66 minutes, and so on.

93
00:05:17,975 --> 00:05:20,635
So we had a question that I asked you,

94
00:05:20,635 --> 00:05:23,915
if the departure_delay's 35 minutes early,

95
00:05:23,915 --> 00:05:25,700
what is a median value?

96
00:05:25,700 --> 00:05:27,015
And the median value,

97
00:05:27,015 --> 00:05:28,570
would be the value in the middle,

98
00:05:28,570 --> 00:05:31,430
right? So, 28 minutes.

99
00:05:31,430 --> 00:05:37,980
So, if you go back to our console,

100
00:05:37,980 --> 00:05:45,005
we now see that Datalab asks us whether we want to continue, and say "Yes."

101
00:05:45,005 --> 00:05:49,910
Go ahead, then accept all of the things.

102
00:05:49,910 --> 00:05:53,110
So let's go ahead, and run these other query.

103
00:05:53,110 --> 00:05:55,810
To go ahead and find the airport-pair.

104
00:05:55,810 --> 00:05:58,720
Airport-pair meaning a specific departure airport and

105
00:05:58,720 --> 00:06:03,215
a specific arrival airport that has a maximum number of flights between them.

106
00:06:03,215 --> 00:06:05,940
So this is again from the same table,

107
00:06:05,940 --> 00:06:08,830
but now, I'm selecting the departure_airport,

108
00:06:08,830 --> 00:06:11,640
the arrival_airport, and counting the number of flights

109
00:06:11,640 --> 00:06:15,085
but grouping by both the arrival_airport, and departure_airport.

110
00:06:15,085 --> 00:06:18,500
And ordering by number of flights descending which means,

111
00:06:18,500 --> 00:06:23,055
that the airport-pair with the maximum number of flights will be the first,

112
00:06:23,055 --> 00:06:24,420
and I'm limiting 10ths.

113
00:06:24,420 --> 00:06:25,950
I'm going to get there first 10.

114
00:06:25,950 --> 00:06:29,675
The 10 most common of those.

115
00:06:29,675 --> 00:06:35,985
So notice that this is something we've processed 17 million records.

116
00:06:35,985 --> 00:06:37,360
And when I did it,

117
00:06:37,360 --> 00:06:40,000
it took me 2.3 seconds.

118
00:06:40,000 --> 00:06:43,415
How is that possible? Well, it's because

119
00:06:43,415 --> 00:06:48,475
the 70 million records weren't done on this one machine that I'm running on, right?

120
00:06:48,475 --> 00:06:51,270
Where I'm running it, it's run on thousands of machines.

121
00:06:51,270 --> 00:06:52,575
It's run at scale.

122
00:06:52,575 --> 00:06:55,885
And this is what it mean when we say we launch services on the Cloud,

123
00:06:55,885 --> 00:06:58,070
we do these things in a serverless way.

124
00:06:58,070 --> 00:07:00,440
But anyway, going back here,

125
00:07:00,440 --> 00:07:04,070
it turns out that if the departure_airport is LAX,

126
00:07:04,070 --> 00:07:06,380
and the arrival_airport as SAN,

127
00:07:06,380 --> 00:07:11,875
that is a 133,000 flights.

128
00:07:11,875 --> 00:07:16,075
So that's the airport-pair with a maximum number of flights between them.

129
00:07:16,075 --> 00:07:19,405
So at this point, now when we go back to Cloud shell.

130
00:07:19,405 --> 00:07:22,360
We see that we might click on the Web preview,

131
00:07:22,360 --> 00:07:25,440
and change port to 8081 to start using Datalab,

132
00:07:25,440 --> 00:07:28,240
that is this item here, Web preview,

133
00:07:28,240 --> 00:07:32,010
so select that, change the port to 8081.

134
00:07:32,010 --> 00:07:33,415
And at this point,

135
00:07:33,415 --> 00:07:35,755
we are now inside Datalab.

136
00:07:35,755 --> 00:07:39,370
Everything that you've done in BigQuery so far has been great.

137
00:07:39,370 --> 00:07:43,740
We have been able to go ahead and run SQL queries on millions of rows of data,

138
00:07:43,740 --> 00:07:45,840
get our answers back in seconds.

139
00:07:45,840 --> 00:07:49,170
That's great, but what we really want,

140
00:07:49,170 --> 00:07:53,740
in addition to getting those answers is to do things like drawing graphs, et cetera.

141
00:07:53,740 --> 00:07:55,545
We want to be able to visualize the data.

142
00:07:55,545 --> 00:07:59,465
And visualization is one of those things that you can't do in the BigQuery Console.

143
00:07:59,465 --> 00:08:02,640
We want to use a custom visualization tool.

144
00:08:02,640 --> 00:08:04,600
In this case, we're going to use Datalab,

145
00:08:04,600 --> 00:08:06,810
which has full access to all of

146
00:08:06,810 --> 00:08:10,920
the Python goodness to go ahead and do all of our graphic.

147
00:08:10,920 --> 00:08:15,065
So what we're going to do here is that we're going to run one of our queries,

148
00:08:15,065 --> 00:08:19,745
but we're going to do this not from the BigQuery Console.

149
00:08:19,745 --> 00:08:21,295
But from within Datalab.

150
00:08:21,295 --> 00:08:23,125
So here we are in Datalab,

151
00:08:23,125 --> 00:08:26,975
I'll go ahead and start in your notebook.

152
00:08:26,975 --> 00:08:30,145
And in this notebook,

153
00:08:30,145 --> 00:08:32,390
what we have here is a code cell,

154
00:08:32,390 --> 00:08:34,880
so I can go and paste the code in that cell,

155
00:08:34,880 --> 00:08:38,175
and hit "Run" to run the code.

156
00:08:38,175 --> 00:08:42,710
So, all of this is being executed by BigQuery.

157
00:08:42,710 --> 00:08:44,765
So in the same order of seconds,

158
00:08:44,765 --> 00:08:47,700
we're going to be analyzing this millions of flights,

159
00:08:47,700 --> 00:08:53,005
and what we're now doing is I'm getting it back as a pandas dataframes.

160
00:08:53,005 --> 00:08:56,290
So.two_dataframe here is a pandas dataframe.

161
00:08:56,290 --> 00:09:03,485
So, it basically shows you the first few rows of that dataframe, and as before,

162
00:09:03,485 --> 00:09:04,740
we have a departure_delay,

163
00:09:04,740 --> 00:09:06,115
we have the number of flights,

164
00:09:06,115 --> 00:09:10,400
and we have the deciles because in this case,

165
00:09:10,400 --> 00:09:12,570
I'm doing the quantiles as 10.

166
00:09:12,570 --> 00:09:13,720
So there are 10 of them,

167
00:09:13,720 --> 00:09:16,505
and they get them back as a Python list.

168
00:09:16,505 --> 00:09:21,990
If you now go ahead and take the same dataframe,

169
00:09:21,990 --> 00:09:27,800
and we will basically go ahead and do a quick rename,

170
00:09:27,800 --> 00:09:34,180
what we now have is we've taken this deciles data,

171
00:09:34,180 --> 00:09:35,795
and we've broken it up,

172
00:09:35,795 --> 00:09:37,340
and gotten 0 percent,

173
00:09:37,340 --> 00:09:39,080
10 percent, 20 percent, 30 percent,

174
00:09:39,080 --> 00:09:41,590
et cetera, as separate columns.

175
00:09:41,590 --> 00:09:44,500
Why am I doing that? By doing separate columns,

176
00:09:44,500 --> 00:09:49,220
it allows me to do the next thing that I want to do.

177
00:09:49,640 --> 00:09:54,680
So, let's go ahead, and

178
00:10:04,260 --> 00:10:08,770
so at this point, I'm going to drop the 0 percent,

179
00:10:08,770 --> 00:10:10,675
or I'm going to drop the 100 percent,

180
00:10:10,675 --> 00:10:14,840
and I'm going to take the 10 to 90 percent all of that data,

181
00:10:14,840 --> 00:10:18,475
and I'm going to basically go ahead and plot them as graphs.

182
00:10:18,475 --> 00:10:20,700
So at this point,

183
00:10:20,700 --> 00:10:22,975
how do you read this graph?

184
00:10:22,975 --> 00:10:25,240
What you get here is that for example,

185
00:10:25,240 --> 00:10:27,065
in the departure_delay is 10.

186
00:10:27,065 --> 00:10:29,385
That's 10 minutes delay.

187
00:10:29,385 --> 00:10:33,870
10 percent of flights, nevertheless arrive early.

188
00:10:33,870 --> 00:10:39,605
But my end, 90 percent of flights arrive within above 21 minutes.

189
00:10:39,605 --> 00:10:40,975
So these are the deciles.

190
00:10:40,975 --> 00:10:44,640
The medium on the other hand is a departure-delay,

191
00:10:44,640 --> 00:10:47,545
and arrival_delay of perhaps,

192
00:10:47,545 --> 00:10:48,910
three or four minutes.

193
00:10:48,910 --> 00:10:51,260
So that's essentially, what these lines are?

194
00:10:51,260 --> 00:10:55,500
These lines give us a distribution at a specific departure_delay.

195
00:10:55,500 --> 00:11:00,990
Looking at this, you notice that the relationship is essentially linear

196
00:11:00,990 --> 00:11:07,915
for all departure_delays until it gets to maybe below minus 20.

197
00:11:07,915 --> 00:11:14,195
So far, flights that depart more than 20 minutes early, right?

198
00:11:14,195 --> 00:11:15,960
So they depart really early.

199
00:11:15,960 --> 00:11:18,640
The relationship is rather scattershot.

200
00:11:18,640 --> 00:11:19,800
It's not very linears.

201
00:11:19,800 --> 00:11:21,885
If we're going to be building a linear model,

202
00:11:21,885 --> 00:11:24,920
we will be okay with doing such a linear model,

203
00:11:24,920 --> 00:11:28,170
somewhere in the middle of the distribution but not at the edges.

204
00:11:28,170 --> 00:11:31,330
And this is the kind of thing that you cannot get

205
00:11:31,330 --> 00:11:34,685
this kind of an insight easily any other way.

206
00:11:34,685 --> 00:11:37,125
You need to basically plot distributions,

207
00:11:37,125 --> 00:11:40,015
and plotting distributions is a lot easier

208
00:11:40,015 --> 00:11:43,120
when you have the full power of Python at your disposal.