Dans cet atelier,
vous allez utiliser un format très utile. Vous allez vous servir de BigQuery
pour calculer des données agrégées, des valeurs de centile, etc.,
sur plus de 70 millions de lignes. Le résultat s'affichera dans un dataframe
Pandas d'une dizaine de lignes. Vous pourrez alors utiliser le dataframe
en mémoire à des fins de visualisation. Ce genre d'opération prendrait des heures
si vous utilisiez une autre méthode. Toutefois, dans cet atelier, vous allez créer un graphique
en quelques secondes. Il est crucial de créer ce type de flux
de développement interactif. Autrement, vous aurez du mal à utiliser
de larges ensembles de données. Vous pensez peut-être
que toutes les données ne sont pas utiles, que vous pourriez échantillonner
l'ensemble de données pour le réduire. Toutefois, ce n'est pas du tout recommandé
avec le machine learning. Je dis souvent que la grande différence entre
les statistiques et le machine learning, c'est le traitement
des anomalies. En statistique,
les anomalies sont supprimées, mais dans le machine learning, les anomalies sont apprises. Si vous voulez les apprendre, vous devez disposer
de suffisamment d'exemples, ce qui revient à dire
qu'il faut utiliser toutes vos données. Vous avez besoin d'une distribution
d'anomalies, de distributions de valeurs rares
au sein de l'ensemble de données. Et pour cela, vous devez utiliser
tout l'ensemble de données. L'une des méthodes préconisées est de procéder comme dans cet atelier. Vous devez utiliser
des services gérés tels que BigQuery pour traiter des données à grande échelle, puis les ramener dans des structures
en mémoire de type Pandas et vous servir d'outils 
comme les bibliothèques de traçage Python. C'est une méthode de travail courante
à laquelle nous devons nous habituer. C'est ce que nous allons faire maintenant.