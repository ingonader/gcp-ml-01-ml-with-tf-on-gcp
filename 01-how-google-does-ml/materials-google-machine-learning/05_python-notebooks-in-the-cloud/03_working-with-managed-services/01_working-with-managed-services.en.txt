Datalab works with the same technologies that you're comfortable with. So that you can start developing now, and then work on scale later. For example, we'll be doing an exercise where we read from a CSV file. You could then process it in pandas and Apache Beam, before training the model in TensorFlow. Make sure they all work. And then, improve the model through training. Eventually though, when you're ready to scale to train this model on all of your data, you can use Google Cloud Storage to hold your data, process it with Cloud Dataflow on an ephemeral cluster, and then run distributor training, and hyperparameter optimization in Cloud ML Engine. And you can do all those because Datalab integrates seamlessly with all other GCP products. In a few minutes, you'll do a lab that shows you how easy it is to connect to BigQuery, and harness thousands of machines to explore and analyze your data. You can also write TensorFlow code, and connect with Google machine learning APIs. Authentication is a breeze. You can even start big computational jobs in Cloud ML Engine and Dataflow. And of course, you can do all the things that you can do in a Python Notebook. Doing analysis with pandas, or visualizing query results using seaborn or plotly. So starting up Cloud Datalab is pretty simple. You go to Cloud Shell, and you type in "datalab create". Simple that is if you know what all these things mean. Cloud shell, compute zone, machine type. Let's back up a little. We need to tell you about Compute Engine. The point about using Compute Engine is that it's rented infrastructure. You're not going to keep it around forever. But when the machine goes away, your work also vanishes. So you need to save your notebook source code into Git. That's easy. So how about the results of that analysis, data, etc? You can't check those in, can you? So we also need to tell you about Cloud Storage.