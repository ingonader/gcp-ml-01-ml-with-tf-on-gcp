O Datalab trabalha com as mesmas
tecnologias com que você está acostumado. Assim, você pode começar a desenvolver
agora e trabalhar no escalonamento depois. Por exemplo, eu farei um exercício em que
leremos de um arquivo CSV. E você poderá então processar isso no
Pandas e no Apache Beam, antes de treinar o modelo no Tensor Flow. Certifique-se de que todos funcionam. E então melhore o modelo
por meio de treinamento. Em certo momento, quando estiver pronto
para escalonar para treinar esse modelo em todos os seus dados, você poderá usar o Google Cloud Storage
para manter seus dados, processá-los com o Cloud Dataflow em um
Cluster FML, executar o distribuidor de treinamento e otimizar os hiperparâmetros no
Cloud ML Engine. Você pode fazer tudo isso porque o Datalab se integra facilmente a todos os outros
produtos do GCP. Em poucos minutos você fará um laboratório que mostra como é
fácil se conectar ao BigQuery e ligar milhares de máquinas para explorar
e analisar seus dados. Você também pode gravar um código
do TensorFlow e conectar com APIs de aprendizado de
máquina do Google. A autenticação é muito fácil. Você pode até iniciar jobs de grande
computação no Cloud ML Engine e no Dataflow. E, claro, você pode fazer tudo o que for
possível em um bloco de notas do Python. Fazer análises com o Pandas ou visualizar resultados de consultas, usando Seaborn ou Plotly. Iniciar o Cloud Datalab é muito simples. Vá ao Cloud Shell e digite "datalab create". Simples assim, se você sabe
o que é tudo isso. Cloud Shell, zona de computação, tipo de máquina. Vamos voltar
um pouco. Precisamos falar a você sobre o
Compute Engine. O ponto sobre usar o Compute Engine
é que se trata de uma infraestrutura alugada. Você não vai
mantê-lo para sempre. Mas se uma máquina se vai, seu trabalho também desaparece. Você precisa salvar o código-fonte 
do seu bloco de notas no Git. Isso é fácil. Quais foram os resultados
daquela análise? Dados etc. Você pode verificá-los, não? Então também precisamos falar
sobre o Cloud Storage.