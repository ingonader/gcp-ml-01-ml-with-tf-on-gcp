Datalab arbeitet
mit vertrauten Technologien. So können Sie gleich
mit dem Entwickeln beginnen und bei Bedarf 
später Skalierungen vornehmen. Als Beispiel lesen wir
Daten aus einer CSV-Datei aus. Dann könnten Sie sie
in pandas und Apache Beam verarbeiten, ehe Sie das Modell
in TensorFlow trainieren. Prüfen Sie, ob alles funktioniert. Dann können Sie
das Modell durch Training optimieren. Wenn Sie das Modell skalieren,
um es mit all Ihren Daten zu trainieren, können Sie die Daten
in Google Cloud Storage speichern, sie mit Cloud Dataflow
in einem FML-Cluster verarbeiten und dann ein verteiltes Training und Hyper-Parameter-Optimierung
in Cloud ML Engine ausführen. All das ist möglich, weil sich Datalab nahtlos in alle
anderen GCP-Produkte integrieren lässt. In ein paar Minuten zeigen wir, wie leicht es ist,
eine Verbindung zu BigQuery herzustellen, um Daten von Tausenden VMs
erkunden und analysieren zu lassen. Sie können auch TensorFlow-Code schreiben und Verbindungen zu Machine Learning APIs
von Google herstellen. Die Authentifizierung ist ganz einfach. Auch große Rechenaufträge in
Cloud ML Engine und Dataflow sind möglich. Sie können auch alles tun, 
was in einem Python-Notebook möglich ist. Ob Analysen mit pandas oder das Visualisieren
von Abfrageergebnissen mit Seaborn oder Plotly. Cloud Datalab ist einfach zu starten. Öffnen Sie Cloud Shell und geben Sie "datalab create" ein. Das ist einfach, wenn Sie wissen,
was diese Begriffe bedeuten: Cloud Shell, Compute-Zone, Maschinentyp? Gehen wir kurz zurück. Sie müssen mehr
über Compute Engine wissen. Entscheidend ist,
dass die Infrastruktur gemietet wird. Sie steht nicht ständig zur Verfügung. Doch wenn die Maschine beendet wird, verschwindet auch Ihre Arbeit. Deshalb müssen Sie Ihren 
Notebook-Quellcode in Git speichern. So einfach ist das. Wie sieht es mit den Analyseergebnissen aus? Mit Daten usw. Sie können sie nicht einchecken. Deshalb müssen Sie mehr
über Cloud Storage wissen.