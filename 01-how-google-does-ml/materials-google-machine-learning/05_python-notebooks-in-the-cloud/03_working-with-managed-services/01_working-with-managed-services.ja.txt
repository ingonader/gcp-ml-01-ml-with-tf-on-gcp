皆さんが慣れているテクノロジーを
Datalabでも使えます それで 今すぐ開発を始めて あとでスケーリングすることができます たとえば 演習でCSVファイルから
読み取ります 次にpandaやApache Beamでそれを処理して TensorFlowでモデルを
トレーニングできます トレーニングでモデルを改善する前に すべての動作を確認すべきです 最終的にすべてのデータで
このモデルをトレーニングするために スケーリングの準備ができたら Google Cloud Storageでデータを格納し 短命クラスタ上のCloud Dataflowで
データを処理した後 ディストリビュータートレーニングや ハイパーパラメータの最適化を
Cloud MLエンジンで実行できます こうした操作が可能になるのは Datalabを他のGCPプロダクトと
シームレスに統合できるからです 今から数分後にラボを行います そこではBigQueryに簡単に接続できること 何千台ものマシンで簡単にデータを
探索/分析できることを見ていきます またTensorFlowコードを書いて Google機械学習APIで接続できます 認証はすごく簡単です クラウド上の巨大なジョブを
MLエンジンやDataflowで開始できます もちろん このすべてを
Python Notebookで行えます pandaで分析したり クエリ結果を可視化したりするには seaborn または plotly を使用します ですからCloud Datalabを始めるのは
とても簡単です Cloud Shellで「datalab create」と入力します これらの意味がわかっていれば簡単です Cloud Shell、
コンピューティング ゾーン、 マシンの種類などです 少し補足しましょう Compute Engineの説明をする必要があります Compute Engineを使用するときは インフラストラクチャを借ります 永遠に所有するわけではありません でも マシンがなくなれば作業も消えます それでNotebookソースコードをGitに
保存する必要があります 簡単ですね では その分析結果はどうですか データやアナリシスなどは チェックインできませんね そのためCloud Storageについても
説明する必要があります