Datalab funciona
con las mismas tecnologías que ya conocen. Pueden comenzar a desarrollar
de inmediato y escalar más tarde. Por ejemplo, haremos un ejercicio
en el que leeremos de un archivo CSV. Luego, pueden procesarlo
con Pandas y Apache Beam antes de entrenar el modelo en TensorFlow. Se asegurarán de que todo funciona. Luego, mejorarán el modelo
mediante el entrenamiento. Finalmente,
cuando estén listos para escalar y entrenar este modelo con sus datos podrán usar Google Cloud Storage
para almacenar sus datos procesarlo con Cloud Dataflow,
en un clúster FML y, luego,
ejecutar el entrenamiento distribuido y la optimización de los hiperparámetros
en Cloud ML Engine. Pueden hacer todo eso
porque Datalab se integra sin interrupciones
con todos los productos de GCP. En unos minutos,
harán un lab que les mostrará lo fácil que es conectarse a BigQuery
y aprovechar miles de máquinas para explorar y analizar los datos. También,
pueden escribir código de TensorFlow y conectarse con las API
de Google Machine Learning. La autenticación es muy fácil. Incluso pueden comenzar
trabajos de computación grandes en Cloud ML Engine y Dataflow. Por supuesto, podrán hacer lo mismo
que pueden hacer en un cuaderno de Python. Hacer análisis con Pandas
o visualizar los resultados de consultas con Seaborn o Plotly. Iniciar Cloud Datalab es muy simple. Van a Cloud Shell y escriben
"datalab create". Es simple si saben
lo que significa todo esto. Cloud Shell, "zone", "machine-type". Paremos un momento.
Debemos hablar de Compute Engine. El asunto sobre usar Compute Engine
es que es infraestructura alquilada. No la conservarán para siempre. Y si la máquina desaparece,
su trabajo también. Necesitan guardar el código fuente
de sus cuadernos en Git. Eso es sencillo. ¿Cuáles fueron los resultados
de ese análisis? Los datos, etcétera. No pueden verlos, ¿o sí? Por eso,
tenemos que hablar de Cloud Storage.