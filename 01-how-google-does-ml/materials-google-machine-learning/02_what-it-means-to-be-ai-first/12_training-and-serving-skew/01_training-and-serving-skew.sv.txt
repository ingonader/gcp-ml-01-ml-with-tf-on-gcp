So, how do you get started on Machine Learning? In our experience, we have seen that the typical customer journey, the one that's most likely to be successful, is to select the use case for which you are doing manual data analysis today. This is what Global Fishing Watch, a nonprofit that tries to identify poaching, this is what they did. They used to manually analyze fishing trips and then they scaled up their processing using Machine Learning to the point that they could analyze 22 million data points daily. So there are several reasons why you want to go through manual data analysis to get the Machine Learning. Number one, if you're doing manual data analysis, you probably have the data already and that is the hard part. Collecting data is often the longest and hardest part of a Machine Learning project and the most likely to fail. So, if you have the data already, your chances of success just went up. So that's one reason to basically go through manual data analysis. Second reason, even if you don't have the data today, so your ML project involves first collecting and rating the data, rating meaning finding labels for the data, you want to go through a manual analysis stage. The reason is, that if you cannot analyze your data to get reasonable inputs towards making decisions, then there's no point in doing Machine Learning. Manual analysis helps you fail fast, try new ideas. So, don't skip this analysis step. The analysis step will often tell you if there are insights to be had from the data. Third reason why you'd want to go to manual data analysis and not skip it, is that to build a good Machine Learning model, you have to know your data. And since that's the first step, why don't you go through the process of doing manual data analysis? Don't jump straight into ML. So, we'll talk about this more in the next module. But the fourth reason is that ML is a journey towards automation and scale. You are automating manual analysis because you want it to scale. Perhaps like Global Fishing Watch, you're manually analyzing a small fraction of fishing trips and you want to automate this so that you can scale up to analyzing a great deal more fishing trips. So, more pitily, if you can't do analytics, you can't do ML. So when we say Machine Learning to engineers, they keep thinking training. But the true utility of Machine Learning comes during predictions. That's when you're getting value from it. So one key thing then is that your models have to work on streaming data. You need to build up your streaming data sophistication. If you are thinking that you could get away with doing things weekly as batch processing, guess what? Your business is only getting faster. So, one common reason that Machine Learning products fail is because of something called training serving skew. This is where you had a certain system for processing historical data so that you could train on it. Perhaps, it was a batch processing system written by a data science team. And then you have a different system that needs to use the Machine Learning model during prediction. The system that serves these predictions is probably written something that your production engineering team writes and maintains. Perhaps, it's written in Java using web frameworks. The problem is that, unless the model sees the exact same data in serving as it was used to seeing during training, the model predictions are going to be off. So that is a problem that is referred to as training serving skew. So, the problem is that the result of stream processing and the result of batch processing have to be the same. So, one way to reduce the chances of this problem, one way to reduce the chances of training serving skew, is to take the same code that was used to process historical data during training and reuse it during predictions. But for that to happen, your data pipelines have to process both batch and stream. This is a key insight behind dataflow. A way to author data pipelines in Python, Java, or even visually with Cloud Data brand. It's open source is Apache Beam. Where B stands for batch, and the eam stands for stream. So a single system to do batch and stream. Because in Machine Learning, it's helpful. They use the same system in board, training, and prediction. The performance metrics that you care about change between training and predictions as well. During training, the key performance aspect we care about is scaling to a lot of data. Distributor training, if you will. During prediction, though, the key performance aspect is speed of response, high QPS. So, this is a key insight behind TensorFlow. Lots of Machine Learning frameworks exist for training. Not so many are equally capable of operationalization.