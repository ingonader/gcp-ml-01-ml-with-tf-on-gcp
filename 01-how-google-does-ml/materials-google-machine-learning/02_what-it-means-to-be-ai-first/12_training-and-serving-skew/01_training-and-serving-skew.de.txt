Wie gelingt der Einstieg
in das maschinelle Lernen? Unserer Erfahrung nach
wird folgender Weg oft genutzt und bietet Kunden
die höchsten Erfolgschancen: Man wählt einen Anwendungsfall
mit bislang manueller Datenanalyse. So war es auch
bei Global Fishing Watch, einer Organisation, die
gegen Fischwilderei kämpft. Sie werteten
Fangfahrten früher manuell aus. Dann skalierten sie ihre
Prozesse mit maschinellem Lernen und konnten dadurch 22 Mio.
Datenpunkte pro Tag analysieren. Es gibt also mehrere Gründe dafür, die manuelle Datenanalyse
vor das maschinelle Lernen zu stellen. Erstens liegen bei der manuellen
Datenanalyse die Daten meist schon vor. Das ist oft der schwierigste Teil. Die Datenerfassung
ist der Teil eines ML-Projekts, der oft am aufwendigsten ist
und am häufigsten misslingt. Wenn die Daten also schon vorliegen, sind Ihre Erfolgschancen schon höher. Dies spricht also dafür, zunächst eine
manuelle Datenanalyse zu durchlaufen. Zweitens, selbst wenn Sie
noch keine Daten besitzen, müssen Sie für Ihr ML-Projekt
zunächst Daten sammeln und bewerten, das heißt, den Daten ein Label zuweisen. Deshalb sollte man anfangs
eine manuelle Analyse durchlaufen. Wenn Sie Ihre Daten
nicht analysieren können, um belastbare Aussagen
für die Entscheidungsfindung zu erhalten, ergibt das maschinelle Lernen keinen Sinn. Die manuelle Analyse
lässt Sie schnell scheitern, damit Sie neue Ansätze probieren können. Darum sollten Sie
diesen Schritt nicht überspringen. Der Analyseschritt zeigt häufig,
ob die Daten neue Erkenntnisse enthalten. Drittens spricht Folgendes dafür, die manuelle
Datenanalyse nicht zu überspringen: Um ein gutes
ML-Modell zu entwickeln, müssen Sie Ihre Daten kennen. Da dies der erste Schritt ist, warum hierbei nicht eine
manuelle Datenanalyse durchlaufen? Springen Sie nicht sofort
zum maschinellen Lernen. Dieses Thema besprechen
wir detailliert im nächsten Modul. Viertens führt das maschinelle Lernen
zu Automatisierung und Massenverarbeitung. Sie automatisieren die manuelle
Analyse, um große Mengen zu verarbeiten. Vielleicht geht es Ihnen
wie Global Fishing Watch: Sie analysieren eine geringe
Anzahl an Fangfahrten manuell und möchten dies automatisieren,
um deutlich mehr analysieren zu können. Leider ist maschinelles Lernen
ohne Analysetools nicht möglich. Engineers verstehen unter
maschinellem Lernen meist nur Training. Dabei liegt der wahre
Nutzen von ML in den Vorhersagen. Dies ist der eigentliche
Wert für den Anwender. Darum müssen Ihre Modelle
Streamingdaten verarbeiten können. Sie müssen Ihre Prozesse
für Daten in Echtzeit ausbauen. Falls Sie glauben, Sie können Prozesse einmal pro
Woche per Batch-Verarbeitung erledigen, werden Sie feststellen,
dass Ihr Unternehmen nur schneller wird. Ein Hauptgrund für den
Misserfolg von ML-Projekten liegt darin, dass Verzerrungseffekte auftreten können. Man hat ein bestimmtes System
zum Verarbeiten der Verlaufsdaten, die dann zum Training verwendet werden. Vielleicht ein Batchverarbeitungssystem,
das von Data Scientists geschrieben wurde. Dann gibt es ein zweites System, das für die Prognose auf das Modell
für maschinelles Lernen zugreifen muss. Das System,
das diese Vorhersagen liefert, wird meist von Produktionstechnikern
entwickelt und von ihnen gewartet. Es wurde vielleicht in Java
geschrieben und verwendet Webframeworks. Es entsteht folgendes Problem: Wenn bei Prognose und Training nicht
exakt dieselben Daten genutzt werden, werden die
Vorhersagen im Modell verfälscht. Es entsteht eine Verzerrung
zwischen Training und Vorhersage. Die Ergebnisse von Stream-Verarbeitung und
Batch-Verarbeitung müssen identisch sein. Um dieses Problem zu vermeiden und das Risiko
für Verzerrungseffekte zu senken, verwendet man denselben Code, der schon beim Training
mit den Verlaufsdaten eingesetzt wurde, erneut während der Vorhersagen. Damit dies geschehen kann, müssen Ihre Datenpipelines eine Batch-
und Echtzeitverarbeitung ermöglichen. Dies ist ein Hauptvorteil von Dataflow. Datenpipelines können in Python, Java oder auch visuell
mit Cloud Data erstellt werden. Die Open-Source-Software ist Apache Beam. "B" steht für "Batch"
und "eam" steht für "Stream". Dieses System erlaubt also
Batch- und Streamingprozesse. Dies ist hilfreich
beim maschinellen Lernen. Dasselbe System wird für Datenerfassung,
Training und Vorhersage verwendet. Die gewünschten Leistungsmesswerte wechseln ebenfalls
zwischen Training und Vorhersage. Beim Training geht es vor allem
um Skalierbarkeit auf hohe Datenmengen. Sozusagen ein Distributor-Training. Bei der Vorhersage geht es
hingegen vor allem um schnelle Antworten und viele Abfragen pro Sekunde. Dies ist der Hauptvorteil von TensorFlow. Beim maschinellen Lernen gibt es
viele Frameworks für Trainingsaufgaben. Aber nur wenige ermöglichen
auch eine Operationalisierung.