¿Cómo pueden comenzar
con el aprendizaje automático? Según nuestra experiencia,
la ruta típica de un cliente la que tiene más
probabilidades de tener éxito es seleccionar un caso práctico
en el que los datos se analizan manualmente
en la actualidad. Esto es lo que hicieron
en Global Fishing Watch una organización sin fines
de lucro, para identificar la caza furtiva. Solían analizar de forma manual los viajes de pesca,
y luego escalaron el procesamiento mediante el AA hasta llegar
a analizar 22 millones de puntos de datos diariamente. Así que hay muchas razones para pasar por el análisis manual de datos
para llegar al aprendizaje automático. Número uno, si realizan
un análisis manual de los datos probablemente ya tienen
los datos y esa es la parte difícil. La recopilación de datos
suele ser la parte más larga y complicada de un proyecto de AA
y la que probablemente fallará. Así que, si ya tienen los datos sus probabilidades de éxito aumentan. Esa es una razón para pasar
por el análisis manual de datos. La segunda razón,
aún si no tienen los datos hoy y su proyecto de AA implica
primero recopilar y calificar los datos calificar significa
encontrar etiquetas para los datos les conviene pasar
por la etapa de análisis manual. La razón es que,
si no pueden analizar sus datos para obtener entradas
razonables a fin de tomar decisiones no tiene sentido realizar el AA. El análisis manual los ayuda
a probar nuevas ideas cuando fallan. No omitan este paso de análisis. El análisis les dirá si se pueden
obtener estadísticas con los datos. La tercera razón por la que deberían
realizar el análisis manual de los datos y no omitirlo
es que para crear un buen modelo de AA tienen que conocer sus datos. Y como ese es el primer paso ¿por qué no llevar a cabo el proceso
de análisis manual de datos? No pasen directamente al AA. Hablaremos más
sobre esto en el siguiente módulo. La cuarta razón
es que el AA es una transición hacia la automatización y el escalamiento. Están automatizando el análisis
manual porque quieren que escale. Tal vez, como Global Fishing Watch analizan de forma manual una pequeña
fracción de viajes de pesca y quieren automatizarlo para poder
escalar y así analizar una gran cantidad. En pocas palabras, si no pueden
realizar análisis, no podrán usar el AA. Cuando mencionamos el AA a los ingenieros solo piensan en entrenamiento. Pero el uso real del AA
está en las predicciones. Ahí es donde les ofrece más valor. Un aspecto clave es que sus modelos
deben trabajar con datos de transmisión. Tienen que lograr una sofisticación
en la transmisión de datos. Si creen que pueden hacer las cosas
una vez a la semana como procesamiento por lotes,
adivinen qué. Su negocio solo será más rápido. Una razón común
de por qué fallan los productos de AA es por algo llamado desviación
entre el entrenamiento y la entrega. Es cuando tienen
un determinado sistema para procesar los datos históricos
para entrenar en ellos. Tal vez era un procesamiento por lotes
escrito por un equipo de ciencia de datos. Y, luego, tienen un sistema diferente que necesita usar
el modelo de AA durante la predicción. El sistema que entrega estas predicciones tal vez está en un formato que su equipo
de ingeniería escribe y mantiene. Quizá está escrito
en Java con marcos de trabajo web. El problema es que a menos que el modelo vea
los mismos datos en la entrega que estaba acostumbrado
a ver durante el entrenamiento las predicciones
del modelo serán erróneas. Es un problema denominado desviación
entre el entrenamiento y la entrega. El problema es que el resultado
del procesamiento de transmisión y el resultado del procesamiento
por lotes tiene que ser el mismo. Una forma de disminuir
las probabilidades de este problema y reducir la probabilidad de desviación
entre el entrenamiento y la entrega es tomar el mismo código que se utilizó
para procesar los datos históricos durante el entrenamiento
y reutilizarlo durante las predicciones. Pero para que ocurra eso sus canalizaciones de datos tienen
que procesar por lotes y transmisión. Este es un punto clave
detrás del Dataflow. Una forma de crear
canalizaciones de datos en Python Java o incluso de forma
visual con Cloud Dataproc. Su código abierto es Apache Beam. Donde la “B” significa lote (batch) y el resto, “eam”, es transmisión (stream). Entonces, un solo sistema
para procesar por lotes y transmitir porque en el aprendizaje automático
es útil usar el mismo sistema en el entrenamiento y la predicción. Las métricas de rendimiento importantes también cambian
entre entrenamiento y predicciones. Durante el entrenamiento, un aspecto clave
del rendimiento es escalar a muchos datos. Entrenamiento distribuido,
si lo prefieren. En la predicción, el aspecto clave del
rendimiento es la velocidad de respuesta. Una QPS alta. Este es un aspecto
clave detrás de TensorFlow. Muchos marcos de trabajo
de AA existen para el entrenamiento. No muchos son
capaces de la operacionalización.