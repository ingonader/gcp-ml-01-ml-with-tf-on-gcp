1
00:00:00,000 --> 00:00:03,270
So, how do you get started on Machine Learning?

2
00:00:03,270 --> 00:00:07,815
In our experience, we have seen that the typical customer journey,

3
00:00:07,815 --> 00:00:10,875
the one that's most likely to be successful,

4
00:00:10,875 --> 00:00:16,650
is to select the use case for which you are doing manual data analysis today.

5
00:00:16,650 --> 00:00:18,390
This is what Global Fishing Watch,

6
00:00:18,390 --> 00:00:22,740
a nonprofit that tries to identify poaching, this is what they did.

7
00:00:22,740 --> 00:00:24,300
They used to manually analyze

8
00:00:24,300 --> 00:00:27,750
fishing trips and then they scaled up their processing using

9
00:00:27,750 --> 00:00:33,585
Machine Learning to the point that they could analyze 22 million data points daily.

10
00:00:33,585 --> 00:00:36,224
So there are several reasons why you want to go through

11
00:00:36,224 --> 00:00:40,400
manual data analysis to get the Machine Learning.

12
00:00:40,400 --> 00:00:43,910
Number one, if you're doing manual data analysis,

13
00:00:43,910 --> 00:00:47,600
you probably have the data already and that is the hard part.

14
00:00:47,600 --> 00:00:50,840
Collecting data is often the longest and hardest part of

15
00:00:50,840 --> 00:00:54,605
a Machine Learning project and the most likely to fail.

16
00:00:54,605 --> 00:00:57,110
So, if you have the data already,

17
00:00:57,110 --> 00:00:59,480
your chances of success just went up.

18
00:00:59,480 --> 00:01:04,190
So that's one reason to basically go through manual data analysis.

19
00:01:04,190 --> 00:01:08,150
Second reason, even if you don't have the data today,

20
00:01:08,150 --> 00:01:13,205
so your ML project involves first collecting and rating the data,

21
00:01:13,205 --> 00:01:16,755
rating meaning finding labels for the data,

22
00:01:16,755 --> 00:01:20,375
you want to go through a manual analysis stage.

23
00:01:20,375 --> 00:01:23,710
The reason is, that if you cannot analyze your data

24
00:01:23,710 --> 00:01:27,595
to get reasonable inputs towards making decisions,

25
00:01:27,595 --> 00:01:30,145
then there's no point in doing Machine Learning.

26
00:01:30,145 --> 00:01:35,790
Manual analysis helps you fail fast, try new ideas.

27
00:01:35,790 --> 00:01:38,500
So, don't skip this analysis step.

28
00:01:38,500 --> 00:01:44,485
The analysis step will often tell you if there are insights to be had from the data.

29
00:01:44,485 --> 00:01:49,968
Third reason why you'd want to go to manual data analysis and not skip it,

30
00:01:49,968 --> 00:01:52,377
is that to build a good Machine Learning model,

31
00:01:52,377 --> 00:01:54,040
you have to know your data.

32
00:01:54,040 --> 00:01:56,350
And since that's the first step,

33
00:01:56,350 --> 00:02:00,550
why don't you go through the process of doing manual data analysis?

34
00:02:00,550 --> 00:02:03,015
Don't jump straight into ML.

35
00:02:03,015 --> 00:02:05,725
So, we'll talk about this more in the next module.

36
00:02:05,725 --> 00:02:12,195
But the fourth reason is that ML is a journey towards automation and scale.

37
00:02:12,195 --> 00:02:16,685
You are automating manual analysis because you want it to scale.

38
00:02:16,685 --> 00:02:19,105
Perhaps like Global Fishing Watch,

39
00:02:19,105 --> 00:02:23,440
you're manually analyzing a small fraction of fishing trips and you want to

40
00:02:23,440 --> 00:02:28,180
automate this so that you can scale up to analyzing a great deal more fishing trips.

41
00:02:28,180 --> 00:02:35,925
So, more pitily, if you can't do analytics, you can't do ML.

42
00:02:35,925 --> 00:02:38,720
So when we say Machine Learning to engineers,

43
00:02:38,720 --> 00:02:41,305
they keep thinking training.

44
00:02:41,305 --> 00:02:46,235
But the true utility of Machine Learning comes during predictions.

45
00:02:46,235 --> 00:02:48,590
That's when you're getting value from it.

46
00:02:48,590 --> 00:02:53,295
So one key thing then is that your models have to work on streaming data.

47
00:02:53,295 --> 00:02:56,965
You need to build up your streaming data sophistication.

48
00:02:56,965 --> 00:02:59,340
If you are thinking that you could get away with doing things

49
00:02:59,340 --> 00:03:02,960
weekly as batch processing, guess what?

50
00:03:02,960 --> 00:03:08,545
Your business is only getting faster.

51
00:03:08,545 --> 00:03:11,160
So, one common reason that Machine Learning products

52
00:03:11,160 --> 00:03:15,015
fail is because of something called training serving skew.

53
00:03:15,015 --> 00:03:17,910
This is where you had a certain system for processing

54
00:03:17,910 --> 00:03:20,895
historical data so that you could train on it.

55
00:03:20,895 --> 00:03:25,395
Perhaps, it was a batch processing system written by a data science team.

56
00:03:25,395 --> 00:03:28,380
And then you have a different system that

57
00:03:28,380 --> 00:03:31,840
needs to use the Machine Learning model during prediction.

58
00:03:31,840 --> 00:03:35,760
The system that serves these predictions is probably

59
00:03:35,760 --> 00:03:40,575
written something that your production engineering team writes and maintains.

60
00:03:40,575 --> 00:03:45,360
Perhaps, it's written in Java using web frameworks.

61
00:03:45,360 --> 00:03:46,965
The problem is that,

62
00:03:46,965 --> 00:03:51,060
unless the model sees the exact same data in serving

63
00:03:51,060 --> 00:03:55,150
as it was used to seeing during training,

64
00:03:55,150 --> 00:03:58,930
the model predictions are going to be off.

65
00:03:58,930 --> 00:04:04,250
So that is a problem that is referred to as training serving skew.

66
00:04:04,250 --> 00:04:08,870
So, the problem is that the result of stream processing and

67
00:04:08,870 --> 00:04:14,230
the result of batch processing have to be the same.

68
00:04:14,230 --> 00:04:17,860
So, one way to reduce the chances of this problem,

69
00:04:17,860 --> 00:04:21,018
one way to reduce the chances of training serving skew,

70
00:04:21,018 --> 00:04:24,950
is to take the same code that was used to process

71
00:04:24,950 --> 00:04:30,125
historical data during training and reuse it during predictions.

72
00:04:30,125 --> 00:04:31,840
But for that to happen,

73
00:04:31,840 --> 00:04:36,975
your data pipelines have to process both batch and stream.

74
00:04:36,975 --> 00:04:39,330
This is a key insight behind dataflow.

75
00:04:39,330 --> 00:04:42,978
A way to author data pipelines in Python,

76
00:04:42,978 --> 00:04:46,125
Java, or even visually with Cloud Data brand.

77
00:04:46,125 --> 00:04:48,750
It's open source is Apache Beam.

78
00:04:48,750 --> 00:04:51,195
Where B stands for batch,

79
00:04:51,195 --> 00:04:53,935
and the eam stands for stream.

80
00:04:53,935 --> 00:04:57,465
So a single system to do batch and stream.

81
00:04:57,465 --> 00:05:01,731
Because in Machine Learning, it's helpful.

82
00:05:01,731 --> 00:05:07,850
They use the same system in board, training, and prediction.

83
00:05:07,850 --> 00:05:11,090
The performance metrics that you care

84
00:05:11,090 --> 00:05:14,755
about change between training and predictions as well.

85
00:05:14,755 --> 00:05:22,180
During training, the key performance aspect we care about is scaling to a lot of data.

86
00:05:22,180 --> 00:05:24,745
Distributor training, if you will.

87
00:05:24,745 --> 00:05:33,115
During prediction, though, the key performance aspect is speed of response, high QPS.

88
00:05:33,115 --> 00:05:36,650
So, this is a key insight behind TensorFlow.

89
00:05:36,650 --> 00:05:40,890
Lots of Machine Learning frameworks exist for training.

90
00:05:40,890 --> 00:05:46,000
Not so many are equally capable of operationalization.