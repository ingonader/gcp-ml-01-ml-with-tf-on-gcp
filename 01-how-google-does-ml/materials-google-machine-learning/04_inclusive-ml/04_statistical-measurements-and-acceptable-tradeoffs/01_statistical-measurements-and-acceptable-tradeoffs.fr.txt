Après avoir défini la matrice
de confusion, on peut calculer toutes sortes
de métriques d'évaluation pour déterminer comment
améliorer l'inclusion du système de ML. Pour rendre un système de ML
plus inclusif, il faut se concentrer sur les taux
de faux positifs et de faux négatifs, pour comprendre comment un sous-groupe
peut avoir des performances négatives. On peut calculer le taux de vrais
positifs, la sensibilité ou le rappel, qui représentent le nombre de fois où le
modèle prédit un visage dans une image lorsque le libellé indique
aussi la présence d'un visage. Dans ce cas, vous avez seulement
besoin des vrais positifs et des faux négatifs correspondants
pour calculer le rappel. Autre exemple de calcul possible
avec une matrice de confusion : la précision, qui représente
le nombre de fois où le modèle prédit
les libellés correctement. En prenant en compte s'il s'agit
d'un libellé positif, par exemple, si l'image comporte un visage,
et si le modèle prédit un libellé positif, ou s'il s'agit d'un libellé négatif,
si l'image ne comporte pas de visage, et si le modèle prédit
un libellé négatif. Dans ce calcul, vous avez seulement besoin
de mesurer les vrais positifs et les faux positifs correspondants. Les taux de faux positifs,
de faux négatifs et de vrais positifs, la précision et le rappel constituent
de nombreuses métriques à gérer. Comment choisir les métriques
à privilégier pour rendre votre système de ML
plus inclusif ? Tout dépend des taux de faux positifs
et de faux négatifs obtenus. En faisant des compromis,
vous pouvez préférer que votre modèle présente un faible rappel
et ignore beaucoup de choses, mais offre une meilleure précision,
ou que le volume classifié soit faible, mais correct. Prenons l'exemple d'un modèle de ML
qui détermine si une image doit être floutée
pour protéger la vie privée. Un faux positif entraîne le floutage
d'un élément qui ne doit pas l'être, car le modèle prédit qu'il doit l'être. Cela peut être un problème. Avec un faux négatif, un élément
qui devrait être flouté ne l'est pas, car le modèle ne prédit pas
qu'il devrait l'être. Cela peut entraîner des situations
de non-respect de la vie privée, car la personne peut être exposée
dans l'image. Dans cet exemple, vous pouvez essayer de réduire au maximum
le nombre de faux négatifs. Vous devez analyser les métriques pour
atteindre un faible taux de faux négatifs. Dans d'autres cas,
il peut être préférable d'avoir des faux négatifs
plutôt que des faux positifs. Imaginons que vous travaillez
sur un modèle de filtrage de spam. Avec un faux négatif, un spam
ne sera pas détecté par le modèle, et il apparaîtra dans
votre boîte de réception. Mais que se passe-t-il
en cas de faux positif ? Un message d'un ami
ou d'un proche risque d'être marqué comme spam et d'être supprimé
de votre messagerie. Ce serait très problématique. Dans ce cas, il est préférable
de réduire le taux de faux positifs autant que possible. Une fois que vous avez identifié
les métriques d'évaluation à analyser, calculez ces métriques dans les différents
sous-groupes de vos données. Comme illustré dans ce graphique,
vous pouvez visualiser les distributions de vos métriques d'évaluation
dans un sous-groupe, comme le montrent
les courbes bleue et verte, qui représentent chacune
un sous-groupe distinct de vos données. Une fois cela en place, il ne reste plus
qu'à déterminer la valeur acceptable et à comparer les valeurs
entre les sous-groupes. Par exemple, un taux de faux négatifs
de 0,1 peut être acceptable pour le problème que vous tentez
de résoudre avec votre système de ML. À quoi ressemble le taux global
sur tous vos sous-groupes ? Ces méthodologies vous aideront
à identifier des solutions pour rendre votre système de ML
plus inclusif. Pour résumer, les métriques d'évaluation
sont essentielles pour mesurer le niveau d'inclusion d'un système
de machine learning. Cette mesure est importante
pour faire des compromis acceptables entre vos faux positifs
et vos faux négatifs. Choisissez vos métriques d'évaluation
selon les compromis acceptables entre vos faux positifs
et vos faux négatifs.