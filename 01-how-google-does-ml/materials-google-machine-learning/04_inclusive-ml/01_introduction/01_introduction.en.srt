1
00:00:00,100 --> 00:00:03,320
Welcome back. I'm Andrew Zaldivar,

2
00:00:03,320 --> 00:00:07,100
a Data Scientist in the Research and Machine Intelligence group at Google.

3
00:00:07,100 --> 00:00:08,720
I work on developing the benefits of

4
00:00:08,720 --> 00:00:11,490
artificial intelligence and machine learning for everyone.

5
00:00:11,490 --> 00:00:14,390
And this is part of what I'll be talking about in this course.

6
00:00:14,390 --> 00:00:17,345
So far, we've talked about the strategy behind machine learning,

7
00:00:17,345 --> 00:00:18,845
about what machine learning means,

8
00:00:18,845 --> 00:00:20,285
what problems it can solve,

9
00:00:20,285 --> 00:00:22,705
and how to put into practice at your company.

10
00:00:22,705 --> 00:00:25,165
Beside these technical and business aspects,

11
00:00:25,165 --> 00:00:30,230
another thing to consider is how fair and inclusive your models are treating your users.

12
00:00:30,230 --> 00:00:32,390
A key aspect in your machine learning strategy is

13
00:00:32,390 --> 00:00:35,105
to build machine learning systems in an inclusive way.

14
00:00:35,105 --> 00:00:37,040
So, in this module, I will show you how to

15
00:00:37,040 --> 00:00:39,590
identify the origins of bias in machine learning.

16
00:00:39,590 --> 00:00:43,070
And sometimes, it comes down to the training data itself.

17
00:00:43,070 --> 00:00:45,215
Then I will show you ways in which you can apply

18
00:00:45,215 --> 00:00:48,440
an inclusive lens throughout the machine learning development process,

19
00:00:48,440 --> 00:00:50,660
from the data exploration all the

20
00:00:50,660 --> 00:00:53,895
way to evaluating the performance of your training model.

21
00:00:53,895 --> 00:00:56,240
So, let's delve in. We'll first watch

22
00:00:56,240 --> 00:00:59,450
a video that explains where bias and machine learning originates,

23
00:00:59,450 --> 00:01:03,075
and the importance of building inclusive machine learning systems.

24
00:01:03,075 --> 00:01:06,150
After the video, I'll walk through some of the ways in which you can understand

25
00:01:06,150 --> 00:01:09,875
the trade-offs between the outcomes of your machine learning system and your users,

26
00:01:09,875 --> 00:01:13,625
and how these trade offs map to evaluate the metrics that you can compute.

27
00:01:13,625 --> 00:01:16,745
From there, I'll introduce equality of opportunity,

28
00:01:16,745 --> 00:01:18,530
a methodology that builds on top of

29
00:01:18,530 --> 00:01:22,790
these evaluation metrics in order to achieve a more desirable outcome.

30
00:01:22,790 --> 00:01:25,010
An outcome where there is an equal chance of

31
00:01:25,010 --> 00:01:27,980
a machine learning system correctly classifying an outcome,

32
00:01:27,980 --> 00:01:30,485
irrespective of any sensitive attributes.

33
00:01:30,485 --> 00:01:32,210
And finally, as we know,

34
00:01:32,210 --> 00:01:34,360
machine learning systems are fueled by data.

35
00:01:34,360 --> 00:01:35,890
So, getting the best results out of

36
00:01:35,890 --> 00:01:39,200
a machine learning system requires that you truly understand your data,

37
00:01:39,200 --> 00:01:42,320
and that holds true for making machine learning systems inclusive.

38
00:01:42,320 --> 00:01:43,775
So, in this last section,

39
00:01:43,775 --> 00:01:48,440
I'll showcase an open-source visualization tool for machine learning data called Facets,

40
00:01:48,440 --> 00:01:51,515
which helps you explore the intricacies of your dataset,

41
00:01:51,515 --> 00:01:53,600
and provide some suggestions on what to look

42
00:01:53,600 --> 00:01:57,760
for when assessing the inclusiveness of your training data.