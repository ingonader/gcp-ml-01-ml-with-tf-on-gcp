Votre système de machine learning
va faire des erreurs. Il est important de comprendre ces erreurs
et comment elles peuvent affecter l'expérience utilisateur générée
par les résultats de votre modèle de ML. Dans ce module, on va voir
comment évaluer l'inclusion lors du développement
et des tests de votre modèle de ML. Il est essentiel de connaître
la matrice de confusion pour comprendre l'inclusion
et savoir comment l'intégrer dans différents sous-groupes
de vos données. Même si vous sachez évaluer votre
modèle sur votre ensemble de données, vous devez aussi l'évaluer
sur des sous-groupes. Au lieu d'analyser les performances
globales de votre modèle sur votre ensemble de données,
vous allez évaluer les performances au niveau du sous-groupe
que vous souhaitez optimiser. Prenons l'exemple
de la détection de visages. Vous créez un modèle de ML pour déterminer si une photo comporte
un visage humain ou non. Ce n'est pas un problème si simple. Vos sous-groupes peuvent être
des hommes, des femmes, des adultes, des enfants,
des gens avec des cheveux, des gens chauves. Il faut analyser les performances
sur tous ces sous-groupes pour identifier
les améliorations à apporter. La matrice de confusion
est une méthode courante pour évaluer les performances
d'un modèle de ML. D'autres méthodes existent
pour d'autres types de problèmes, mais dans le cadre de ce module, nous allons nous concentrer
sur la matrice de confusion pour présenter ces points. La matrice de confusion va nous permettre
d'analyser l'inclusion. Pour cela, il faut d'abord créer
une matrice de confusion pour chaque sous-groupe de données dont vous souhaitez
analyser les performances. La matrice de confusion présente
une comparaison de vos libellés, qui ne reflètent pas
forcément la réalité de terrain, car vous pouvez ne pas y avoir accès. Néanmoins, vous comparez ces
libellés aux prédictions de votre modèle. Il faut ensuite analyser
les positifs et les négatifs. Parmi les libellés,
certains sont considérés comme corrects, les libellés positifs, et certains sont considérés
comme incorrects, les libellés négatifs. Du côté du ML, il existe des prédictions positives
sur les éléments présents et des prédictions négatives
sur les éléments absents. On compare ces points
dans la matrice de confusion pour comprendre les décisions
déduites par le système de ML. On commence par les vrais positifs, quand le libellé indique
la présence d'un élément et que le modèle le prédit. Pour la détection de visages, on obtient un vrai positif quand
le modèle prédit avec précision la présence d'un visage dans l'image. Si le libellé indique
la présence d'un élément, et que le modèle ne le prédit pas,
il s'agit alors d'un faux négatif. Dans cet exemple
de détection de visages, le modèle ne prédit pas
la présence d'un visage dans l'image alors que le libellé suggère
qu'il y en a bien un. Si le libellé indique qu'il n'y en a pas, et que le modèle
n'en prédit pas non plus, il s'agit d'un vrai négatif. Ainsi, dans notre exemple
de détection de visages, le modèle a raison de ne pas prédire
la présence d'un visage dans l'image, car il est également absent
dans le libellé. Enfin, on obtient un faux positif, quand le libellé indique
l'absence de visage, mais que le modèle de ML en prédit un. Dans notre exemple,
il peut s'agir d'une statue sur l'image, que le modèle a confondue
avec un visage humain. Il est important de se concentrer sur
les faux négatifs et les faux positifs. Les faux négatifs sont les éléments
qui existent et que vous ne prédisez pas, les éléments que vous excluez
alors qu'ils devraient être inclus. Les faux positifs sont les éléments
que vous prédisez à tort, les éléments que vous incluez alors
qu'ils n'existent pas dans le libellé et qui devraient donc être exclus. On parle souvent d'erreurs de type 1
et de type 2 dans d'autres domaines. Cette répartition en quatre types
de correspondances avec les libellés vous permet de calculer
de nombreuses métriques différentes qui vous aideront à évaluer
le niveau d'inclusion de votre modèle.