1
00:00:00,000 --> 00:00:03,480
Now, your machine learning system will make mistakes.

2
00:00:03,480 --> 00:00:06,720
It's important to understand what these errors look like and how they might

3
00:00:06,720 --> 00:00:10,820
affect the user experience that's driven by the output of your machine learning model.

4
00:00:10,820 --> 00:00:14,040
In this module, we'll discuss some of the ways in which you can

5
00:00:14,040 --> 00:00:18,110
evaluate inclusion as you're developing and testing your machine learning model.

6
00:00:18,110 --> 00:00:20,010
One of the key things to really know,

7
00:00:20,010 --> 00:00:22,380
which will help in understanding inclusion and how to

8
00:00:22,380 --> 00:00:25,425
introduce inclusion across different subgroups within your data,

9
00:00:25,425 --> 00:00:27,960
is by understanding the confusion matrix.

10
00:00:27,960 --> 00:00:31,770
While you may be familiar with evaluating your model over the entire dataset,

11
00:00:31,770 --> 00:00:35,155
it's also important to evaluate your model over subgroups.

12
00:00:35,155 --> 00:00:39,855
So, instead of just looking at how your model performs overall over your entire dataset,

13
00:00:39,855 --> 00:00:42,395
we'll focus instead on breaking the performance down to

14
00:00:42,395 --> 00:00:45,680
the subgroup that you wish to improve performance on.

15
00:00:45,680 --> 00:00:48,805
For example, suppose you're doing face detection.

16
00:00:48,805 --> 00:00:51,240
Essentially, you're building a machine learning model to

17
00:00:51,240 --> 00:00:54,160
say whether or not there is a human face in a photograph.

18
00:00:54,160 --> 00:00:56,265
This is not necessarily an easy problem.

19
00:00:56,265 --> 00:00:58,680
Your subgroups might be men, women,

20
00:00:58,680 --> 00:01:01,165
adults, children, people with hair,

21
00:01:01,165 --> 00:01:02,550
people who are bald.

22
00:01:02,550 --> 00:01:04,650
You want to look at the performance of your model across

23
00:01:04,650 --> 00:01:07,275
all these subgroups to identify areas of improvement.

24
00:01:07,275 --> 00:01:09,930
So, a common way that we evaluate performance in

25
00:01:09,930 --> 00:01:13,230
machine learning is by using a confusion matrix.

26
00:01:13,230 --> 00:01:16,200
Now, there are other methods for other types of problems,

27
00:01:16,200 --> 00:01:18,120
but for the purposes of this module,

28
00:01:18,120 --> 00:01:21,420
we'll focus on the confusion matrix to explain these points.

29
00:01:21,420 --> 00:01:25,380
The idea is using the confusion matrix in order to look at inclusion.

30
00:01:25,380 --> 00:01:28,005
And you do this by first creating the confusion matrix,

31
00:01:28,005 --> 00:01:30,550
but you do so for every subgroup in your data,

32
00:01:30,550 --> 00:01:33,360
subgroups of what you're interested in measuring performance.

33
00:01:33,360 --> 00:01:34,980
Now, in the confusion matrix,

34
00:01:34,980 --> 00:01:38,040
you have comparisons between your labels, which, of course,

35
00:01:38,040 --> 00:01:40,590
may or may not necessarily reflect your ground truth because

36
00:01:40,590 --> 00:01:43,395
sometimes we don't necessarily have access to the ground truth.

37
00:01:43,395 --> 00:01:47,530
But nevertheless, you're comparing those labels to your model predictions.

38
00:01:47,530 --> 00:01:49,980
From here, we look at the positives and negatives.

39
00:01:49,980 --> 00:01:53,034
So in our labels, there are some things that are considered correct,

40
00:01:53,034 --> 00:01:54,790
we will call those a positive label,

41
00:01:54,790 --> 00:01:57,435
and there are some things that are considered incorrect,

42
00:01:57,435 --> 00:01:59,295
and we call those negative labels.

43
00:01:59,295 --> 00:02:00,875
On the machine learning side,

44
00:02:00,875 --> 00:02:03,900
we have positive predictions about what there is and we

45
00:02:03,900 --> 00:02:07,140
have predictions about what's not there, and those are called native.

46
00:02:07,140 --> 00:02:09,750
We compare this in the confusion matrix in order to

47
00:02:09,750 --> 00:02:12,525
understand the decisioning machine learning system is inferring,

48
00:02:12,525 --> 00:02:14,145
starting with the true positives,

49
00:02:14,145 --> 00:02:17,625
which is when the label says something is there and the model predicts it.

50
00:02:17,625 --> 00:02:19,530
So, in the case of face detection,

51
00:02:19,530 --> 00:02:21,525
a true positive would be when the model

52
00:02:21,525 --> 00:02:24,690
accurately predicted that there is a face in the image.

53
00:02:24,690 --> 00:02:27,150
Now, when the label says something exists and

54
00:02:27,150 --> 00:02:30,610
a model doesn't predict it, that's a false negative.

55
00:02:30,610 --> 00:02:33,165
So, using the same face detection example,

56
00:02:33,165 --> 00:02:36,630
the model does not predict there being a face in the image when

57
00:02:36,630 --> 00:02:40,170
in fact the label suggests that there is a face.

58
00:02:40,170 --> 00:02:43,988
When the label says it doesn't exist and your model also doesn't predict it,

59
00:02:43,988 --> 00:02:46,005
that's what's called a true negative.

60
00:02:46,005 --> 00:02:47,985
Basically, what that means is,

61
00:02:47,985 --> 00:02:50,080
using this face detection example,

62
00:02:50,080 --> 00:02:52,980
the model not predicting that the face is present in

63
00:02:52,980 --> 00:02:57,725
the image is correct because it's also not present in the label.

64
00:02:57,725 --> 00:03:00,255
And lastly, this is the false positive case,

65
00:03:00,255 --> 00:03:02,550
where the label says there is no face but

66
00:03:02,550 --> 00:03:05,725
the machine learning model predicts that there should be a face.

67
00:03:05,725 --> 00:03:07,140
So, in this instance,

68
00:03:07,140 --> 00:03:09,780
perhaps there is a statue in the image and

69
00:03:09,780 --> 00:03:13,245
the model falsely identifies that statue as having a face.

70
00:03:13,245 --> 00:03:15,240
But really, what I want you to focus in on

71
00:03:15,240 --> 00:03:18,250
here are the false negatives and false positives.

72
00:03:18,250 --> 00:03:22,380
Remember, false negatives are the things you incorrectly do not predict,

73
00:03:22,380 --> 00:03:25,735
things you exclude when instead it should have been included,

74
00:03:25,735 --> 00:03:28,935
and false positives are things that you incorrectly predicted,

75
00:03:28,935 --> 00:03:31,500
things you include that aren't actually there in

76
00:03:31,500 --> 00:03:34,380
the label and should have instead been excluded.

77
00:03:34,380 --> 00:03:38,865
And these are often referred to as type I errors in type II errors in other domains.

78
00:03:38,865 --> 00:03:41,730
But the cool thing about this sort of basic breakdown into

79
00:03:41,730 --> 00:03:44,610
four different kinds of matches to the label is that you

80
00:03:44,610 --> 00:03:47,115
can start to calculate a ton of different metrics

81
00:03:47,115 --> 00:03:51,000
that can be used to gauge the amount of inclusiveness in your model.