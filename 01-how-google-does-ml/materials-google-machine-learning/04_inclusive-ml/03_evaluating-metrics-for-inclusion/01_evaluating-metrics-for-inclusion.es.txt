Su sistema de aprendizaje
automático cometerá errores. Es importante saber cómo lucen
estos errores y cómo podrían afectar la experiencia del usuario generada
por el resultado de su modelo de AA. En este módulo, analizaremos
algunas de las formas en las que pueden evaluar la inclusión, a medida
que desarrollan y prueban su modelo de AA. Uno de los aspectos clave que los ayudará
a comprender la inclusión y cómo introducirla en los
diferentes subgrupos de sus datos es comprender la matriz de confusión. Probablemente saben bien cómo evaluar
su modelo en todo el conjunto de datos pero también es importante
que lo evalúen en subgrupos. Así que, en lugar de ver cómo
se desempeña en todo el conjunto de datos nos enfocaremos
en analizar el rendimiento en el subgrupo en el que desean
mejorar el rendimiento. Por ejemplo, supongamos
que trabajan en la detección de rostros. Básicamente, están creando un modelo de AA para que diga si hay o no
un rostro humano en una foto. Esto en sí no es un problema sencillo. Los subgrupos podrían ser hombres, mujeres adultos, niños, personas con cabello personas sin cabello. Es conveniente analizar
el rendimiento de su modelo en estos subgrupos para identificar
las áreas que deben mejorar. Así que, una forma común
de evaluar el rendimiento en el AA es usar una matriz de confusión. Hay otros métodos
para los diferentes tipos de problemas pero, a los fines de este módulo nos enfocaremos en la matriz
de confusión para explicar estos puntos. La idea es usar la matriz
de confusión para analizar la inclusión. Y pueden hacerlo
mediante la creación de esa matriz para cada subgrupo presente en sus datos en los que les interesa
medir el rendimiento. Ahora, en la matriz de confusión se tienen comparaciones
entre las etiquetas que pueden o no reflejar
su conjunto de etiquetas confiables ya que no necesariamente
tenemos acceso a ellas. Sin embargo, comparan esas etiquetas
con las predicciones del modelo. Desde aquí vemos los
aspectos positivos y negativos. En nuestras etiquetas
hay cosas consideradas correctas que llamaremos etiquetas positivas y otras que se consideran incorrectas que llamaremos etiquetas negativas. En el lado del AA tenemos predicciones
positivas sobre lo que hay y predicciones sobre lo
que no hay, llamadas negativas. Las comparamos
en la matriz de confusión para entender la decisión que infiere
el sistema de aprendizaje automático desde los verdaderos positivos que es cuando la etiqueta
dice que hay algo y el modelo lo predice. En el caso de la detección de rostros un verdadero positivo
sería cuando el modelo predice correctamente
que hay un rostro en la imagen. Ahora, cuando la etiqueta
dice que hay algo y el modelo no
lo predice, es un falso negativo. Así que, en el mismo ejemplo
de detección de rostros el modelo no predice
que hay un rostro en la imagen cuando la etiqueta sí lo sugiere. Cuando la etiqueta dice que no existe
y su modelo no lo predice se denomina como verdadero negativo. Básicamente, eso significa que en este
ejemplo de detección de rostros cuando el modelo
no predice que hay un rostro presente en la imagen es correcto,
porque tampoco está en la etiqueta. Y, finalmente,
este es el caso del falso positivo donde la etiqueta
dice que no hay un rostro pero el modelo de AA
predice que debería haber uno. En esta instancia tal vez hay una estatua en la imagen y el modelo identifica
erróneamente el rostro de la estatua. Pero en lo quiero que se enfoquen es en que hay falsos negativos
y falsos positivos. Recuerden, los falsos negativos
son los elementos que erróneamente no se predicen lo que se excluye
cuando se debería incluir. Y los falsos positivos son los elementos que se predicen incorrectamente lo que se incluye
y que en realidad no está en la etiqueta y que debió excluirse. Y se los suele llamar errores
de tipo 1 y tipo 2 en otros dominios. Pero lo genial de este tipo de desglose en cuatro tipos
de coincidencias de etiqueta es que pueden comenzar a calcular
muchas métricas diferentes para medir el nivel
de inclusión en su modelo.