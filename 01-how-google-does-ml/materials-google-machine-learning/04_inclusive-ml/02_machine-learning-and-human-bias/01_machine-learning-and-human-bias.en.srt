1
00:00:00,000 --> 00:00:00,499

2
00:00:00,499 --> 00:00:02,050
SPEAKER: Let's play a game.

3
00:00:02,050 --> 00:00:05,331
Close your eyes
and picture a shoe.

4
00:00:05,331 --> 00:00:05,830
OK.

5
00:00:05,830 --> 00:00:08,044
Did anyone picture this?

6
00:00:08,044 --> 00:00:09,850
This?

7
00:00:09,850 --> 00:00:12,020
How about this?

8
00:00:12,020 --> 00:00:14,300
We may not even know
why, but each of us

9
00:00:14,300 --> 00:00:17,450
is biased toward one
shoe over the others.

10
00:00:17,450 --> 00:00:19,970
Now, imagine that you're
trying to teach a computer

11
00:00:19,970 --> 00:00:21,500
to recognize a shoe.

12
00:00:21,500 --> 00:00:24,290
You may end up exposing
it to your own bias.

13
00:00:24,290 --> 00:00:27,390
That's how bias happens
in machine learning.

14
00:00:27,390 --> 00:00:29,990
But first, what is
machine learning?

15
00:00:29,990 --> 00:00:33,950
Well, it's used in a lot
of technology we use today.

16
00:00:33,950 --> 00:00:36,920
Machine learning helps us
get from place to place,

17
00:00:36,920 --> 00:00:40,490
gives us suggestions,
translates stuff, even

18
00:00:40,490 --> 00:00:42,750
understands what you say to it.

19
00:00:42,750 --> 00:00:44,030
How does it work?

20
00:00:44,030 --> 00:00:45,960
With traditional
programming, people

21
00:00:45,960 --> 00:00:49,640
hand code the solution to
a problem, step by step.

22
00:00:49,640 --> 00:00:52,520
With machine learning,
computers learn the solution

23
00:00:52,520 --> 00:00:55,010
by finding patterns
in data, so it's

24
00:00:55,010 --> 00:00:57,405
easy to think there's
no human bias in that.

25
00:00:57,405 --> 00:00:59,990
But just because
something is based on data

26
00:00:59,990 --> 00:01:02,570
doesn't automatically
make it neutral.

27
00:01:02,570 --> 00:01:05,090
Even with good intentions,
it's impossible to separate

28
00:01:05,090 --> 00:01:07,940
ourselves from our
own human biases,

29
00:01:07,940 --> 00:01:10,640
so our human biases become
part of the technology

30
00:01:10,640 --> 00:01:13,860
we create in many
different ways.

31
00:01:13,860 --> 00:01:17,480
There's interaction bias,
like this recent game

32
00:01:17,480 --> 00:01:20,240
where people were asked to
draw shoes for the computer.

33
00:01:20,240 --> 00:01:22,350
Most people drew ones like this.

34
00:01:22,350 --> 00:01:24,380
So as more people
interacted with the game,

35
00:01:24,380 --> 00:01:27,740
the computer didn't
even recognize these.

36
00:01:27,740 --> 00:01:31,250
Latent bias-- for example, if
you were training a computer

37
00:01:31,250 --> 00:01:34,190
on what a physicist looks
like, and you're using pictures

38
00:01:34,190 --> 00:01:36,230
of past physicists,
your algorithm

39
00:01:36,230 --> 00:01:40,550
will end up with a latent
bias skewing towards men.

40
00:01:40,550 --> 00:01:43,130
And selection bias-- say
you're training a model

41
00:01:43,130 --> 00:01:44,992
to recognize faces.

42
00:01:44,992 --> 00:01:47,450
Whether you grab images from
the internet or your own photo

43
00:01:47,450 --> 00:01:49,520
library, are you
making sure to select

44
00:01:49,520 --> 00:01:51,830
photos that represent everyone?

45
00:01:51,830 --> 00:01:55,100
Since some of our most advanced
products use machine learning,

46
00:01:55,100 --> 00:01:57,080
we've been working to
prevent that technology

47
00:01:57,080 --> 00:01:58,980
from perpetuating
negative human bias--

48
00:01:58,980 --> 00:02:01,670

49
00:02:01,670 --> 00:02:04,034
from tackling offensive
or clearly misleading

50
00:02:04,034 --> 00:02:06,200
information from appearing
at the top of your search

51
00:02:06,200 --> 00:02:10,780
results page to adding a
feedback tool in the search bar

52
00:02:10,780 --> 00:02:13,140
so people can flag
hateful or inappropriate

53
00:02:13,140 --> 00:02:17,020
autocomplete suggestions.

54
00:02:17,020 --> 00:02:19,540
It's a complex issue, and
there is no magic bullet,

55
00:02:19,540 --> 00:02:21,670
but it starts with all
of us being aware of it,

56
00:02:21,670 --> 00:02:24,100
so we can all be part
of the conversation,

57
00:02:24,100 --> 00:02:27,540
because technology
should work for everyone.