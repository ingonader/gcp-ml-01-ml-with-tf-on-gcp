1
00:00:00,000 --> 00:00:01,830
Now, that we know how to calculate

2
00:00:01,830 --> 00:00:05,314
evaluation metrics of a model's predictions across subgroups,

3
00:00:05,314 --> 00:00:08,550
let's talk about an approach that builds on top of these metrics,

4
00:00:08,550 --> 00:00:11,610
in order to achieve a better performance across subgroups.

5
00:00:11,610 --> 00:00:14,850
The approach I'm going to introduce is known as equality of opportunity,

6
00:00:14,850 --> 00:00:16,690
and it goes something like this.

7
00:00:16,690 --> 00:00:20,310
Let's say you have a model that's intended to work across all users,

8
00:00:20,310 --> 00:00:23,402
irrespective of who they are or where they come from.

9
00:00:23,402 --> 00:00:28,935
Ideally, all users who qualify for a desirable outcome generated by our model,

10
00:00:28,935 --> 00:00:31,500
should have an equal chance amongst all users of

11
00:00:31,500 --> 00:00:34,800
being correctly classified for that desirable outcome.

12
00:00:34,800 --> 00:00:37,005
So, let's say we're working for a bank,

13
00:00:37,005 --> 00:00:39,150
and we are building a machine learning model to help

14
00:00:39,150 --> 00:00:42,485
determine whether or not to approve a loan.

15
00:00:42,485 --> 00:00:45,490
What is equality of opportunity mean in this context?

16
00:00:45,490 --> 00:00:49,755
Ideally, all users who qualify for a loan have an equal chance

17
00:00:49,755 --> 00:00:53,989
amongst all users of being correctly classified for that loan approval.

18
00:00:53,989 --> 00:00:57,735
In other words, the chances of a person being qualified for a loan,

19
00:00:57,735 --> 00:01:01,155
should be the same regardless of which protected subgroup they are part of.

20
00:01:01,155 --> 00:01:05,100
So, what we are seeing here is that if you keep everything about a person the same,

21
00:01:05,100 --> 00:01:08,325
and change them from being a member once a group to another,

22
00:01:08,325 --> 00:01:11,640
their chances of qualifying for the loan should remain the same.

23
00:01:11,640 --> 00:01:15,405
So, why should you incorporate such an approach into your machine learning system?

24
00:01:15,405 --> 00:01:19,245
Well, because an approach like this gives you a way to scrutinize your model,

25
00:01:19,245 --> 00:01:22,140
in order to discover possible areas of concern.

26
00:01:22,140 --> 00:01:24,720
Once you identify opportunities for improvement,

27
00:01:24,720 --> 00:01:27,660
you can now make this necessary adjustments to strike

28
00:01:27,660 --> 00:01:30,915
a better trade off between accuracy and non-discrimination,

29
00:01:30,915 --> 00:01:34,320
which in turn can make your machine learning model more inclusive.

30
00:01:34,320 --> 00:01:37,560
Let's now illustrate this approach using a toy classifier,

31
00:01:37,560 --> 00:01:39,524
that is not a real model,

32
00:01:39,524 --> 00:01:42,980
it's just a synthetic example to explain the concepts.

33
00:01:42,980 --> 00:01:47,610
The purpose of the model is to predict with high accuracy who will pay back the loan,

34
00:01:47,610 --> 00:01:50,280
and the bank can then use this model to help decide

35
00:01:50,280 --> 00:01:53,755
whether or not to give the loan to the applicant.

36
00:01:53,755 --> 00:01:55,850
So, in the diagram that you see here,

37
00:01:55,850 --> 00:01:59,045
the dark dots represent people who pay off the loan,

38
00:01:59,045 --> 00:02:01,735
and the light dots are those who wouldn't.

39
00:02:01,735 --> 00:02:04,535
The numbers at the top row represent credit score,

40
00:02:04,535 --> 00:02:07,165
which is simplified to a range of 0 to 100,

41
00:02:07,165 --> 00:02:11,425
where a higher score represents a higher likelihood of repaying the loan.

42
00:02:11,425 --> 00:02:13,080
In an ideal world,

43
00:02:13,080 --> 00:02:16,935
we would work with statistics that cleanly separate categories,

44
00:02:16,935 --> 00:02:19,125
as you can see in the example on the left.

45
00:02:19,125 --> 00:02:21,940
Unfortunately, it's far more common to see

46
00:02:21,940 --> 00:02:25,515
the situation on the right where the groups overlap.

47
00:02:25,515 --> 00:02:30,840
Now, a single statistic like a credit score can stand in for many different variables.

48
00:02:30,840 --> 00:02:32,910
You'll see later in the specialization that

49
00:02:32,910 --> 00:02:35,415
most machine learning models return a probability,

50
00:02:35,415 --> 00:02:38,835
and so the credit score here could stand in for that probability.

51
00:02:38,835 --> 00:02:43,815
The resulting probability from a machine learning model like credit score for example,

52
00:02:43,815 --> 00:02:46,470
which factor in a lot of things including income,

53
00:02:46,470 --> 00:02:49,100
provenance of paying debt and so on.

54
00:02:49,100 --> 00:02:51,420
So, the number might actually represent the likelihood that

55
00:02:51,420 --> 00:02:54,255
a person will pay off the loan or default.

56
00:02:54,255 --> 00:02:56,450
But it also might not.

57
00:02:56,450 --> 00:02:59,670
This is where the idea of setting a threshold can come in.

58
00:02:59,670 --> 00:03:02,160
Basically, you pick a particular cutoff point,

59
00:03:02,160 --> 00:03:05,685
and people whose credit score are below it are denied the loan,

60
00:03:05,685 --> 00:03:08,260
and people above it are granted the loan.

61
00:03:08,260 --> 00:03:10,080
As you can see in this diagram,

62
00:03:10,080 --> 00:03:12,490
picking the threshold requires some tradeoffs.

63
00:03:12,490 --> 00:03:16,050
Too low and he may grant more loans at default,

64
00:03:16,050 --> 00:03:19,910
too high and many people who deserve a loan won't get one.

65
00:03:19,910 --> 00:03:21,631
So, what is the best threshold?

66
00:03:21,631 --> 00:03:25,290
What threshold to use depends on your goals and motivation.

67
00:03:25,290 --> 00:03:28,140
One goal might be to maximize the number of correct decisions,

68
00:03:28,140 --> 00:03:29,865
as you can see in this diagram.

69
00:03:29,865 --> 00:03:35,400
On the left, the dark blue dots represent loans that were granted and paid back,

70
00:03:35,400 --> 00:03:40,325
and the light gray dots represent loans that were denied because they would default,

71
00:03:40,325 --> 00:03:43,830
all of these dots would represent correct predictions.

72
00:03:43,830 --> 00:03:45,710
Now on the right,

73
00:03:45,710 --> 00:03:49,925
you have the light blue dots which represent loans that were granted and defaulted,

74
00:03:49,925 --> 00:03:53,120
and dark grey dots representing loans that

75
00:03:53,120 --> 00:03:56,285
were denied to people that would have actually paid them off,

76
00:03:56,285 --> 00:03:58,650
these dots represent incorrect predictions.

77
00:03:58,650 --> 00:04:02,570
But some decisions are financially more costly than others.

78
00:04:02,570 --> 00:04:04,550
Perhaps there's a category of loans,

79
00:04:04,550 --> 00:04:09,450
perhaps loans for 15 year mortgages that are more profitable than other loans.

80
00:04:09,450 --> 00:04:12,570
So, you may not want to treat all decisions the same.

81
00:04:12,570 --> 00:04:15,980
So, another goal in a financial situation might

82
00:04:15,980 --> 00:04:19,475
be to maximize not the number of correct decisions,

83
00:04:19,475 --> 00:04:21,200
but the overall profit.

84
00:04:21,200 --> 00:04:23,750
And the bottom diagram that you're seeing here,

85
00:04:23,750 --> 00:04:26,360
represents the hypothetical profit based

86
00:04:26,360 --> 00:04:30,020
on our estimate of the profit associated with each loan.

87
00:04:30,020 --> 00:04:31,655
So, the question then becomes,

88
00:04:31,655 --> 00:04:33,825
what is the most profitable threshold,

89
00:04:33,825 --> 00:04:37,340
and does it match with the threshold with the most correct decisions?

90
00:04:37,340 --> 00:04:40,790
Questions like these become particularly thorny when a statistic like

91
00:04:40,790 --> 00:04:44,980
a credit score ends up distributed differently between two groups.

92
00:04:44,980 --> 00:04:47,375
This is where equality of opportunity can come in.

93
00:04:47,375 --> 00:04:51,140
The formal set up for equality of opportunity looks something like this.

94
00:04:51,140 --> 00:04:55,285
Let's say you have A which represents a predicted attribute.

95
00:04:55,285 --> 00:04:57,470
For simplicity, let's treat A as binary,

96
00:04:57,470 --> 00:05:00,710
and have it represent a membership of some protected group.

97
00:05:00,710 --> 00:05:02,690
Now, I'm not a lawyer,

98
00:05:02,690 --> 00:05:06,605
so I can't tell you what constitutes a protected group in your publication area,

99
00:05:06,605 --> 00:05:08,390
you should talk to your company's legal department

100
00:05:08,390 --> 00:05:10,415
to find out what is protected and what isn't.

101
00:05:10,415 --> 00:05:11,855
But to give you an example,

102
00:05:11,855 --> 00:05:13,175
in the United States,

103
00:05:13,175 --> 00:05:17,310
federal laws protect employees from discrimination based on age.

104
00:05:17,310 --> 00:05:19,895
So, depending on the application you're building,

105
00:05:19,895 --> 00:05:22,715
age might be a protected group.

106
00:05:22,715 --> 00:05:25,825
You also have a binary outcome which we'll call Y,

107
00:05:25,825 --> 00:05:30,200
where we can interpret the value of Y equals one as a desirable outcome.

108
00:05:30,200 --> 00:05:33,135
In this case, the acceptance of a loan.

109
00:05:33,135 --> 00:05:36,780
Consider Y in this example as your ground truth or label,

110
00:05:36,780 --> 00:05:38,835
but we're building a model of Y.

111
00:05:38,835 --> 00:05:42,250
So, we'll need to also have Y hat our predictor.

112
00:05:42,250 --> 00:05:44,590
In our example, the predictor is always

113
00:05:44,590 --> 00:05:47,545
a threshold defined using a score between zero and one.

114
00:05:47,545 --> 00:05:50,625
The predictor may use thresholds that depend on A,

115
00:05:50,625 --> 00:05:53,500
where we can use the different thresholds for different groups.

116
00:05:53,500 --> 00:05:59,065
So, the idea here is that individuals in A who qualify for a positive outcome,

117
00:05:59,065 --> 00:06:01,915
should have the same chance of getting positively classified,

118
00:06:01,915 --> 00:06:04,295
as individual who are not in A.

119
00:06:04,295 --> 00:06:07,050
More formally speaking, this desire

120
00:06:07,050 --> 00:06:10,180
coincides with an equal true positive rate in both groups.

121
00:06:10,180 --> 00:06:14,000
And this is the principle behind equality of opportunity.