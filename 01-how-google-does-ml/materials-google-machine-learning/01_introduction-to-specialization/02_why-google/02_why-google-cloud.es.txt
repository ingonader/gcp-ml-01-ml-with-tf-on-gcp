Una de las lecciones clave que aprendimos es que es importante pensar
en la entrega del aprendizaje automático no solo en el entrenamiento. Cuando se habla de AA,
casi todo el mundo piensa en la canalización compleja
a la izquierda en este diagrama. Allí es donde usted,
como ingeniero o científico de datos, trabajará gran parte del tiempo. Sin embargo,
el motivo clave para usar el AA se encuentra
en el lado derecho del diagrama. Es necesario entregar predicciones
a los encargados de tomar decisiones con notebooks, paneles,
aplicaciones o informes. La puesta en práctica de un modelo de AA
implica seleccionar un modelo entrenado y llegar al punto en el que puede
usarse para entregar predicciones. Es difícil poner
un modelo de AA en práctica. Muchos proyectos
nunca llegan a esta etapa de predicción. Una de las lecciones que aprendió Google es que para reducir
las probabilidades de fracasar necesitábamos la capacidad de procesar
datos de transmisión y de lotes por igual. Cloud Data Flow, en este diagrama… La versión de código abierto
es Apache Beam. Cloud Data Flow nos ayuda a procesar
datos de transmisión y de lotes por igual. Cloud Data Flow es solo un ejemplo de la forma en la que Google Cloud
le permite aprovechar nuestra experiencia en la creación de infraestructuras de AA. Si no realizó nuestra especialización
en ingeniería de datos en Coursera, le recomiendo enfáticamente que la haga. Pero en esta opción, hablaremos
de las partes clave mientras avanzamos. Por suerte, si es científico de datos, la ingeniería de datos
no es muy difícil de aprender. En GCP, los servicios clave
no usan servidores y toda la infraestructura es administrada. En este curso le mostraremos cómo crear canalizaciones de datos
de lotes y de transmisión. Cuando se crea una canalización en Cloud,
se puede aprovechar la escalabilidad la confiabilidad y el dominio
de ingeniería que Google aplica a la ejecución de sistemas
de aprendizaje automático.